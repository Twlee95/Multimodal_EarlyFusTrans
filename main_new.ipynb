{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\taewon_project\\\\Multimodal_Transformer-main_early')\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(Transformer, Transformer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Transformer.zero_grad()\n",
    "        Transformer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "\n",
    "        Transf_out = Transformer(x)\n",
    "\n",
    "        loss = args.loss_fn(Transf_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Transformer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Transformer, train_loss\n",
    "\n",
    "def validation(Transformer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Transformer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out = Transformer(x)\n",
    "                        \n",
    "            loss = args.loss_fn(Transf_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Transformer, val_loss\n",
    "\n",
    "def test(Transformer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out = Transformer(x)\n",
    "\n",
    "            output_ = torch.where(Transf_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\MM_results\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from Transformer_Encoder import Transformer\n",
    "\n",
    "args.Transformer = Transformer\n",
    "args.input_feature_size = 40\n",
    "\n",
    "args.Transformer_feature_size = 64\n",
    "\n",
    "args.nhead = 4\n",
    "\n",
    "args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "Epoch 0, Loss(train/val) 0.68923/0.68929. Took 1.36 sec\n",
      "Epoch 1, Loss(train/val) 0.68711/0.68993. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68640/0.69061. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68602/0.69138. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68593/0.69191. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68553/0.69243. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68482/0.69303. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68521/0.69327. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68471/0.69366. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68443/0.69397. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68441/0.69426. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68353/0.69478. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68425/0.69487. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68314/0.69488. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68296/0.69535. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68243/0.69562. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68273/0.69569. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68229/0.69591. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68109/0.69648. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68143/0.69668. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68027/0.69665. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67943/0.69696. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67964/0.69724. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67897/0.69707. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67875/0.69706. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67692/0.69725. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67752/0.69755. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67675/0.69766. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67525/0.69776. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67417/0.69810. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67445/0.69824. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67246/0.69798. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67255/0.69837. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67092/0.69967. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66997/0.70015. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66900/0.69977. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66694/0.69886. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66565/0.69947. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66489/0.70039. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66239/0.70166. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66100/0.70172. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65839/0.70144. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65787/0.70119. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65379/0.70216. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65329/0.70373. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65227/0.70322. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65000/0.70461. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64783/0.70658. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64366/0.70734. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64155/0.71039. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63844/0.71101. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63593/0.71275. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63392/0.71451. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63344/0.71834. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63114/0.71986. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62623/0.72561. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62450/0.72720. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62245/0.73474. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62281/0.73145. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62030/0.73595. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61857/0.74133. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62061/0.74136. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61364/0.74675. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61603/0.75395. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61277/0.75118. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61340/0.75359. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60823/0.75468. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60629/0.76816. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60150/0.76238. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59944/0.77153. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59672/0.76829. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59594/0.78317. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59770/0.77598. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58962/0.77640. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59020/0.78380. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58654/0.78693. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58502/0.79240. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57944/0.79030. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58247/0.80649. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58269/0.80717. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57827/0.79569. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57618/0.80227. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57499/0.80628. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57274/0.81413. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56918/0.81882. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56720/0.81851. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56609/0.82610. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56295/0.82442. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56132/0.82732. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55382/0.83821. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55342/0.83113. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54900/0.83616. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55907/0.84858. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55498/0.83965. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55605/0.85189. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54682/0.84501. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54406/0.84868. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54394/0.86041. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54036/0.85739. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53655/0.87004. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69225/0.68830. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68876/0.68598. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68803/0.68572. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68812/0.68562. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68793/0.68548. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68677/0.68553. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68710/0.68591. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68718/0.68593. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68657/0.68604. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68649/0.68605. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68592/0.68608. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68527/0.68617. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68544/0.68632. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68377/0.68643. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68355/0.68651. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68335/0.68638. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68317/0.68653. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68203/0.68676. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68206/0.68688. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68001/0.68689. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68069/0.68704. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67982/0.68716. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67852/0.68726. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67714/0.68747. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67717/0.68758. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67605/0.68751. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67523/0.68704. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67579/0.68646. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67492/0.68609. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67374/0.68574. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67106/0.68510. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67008/0.68460. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66905/0.68366. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66761/0.68304. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66724/0.68266. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66304/0.68261. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66295/0.68168. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66122/0.68051. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66005/0.67904. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65815/0.67830. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65481/0.67737. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65748/0.67620. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65094/0.67491. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64830/0.67586. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64769/0.67433. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64515/0.67547. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64257/0.67556. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64087/0.67522. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63951/0.67450. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63587/0.67405. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63114/0.67606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62961/0.67599. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62850/0.67725. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62581/0.67735. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62495/0.67991. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62116/0.68178. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62019/0.68271. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61430/0.68289. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61026/0.68621. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61348/0.68404. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61189/0.69084. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60214/0.69457. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60393/0.69358. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60109/0.69617. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60329/0.69515. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59101/0.69991. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59456/0.70762. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58718/0.71139. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59374/0.70926. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58666/0.71593. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58448/0.71734. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57978/0.71504. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58074/0.71683. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57683/0.71958. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57322/0.71872. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57541/0.73092. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57020/0.72855. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56945/0.73518. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56654/0.74070. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56048/0.74272. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55669/0.73852. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55704/0.75026. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55122/0.74897. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54639/0.75598. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54836/0.76149. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54228/0.76644. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54136/0.75822. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54456/0.77384. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54029/0.77570. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53815/0.77607. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54091/0.77658. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53928/0.77808. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52861/0.78483. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51693/0.78656. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52029/0.79053. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51904/0.79988. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51000/0.80608. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51203/0.80496. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51546/0.80906. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51298/0.81526. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69028/0.69143. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69062/0.69142. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68975/0.69147. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68913/0.69155. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68851/0.69170. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68710/0.69187. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68663/0.69208. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68611/0.69223. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68629/0.69239. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68596/0.69246. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68601/0.69221. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68503/0.69210. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68530/0.69208. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68403/0.69191. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68359/0.69163. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68215/0.69129. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68142/0.69103. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68088/0.69058. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67927/0.69094. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67859/0.69088. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67897/0.69069. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67625/0.69033. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67667/0.69003. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67387/0.68957. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67298/0.68951. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67203/0.68911. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67118/0.68815. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67014/0.68779. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66916/0.68728. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66744/0.68595. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66567/0.68523. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66285/0.68432. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66215/0.68404. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66231/0.68349. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65660/0.68254. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65688/0.68193. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65457/0.68129. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65268/0.67840. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64928/0.67893. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64795/0.67764. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.64566/0.67696. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64342/0.67660. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.64230/0.67506. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64019/0.67682. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63424/0.67283. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.63463/0.67113. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63415/0.67305. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62850/0.67368. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63121/0.67390. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62927/0.67248. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62225/0.67090. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62349/0.66993. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61910/0.66698. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61435/0.66633. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61602/0.66574. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.61003/0.66927. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61357/0.67289. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60536/0.67082. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60475/0.66807. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60120/0.66628. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59554/0.67357. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59500/0.67428. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59039/0.67416. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59083/0.67601. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59007/0.67370. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58759/0.67584. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58597/0.67207. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58424/0.67057. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57892/0.67325. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57678/0.67373. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57329/0.67160. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56908/0.67458. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56616/0.67747. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56228/0.68394. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56155/0.67918. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56019/0.68829. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55860/0.68955. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55045/0.68720. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55210/0.68584. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54292/0.68358. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54551/0.68573. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54380/0.68479. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53676/0.68535. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.53550/0.68587. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53625/0.68415. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52898/0.69098. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52861/0.69648. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52180/0.68929. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52007/0.69474. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51855/0.69376. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50929/0.70211. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50783/0.69964. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51229/0.70398. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51133/0.71252. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50053/0.71211. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50238/0.70154. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49982/0.71166. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49400/0.71418. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49602/0.71490. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49076/0.71379. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69480/0.69276. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69635. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.69970. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69010/0.70302. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69046/0.70614. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68956/0.70808. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68791/0.70984. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68839/0.71082. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.71109. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68741/0.71254. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68706/0.71286. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68773/0.71267. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68671/0.71232. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68642/0.71072. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68601/0.71104. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68660/0.71208. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68644/0.71154. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68570/0.71154. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68338/0.71138. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68350/0.70971. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68437/0.70927. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68236/0.70746. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68486/0.70839. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68312/0.70683. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67930/0.70587. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68120/0.70668. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68050/0.70789. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68105/0.70704. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68111/0.70745. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67869/0.70793. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67858/0.70591. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67557/0.70563. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67830/0.70626. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67394/0.70503. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67387/0.70275. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67441/0.70604. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67210/0.70423. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67259/0.70617. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66935/0.70276. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66818/0.70380. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66887/0.70448. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66620/0.70537. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66572/0.69989. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66271/0.70437. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66011/0.70058. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66170/0.70127. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66015/0.70009. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65989/0.69957. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65642/0.69877. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65591/0.70064. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65234/0.69489. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65062/0.69543. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65322/0.70176. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64536/0.69838. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64518/0.69616. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64434/0.69875. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64255/0.69675. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64077/0.69934. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63472/0.69914. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63560/0.69973. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62813/0.70302. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63297/0.70091. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62558/0.70681. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62357/0.70124. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62153/0.70003. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62064/0.70231. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61727/0.70667. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62010/0.71423. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61063/0.71541. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61382/0.71941. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61704/0.71802. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60928/0.72092. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60735/0.72082. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60248/0.72169. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60060/0.72488. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59468/0.72467. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59854/0.73258. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59436/0.74049. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58999/0.74070. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58794/0.73777. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59134/0.74716. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57998/0.74640. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58808/0.74685. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58451/0.75323. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57414/0.75184. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57825/0.75259. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57015/0.75675. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57020/0.75932. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57240/0.76358. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56360/0.76378. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56848/0.76939. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56492/0.77704. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56027/0.77744. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56148/0.77543. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55659/0.77967. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56252/0.78773. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55314/0.78756. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54676/0.79594. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54610/0.79387. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54985/0.80419. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69938/0.69562. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69605/0.69496. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69263/0.69384. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69217/0.69314. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69120/0.69243. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69052/0.69182. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69080/0.69106. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68956/0.69013. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68885/0.68944. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68835/0.68869. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68794/0.68804. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68741/0.68758. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68775/0.68707. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68635/0.68657. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68638/0.68584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68602/0.68526. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68511/0.68461. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68448/0.68399. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68495/0.68363. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68368/0.68317. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68411/0.68254. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68229/0.68178. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68242/0.68107. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68233/0.68058. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68026/0.68001. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68032/0.67953. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68126/0.67923. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67777/0.67899. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67682/0.67822. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67790/0.67798. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67684/0.67717. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67611/0.67632. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67486/0.67599. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67355/0.67559. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67292/0.67494. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67156/0.67413. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67135/0.67366. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66751/0.67321. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66933/0.67315. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66738/0.67295. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66743/0.67252. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66489/0.67207. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66366/0.67139. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66292/0.67110. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66118/0.67100. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65920/0.67204. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65937/0.67139. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65798/0.67081. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65660/0.67025. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65502/0.67102. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65361/0.67156. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65198/0.67060. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65049/0.67211. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64858/0.67242. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65082/0.67108. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64698/0.67258. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64555/0.67366. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64511/0.67260. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64124/0.67326. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64143/0.67418. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63874/0.67372. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63755/0.67294. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63311/0.67500. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63472/0.67572. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63149/0.67572. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62959/0.67779. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63268/0.67546. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62949/0.67720. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62807/0.67662. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62764/0.67846. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62576/0.67648. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62321/0.67865. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62123/0.67843. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62020/0.67935. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61818/0.67955. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61481/0.68094. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61631/0.68385. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61632/0.68345. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60960/0.68072. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60804/0.68400. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60950/0.68273. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60658/0.68482. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60830/0.68436. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60494/0.68606. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60258/0.68337. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60183/0.68941. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59617/0.68832. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60000/0.68827. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59433/0.68886. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59958/0.69227. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58829/0.69459. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59065/0.69387. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58745/0.69374. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59245/0.69718. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58705/0.69661. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58817/0.70033. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58156/0.69948. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58202/0.70368. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57646/0.69825. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57692/0.70246. Took 0.09 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.69971/0.69186. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69257/0.69145. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69079. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69130/0.69023. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69202/0.68991. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69115/0.68950. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68971/0.68909. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68963/0.68871. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68802/0.68832. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68834/0.68776. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.68726. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68627/0.68658. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68682/0.68605. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68543/0.68574. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68460/0.68525. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68315/0.68452. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68223/0.68404. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68088/0.68348. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67810/0.68263. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67786/0.68206. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67471/0.68114. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67463/0.68026. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67016/0.67954. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66964/0.67837. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66622/0.67747. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66350/0.67687. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66383/0.67605. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65869/0.67596. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65465/0.67583. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65443/0.67510. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64879/0.67506. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64700/0.67418. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64628/0.67373. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64377/0.67397. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63976/0.67312. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63822/0.67339. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63921/0.67429. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63264/0.67463. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63071/0.67399. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62846/0.67383. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62944/0.67353. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62900/0.67225. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62385/0.67330. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62120/0.67421. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61938/0.67542. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61587/0.67591. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61451/0.67577. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61406/0.67770. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61194/0.67711. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61102/0.67891. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60935/0.67905. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60463/0.68052. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60281/0.68249. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60300/0.68157. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59769/0.68165. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60061/0.68432. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59377/0.68502. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59154/0.68842. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59230/0.69040. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58868/0.69177. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.57916/0.69381. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58484/0.69777. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58997/0.69572. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58413/0.69585. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57889/0.69526. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57775/0.69321. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57333/0.69412. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57585/0.69993. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57884/0.70013. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56501/0.70171. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56937/0.70716. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56063/0.70910. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56335/0.70665. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.56249/0.70734. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56262/0.70992. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55824/0.71489. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55468/0.71503. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55294/0.71354. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54843/0.71046. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55153/0.72003. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53827/0.71529. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54306/0.71572. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54315/0.71545. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54253/0.71710. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53807/0.72267. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54318/0.72055. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52743/0.72247. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53739/0.72902. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53308/0.72242. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53105/0.72635. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51920/0.72574. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51996/0.72583. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52001/0.72699. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51646/0.72131. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51295/0.73080. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51185/0.72745. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50974/0.73078. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50381/0.72813. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49982/0.73150. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50439/0.73485. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69532/0.69754. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69483/0.69634. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69361/0.69519. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69413. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69198/0.69312. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.69267. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69141/0.69234. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.69232. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69066/0.69251. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69009/0.69317. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69005/0.69388. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68859/0.69480. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68724/0.69564. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68789/0.69676. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68650/0.69836. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68518/0.69971. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68465/0.70139. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68285/0.70397. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68147/0.70571. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68079/0.70757. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67943/0.70907. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67768/0.71197. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67672/0.71418. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67539/0.71674. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67813/0.71835. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67540/0.71857. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67307/0.72335. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67156/0.72570. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67278/0.72501. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67082/0.72769. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66895/0.73054. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66836/0.73152. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66650/0.73409. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66720/0.73488. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66485/0.73868. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66782/0.73749. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66419/0.74174. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66554/0.73983. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66330/0.74413. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66079/0.74632. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66437/0.74395. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66067/0.74863. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66154/0.75023. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66115/0.75194. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65951/0.75404. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66041/0.75644. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65778/0.75544. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65218/0.75553. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65343/0.76017. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65637/0.75864. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65582/0.76002. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65333/0.76071. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65429/0.76046. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65124/0.76441. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65179/0.76641. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64973/0.76768. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64617/0.76893. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64805/0.76862. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64917/0.77147. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64427/0.77201. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64455/0.77432. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64428/0.77432. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64074/0.77251. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64101/0.77596. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63725/0.77884. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63889/0.78159. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63706/0.78034. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63841/0.78296. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63728/0.78510. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63342/0.78705. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63435/0.78902. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63168/0.79189. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63018/0.79569. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62938/0.79710. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62646/0.80018. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62213/0.80401. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62375/0.80217. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62371/0.80426. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61963/0.80219. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61910/0.80647. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61519/0.80891. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61463/0.81173. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61792/0.81777. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60806/0.81976. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60948/0.82011. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60724/0.82445. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60971/0.82937. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60289/0.82753. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60358/0.82686. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59993/0.83190. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60044/0.83539. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59274/0.83974. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59481/0.84393. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59688/0.84668. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59445/0.85050. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58734/0.85189. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59153/0.85613. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58588/0.85905. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58366/0.86007. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58139/0.86309. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69273. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69250/0.69278. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69136/0.69293. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69069/0.69314. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69075/0.69338. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68959/0.69364. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68951/0.69392. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68912/0.69424. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68964/0.69430. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68756/0.69459. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.69484. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68576/0.69522. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68546/0.69563. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68415/0.69565. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68400/0.69611. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68229/0.69637. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67973/0.69720. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68002/0.69747. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67717/0.69724. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67583/0.69747. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67495/0.69797. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67373/0.69816. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67343/0.69828. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66921/0.69851. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66856/0.69896. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66829/0.69735. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66453/0.69789. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66474/0.69601. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66209/0.69612. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65977/0.69627. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65712/0.69452. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65675/0.69310. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65851/0.69153. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65080/0.68982. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65249/0.68951. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.64726/0.68941. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.64578/0.68698. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64691/0.68529. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64084/0.68789. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64143/0.68364. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.63881/0.68672. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63426/0.68637. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63470/0.68634. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63959/0.68307. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63486/0.68097. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.63016/0.68305. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62691/0.68126. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62683/0.67966. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.62733/0.68117. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62009/0.68163. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61927/0.68296. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62036/0.68208. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62127/0.67924. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61627/0.68246. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.61326/0.68491. Took 0.12 sec\n",
      "Epoch 55, Loss(train/val) 0.61342/0.68731. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60763/0.68325. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60979/0.68447. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60464/0.68494. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60277/0.68497. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60439/0.68758. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59695/0.68913. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59934/0.69341. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59621/0.69203. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59292/0.69420. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59071/0.69762. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58908/0.69743. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58805/0.69855. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58427/0.70050. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57812/0.70375. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57250/0.70484. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57858/0.70476. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57673/0.70533. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56708/0.71146. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57018/0.71397. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56776/0.71789. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56529/0.72041. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56099/0.72133. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56026/0.72496. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56324/0.72018. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55372/0.72574. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55718/0.72813. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54855/0.73357. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54346/0.73592. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54768/0.74173. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54081/0.74212. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54085/0.73858. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53627/0.74687. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53198/0.75643. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53167/0.75946. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53140/0.76604. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52340/0.76842. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.51836/0.77041. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53158/0.77053. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51240/0.77425. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51938/0.78286. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51407/0.78711. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51646/0.78977. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50213/0.79033. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50524/0.79804. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69622/0.69775. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69380/0.70075. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69305/0.70251. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69160/0.70396. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69126/0.70519. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68995/0.70629. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68920/0.70742. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68896/0.70878. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68833/0.70989. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68794/0.71076. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68681/0.71209. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68612/0.71229. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68637/0.71310. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68470/0.71409. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68450/0.71483. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68352/0.71514. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68272/0.71545. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68199/0.71602. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68166/0.71576. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68039/0.71614. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67881/0.71560. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67807/0.71631. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67640/0.71603. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67377/0.71641. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67469/0.71478. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67507/0.71348. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67057/0.71424. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67126/0.71445. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66812/0.71471. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66795/0.71438. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66366/0.71409. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66405/0.71214. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66265/0.71104. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65789/0.71073. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65747/0.71010. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65381/0.70979. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65090/0.70915. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65066/0.70912. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.64675/0.70984. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64613/0.70908. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64303/0.71159. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64148/0.71177. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63751/0.71093. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63475/0.71198. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63334/0.71203. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63105/0.71434. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62995/0.71536. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62611/0.71505. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62396/0.71836. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62567/0.71667. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62328/0.71856. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61813/0.71902. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61444/0.71949. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61188/0.72359. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61145/0.72542. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60953/0.72932. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60921/0.72829. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60676/0.73102. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60660/0.73230. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59896/0.73534. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60154/0.73587. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59389/0.74200. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59865/0.74621. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59725/0.74836. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59283/0.74779. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58828/0.74928. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59304/0.75223. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58804/0.75330. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58506/0.75492. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58276/0.76163. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58108/0.76293. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58235/0.76686. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57579/0.77033. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57354/0.77493. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57356/0.77648. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57152/0.78141. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57308/0.78442. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56728/0.78681. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56802/0.78599. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56669/0.79115. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56479/0.78855. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56710/0.79249. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56426/0.79000. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55679/0.79691. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55618/0.80003. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55309/0.80040. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55084/0.80726. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55331/0.81102. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54859/0.80638. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54811/0.80869. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54236/0.81327. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54786/0.81816. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54376/0.81566. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54013/0.81836. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53781/0.82532. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.53486/0.82850. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53602/0.82589. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53365/0.83546. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52548/0.83611. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51393/0.84873. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69494/0.69371. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69361/0.69318. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69295. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.69241/0.69282. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69282. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69222/0.69278. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69218/0.69278. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69110/0.69280. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69183/0.69288. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69308. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69122/0.69319. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69021/0.69334. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69056/0.69352. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68959/0.69354. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68867/0.69378. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68797/0.69411. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.69461. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68632/0.69514. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68586/0.69570. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68430/0.69638. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68388/0.69730. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68430/0.69822. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68309/0.69944. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68171/0.70023. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68095/0.70175. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68142/0.70225. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68000/0.70332. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68047/0.70395. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67644/0.70531. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67959/0.70600. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67735/0.70682. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67603/0.70783. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67497/0.70919. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67609/0.70916. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67289/0.71046. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67295/0.71130. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67157/0.71191. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66980/0.71436. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66965/0.71516. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66868/0.71587. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66699/0.71706. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66629/0.71818. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66467/0.71887. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66328/0.71953. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66190/0.72068. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65998/0.72279. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66136/0.72388. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66099/0.72396. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65577/0.72525. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65617/0.72527. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65708/0.72500. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65608/0.72614. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65277/0.72772. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65235/0.72843. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64932/0.73046. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64666/0.72940. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64673/0.73360. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64823/0.73329. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64291/0.73528. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64349/0.73549. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63998/0.73526. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64313/0.73410. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64113/0.73611. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63901/0.73941. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63250/0.74112. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63299/0.74233. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62981/0.74394. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63349/0.74323. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63116/0.74259. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62840/0.74383. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62229/0.74609. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62738/0.74879. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62454/0.75035. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61832/0.75174. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62431/0.75209. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61507/0.74985. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61588/0.75554. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61151/0.75853. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61807/0.75704. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60993/0.75867. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61388/0.75989. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60360/0.76529. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60341/0.76321. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60092/0.76597. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60185/0.76886. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59129/0.76926. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59189/0.77700. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58938/0.77542. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59341/0.77304. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58941/0.77853. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58129/0.77922. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58305/0.78122. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58634/0.78546. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58461/0.78971. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57987/0.78831. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57680/0.78696. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57438/0.78989. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57270/0.79320. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57399/0.79598. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56885/0.80168. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69647/0.69961. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69430/0.69750. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69370/0.69619. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69318/0.69499. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69243/0.69421. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.69357. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.69254. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69081/0.69188. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69044/0.69103. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69051/0.69023. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68987/0.68964. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.68894. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68912/0.68862. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68827/0.68778. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68854/0.68731. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68767/0.68724. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68685/0.68695. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68700/0.68616. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68559/0.68609. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68562/0.68570. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68580/0.68584. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68507/0.68592. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68481/0.68621. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68410/0.68629. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68463/0.68687. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68328/0.68727. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68311/0.68755. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68212/0.68766. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68171/0.68786. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68094/0.68851. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68040/0.68876. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67963/0.68864. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67817/0.68946. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67864/0.68979. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67714/0.69102. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67877/0.69104. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67719/0.69211. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67742/0.69275. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67606/0.69311. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67307/0.69346. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67401/0.69422. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67423/0.69460. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67392/0.69533. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67096/0.69623. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67159/0.69667. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67081/0.69726. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66962/0.69689. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66971/0.69823. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66984/0.69935. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66712/0.69922. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66591/0.69978. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66494/0.70069. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66635/0.70085. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66349/0.70191. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66295/0.70190. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66110/0.70314. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66181/0.70362. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65816/0.70434. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65802/0.70651. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65748/0.70671. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65698/0.70730. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65445/0.70855. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65407/0.70858. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65613/0.71048. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65286/0.71017. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65213/0.71098. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65291/0.71173. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65054/0.71314. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64940/0.71305. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64800/0.71350. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64402/0.71350. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64458/0.71748. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64350/0.71852. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64383/0.71834. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64227/0.71895. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63825/0.72044. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63921/0.72251. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63711/0.72400. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63568/0.72521. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63707/0.72426. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63805/0.72560. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63938/0.72667. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63617/0.72724. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63071/0.72896. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63241/0.72918. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63503/0.72968. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63257/0.73058. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62628/0.73158. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62630/0.73535. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62905/0.73782. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62621/0.73623. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62362/0.73847. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62685/0.73794. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61959/0.74038. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62170/0.74091. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62075/0.74221. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61994/0.74652. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62105/0.74589. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61428/0.74672. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61377/0.74627. Took 0.08 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69479/0.69163. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69330/0.69173. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69290/0.69152. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69275/0.69132. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69226/0.69139. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69200/0.69131. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69238/0.69115. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69132/0.69111. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69129/0.69130. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69140/0.69121. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69018/0.69111. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69016/0.69118. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68916/0.69108. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68921/0.69131. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68890/0.69161. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68795/0.69203. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68636/0.69224. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68777/0.69254. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68701/0.69327. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68688/0.69346. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68581/0.69396. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68591/0.69367. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68480/0.69348. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68351/0.69398. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68324/0.69429. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68512/0.69429. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68323/0.69508. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68493/0.69516. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68116/0.69632. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68209/0.69635. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68056/0.69683. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68102/0.69660. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.69705. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67985/0.69741. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67874/0.69861. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67864/0.69933. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67772/0.69968. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67710/0.70044. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67592/0.70126. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67415/0.70119. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67133/0.70244. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67384/0.70273. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66963/0.70418. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66990/0.70626. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66932/0.70759. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66601/0.70771. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66602/0.70830. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66631/0.71073. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66339/0.71199. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66340/0.71197. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66139/0.71395. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66004/0.71594. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65921/0.71671. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65759/0.71820. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65615/0.72181. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65419/0.72252. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65445/0.72325. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65283/0.72481. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65413/0.72557. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65020/0.72716. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65168/0.72888. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64643/0.72898. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64417/0.73172. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64646/0.73428. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64119/0.73700. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64277/0.73874. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64110/0.74016. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63616/0.74083. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63763/0.74151. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63291/0.74334. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63479/0.74717. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63116/0.74963. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62951/0.75224. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63056/0.75479. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62769/0.75752. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62502/0.75851. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62342/0.75927. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62609/0.76082. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62555/0.76263. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61923/0.76365. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61708/0.76857. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61383/0.77008. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61299/0.77476. Took 0.12 sec\n",
      "Epoch 83, Loss(train/val) 0.61259/0.77534. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61016/0.78046. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61032/0.77885. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60784/0.78205. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60754/0.78343. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60233/0.78515. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60081/0.78660. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60336/0.79002. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59986/0.79178. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59814/0.79403. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59570/0.79750. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59462/0.80222. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59457/0.80077. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.58579/0.80528. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58437/0.81199. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58359/0.81584. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58323/0.82211. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69354/0.69245. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69258. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69227/0.69256. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 0.69191/0.69224. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69183/0.69202. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69167/0.69184. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69093/0.69170. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69161. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68983/0.69140. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68978/0.69132. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68966/0.69139. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68859/0.69155. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68711/0.69126. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68681/0.69155. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68674/0.69221. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68641/0.69164. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68610/0.69190. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68433/0.69216. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68446/0.69261. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68400/0.69325. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68246/0.69381. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68199/0.69399. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68125/0.69466. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68095/0.69539. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68076/0.69598. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67794/0.69695. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67678/0.69838. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67701/0.69831. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67580/0.69937. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67434/0.70057. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67167/0.70161. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67151/0.70408. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66992/0.70536. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66610/0.70684. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66624/0.70937. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66563/0.71038. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66492/0.71196. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66261/0.71263. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65649/0.71562. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65585/0.71813. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64998/0.72188. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64983/0.72704. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65018/0.72822. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64887/0.72918. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64782/0.72858. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64465/0.72983. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63903/0.73425. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63940/0.73686. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63594/0.73979. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63716/0.74188. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63689/0.74399. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63611/0.74421. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62950/0.74708. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63273/0.74792. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62743/0.75004. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62583/0.74925. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62062/0.75067. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61922/0.75308. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62005/0.75821. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61533/0.76033. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61913/0.76025. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60843/0.76304. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60688/0.76406. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60389/0.77195. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60514/0.77159. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59696/0.77448. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60011/0.77413. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59903/0.77784. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59735/0.78225. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59295/0.78416. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58810/0.78543. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58778/0.79001. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59087/0.78975. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58461/0.79539. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57493/0.80059. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57933/0.80474. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57569/0.80661. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56814/0.80877. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56635/0.81302. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56926/0.81288. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56940/0.81591. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56328/0.81680. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55812/0.82334. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55920/0.82550. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55390/0.82596. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55424/0.83295. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54737/0.83874. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.55037/0.84469. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54291/0.84564. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54652/0.84577. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53860/0.84920. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54226/0.84791. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52693/0.85847. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53027/0.85923. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52716/0.86539. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53020/0.86752. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51749/0.87070. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51742/0.87644. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51074/0.88453. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50854/0.88654. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69414/0.69388. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69397/0.69383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69343/0.69395. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69311/0.69394. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69299/0.69392. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69324/0.69391. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69310/0.69392. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69283/0.69388. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69209/0.69391. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69285/0.69395. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69126/0.69406. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69086/0.69405. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69074/0.69419. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69068/0.69421. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69130/0.69424. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69078/0.69433. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69083/0.69458. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68956/0.69483. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68808/0.69506. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68922/0.69529. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68762/0.69544. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68749/0.69574. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68628/0.69602. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68675/0.69630. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68476/0.69653. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68515/0.69660. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68152/0.69701. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68153/0.69717. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.69808. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68001/0.69845. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67931/0.69919. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67771/0.69938. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67668/0.70048. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67636/0.70035. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67405/0.70148. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67553/0.70145. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67383/0.70173. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67253/0.70059. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66832/0.70101. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67047/0.70118. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66781/0.70254. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66745/0.70322. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66685/0.70150. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66201/0.70380. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66260/0.70427. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66329/0.70504. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66171/0.70523. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65278/0.70903. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65583/0.70982. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65413/0.70906. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65396/0.70957. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65361/0.70925. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64811/0.70993. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65133/0.71137. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64620/0.71290. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64127/0.71368. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64591/0.71377. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63912/0.71713. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63890/0.71819. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63410/0.71733. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63389/0.71884. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63276/0.71844. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63289/0.71829. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63100/0.71975. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62454/0.72113. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62574/0.72557. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62540/0.72635. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61818/0.72711. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62067/0.72923. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61282/0.73079. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61233/0.73064. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61132/0.73166. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.60967/0.72832. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60414/0.73428. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60571/0.73390. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59900/0.73527. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59891/0.73336. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59961/0.73482. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59674/0.73584. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59016/0.73965. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58860/0.73860. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58898/0.73776. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58319/0.74101. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58218/0.74281. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57652/0.74537. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57539/0.74639. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56767/0.74526. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56982/0.74667. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56333/0.75576. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56822/0.75404. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56383/0.75206. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55175/0.75609. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55417/0.75570. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55231/0.75879. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55415/0.76032. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54472/0.76298. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55155/0.76254. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54297/0.76981. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53613/0.77188. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53403/0.77478. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69586/0.68831. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69463/0.68983. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69354/0.69106. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69344/0.69178. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69284/0.69275. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.69324. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69180/0.69374. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69081/0.69462. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69072/0.69535. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69046/0.69622. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.69714. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68921/0.69794. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68822/0.69887. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68758/0.69964. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68627/0.70063. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68633/0.70161. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68628/0.70206. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68457/0.70321. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68523/0.70390. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68269/0.70420. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68232/0.70549. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68209/0.70697. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68138/0.70716. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68121/0.70788. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67913/0.70884. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67826/0.71097. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67822/0.71084. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67655/0.71137. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67591/0.71141. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67437/0.71220. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67285/0.71336. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67426/0.71286. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67359/0.71393. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66812/0.71417. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66934/0.71429. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66714/0.71583. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66594/0.71622. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66552/0.71582. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66274/0.71705. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66233/0.71741. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66299/0.71652. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65798/0.71663. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65695/0.71493. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65767/0.71515. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65337/0.71525. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65099/0.71316. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64844/0.71443. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64562/0.71413. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64884/0.71392. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64380/0.71189. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64095/0.70898. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64054/0.70856. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63630/0.70872. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63544/0.71323. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63078/0.70990. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63360/0.70606. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62835/0.70982. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62561/0.70750. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62215/0.70729. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61994/0.70654. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61849/0.71021. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61523/0.70556. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61171/0.70958. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.61439/0.70465. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.60695/0.70159. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.60335/0.70563. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.61086/0.70970. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60972/0.70691. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59736/0.70504. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59349/0.70951. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59756/0.70839. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59827/0.70851. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58825/0.70659. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58921/0.70826. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58643/0.70550. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58740/0.71118. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58724/0.71058. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57918/0.70602. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57473/0.71184. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57709/0.70865. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57247/0.70918. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56806/0.70816. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56505/0.71924. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55657/0.71390. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56177/0.72294. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55840/0.72905. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55538/0.71327. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55200/0.72314. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55240/0.71708. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54713/0.72448. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54594/0.71878. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54166/0.72513. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54337/0.73161. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53576/0.72267. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53114/0.72522. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53503/0.72830. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53422/0.73389. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52613/0.73079. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52807/0.75639. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52658/0.72982. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69548/0.69249. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69445/0.69239. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69399/0.69229. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69319/0.69226. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69321/0.69231. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69279/0.69237. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69274/0.69254. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69314/0.69267. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69251/0.69292. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.69317. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69086/0.69331. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69153/0.69337. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69091/0.69357. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69082/0.69375. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69085/0.69402. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68914/0.69419. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68957/0.69450. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68883/0.69487. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68859/0.69535. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68739/0.69607. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68629/0.69682. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68524/0.69773. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68598/0.69830. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68438/0.69936. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68372/0.70012. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68274/0.70120. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68079/0.70241. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68099/0.70335. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67923/0.70435. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67815/0.70482. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67650/0.70466. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67550/0.70543. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67636/0.70503. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67420/0.70598. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67297/0.70693. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67229/0.70692. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67246/0.70739. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67109/0.70748. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66724/0.70830. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66925/0.70905. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.66500/0.70827. Took 0.11 sec\n",
      "Epoch 41, Loss(train/val) 0.66663/0.70838. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66375/0.70848. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66340/0.70866. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65976/0.70719. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66162/0.70780. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65681/0.70755. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65978/0.70629. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65689/0.70683. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65552/0.70693. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65327/0.70660. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65386/0.70475. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64903/0.70541. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65072/0.70406. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64639/0.70543. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64702/0.70461. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64286/0.70163. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64360/0.70472. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63927/0.70488. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63525/0.70433. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63646/0.70283. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63331/0.70141. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63705/0.69935. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63130/0.70015. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62599/0.70352. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63004/0.70078. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62747/0.69797. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62345/0.69793. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61968/0.69704. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61864/0.69464. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61694/0.69538. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61248/0.69131. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60989/0.69715. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61168/0.69351. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61090/0.69720. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60173/0.69589. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60165/0.69528. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59813/0.69412. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60099/0.69490. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59769/0.69781. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59837/0.69240. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59597/0.69284. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59535/0.68794. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58503/0.69019. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58804/0.68792. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58395/0.68985. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57580/0.68885. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57899/0.68855. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56902/0.68590. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56966/0.69083. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57843/0.69036. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56692/0.69156. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55687/0.69016. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56320/0.69407. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55767/0.69059. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55847/0.69818. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55568/0.70047. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55761/0.69449. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54695/0.69103. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54410/0.68113. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69311/0.69393. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69308/0.69388. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69338/0.69388. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69393. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69280/0.69399. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69204/0.69402. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69134/0.69411. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69078/0.69424. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69187/0.69434. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69075/0.69446. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69058/0.69463. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69075/0.69470. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69002/0.69491. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68852/0.69518. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68952/0.69547. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68821/0.69576. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68775/0.69618. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68652/0.69671. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68594/0.69753. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68459/0.69849. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68386/0.69945. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68364/0.70064. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68076/0.70218. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67762/0.70364. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67717/0.70578. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67683/0.70860. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67672/0.71104. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67284/0.71390. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67317/0.71592. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67102/0.71801. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67013/0.72015. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66678/0.72235. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66745/0.72600. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66521/0.72869. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66493/0.73148. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66023/0.73336. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65961/0.73558. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65770/0.73737. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65664/0.74139. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65502/0.74252. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65646/0.74700. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65333/0.74819. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64777/0.75004. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65080/0.75330. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64577/0.75715. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64258/0.75989. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64362/0.76128. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64211/0.76383. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64417/0.76329. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63585/0.76751. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63832/0.77171. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63378/0.77328. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63487/0.77701. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63040/0.78101. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62715/0.78293. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62717/0.78282. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62422/0.78441. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62323/0.78837. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61851/0.79277. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.61483/0.79804. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61484/0.80231. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60740/0.80884. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60749/0.81352. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60822/0.81480. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60039/0.82043. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59882/0.82623. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59954/0.82652. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59699/0.82956. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58866/0.83645. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58429/0.84378. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58642/0.85035. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58340/0.84792. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58023/0.85394. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57148/0.86315. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57302/0.86828. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56951/0.87237. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56999/0.87348. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56326/0.88062. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56303/0.87819. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55741/0.88362. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55639/0.89284. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54897/0.90096. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55600/0.89891. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54962/0.90142. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54164/0.90627. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54185/0.91179. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53628/0.91088. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54259/0.91868. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53046/0.92009. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53636/0.92293. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52879/0.92375. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52163/0.93562. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51497/0.93853. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51880/0.94939. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51124/0.95813. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51945/0.94472. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51290/0.95578. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49755/0.96771. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49951/0.95729. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50239/0.97000. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69333/0.69395. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69186/0.69342. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68998/0.69255. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68952/0.69172. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68817/0.69088. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68746/0.69030. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68661/0.68991. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68606/0.68927. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68540/0.68887. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68367/0.68876. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68446/0.68840. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68282/0.68850. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68217/0.68794. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68106/0.68773. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68062/0.68744. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68158/0.68728. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67873/0.68749. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67889/0.68709. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67891/0.68669. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67820/0.68570. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67614/0.68560. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67502/0.68592. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67421/0.68518. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67520/0.68472. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67595/0.68472. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67239/0.68430. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67097/0.68457. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67029/0.68238. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67116/0.68189. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66687/0.68230. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66709/0.68254. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66515/0.68149. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66364/0.68098. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66342/0.67972. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66064/0.67916. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65936/0.67859. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65546/0.68029. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65586/0.67914. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65426/0.67970. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65216/0.67868. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65365/0.67725. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65120/0.67682. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64743/0.67746. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64671/0.67701. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64428/0.67669. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63896/0.67948. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64214/0.67893. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64121/0.67922. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63740/0.68032. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63626/0.68036. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63248/0.67825. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63438/0.68108. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63059/0.68034. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62779/0.67795. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62717/0.67982. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62682/0.68034. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62155/0.68369. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61954/0.68335. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61667/0.68374. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61577/0.68649. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61064/0.68720. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61195/0.68757. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60832/0.68860. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60441/0.69390. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60326/0.68884. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60172/0.69514. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60938/0.69258. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59853/0.69353. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59786/0.69526. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59186/0.69543. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58996/0.69840. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58519/0.70134. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58437/0.69980. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58230/0.70493. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58226/0.70568. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58012/0.70729. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57596/0.70905. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57577/0.70872. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57345/0.71280. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57190/0.71233. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56971/0.71256. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56681/0.71121. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56785/0.71918. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56235/0.71479. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56376/0.71693. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56199/0.71767. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55838/0.72004. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54945/0.72187. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55183/0.72443. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55006/0.72574. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54869/0.72849. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54289/0.72631. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53807/0.73215. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53927/0.73402. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52979/0.73586. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52971/0.73762. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53483/0.73914. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52631/0.73739. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52661/0.74168. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52552/0.74890. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69480. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69389/0.69368. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69259. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69235/0.69166. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69218/0.69085. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69178/0.69002. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69171/0.68945. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69105/0.68909. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69081/0.68864. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.68807. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69027/0.68769. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68970/0.68729. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68976/0.68686. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68956/0.68661. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68771/0.68588. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68787/0.68547. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68637/0.68499. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68598/0.68454. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68633/0.68465. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68497/0.68462. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68245/0.68458. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68256/0.68472. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68255/0.68469. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68136/0.68528. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68093/0.68635. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68072/0.68670. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67904/0.68793. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67855/0.68861. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67768/0.68979. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67686/0.69097. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67770/0.69220. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67666/0.69297. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67440/0.69443. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67508/0.69545. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67163/0.69661. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67287/0.69792. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67292/0.69875. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67244/0.70096. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66982/0.70189. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66771/0.70331. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66838/0.70444. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66840/0.70602. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66490/0.70757. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66518/0.70978. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66649/0.71119. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66312/0.71198. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66250/0.71442. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66273/0.71653. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65913/0.71828. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65786/0.71923. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.65939/0.72070. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65612/0.72330. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65309/0.72466. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65200/0.72690. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65358/0.72743. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65504/0.72999. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64880/0.73255. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65099/0.73547. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64902/0.73660. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64387/0.73990. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64560/0.74186. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63932/0.74362. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64350/0.74520. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63998/0.74744. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63767/0.75296. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63459/0.75377. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62994/0.75898. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63410/0.76264. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63006/0.76444. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63089/0.76601. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62622/0.76511. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62824/0.77050. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62328/0.77194. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62081/0.77581. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61983/0.78022. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61959/0.78181. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61961/0.78726. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61903/0.78820. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61316/0.78892. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61515/0.79065. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60841/0.78817. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60596/0.79053. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60739/0.79952. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60309/0.80177. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60253/0.80185. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59949/0.80353. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59864/0.80778. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60255/0.81214. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59129/0.81476. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59230/0.81711. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58569/0.82105. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58137/0.82543. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58818/0.82588. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58872/0.82900. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58218/0.82818. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58476/0.82919. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58455/0.83359. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57963/0.83966. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56903/0.83880. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57503/0.84068. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69406/0.68975. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69219/0.68905. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69131/0.68854. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69076/0.68799. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69020/0.68750. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68921/0.68714. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68867/0.68662. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68781/0.68620. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68691/0.68602. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68592/0.68583. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68515/0.68575. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68480/0.68567. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68412/0.68591. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68324/0.68610. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68289/0.68649. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68196/0.68684. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68163/0.68703. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68050/0.68756. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67982/0.68800. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68004/0.68838. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67919/0.68867. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67840/0.68898. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67624/0.68942. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67463/0.68985. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67511/0.69044. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67634/0.69092. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67436/0.69137. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67395/0.69191. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67190/0.69273. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67189/0.69315. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66972/0.69393. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67131/0.69449. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66917/0.69450. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66668/0.69537. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66517/0.69487. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66495/0.69540. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66545/0.69673. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66521/0.69789. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66340/0.69836. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66025/0.69887. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66123/0.69932. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66168/0.70055. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65734/0.70100. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65882/0.70173. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65707/0.70266. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65748/0.70330. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65558/0.70349. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65479/0.70336. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65219/0.70426. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65090/0.70568. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65262/0.70741. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65123/0.70822. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64788/0.70617. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64788/0.70782. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64703/0.70990. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64721/0.71027. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64711/0.71199. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64782/0.71362. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64757/0.71488. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64237/0.71380. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64411/0.71596. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64331/0.71580. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64017/0.71545. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63903/0.71709. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63896/0.71567. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63674/0.71816. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64128/0.72078. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63747/0.71953. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63563/0.72028. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63621/0.72474. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63203/0.72356. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62890/0.72765. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62953/0.72912. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62806/0.72910. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62751/0.72705. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62771/0.72756. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62461/0.72886. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62460/0.73526. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62299/0.73609. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61882/0.73504. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61761/0.73801. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61818/0.73837. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61650/0.73938. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61724/0.74055. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61444/0.74307. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61647/0.74704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61003/0.75160. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60858/0.74748. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61011/0.74919. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60761/0.75379. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60658/0.75644. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60180/0.75712. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59995/0.75830. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60084/0.75610. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59442/0.76343. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59456/0.75987. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59393/0.75989. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59379/0.76669. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59146/0.77384. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59234/0.76996. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69281/0.69409. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69264/0.69414. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69214/0.69397. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69088/0.69396. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69069/0.69402. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69023/0.69420. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69112/0.69415. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68916/0.69415. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68845/0.69426. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68878/0.69429. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68809/0.69407. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68748/0.69410. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68621/0.69433. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68512/0.69468. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68502/0.69481. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68447/0.69474. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68317/0.69474. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68202/0.69523. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68045/0.69531. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67997/0.69566. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67942/0.69635. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67641/0.69725. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67557/0.69795. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67450/0.69826. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67261/0.69929. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67108/0.69969. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66678/0.69963. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66631/0.70018. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66461/0.70087. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66171/0.70192. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65958/0.70252. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65597/0.70346. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65574/0.70482. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65642/0.70723. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65309/0.70969. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65243/0.71101. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.64689/0.71406. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64936/0.71485. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.64520/0.71975. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63936/0.71814. Took 0.12 sec\n",
      "Epoch 40, Loss(train/val) 0.63971/0.72065. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63757/0.72163. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63237/0.72586. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63223/0.72901. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63126/0.73269. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62936/0.73643. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62971/0.73828. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62649/0.74281. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62757/0.74225. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62522/0.74544. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61657/0.74529. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61915/0.75146. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61509/0.75279. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.61804/0.75164. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61402/0.75637. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60817/0.75778. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60704/0.76116. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60622/0.76380. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.60327/0.76645. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.60591/0.76933. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59925/0.77148. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59914/0.77572. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59634/0.77707. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59322/0.78185. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59518/0.78713. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58678/0.78428. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58947/0.78275. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58160/0.78777. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58134/0.78511. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58193/0.78878. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57683/0.79238. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57707/0.79378. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57109/0.80013. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57431/0.80004. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56726/0.80107. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56395/0.80814. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56371/0.80913. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56591/0.80792. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56152/0.80709. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55351/0.81056. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55823/0.80970. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54848/0.80695. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54813/0.82447. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54242/0.81670. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53986/0.82134. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53921/0.81959. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54104/0.81929. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53817/0.81793. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53196/0.81923. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52976/0.82730. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52590/0.82607. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53063/0.81679. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52065/0.82615. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52418/0.82440. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51951/0.82797. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52118/0.83498. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51397/0.81560. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51405/0.81970. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.50235/0.83109. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50739/0.82630. Took 0.08 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69349/0.69693. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69330/0.69668. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69630. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69622. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69222/0.69616. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69199/0.69626. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69241/0.69627. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69183/0.69623. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69158/0.69633. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69079/0.69679. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.69703. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69042/0.69727. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69071/0.69764. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69047/0.69782. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68912/0.69813. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.69843. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68804/0.69868. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68715/0.69913. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68676/0.69934. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68444/0.69919. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68544/0.69984. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68436/0.69964. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68227/0.69947. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.69901. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68051/0.69816. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67954/0.69920. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67722/0.69790. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67755/0.69879. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67432/0.69780. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67415/0.69710. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67088/0.69754. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66956/0.69606. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66842/0.69667. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66507/0.69815. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66305/0.69720. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66503/0.69772. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66116/0.69848. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65890/0.69880. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65876/0.70211. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65660/0.69751. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65300/0.69892. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65047/0.70098. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64982/0.69772. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64905/0.70075. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64682/0.69999. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64074/0.70326. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64001/0.70343. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63950/0.70275. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63691/0.70250. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63987/0.70826. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63228/0.70543. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63485/0.70634. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63144/0.70481. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63038/0.70647. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62819/0.70741. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62813/0.70923. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62673/0.70819. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62340/0.70941. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61588/0.70890. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62141/0.71135. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61535/0.71130. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61307/0.71038. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61220/0.70870. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60943/0.70780. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60895/0.71182. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60495/0.71498. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60137/0.71530. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60287/0.71480. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59803/0.72049. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59950/0.71349. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59429/0.71348. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59336/0.71382. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58931/0.71601. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58278/0.71556. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58118/0.71857. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58180/0.72006. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57519/0.71887. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57614/0.71689. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56726/0.71758. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56836/0.72010. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56582/0.71899. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56383/0.72074. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56370/0.72844. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55549/0.72444. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55133/0.72423. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54348/0.73570. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55026/0.73410. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54878/0.73624. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54382/0.73997. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53539/0.73439. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53836/0.74361. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53686/0.73319. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53418/0.72490. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52959/0.72673. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52967/0.74417. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52308/0.75064. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51906/0.74003. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51826/0.76368. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51869/0.76207. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50697/0.76104. Took 0.08 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69434/0.70061. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69338/0.70009. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69272/0.69938. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69218/0.69880. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69167/0.69805. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69135/0.69758. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69136/0.69718. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69171/0.69675. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69154/0.69647. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69118/0.69628. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69039/0.69635. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69042/0.69630. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69014/0.69602. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68938/0.69615. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68944/0.69566. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68912/0.69572. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68885/0.69601. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68778/0.69622. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68835/0.69674. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68760/0.69710. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68737/0.69734. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68764/0.69788. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68610/0.69911. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68524/0.69884. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68545/0.69925. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68436/0.69894. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68379/0.70047. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68379/0.70070. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68273/0.70180. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68064/0.70251. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68155/0.70338. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68059/0.70304. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67747/0.70429. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67871/0.70525. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67889/0.70479. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67488/0.70695. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67570/0.70748. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67272/0.70856. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67266/0.71088. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67021/0.71055. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66740/0.71370. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66739/0.71546. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66720/0.71700. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66533/0.71676. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66181/0.71917. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66329/0.72264. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65955/0.72268. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65790/0.72830. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65407/0.72888. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65450/0.73286. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65109/0.73603. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65291/0.73709. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64391/0.73877. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64705/0.74029. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64342/0.74554. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64049/0.74852. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63957/0.74840. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63658/0.74945. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63376/0.75129. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63173/0.75868. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63127/0.76052. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63015/0.76034. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62504/0.76751. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62506/0.77237. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62074/0.77292. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61774/0.77793. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61676/0.77859. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61685/0.78482. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60939/0.78642. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60820/0.79238. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61098/0.79076. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60873/0.79288. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60515/0.79413. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60358/0.79899. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60542/0.80126. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60180/0.80308. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60020/0.80836. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59501/0.81355. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59035/0.81652. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59400/0.81313. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59041/0.82230. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59075/0.81957. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58749/0.82546. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58015/0.81712. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58258/0.82631. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57640/0.82824. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57299/0.83887. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56881/0.83679. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57378/0.84242. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56760/0.84580. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55560/0.85285. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56457/0.85225. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56664/0.84941. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55792/0.85219. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55486/0.85663. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55534/0.85353. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54724/0.85791. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54198/0.87525. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54653/0.87780. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53592/0.86997. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69561/0.68782. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69388/0.69056. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69282/0.69021. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.69028. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69225/0.69062. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.69152. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69125/0.69157. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69174/0.69161. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69105/0.69239. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69136/0.69277. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69023/0.69276. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69061/0.69383. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68979/0.69405. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68974/0.69384. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68933/0.69515. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68868/0.69468. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68797/0.69563. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68858/0.69452. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68736/0.69559. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68708/0.69488. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68782/0.69501. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68746/0.69561. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68613/0.69520. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68678/0.69611. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68492/0.69551. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68436/0.69625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68388/0.69582. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68357/0.69628. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68368/0.69677. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68138/0.69615. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68082/0.69562. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68150/0.69729. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68007/0.69631. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67949/0.69639. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67814/0.69590. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67553/0.69693. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67742/0.69768. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67452/0.69614. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67504/0.69783. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67380/0.70047. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67289/0.69713. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67082/0.69970. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67016/0.69962. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67050/0.70070. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66929/0.70117. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66869/0.70329. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66907/0.70157. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66816/0.70185. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66695/0.70184. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66468/0.70246. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66473/0.70358. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66352/0.70735. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65939/0.70502. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66164/0.70616. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65852/0.70611. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65696/0.70677. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65963/0.70933. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65633/0.71054. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65244/0.71174. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65787/0.71289. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65031/0.71430. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64704/0.71511. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64755/0.71486. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64618/0.71659. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64385/0.71850. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64200/0.71911. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64166/0.71791. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64030/0.72426. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63875/0.72430. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63344/0.72626. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63544/0.72521. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63002/0.72971. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63332/0.73084. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62434/0.73240. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62871/0.73513. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62863/0.73412. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62226/0.73665. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62141/0.73822. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61817/0.73910. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61583/0.73905. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61384/0.74323. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61035/0.74411. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60946/0.74526. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60936/0.74774. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60050/0.74923. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60645/0.74970. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60150/0.75124. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60165/0.75133. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59578/0.75234. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59608/0.75677. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59811/0.75410. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58908/0.75851. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58838/0.75808. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58619/0.75597. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59112/0.76240. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58054/0.76380. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57993/0.76704. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58065/0.76629. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57595/0.76972. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57252/0.76992. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69291/0.68756. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69178/0.68724. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69052/0.68811. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69159/0.68905. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69135/0.68958. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69132/0.69069. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69107/0.69155. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69031/0.69194. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.69258. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69061/0.69351. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68934/0.69374. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68923/0.69427. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68952/0.69532. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69012/0.69581. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68875/0.69637. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68863/0.69659. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68802/0.69714. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68765/0.69771. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68764/0.69835. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68644/0.69880. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68700/0.69912. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68581/0.69958. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68771/0.69994. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68682/0.70069. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68608/0.70123. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68602/0.70151. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68509/0.70178. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68457/0.70176. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68481/0.70151. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68428/0.70214. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68338/0.70196. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68328/0.70281. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68266/0.70278. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68207/0.70334. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68278/0.70318. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68078/0.70430. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68119/0.70455. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68197/0.70466. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68233/0.70557. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68096/0.70585. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67700/0.70648. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67780/0.70645. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67769/0.70716. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67663/0.70797. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67460/0.70805. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67582/0.70748. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67368/0.70736. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67295/0.70972. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67224/0.70996. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67352/0.70970. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66980/0.71073. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67192/0.71188. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66736/0.71208. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66942/0.71386. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66804/0.71330. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66761/0.71407. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66466/0.71539. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66750/0.71622. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66548/0.71536. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66417/0.71610. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65957/0.71703. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66073/0.71870. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65609/0.71912. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65628/0.72097. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65572/0.72227. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65516/0.72223. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65521/0.72226. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65022/0.72477. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65029/0.72534. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65282/0.72591. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65182/0.72607. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64374/0.72734. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64795/0.73218. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64541/0.73194. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64492/0.73241. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64195/0.73199. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64056/0.73439. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63804/0.73645. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63968/0.73450. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64088/0.73444. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63554/0.73579. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63183/0.73877. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63525/0.73932. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63683/0.74095. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63424/0.74032. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63245/0.74065. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62583/0.74220. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63102/0.74521. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62632/0.74611. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62628/0.74798. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62350/0.74876. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62163/0.75038. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61977/0.74845. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61715/0.75315. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61717/0.75390. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61766/0.75331. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61552/0.75349. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61159/0.75688. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61697/0.75848. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61013/0.76357. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69310/0.69360. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69145/0.69308. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69124/0.69266. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69066/0.69223. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69013/0.69168. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69032/0.69130. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.69107. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68981/0.69066. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68877/0.69001. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68999/0.68971. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68960/0.68925. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68897/0.68877. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68806/0.68820. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68764/0.68758. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68757/0.68706. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68547/0.68665. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68647/0.68598. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68588/0.68554. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68537/0.68486. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68500/0.68471. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68566/0.68439. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68406/0.68347. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68465/0.68372. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68297/0.68348. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68156/0.68320. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68173/0.68263. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68045/0.68273. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67932/0.68245. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67851/0.68275. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67871/0.68271. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67778/0.68201. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67610/0.68282. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67437/0.68150. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67444/0.68203. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67215/0.68279. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67114/0.68072. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67020/0.68225. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66736/0.68094. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66712/0.68121. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66569/0.68268. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66216/0.68492. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66112/0.68270. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66112/0.68345. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65638/0.68554. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65543/0.68455. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65284/0.68499. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65253/0.68388. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65324/0.68656. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64852/0.68967. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64775/0.69023. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64271/0.68923. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64221/0.69028. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63618/0.69494. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63486/0.69390. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63268/0.69425. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63499/0.69371. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62806/0.69759. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62902/0.69791. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62613/0.69756. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62113/0.70677. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62178/0.70725. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61576/0.70423. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61991/0.71346. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61668/0.70939. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61524/0.71126. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61253/0.71275. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60462/0.71440. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60726/0.71549. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60424/0.71972. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60387/0.71957. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60535/0.71536. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60146/0.71664. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59803/0.71718. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59414/0.72493. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58826/0.72761. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59307/0.72598. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58842/0.72363. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58872/0.72430. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59155/0.72825. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58155/0.72677. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58374/0.72872. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58263/0.72404. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57714/0.72612. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57399/0.72582. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57006/0.72814. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58083/0.72710. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57305/0.72900. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56409/0.72178. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56756/0.72750. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56573/0.72776. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55980/0.72735. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55907/0.72771. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55640/0.73374. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55561/0.73793. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55220/0.72628. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55617/0.73076. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55479/0.72666. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55499/0.72576. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54730/0.72662. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54952/0.73388. Took 0.08 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69423. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.69441. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69160/0.69470. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69167/0.69503. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69111/0.69536. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69178/0.69581. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69184/0.69609. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69019/0.69645. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68991/0.69709. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68948/0.69766. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68872/0.69830. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68804/0.69907. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68890/0.69972. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68842/0.70044. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68696/0.70159. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68860/0.70240. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68673/0.70310. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68530/0.70428. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68690/0.70555. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68566/0.70686. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68363/0.70860. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68288/0.71060. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67996/0.71325. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68128/0.71546. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67927/0.71780. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67571/0.72079. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67587/0.72367. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67530/0.72618. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67228/0.72868. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67156/0.73136. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67151/0.73432. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66575/0.73807. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66758/0.74207. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66384/0.74568. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66319/0.74938. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66176/0.75377. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65939/0.75819. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65499/0.76347. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65658/0.76648. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65064/0.77020. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65212/0.77658. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64695/0.78164. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64284/0.78613. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63950/0.79069. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64231/0.79426. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63842/0.79694. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63502/0.80248. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63633/0.80641. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63569/0.80886. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63147/0.81162. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63081/0.81463. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62556/0.81864. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62825/0.81800. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62379/0.82447. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62075/0.82512. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62138/0.83163. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62094/0.83240. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61291/0.83459. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61400/0.83804. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61100/0.84010. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60951/0.84472. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60545/0.84758. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60786/0.85035. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60113/0.85464. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59935/0.85889. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59588/0.86025. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59425/0.86318. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59520/0.86188. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59540/0.86642. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59102/0.86864. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59317/0.86886. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57966/0.87274. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58294/0.87498. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.58289/0.87872. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57706/0.88467. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57832/0.88609. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57403/0.89252. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56727/0.89612. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56529/0.90071. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56564/0.90378. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55995/0.90767. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56206/0.91342. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56044/0.91425. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56141/0.92004. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55376/0.92129. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54408/0.93290. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54393/0.92979. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54808/0.93282. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54942/0.93611. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54130/0.93898. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54243/0.94294. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54106/0.95291. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.52986/0.96027. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52849/0.96248. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52552/0.97390. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51849/0.96730. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52302/0.97045. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51356/0.97995. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51087/0.97514. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52320/0.98422. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69134/0.69049. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69016/0.69034. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69010/0.68995. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68978/0.68969. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68934/0.68922. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68881/0.68892. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68788/0.68832. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68819/0.68762. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68749/0.68667. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68632/0.68596. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68650/0.68544. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68477/0.68427. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68465/0.68330. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68435/0.68235. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68353/0.68153. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68253/0.68037. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68394/0.67971. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68270/0.67874. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68062/0.67805. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68078/0.67703. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67832/0.67590. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67887/0.67538. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67645/0.67410. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.67548/0.67223. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67431/0.67170. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67244/0.66993. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67082/0.66970. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66782/0.66902. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66871/0.66759. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66802/0.66699. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66509/0.66562. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66387/0.66294. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66214/0.66298. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66386/0.66214. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65691/0.66115. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65711/0.66064. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65333/0.65954. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64922/0.65833. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65066/0.65917. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64464/0.65600. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64434/0.65622. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64065/0.65605. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63687/0.65349. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63671/0.65143. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.63402/0.65258. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63327/0.65224. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62727/0.65446. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62699/0.65051. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62943/0.65302. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61985/0.65275. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61853/0.65117. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61818/0.65251. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61078/0.65529. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60915/0.65194. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60774/0.65605. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60300/0.66286. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59808/0.66614. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60230/0.66374. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59584/0.66184. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59222/0.67106. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58687/0.67225. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58049/0.66742. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58534/0.66729. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58095/0.67289. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57717/0.67596. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57812/0.68155. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.56871/0.68465. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57303/0.69016. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56655/0.69183. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56480/0.69729. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.55796/0.70527. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55453/0.70763. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.54578/0.70704. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55477/0.70535. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54643/0.71042. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54697/0.72132. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54214/0.72589. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53453/0.72917. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54170/0.73546. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.53751/0.73492. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53399/0.74043. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.51994/0.74646. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.52910/0.74921. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51494/0.75613. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51572/0.75999. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52112/0.75631. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.51412/0.76279. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.50230/0.76508. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.50601/0.76999. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51147/0.77301. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50378/0.76893. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.49398/0.77549. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.49515/0.78230. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49460/0.78367. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.49311/0.78335. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48678/0.78586. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.48536/0.79212. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.47702/0.79771. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.47170/0.80475. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47528/0.81197. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69104/0.69535. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69076/0.69544. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69020/0.69557. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69001/0.69575. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69023/0.69575. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68967/0.69581. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68896/0.69587. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68857/0.69590. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68800/0.69606. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68847/0.69635. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68714/0.69676. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68646/0.69692. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68695/0.69736. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68588/0.69758. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68531/0.69807. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68400/0.69844. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68333/0.69897. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68239/0.69982. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68193/0.70069. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68008/0.70117. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67847/0.70200. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67683/0.70329. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67318/0.70476. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67462/0.70583. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67483/0.70691. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67407/0.70742. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66990/0.70765. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67059/0.70816. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66940/0.70873. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66449/0.71022. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66457/0.71124. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66394/0.71185. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66302/0.71269. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65675/0.71266. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65634/0.71373. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65696/0.71475. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65088/0.71636. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65120/0.71811. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64937/0.71872. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64800/0.71929. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64460/0.71825. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64200/0.72031. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63652/0.72135. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63717/0.72153. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63275/0.72296. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63054/0.72434. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62543/0.72398. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62363/0.72658. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62383/0.72869. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61752/0.72984. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61491/0.73213. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61171/0.73596. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60965/0.73692. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60841/0.73925. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60959/0.73915. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60015/0.74152. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60277/0.74457. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59605/0.74782. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59246/0.74998. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59003/0.75078. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58983/0.75461. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58331/0.76022. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58763/0.76623. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57966/0.76710. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57630/0.77325. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.57569/0.77089. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56796/0.77842. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.56453/0.78333. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56536/0.78428. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55805/0.78816. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.56134/0.79360. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55410/0.79905. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55413/0.79923. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.55076/0.80722. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.54662/0.81285. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54978/0.81325. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.54917/0.81536. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.54852/0.81561. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.53295/0.82465. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54174/0.83178. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.52531/0.84045. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.52284/0.85093. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.52986/0.85417. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52571/0.84960. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51579/0.85381. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.52468/0.85662. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.51471/0.86655. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.51003/0.86496. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.51233/0.86466. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50784/0.87291. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50702/0.87738. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.50221/0.88389. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50040/0.89130. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49534/0.89439. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.50267/0.88911. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.49112/0.90582. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49603/0.91366. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.48638/0.91391. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49482/0.91100. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48763/0.91082. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69256/0.69124. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69132/0.69146. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69135/0.69197. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69107/0.69254. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69038/0.69298. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69026/0.69382. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69000/0.69429. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69041/0.69478. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69045/0.69536. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69050/0.69563. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69006/0.69610. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68961/0.69638. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68962/0.69688. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68897/0.69788. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68889/0.69834. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68962/0.69850. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68979/0.69844. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68900/0.69868. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68868/0.69927. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68824/0.69974. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68813/0.70007. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68772/0.70048. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68766/0.70073. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68778/0.70122. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68764/0.70168. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68749/0.70214. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68661/0.70277. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68621/0.70297. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68612/0.70345. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68648/0.70392. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68565/0.70428. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68517/0.70483. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68432/0.70548. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68425/0.70634. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68341/0.70763. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68467/0.70825. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68357/0.70893. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68266/0.70934. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68221/0.71011. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68163/0.71174. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68109/0.71249. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68092/0.71232. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67939/0.71339. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68039/0.71415. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67890/0.71452. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67830/0.71486. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67696/0.71601. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67701/0.71712. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67441/0.71918. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67357/0.72039. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67499/0.72088. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67231/0.72275. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67356/0.72339. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67192/0.72397. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67054/0.72542. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66833/0.72696. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66644/0.72880. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66609/0.72861. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66607/0.73156. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66558/0.73250. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66448/0.73336. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66077/0.73662. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65955/0.73665. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65886/0.73622. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65793/0.73732. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65657/0.73829. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65714/0.73967. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65507/0.74241. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65455/0.74073. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65268/0.74134. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65140/0.74315. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65060/0.74473. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64813/0.74565. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64553/0.74737. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64611/0.74657. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64683/0.74878. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64344/0.74971. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64040/0.75080. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64041/0.74841. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64169/0.75162. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63682/0.75313. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63377/0.75293. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63398/0.75304. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63283/0.75633. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62847/0.75460. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63059/0.75898. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62743/0.75462. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62366/0.75396. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62434/0.75844. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61813/0.75957. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62225/0.76271. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62353/0.75869. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61701/0.76086. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60840/0.76646. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61588/0.76515. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61195/0.76704. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60886/0.76838. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61149/0.76879. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61103/0.76514. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60490/0.77294. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69556/0.68948. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69425/0.68845. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69345/0.68890. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69243/0.68891. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69199/0.68893. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69194/0.68885. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69171/0.68929. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.68947. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.68939. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69096/0.68932. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69035/0.68990. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68999/0.69081. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68961/0.69120. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68845/0.69071. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68767/0.69114. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68725/0.69214. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68634/0.69196. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68632/0.69226. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68567/0.69264. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68594/0.69285. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68559/0.69272. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68419/0.69501. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68434/0.69305. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68447/0.69429. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68197/0.69365. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68339/0.69474. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68122/0.69510. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68182/0.69475. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68060/0.69689. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67942/0.69642. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67997/0.69413. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67736/0.69612. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67781/0.69656. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67524/0.69693. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67708/0.69484. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67379/0.69879. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67271/0.69627. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67197/0.69748. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67042/0.69738. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66916/0.69732. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66927/0.69814. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66696/0.69782. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66831/0.69700. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66603/0.69734. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66454/0.69591. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66300/0.69768. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66165/0.69713. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66060/0.69681. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65860/0.69723. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65911/0.69983. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65798/0.69758. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65466/0.69696. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65403/0.69772. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64868/0.69917. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64771/0.69829. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64869/0.69632. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64610/0.69549. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64429/0.69591. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64332/0.69697. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64029/0.69940. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64098/0.70033. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63049/0.70188. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63062/0.70308. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63314/0.70219. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63265/0.70359. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62965/0.70299. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62317/0.70252. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62238/0.70380. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61993/0.70629. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61896/0.70824. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61348/0.71077. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61580/0.71039. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61102/0.71477. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60899/0.71563. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60789/0.71618. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60342/0.71861. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60379/0.71862. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60269/0.71907. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60427/0.71990. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59814/0.72137. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59634/0.72220. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59229/0.72386. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59257/0.72430. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58686/0.72965. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58982/0.73197. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57979/0.73139. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57838/0.73312. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57298/0.73720. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57494/0.73994. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56837/0.74183. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57168/0.74087. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56825/0.74110. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56788/0.74814. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55968/0.75023. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55789/0.75084. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55613/0.75034. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54865/0.75489. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55267/0.75684. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54535/0.75914. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54884/0.75959. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69461/0.70175. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69326/0.70240. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69175/0.70264. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.70340. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69080/0.70320. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69087/0.70369. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.70367. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69048/0.70383. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69006/0.70443. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68870/0.70503. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68793/0.70532. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68788/0.70555. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68767/0.70576. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68774/0.70588. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68673/0.70725. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68712/0.70682. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68622/0.70699. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68479/0.70802. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68433/0.70862. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68291/0.70924. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68458/0.70906. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68234/0.70996. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68133/0.71050. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68056/0.71074. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67989/0.71097. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67933/0.71170. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67881/0.71182. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67672/0.71250. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67582/0.71193. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67456/0.71241. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67281/0.71276. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67129/0.71339. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66943/0.71313. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67031/0.71264. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66786/0.71188. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66452/0.71173. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66536/0.71177. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66035/0.71189. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65931/0.71279. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65632/0.71299. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65413/0.71379. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65286/0.71381. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65150/0.71384. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64825/0.71330. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64641/0.71248. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64813/0.71282. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64656/0.71185. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63924/0.71291. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63834/0.71432. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63844/0.71362. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63300/0.71570. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63277/0.71505. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63012/0.71581. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62777/0.71684. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62503/0.71537. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62791/0.71457. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62163/0.71262. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61532/0.71374. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61864/0.71242. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61288/0.71196. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61269/0.71057. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61095/0.71087. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60956/0.71197. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60575/0.71138. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60472/0.70946. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60002/0.70810. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59487/0.71042. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58991/0.71049. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59423/0.70983. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59252/0.70918. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58880/0.70971. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58643/0.70866. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58701/0.70652. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58304/0.70442. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57776/0.70678. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57323/0.70516. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57128/0.70726. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56598/0.70821. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57176/0.70469. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57123/0.70607. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56441/0.71106. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55672/0.70942. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56350/0.71344. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55000/0.71126. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56347/0.70659. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54825/0.71066. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54682/0.71227. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54711/0.71341. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54538/0.71328. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54760/0.70612. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54092/0.70977. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54139/0.71152. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53390/0.71503. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53131/0.71112. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53577/0.71149. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52555/0.71771. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52432/0.71483. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52384/0.72010. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51948/0.72012. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51656/0.72063. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69599/0.69736. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69806. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69222/0.69856. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69146/0.69890. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69149/0.69958. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69009/0.70019. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69004/0.70088. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.70114. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68893/0.70174. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68823/0.70241. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68692/0.70294. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68730/0.70319. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68633/0.70408. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68617/0.70428. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68549/0.70445. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68460/0.70443. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68442/0.70481. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68277/0.70470. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68241/0.70510. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68054/0.70482. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68126/0.70458. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67907/0.70414. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67904/0.70327. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67760/0.70362. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67619/0.70207. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67488/0.70116. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67409/0.70057. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67242/0.69930. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67161/0.69660. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66940/0.69520. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66787/0.69501. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66467/0.69324. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66399/0.69305. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66364/0.69149. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66231/0.68900. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65823/0.68718. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65905/0.68742. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65479/0.68670. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65465/0.68580. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65395/0.68336. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64959/0.68295. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64935/0.68203. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64828/0.68077. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64688/0.67992. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64264/0.67842. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64230/0.67799. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64226/0.67612. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64104/0.67737. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63737/0.67610. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63838/0.67427. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63406/0.67350. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63495/0.67394. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63226/0.67042. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62913/0.66961. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62985/0.66869. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62753/0.66643. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62460/0.66587. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62483/0.66561. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62044/0.66391. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61909/0.66184. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61758/0.66034. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61587/0.65986. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.61221/0.65882. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.60596/0.65540. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60987/0.65409. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.60146/0.65442. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60541/0.65149. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60429/0.65006. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59888/0.64964. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59709/0.64792. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59315/0.64808. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59536/0.64887. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58659/0.64791. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58444/0.64527. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58649/0.64518. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58434/0.64175. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58263/0.64431. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57837/0.64458. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57057/0.64192. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57396/0.64265. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57347/0.64316. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56567/0.64589. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56095/0.64235. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56232/0.64476. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56469/0.64273. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55771/0.63932. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55527/0.64086. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55773/0.64139. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54701/0.64623. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54666/0.64658. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55033/0.63653. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54165/0.64282. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54121/0.64618. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53423/0.64284. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52897/0.63975. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52714/0.64472. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52875/0.64199. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52131/0.64648. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52381/0.64604. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51900/0.64521. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69007. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69120. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69335/0.69241. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69357/0.69284. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69264/0.69336. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69233/0.69377. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69410. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69190/0.69379. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69108/0.69362. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69127/0.69364. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69116/0.69329. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69104/0.69315. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68912/0.69303. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68902/0.69302. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68874/0.69290. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68916/0.69381. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68788/0.69321. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68707/0.69343. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68649/0.69372. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68564/0.69297. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68413/0.69331. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68360/0.69300. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68435/0.69386. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68375/0.69297. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68153/0.69241. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68125/0.69327. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68039/0.69343. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67920/0.69189. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67833/0.69216. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67755/0.69285. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67710/0.69231. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67330/0.69140. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67263/0.69134. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67185/0.69208. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67131/0.69188. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66972/0.69071. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66923/0.69017. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66656/0.68881. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66582/0.68665. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66068/0.68661. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66050/0.68521. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65745/0.68484. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65579/0.68199. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65394/0.68102. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65267/0.67921. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65120/0.68005. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65118/0.67801. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64720/0.67893. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64168/0.67418. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64233/0.67246. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63424/0.67098. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63513/0.67239. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63240/0.67149. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63546/0.66932. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62744/0.66881. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62722/0.66860. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62674/0.66618. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62099/0.66771. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62274/0.66671. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61689/0.66674. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61673/0.66840. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61341/0.66808. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61438/0.67030. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60890/0.66886. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60661/0.66910. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60773/0.66934. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60333/0.66860. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60059/0.67147. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59954/0.66721. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59256/0.66654. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59215/0.66808. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58937/0.66908. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59482/0.66470. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58804/0.66897. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58043/0.66971. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57620/0.66865. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58156/0.67036. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57682/0.67083. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57597/0.67031. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57197/0.67330. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.57006/0.67338. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56828/0.67324. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56536/0.67093. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56504/0.67376. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56326/0.67292. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55813/0.67051. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56077/0.67260. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54919/0.67650. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.54604/0.67412. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54575/0.67965. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53949/0.68195. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53849/0.68487. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53546/0.68800. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52758/0.68449. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53135/0.68756. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52773/0.69108. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53046/0.69345. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52948/0.68407. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52405/0.68715. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51733/0.68880. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69453/0.69365. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69436/0.69265. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69378/0.69139. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69374/0.69079. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69318/0.69020. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69291/0.68990. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69295/0.68943. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69193/0.68873. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69246/0.68864. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69194/0.68823. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69178/0.68786. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69153/0.68727. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69083/0.68669. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69103/0.68636. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69085/0.68586. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69019/0.68551. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69032/0.68533. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68956/0.68508. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68918/0.68445. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68852/0.68428. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68831/0.68402. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68804/0.68330. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68715/0.68307. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68668/0.68248. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68648/0.68203. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68627/0.68249. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68439/0.68249. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68464/0.68224. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68468/0.68156. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68280/0.68100. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68404/0.68161. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68168/0.68067. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68248/0.68003. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68136/0.67989. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68031/0.67979. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67884/0.67916. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67842/0.67945. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67605/0.67844. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67617/0.67842. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67454/0.67694. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67201/0.67759. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67267/0.67653. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67173/0.67672. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67071/0.67666. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67060/0.67608. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66978/0.67512. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66647/0.67462. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66789/0.67500. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66487/0.67419. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66326/0.67359. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65938/0.67287. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65895/0.67213. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65797/0.67164. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65675/0.67018. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65516/0.67024. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65269/0.66950. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64886/0.66883. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64817/0.66890. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64582/0.66732. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.64683/0.66735. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64284/0.66802. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64053/0.66785. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63947/0.66662. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63571/0.66441. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63572/0.66514. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62970/0.66479. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62737/0.66329. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63090/0.66420. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62477/0.66357. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62805/0.66194. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62305/0.66429. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61811/0.66685. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61779/0.66459. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61220/0.66509. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61295/0.66367. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61312/0.66137. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60856/0.66253. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61019/0.66736. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60404/0.66648. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60231/0.66363. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59623/0.66128. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59543/0.66657. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59549/0.66492. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58886/0.66638. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59149/0.66348. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58920/0.66769. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58476/0.67330. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58177/0.66887. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57349/0.66926. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57580/0.66683. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57024/0.66519. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57556/0.67178. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57502/0.66784. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56887/0.66535. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56009/0.66635. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55757/0.67211. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55828/0.66816. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55465/0.67437. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54782/0.67115. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55562/0.67129. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69559/0.69773. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69470/0.69599. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69359/0.69447. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69378/0.69344. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69224/0.69260. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69177/0.69185. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69228/0.69134. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69132/0.69102. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69139/0.69071. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69082/0.69051. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69090/0.69031. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69052/0.69004. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68982/0.68972. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68936/0.68952. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68862/0.68918. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68839/0.68910. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68695/0.68894. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68744/0.68861. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68618/0.68798. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68576/0.68798. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68461/0.68768. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68416/0.68782. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68281/0.68784. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68160/0.68726. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68021/0.68777. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68009/0.68767. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67801/0.68732. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67828/0.68791. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67653/0.68781. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67460/0.68819. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67386/0.68793. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67241/0.68776. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67290/0.68826. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67123/0.68761. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66869/0.68860. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66739/0.68818. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66568/0.68800. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66189/0.68827. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66483/0.68803. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65921/0.68832. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65863/0.68824. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65712/0.68786. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65659/0.68832. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65238/0.68776. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65157/0.68821. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65612/0.68905. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64996/0.68892. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64962/0.68967. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64600/0.69005. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64767/0.68988. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64252/0.69086. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64319/0.68991. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63960/0.69103. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63790/0.69116. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63535/0.69182. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63605/0.69376. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63315/0.69353. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63037/0.69527. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62828/0.69576. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62710/0.69556. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62575/0.69828. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62433/0.69781. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62278/0.69962. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61906/0.69954. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62144/0.70190. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61276/0.70193. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61315/0.70385. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61034/0.70363. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60880/0.70428. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60643/0.70687. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60817/0.70772. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60648/0.70748. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60021/0.70840. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59574/0.71018. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59323/0.71043. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59323/0.71112. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59165/0.71628. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58854/0.71814. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58555/0.71745. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57973/0.71964. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58263/0.71660. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57947/0.71966. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57175/0.72362. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57295/0.72453. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56861/0.72719. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56837/0.73156. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56661/0.73128. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56334/0.73443. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55573/0.73638. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55424/0.74444. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54784/0.74863. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54775/0.74901. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54773/0.74534. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54979/0.74765. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54037/0.75288. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53903/0.75910. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53634/0.76264. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53353/0.76215. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52927/0.76633. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53430/0.76844. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69306/0.68903. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69265/0.68909. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.68911. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69119/0.68906. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69166/0.68907. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69124/0.68912. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69108/0.68912. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69161/0.68919. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69196/0.68917. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69025/0.68914. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69108/0.68929. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69015/0.68943. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68891/0.68958. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68867/0.68963. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68771/0.68967. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68627/0.68963. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68516/0.68989. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68499/0.68998. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68332/0.68988. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68249/0.69044. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67996/0.68995. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68022/0.69045. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67897/0.68930. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67798/0.69048. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67606/0.69034. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67546/0.69010. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67456/0.69192. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67022/0.69022. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67122/0.69202. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66721/0.69212. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66935/0.69407. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66647/0.69342. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66417/0.69747. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66255/0.69466. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66189/0.69691. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65804/0.69710. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65957/0.69898. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65445/0.69850. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65762/0.70120. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65498/0.70309. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64886/0.70309. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65182/0.70391. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65313/0.70307. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64581/0.70555. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64489/0.70836. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64202/0.70781. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63950/0.70883. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64349/0.70900. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63777/0.70899. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63851/0.71221. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63033/0.71009. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63180/0.71407. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63347/0.71575. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62780/0.71625. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62567/0.71849. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62091/0.72020. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62427/0.72164. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62093/0.72469. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61636/0.72642. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61561/0.72521. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60906/0.72817. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61033/0.72882. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60916/0.73230. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60263/0.73691. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60173/0.73503. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60055/0.73720. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59830/0.74314. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60166/0.74468. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59401/0.74985. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58434/0.75492. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58468/0.75605. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58430/0.75567. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57994/0.75982. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57729/0.76240. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57356/0.76983. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57756/0.77038. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57390/0.77387. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56541/0.77828. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57124/0.77864. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56221/0.78356. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55663/0.78801. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56327/0.79049. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56159/0.79330. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55268/0.79884. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54732/0.80382. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54701/0.80336. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54206/0.80400. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54128/0.81007. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53935/0.81402. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53125/0.82067. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53641/0.82293. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.53999/0.82732. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52665/0.83050. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52919/0.83515. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52050/0.83938. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51178/0.84308. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52116/0.84685. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51060/0.84527. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51488/0.84816. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50291/0.85286. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69529/0.69221. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69288/0.69120. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69265/0.69095. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69252/0.69082. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69223/0.69067. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69139/0.69061. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69156/0.69039. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69108/0.69002. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69055/0.68975. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69048/0.68955. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69141/0.68949. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69036/0.68933. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68968/0.68905. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68893/0.68893. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68935/0.68876. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68878/0.68843. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68828/0.68817. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68771/0.68810. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68739/0.68807. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68533/0.68808. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68494/0.68830. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68413/0.68850. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68431/0.68889. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68264/0.68918. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68078/0.68947. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68136/0.68977. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68059/0.69039. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67739/0.69109. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67598/0.69212. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67413/0.69289. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67447/0.69321. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67161/0.69468. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67197/0.69597. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67092/0.69667. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66883/0.69817. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66785/0.70064. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66660/0.70230. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66501/0.70440. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66348/0.70655. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66094/0.70890. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66113/0.71067. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65939/0.71200. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65767/0.71371. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65608/0.71638. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65441/0.71859. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65433/0.71876. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64976/0.71937. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65001/0.72189. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64846/0.72434. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64672/0.72720. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64710/0.72870. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64446/0.72864. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64153/0.73078. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64339/0.73306. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63713/0.73494. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63926/0.73547. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63652/0.73678. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63371/0.73835. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63294/0.74052. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63187/0.74010. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62745/0.74265. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63037/0.74491. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62209/0.74534. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62122/0.74782. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61964/0.74987. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61749/0.75248. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61388/0.75300. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61471/0.75354. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61174/0.75661. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60420/0.75483. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60502/0.76159. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60368/0.76322. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59977/0.76258. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59912/0.76274. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59564/0.76560. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58898/0.76450. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58788/0.76764. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58436/0.77072. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58328/0.77300. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58189/0.77268. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57855/0.77429. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57651/0.77940. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57116/0.78127. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57355/0.78192. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56422/0.78876. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56138/0.79221. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55288/0.79544. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55274/0.80324. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55243/0.80255. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55014/0.80237. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53857/0.80869. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54283/0.81003. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53674/0.81747. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53672/0.82137. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53143/0.83305. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52644/0.83597. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52221/0.84500. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52737/0.84859. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52207/0.84994. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51287/0.85513. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69674/0.69308. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.69364. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69309/0.69394. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69324/0.69429. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69141/0.69465. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69219/0.69492. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69168/0.69495. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69172/0.69498. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69153/0.69517. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69115/0.69538. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69049/0.69541. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69001/0.69567. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69009/0.69578. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68994/0.69584. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69064/0.69568. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68860/0.69566. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68875/0.69562. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68918/0.69568. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68927/0.69564. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68839/0.69555. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68855/0.69566. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68723/0.69580. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68845/0.69556. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68754/0.69549. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68681/0.69531. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68765/0.69533. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68753/0.69497. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68719/0.69509. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68700/0.69490. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68613/0.69475. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68568/0.69481. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68544/0.69433. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68440/0.69431. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68414/0.69453. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68318/0.69427. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68290/0.69383. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68122/0.69359. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68274/0.69345. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68276/0.69219. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67999/0.69220. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67862/0.69198. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67718/0.69200. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67822/0.69073. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67660/0.68995. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67537/0.68963. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67541/0.68876. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67151/0.68802. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67288/0.68686. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67112/0.68630. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67102/0.68544. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67044/0.68396. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66568/0.68360. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66799/0.68184. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66544/0.68165. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66429/0.68008. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66354/0.67947. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66133/0.67899. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66065/0.67809. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65749/0.67716. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66029/0.67687. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65663/0.67640. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65754/0.67625. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65510/0.67606. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65468/0.67513. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65353/0.67466. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65559/0.67504. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65335/0.67489. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65238/0.67458. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65102/0.67519. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64751/0.67455. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65262/0.67437. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64883/0.67422. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64659/0.67356. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64761/0.67351. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64464/0.67389. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64465/0.67357. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64342/0.67347. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64170/0.67495. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64192/0.67397. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63861/0.67544. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64051/0.67718. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63758/0.67667. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63736/0.67593. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63567/0.67636. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63518/0.67841. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63224/0.67555. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63184/0.67809. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63249/0.67907. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63044/0.67635. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62750/0.67724. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63517/0.67729. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62601/0.67917. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62511/0.68021. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62668/0.67968. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62478/0.67848. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62289/0.68127. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62385/0.68280. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62340/0.68172. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62126/0.68433. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62330/0.68337. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69198/0.69372. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69061/0.69394. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.69406. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69037/0.69434. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69009/0.69449. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68993/0.69490. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68952/0.69508. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68923/0.69512. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68928/0.69536. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68884/0.69592. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68824/0.69608. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68793/0.69650. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68849/0.69703. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68718/0.69732. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.69745. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68636/0.69769. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68681/0.69853. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68650/0.69856. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68642/0.69909. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68498/0.69970. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68562/0.69980. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68435/0.70027. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68437/0.70054. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68348/0.70083. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68358/0.70123. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68344/0.70159. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68236/0.70209. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68238/0.70202. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68185/0.70229. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68133/0.70179. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68062/0.70193. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68011/0.70164. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67892/0.70241. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67705/0.70205. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67956/0.70194. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67837/0.70198. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67633/0.70195. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67561/0.70225. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67609/0.70169. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67402/0.70138. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67346/0.70191. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67425/0.70291. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67039/0.70379. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67160/0.70300. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67106/0.70257. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66965/0.70380. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66791/0.70570. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66817/0.70573. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66675/0.70695. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66610/0.70728. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66617/0.70739. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66228/0.70822. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66302/0.70964. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66245/0.71003. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66216/0.71084. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66120/0.71165. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66026/0.71220. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65768/0.71273. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65532/0.71370. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66024/0.71471. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65590/0.71615. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65644/0.71712. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65350/0.71757. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65195/0.71795. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65161/0.72072. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64981/0.72279. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64913/0.72308. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64972/0.72363. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64979/0.72429. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65041/0.72625. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64880/0.72789. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64974/0.72862. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64962/0.72941. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64355/0.72932. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64177/0.72925. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64058/0.73109. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63752/0.73424. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63867/0.73650. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63851/0.73694. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63740/0.73560. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63700/0.73688. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63126/0.74020. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63154/0.74236. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63137/0.74452. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63244/0.74345. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63022/0.74537. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62972/0.74719. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62616/0.74884. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62641/0.75153. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62407/0.75397. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62366/0.75504. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62228/0.75629. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62221/0.76046. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62191/0.75775. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62114/0.75673. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61872/0.75993. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61679/0.76215. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61198/0.76454. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61618/0.76379. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61288/0.76562. Took 0.08 sec\n",
      "ACC: 0.65625\n",
      "Epoch 0, Loss(train/val) 0.69120/0.69931. Took 0.18 sec\n",
      "Epoch 1, Loss(train/val) 0.68873/0.70067. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.68839/0.70070. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.68796/0.70083. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68734/0.70096. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68776/0.70097. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68668/0.70114. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68600/0.70059. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68552/0.69991. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68460/0.69978. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68394/0.70016. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68393/0.69901. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68240/0.69803. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68223/0.69765. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68157/0.69726. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68003/0.69697. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67892/0.69755. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67706/0.69683. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67600/0.69467. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67439/0.69638. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67229/0.69426. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67154/0.69570. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67129/0.69498. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66939/0.69504. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66791/0.69409. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66617/0.69242. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66499/0.69252. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66486/0.69115. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66031/0.69060. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65775/0.68932. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65717/0.68686. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65645/0.68681. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65164/0.68478. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65113/0.68468. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64922/0.68403. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65041/0.68427. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64517/0.68354. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64510/0.68414. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64324/0.68189. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64287/0.68275. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63825/0.68243. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63869/0.68226. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63613/0.68336. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63427/0.68361. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63135/0.68391. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63309/0.68425. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62788/0.68483. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62973/0.68566. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62519/0.68483. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62487/0.68384. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62187/0.68548. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62112/0.68523. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61688/0.68583. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62047/0.68396. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61625/0.68535. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61684/0.68508. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61536/0.68481. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61222/0.68467. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61371/0.68541. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60526/0.68492. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60371/0.68525. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60360/0.68554. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59733/0.68916. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60572/0.68990. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59843/0.69080. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59682/0.69233. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59606/0.69254. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59473/0.69266. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59092/0.69385. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59168/0.69344. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58443/0.69483. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58833/0.69565. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57969/0.69483. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57803/0.69785. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58265/0.69737. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57704/0.69795. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57387/0.69787. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57370/0.69791. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57773/0.69843. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56801/0.70065. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56894/0.70096. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56281/0.70418. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56096/0.70728. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55602/0.70627. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55798/0.70489. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55879/0.70378. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55435/0.70152. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54663/0.70609. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54295/0.70355. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54564/0.71145. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54060/0.71000. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53893/0.71082. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53800/0.71186. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53641/0.71625. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53145/0.71282. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53205/0.71254. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52916/0.71429. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52726/0.71378. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52418/0.71323. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52636/0.71126. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69116/0.69499. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69009/0.69498. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68939/0.69553. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68996/0.69595. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68944/0.69654. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68927/0.69693. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68883/0.69749. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68867/0.69761. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68761/0.69785. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68721/0.69837. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68779/0.69892. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68682/0.69908. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68691/0.69968. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68690/0.69978. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68724/0.70018. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68659/0.70050. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68654/0.70081. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68664/0.70095. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68494/0.70147. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68558/0.70164. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68565/0.70180. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68526/0.70212. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68540/0.70242. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68437/0.70247. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68454/0.70273. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68535/0.70288. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68418/0.70319. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68333/0.70350. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68446/0.70346. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68426/0.70386. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68300/0.70385. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68389/0.70409. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68263/0.70433. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68217/0.70486. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68267/0.70510. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68232/0.70552. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68208/0.70564. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68220/0.70579. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68113/0.70585. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68155/0.70592. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68136/0.70619. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68119/0.70648. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67951/0.70691. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68035/0.70714. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67950/0.70703. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67782/0.70717. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67819/0.70749. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67995/0.70785. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67740/0.70815. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67746/0.70821. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67850/0.70952. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67624/0.70949. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67427/0.70939. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67573/0.71041. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67580/0.71087. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67392/0.71166. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67254/0.71211. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67289/0.71194. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.67464/0.71265. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67106/0.71333. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67022/0.71486. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.67149/0.71519. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66995/0.71545. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66884/0.71568. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66948/0.71669. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66761/0.71802. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66770/0.71704. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66477/0.71845. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66566/0.71903. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66450/0.72065. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66565/0.72212. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66396/0.72190. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66091/0.72165. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66341/0.72497. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66264/0.72547. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65874/0.72732. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65638/0.72693. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65821/0.73017. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.65660/0.73002. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65892/0.73005. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65709/0.73274. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65443/0.73374. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65159/0.73398. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.65216/0.73368. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64787/0.73540. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64903/0.74011. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64839/0.74207. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64716/0.74196. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64833/0.74444. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.64475/0.74476. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64206/0.74774. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63970/0.74705. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63826/0.75049. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63610/0.75113. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.63462/0.75548. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63376/0.75306. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63548/0.75660. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63349/0.75665. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62857/0.75984. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62591/0.76089. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69147/0.70130. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69010/0.70171. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69065/0.70111. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68918/0.70024. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68992/0.69987. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68921/0.69925. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68883/0.69854. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68813/0.69812. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68872/0.69751. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68731/0.69717. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68799/0.69682. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68779/0.69694. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68721/0.69713. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68752/0.69725. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68623/0.69740. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68653/0.69723. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68586/0.69687. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68581/0.69710. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68404/0.69795. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68488/0.69885. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68455/0.69884. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68381/0.69972. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68368/0.70023. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68345/0.69992. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68249/0.70072. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68333/0.70142. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68158/0.70159. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68098/0.70278. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68124/0.70306. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67955/0.70399. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67989/0.70585. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67957/0.70637. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67974/0.70671. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67981/0.70772. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67801/0.70804. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67788/0.70822. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67572/0.70982. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67535/0.70971. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67458/0.71216. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67328/0.71374. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67176/0.71378. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67176/0.71440. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66970/0.71474. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67042/0.71541. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66924/0.71701. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66764/0.71759. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66594/0.71885. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66585/0.71984. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66483/0.71988. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66417/0.72314. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66124/0.72579. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65741/0.72651. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66064/0.72713. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65845/0.72722. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65777/0.73228. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65337/0.72877. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65544/0.73261. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65235/0.73315. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64892/0.73643. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64623/0.73939. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64873/0.74073. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64263/0.73963. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64285/0.74157. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64183/0.74366. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63646/0.74439. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64011/0.74616. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63205/0.75034. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63077/0.75095. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62617/0.75199. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63039/0.75922. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62468/0.76790. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62001/0.76404. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61990/0.76126. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61347/0.76481. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61362/0.77119. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60845/0.77213. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60471/0.77630. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61028/0.78155. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60720/0.78468. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60014/0.78210. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59938/0.78513. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59820/0.78385. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59688/0.78215. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59550/0.79273. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58897/0.78992. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58876/0.78824. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58659/0.79891. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57886/0.79827. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57861/0.79839. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57460/0.80282. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57100/0.80738. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56861/0.81827. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56821/0.81094. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56830/0.81481. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55799/0.80666. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55681/0.82018. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54533/0.82240. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54942/0.81474. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54411/0.82989. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54636/0.82220. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69216/0.69267. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69155/0.69288. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69124/0.69309. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69068/0.69326. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69010/0.69339. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69085/0.69367. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69017/0.69393. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69067/0.69410. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69008/0.69431. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69023/0.69448. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69007/0.69466. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68975/0.69492. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68961/0.69519. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69001/0.69561. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68963/0.69600. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68907/0.69635. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68861/0.69676. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68825/0.69715. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68756/0.69749. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68771/0.69782. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68726/0.69809. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68708/0.69828. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68668/0.69881. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68677/0.69934. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68598/0.69971. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68619/0.70042. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68482/0.70084. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68386/0.70136. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68444/0.70173. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68414/0.70194. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68361/0.70249. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68257/0.70296. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68278/0.70345. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68224/0.70398. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68064/0.70455. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68152/0.70474. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68123/0.70524. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67946/0.70606. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67868/0.70651. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67860/0.70711. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67879/0.70745. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67773/0.70797. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67740/0.70850. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67635/0.70919. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67767/0.70968. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67460/0.70994. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67512/0.71028. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67261/0.71014. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67246/0.71069. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67021/0.71096. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67121/0.71109. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66871/0.71181. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67042/0.71230. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66819/0.71227. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66637/0.71321. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66803/0.71371. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66623/0.71304. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66338/0.71300. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66343/0.71353. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66230/0.71347. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65936/0.71340. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65914/0.71435. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65860/0.71352. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65722/0.71308. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65307/0.71393. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65502/0.71363. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65048/0.71298. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64968/0.71253. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64543/0.71429. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64501/0.71422. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64247/0.71521. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64369/0.71510. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63759/0.71492. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63908/0.71496. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63330/0.71520. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63373/0.71679. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63404/0.71613. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62970/0.71905. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62950/0.71871. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62625/0.71864. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62633/0.71991. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62624/0.72116. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61895/0.72337. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61742/0.72292. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61778/0.72613. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61251/0.72699. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61251/0.72985. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60997/0.73156. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60847/0.73339. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60631/0.73122. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61062/0.73637. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60226/0.74033. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59984/0.74178. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60643/0.74037. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59403/0.73566. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59544/0.74964. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58960/0.74929. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59178/0.75780. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58931/0.75431. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58979/0.76019. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69309/0.70643. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69050/0.70698. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.70770. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.70805. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69029/0.70850. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69015/0.70890. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68963/0.70922. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68989/0.70945. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68979/0.70974. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68927/0.71019. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68893/0.70990. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68941/0.71019. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68859/0.71074. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68835/0.71165. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68872/0.71175. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68819/0.71199. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68766/0.71256. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68799/0.71333. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68737/0.71364. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68662/0.71390. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68747/0.71415. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68668/0.71452. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68717/0.71513. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68634/0.71539. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68647/0.71575. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68627/0.71578. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68616/0.71616. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68634/0.71600. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68482/0.71620. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68641/0.71678. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68527/0.71685. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68537/0.71675. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68438/0.71717. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68364/0.71773. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68411/0.71760. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68231/0.71830. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68241/0.71835. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68268/0.71900. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68120/0.71860. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68148/0.71937. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68072/0.71957. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68179/0.71982. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67949/0.71965. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67991/0.72027. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67910/0.71977. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67822/0.71936. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67829/0.71865. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67687/0.71807. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67703/0.71884. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67598/0.71869. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67629/0.71810. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67579/0.71844. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67396/0.71829. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67430/0.71788. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67349/0.71801. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67337/0.71666. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67171/0.71755. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67250/0.71761. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.67057/0.71868. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67069/0.71594. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66878/0.71733. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66656/0.71794. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66837/0.71706. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66772/0.71732. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66524/0.71768. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66454/0.71549. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66297/0.71641. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66290/0.71580. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66172/0.71529. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65972/0.71517. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65906/0.71654. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65666/0.71676. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65870/0.71646. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65243/0.71643. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64990/0.71826. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65455/0.71831. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65110/0.71888. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64929/0.72211. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64433/0.72234. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64798/0.72063. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64499/0.72079. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64433/0.72125. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64494/0.71782. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64068/0.72218. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63545/0.72484. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63490/0.72318. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63167/0.72391. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62908/0.72016. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63415/0.72347. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63058/0.72604. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62554/0.72174. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62530/0.72494. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62851/0.72150. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62098/0.72869. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62122/0.72693. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61661/0.72953. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61512/0.72970. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61090/0.72578. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61279/0.73340. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61121/0.73761. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69388/0.69618. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69309/0.69588. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69313/0.69567. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69268/0.69555. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69248/0.69548. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69226/0.69533. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69520. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69195/0.69500. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69165/0.69486. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69123/0.69466. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69051/0.69454. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69125/0.69441. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68996/0.69422. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69029/0.69394. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68935/0.69363. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68877/0.69348. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68913/0.69338. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68886/0.69283. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68723/0.69182. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68630/0.69077. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68535/0.69011. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68538/0.68930. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68483/0.68817. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68553/0.68707. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68312/0.68654. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68267/0.68613. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68250/0.68554. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68042/0.68511. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68223/0.68477. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68035/0.68419. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67839/0.68279. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67712/0.68280. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67723/0.68210. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67560/0.68074. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67325/0.68060. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67361/0.68018. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67345/0.67892. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67096/0.67884. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67170/0.67899. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67207/0.67879. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66777/0.67830. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66520/0.67795. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66630/0.67577. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66530/0.67503. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66584/0.67709. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66422/0.67537. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66107/0.67451. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66135/0.67441. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65994/0.67302. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65682/0.67291. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65473/0.67355. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65413/0.67213. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65427/0.67216. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65092/0.67159. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65075/0.67267. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65008/0.67144. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64800/0.66979. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64780/0.67260. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64245/0.67466. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64503/0.67177. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64163/0.66834. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63944/0.67170. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64146/0.67140. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63714/0.67644. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63231/0.67303. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63173/0.67020. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63001/0.67487. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63115/0.67473. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62754/0.67358. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62583/0.67179. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62249/0.66827. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62141/0.67021. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62021/0.67439. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61502/0.66762. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61665/0.67304. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61744/0.67401. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60823/0.67278. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60698/0.67307. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60394/0.67800. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60082/0.67598. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60039/0.67726. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60138/0.68194. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60391/0.68453. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59227/0.68171. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59388/0.68180. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58765/0.68259. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58455/0.68381. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58270/0.69350. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58987/0.69025. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58424/0.69019. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58575/0.68770. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57272/0.68356. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57099/0.69357. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56894/0.68761. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57081/0.69227. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56502/0.68768. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56113/0.69498. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55911/0.68752. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55477/0.69523. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55597/0.69534. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69722/0.69853. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69416/0.69696. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69612. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69208/0.69578. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.69557. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69125/0.69550. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69095/0.69571. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69058/0.69585. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69056/0.69591. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69025/0.69635. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68991/0.69686. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68922/0.69706. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68930/0.69732. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68932/0.69772. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68876/0.69788. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68910/0.69802. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68809/0.69826. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68847/0.69849. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68777/0.69870. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68770/0.69895. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68702/0.69909. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68768/0.69919. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68732/0.69933. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68749/0.69921. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68704/0.69927. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68692/0.69930. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68668/0.69951. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68568/0.69970. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68629/0.69964. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68538/0.69943. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68485/0.69928. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68483/0.69927. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68479/0.69899. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68413/0.69871. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68457/0.69841. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68457/0.69828. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68358/0.69818. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68300/0.69840. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68280/0.69783. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68243/0.69734. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68185/0.69716. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68064/0.69706. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67980/0.69683. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68117/0.69645. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67943/0.69626. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68076/0.69613. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67811/0.69553. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67734/0.69515. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67584/0.69472. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67613/0.69368. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67545/0.69372. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67390/0.69365. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67421/0.69270. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67319/0.69187. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67102/0.69097. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67043/0.69057. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66865/0.68949. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66561/0.68908. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66591/0.68813. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66391/0.68789. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65936/0.68758. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66128/0.68712. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65883/0.68630. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65738/0.68620. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65377/0.68677. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64963/0.68464. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65195/0.68409. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64869/0.68514. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64695/0.68570. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64484/0.68621. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64295/0.68859. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63958/0.69204. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63617/0.69066. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63495/0.69156. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63416/0.69412. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63327/0.69390. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63057/0.69532. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62916/0.69687. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62627/0.69938. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62388/0.69706. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62568/0.70128. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61882/0.69549. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61776/0.70563. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61176/0.70753. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61025/0.71052. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60686/0.71697. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60464/0.71486. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60059/0.71246. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59686/0.71610. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59818/0.72953. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59834/0.71889. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59201/0.73350. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58798/0.73678. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59245/0.73704. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58122/0.73586. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58064/0.74526. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58245/0.75273. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57533/0.75067. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56871/0.76397. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56715/0.75824. Took 0.08 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69359/0.69563. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69354/0.69544. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69304/0.69537. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69290/0.69584. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69239/0.69625. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69260/0.69676. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69316/0.69695. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69233/0.69700. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69226/0.69729. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69152/0.69785. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69214/0.69801. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69112/0.69857. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69181/0.69894. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69102/0.69933. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68993/0.70006. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69013/0.70111. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68886/0.70183. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68959/0.70283. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68964/0.70337. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68853/0.70396. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68831/0.70510. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68831/0.70492. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68803/0.70595. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68718/0.70571. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68687/0.70682. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68561/0.70729. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68607/0.70720. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68514/0.70766. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68520/0.70667. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68403/0.70730. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68540/0.70774. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68578/0.70751. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68406/0.70810. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68510/0.70807. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68275/0.70842. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68199/0.70866. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68302/0.70829. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68244/0.70753. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68301/0.70749. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68011/0.70710. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68063/0.70758. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68173/0.70806. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67969/0.70818. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68056/0.70796. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67752/0.70885. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67828/0.70959. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67731/0.70966. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67837/0.70962. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67760/0.70874. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67713/0.70874. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67673/0.70973. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67664/0.70987. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67622/0.70963. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67458/0.70996. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67341/0.70947. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67270/0.70969. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67423/0.70992. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67051/0.70958. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.67014/0.70909. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.67011/0.71055. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66681/0.70957. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66710/0.70967. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66719/0.70926. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66670/0.70883. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66464/0.70947. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66540/0.71150. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66193/0.70939. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66518/0.71061. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66199/0.71159. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66328/0.71174. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66098/0.70980. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65449/0.71213. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65867/0.71345. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65449/0.71595. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65329/0.71635. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65252/0.71644. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65255/0.71787. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65249/0.71741. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64940/0.71670. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64949/0.71891. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64668/0.71868. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64598/0.71960. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64322/0.72125. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64401/0.72138. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64307/0.72400. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63963/0.72448. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64107/0.72453. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63568/0.72773. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63819/0.72332. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63101/0.72810. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63227/0.72885. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.63228/0.73175. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63187/0.73282. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63209/0.73364. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.63289/0.74069. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62431/0.74180. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62551/0.73790. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62370/0.73768. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62419/0.74024. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62078/0.74288. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69428/0.69355. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69315/0.69264. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69330/0.69221. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69231/0.69206. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69167/0.69184. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69162. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69170/0.69157. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69135. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.69139. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68943/0.69134. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68915/0.69136. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68785/0.69166. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68779/0.69194. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68793/0.69231. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68809/0.69289. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68596/0.69362. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68547/0.69424. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68593/0.69489. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68449/0.69569. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68432/0.69654. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68334/0.69745. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68363/0.69887. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68131/0.69986. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68263/0.70048. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68157/0.70175. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68165/0.70279. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68063/0.70420. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67995/0.70447. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68013/0.70575. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67965/0.70702. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67917/0.70840. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67619/0.70931. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67714/0.71024. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67484/0.71155. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67528/0.71302. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67489/0.71372. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67471/0.71487. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67225/0.71655. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67321/0.71704. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67234/0.71910. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67049/0.72091. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67098/0.72259. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67009/0.72373. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66771/0.72527. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66863/0.72725. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66518/0.72819. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66490/0.72898. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66527/0.73130. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66370/0.73462. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66475/0.73480. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65945/0.73817. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65816/0.73853. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66057/0.74164. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65867/0.74160. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65827/0.74273. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66021/0.74442. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65430/0.74586. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65520/0.74937. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65104/0.74991. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65589/0.75105. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65018/0.75287. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65166/0.75504. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64937/0.75604. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64607/0.75676. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64384/0.76095. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64654/0.76260. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64452/0.76387. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64084/0.76704. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64104/0.76673. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64005/0.76934. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63693/0.77096. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63524/0.77303. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63678/0.77470. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63469/0.77511. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63087/0.77712. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63203/0.77698. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62888/0.77976. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62794/0.78147. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62501/0.78492. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62533/0.78791. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61973/0.79056. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61831/0.79408. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62122/0.79400. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62053/0.79391. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62128/0.79333. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61792/0.79787. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61993/0.79905. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61301/0.80101. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61312/0.80512. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61082/0.80620. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60711/0.80674. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60711/0.80504. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60587/0.80878. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60929/0.80983. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60266/0.81361. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60075/0.81848. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59773/0.81884. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59922/0.82187. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59493/0.82372. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59112/0.82414. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69691/0.69897. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69422/0.69628. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69255/0.69665. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69125/0.69656. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69080/0.69691. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69035/0.69729. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68990/0.69759. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68906/0.69818. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68876/0.69827. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68879/0.69870. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68830/0.69902. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68813/0.69947. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68777/0.69977. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68747/0.69989. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68657/0.69998. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68689/0.70007. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68728/0.69998. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68563/0.70031. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68634/0.70020. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68471/0.70039. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68545/0.70046. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68489/0.70027. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68500/0.70008. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68409/0.70028. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68463/0.70057. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68371/0.70045. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68339/0.70019. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68255/0.70025. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68283/0.70052. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68184/0.70051. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68157/0.70104. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68143/0.70075. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68142/0.70072. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68053/0.70106. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68015/0.70084. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68062/0.70042. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68063/0.70088. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67942/0.70079. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67713/0.70166. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67574/0.70160. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67612/0.70188. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67686/0.70225. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67633/0.70255. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67375/0.70338. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67474/0.70360. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67237/0.70543. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67341/0.70409. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67259/0.70435. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67138/0.70504. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67248/0.70600. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66958/0.70663. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67026/0.70718. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66854/0.70864. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66614/0.70988. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66753/0.71125. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66525/0.71242. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66448/0.71219. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66204/0.71422. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66484/0.71486. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66299/0.71575. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65790/0.71763. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66034/0.71963. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65845/0.72134. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65849/0.72263. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65652/0.72286. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65631/0.72479. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65345/0.72673. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65503/0.72768. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65391/0.72932. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65146/0.72891. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65022/0.73211. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64971/0.73303. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64722/0.73441. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64588/0.73547. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64303/0.73807. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64558/0.73884. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64387/0.73945. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64064/0.74249. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64107/0.74521. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64089/0.74403. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64158/0.74628. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64106/0.74766. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63805/0.74790. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63560/0.74900. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63382/0.75025. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63405/0.75262. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63361/0.75084. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63140/0.75248. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62670/0.75603. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62830/0.75619. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62529/0.75661. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62585/0.76170. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62603/0.75982. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62185/0.76558. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62185/0.76734. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62338/0.76531. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62146/0.76437. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61752/0.76626. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61673/0.76998. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61737/0.77064. Took 0.08 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69436/0.70123. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69139/0.70275. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69018/0.70453. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68860/0.70625. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68780/0.70815. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68669/0.70998. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68558/0.71192. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68520/0.71333. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68468/0.71431. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68336/0.71570. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68239/0.71699. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68248/0.71717. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68218/0.71799. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68202/0.71805. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68135/0.71870. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68076/0.71922. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68000/0.72039. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68019/0.72042. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67925/0.72136. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67853/0.72197. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67895/0.72214. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67762/0.72261. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67726/0.72343. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67744/0.72354. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67602/0.72469. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67634/0.72516. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67531/0.72568. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67509/0.72698. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67395/0.72707. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67359/0.72823. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67470/0.72802. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67255/0.72832. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67177/0.72982. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67202/0.73024. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66991/0.73141. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66930/0.73170. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67128/0.73241. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66860/0.73390. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66829/0.73452. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66846/0.73533. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66796/0.73607. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66579/0.73634. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66461/0.73795. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66595/0.73955. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66599/0.74137. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66561/0.74218. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66478/0.74184. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66308/0.74258. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66233/0.74317. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66055/0.74484. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65856/0.74742. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66115/0.74750. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65987/0.74912. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65765/0.75068. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65959/0.75203. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65878/0.75232. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65618/0.75535. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65664/0.75411. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65733/0.75552. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65538/0.75497. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65444/0.75705. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65322/0.75841. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65221/0.75851. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65233/0.75994. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65170/0.76280. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65325/0.76319. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64866/0.76247. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64850/0.76427. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64771/0.76531. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64787/0.76629. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64534/0.76937. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64709/0.77169. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64544/0.77042. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64620/0.77243. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64731/0.77195. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64438/0.77408. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64021/0.77406. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64054/0.77356. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63927/0.77686. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63754/0.77994. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64021/0.78270. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64100/0.78077. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63648/0.78474. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63767/0.78315. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63500/0.78758. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63550/0.78532. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63103/0.78603. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63171/0.79335. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63550/0.78939. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62960/0.79278. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62672/0.79529. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62901/0.79346. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62567/0.79438. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62599/0.79764. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62349/0.79867. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62331/0.80362. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62390/0.80379. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62223/0.80263. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62221/0.80243. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61823/0.80974. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69689/0.69475. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69431/0.69452. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69290/0.69461. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69240/0.69476. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69196/0.69470. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69167/0.69470. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69066/0.69455. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68935/0.69431. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68931/0.69407. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68751/0.69393. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68708/0.69365. Took 0.13 sec\n",
      "Epoch 11, Loss(train/val) 0.68557/0.69325. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68544/0.69298. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68391/0.69343. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68327/0.69365. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68258/0.69407. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68184/0.69450. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68121/0.69507. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67925/0.69583. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67901/0.69653. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67898/0.69772. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67767/0.69765. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67588/0.69847. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67379/0.69928. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67404/0.69996. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67514/0.70140. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67332/0.70143. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67275/0.70226. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67138/0.70255. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67137/0.70257. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67136/0.70216. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67036/0.70278. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66659/0.70300. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66605/0.70259. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66535/0.70287. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66533/0.70278. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66277/0.70343. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66052/0.70350. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65990/0.70402. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65917/0.70461. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65806/0.70498. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65727/0.70522. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65469/0.70498. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65298/0.70392. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65016/0.70452. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64984/0.70621. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64779/0.70680. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64619/0.70771. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64773/0.70777. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64643/0.70730. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64055/0.70823. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63758/0.70936. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63588/0.71207. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63679/0.71063. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63673/0.71129. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63135/0.71336. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63010/0.71506. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62844/0.71625. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62653/0.71863. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62076/0.72098. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62228/0.72010. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61830/0.72333. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61951/0.72265. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61228/0.72628. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61400/0.72605. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61056/0.72871. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60934/0.73104. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61138/0.73011. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60808/0.72967. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60245/0.73591. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60537/0.73526. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59834/0.74067. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59520/0.74135. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59295/0.74351. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59548/0.74621. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58816/0.74872. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58484/0.75081. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58384/0.75035. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58475/0.75357. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58221/0.75452. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57926/0.75452. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57483/0.75583. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56890/0.76199. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57301/0.76282. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57053/0.76177. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56841/0.76812. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56486/0.76710. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56462/0.77166. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55986/0.77125. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55828/0.77538. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56042/0.77390. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55733/0.77014. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55611/0.76992. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55022/0.77878. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54684/0.78250. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54523/0.77677. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53690/0.78686. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54336/0.77905. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54111/0.77925. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53829/0.77796. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69245/0.69248. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69024/0.69453. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68846/0.69552. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68718/0.69695. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68592/0.69851. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68475/0.69998. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68252/0.70217. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68227/0.70395. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68138/0.70585. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68109/0.70749. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.67997/0.70893. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.67846/0.71022. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.67828/0.71236. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67794/0.71388. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67813/0.71429. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67683/0.71488. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67673/0.71640. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67635/0.71740. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67594/0.71771. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67499/0.71901. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67467/0.71986. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67511/0.72089. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67254/0.72214. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67308/0.72267. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67175/0.72342. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67210/0.72453. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67307/0.72545. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66974/0.72643. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67157/0.72710. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67009/0.72777. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66986/0.72859. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66890/0.72980. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66919/0.73049. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66737/0.73228. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66698/0.73248. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66789/0.73302. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66572/0.73453. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66396/0.73631. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66401/0.73758. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66388/0.73814. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66430/0.73875. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66366/0.74017. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66081/0.74303. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66300/0.74470. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66007/0.74433. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65927/0.74643. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66175/0.74842. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65746/0.74973. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65471/0.75232. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65560/0.75344. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65551/0.75523. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65412/0.75662. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65148/0.75794. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65270/0.75974. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65117/0.75923. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65037/0.76136. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64845/0.76295. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64950/0.76436. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64492/0.76553. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64683/0.76561. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64443/0.76998. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64509/0.77145. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64340/0.77268. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64410/0.77219. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64074/0.77501. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64202/0.77565. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63992/0.77557. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64141/0.77708. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63554/0.78026. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63695/0.78501. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63902/0.78524. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63513/0.78587. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63347/0.78688. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63554/0.78706. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63112/0.78784. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63257/0.78738. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63395/0.79038. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62881/0.79482. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62672/0.79537. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62766/0.79576. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63031/0.79767. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62649/0.80022. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62425/0.80111. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62667/0.80332. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61912/0.80583. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61867/0.80571. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61652/0.80475. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61658/0.80912. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61757/0.80868. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61648/0.81114. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61550/0.81248. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61360/0.81250. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60897/0.81750. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60784/0.82223. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60825/0.82584. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60574/0.82869. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60733/0.82978. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60365/0.83203. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60587/0.83388. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60217/0.83456. Took 0.08 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69334/0.69407. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.69460. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69285/0.69502. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69528. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69261/0.69569. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69221/0.69593. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69134/0.69600. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69196/0.69663. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69132/0.69679. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69118/0.69732. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69084/0.69656. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69019/0.69712. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68924/0.69730. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68943/0.69794. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68954/0.69788. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68931/0.69876. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68801/0.69899. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.69784. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68621/0.69860. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68695/0.70019. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68458/0.69970. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68397/0.70105. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68536/0.70111. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68533/0.69893. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68239/0.70070. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68286/0.70119. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68149/0.70077. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68164/0.70095. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68059/0.70161. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68054/0.70158. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67756/0.70408. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67748/0.70152. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67683/0.70520. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67610/0.70298. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67478/0.70309. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67292/0.70406. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67346/0.70463. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67137/0.70455. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67034/0.70443. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66860/0.70297. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66886/0.70584. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66745/0.70454. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66832/0.70250. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66357/0.70055. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66071/0.70304. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66201/0.70200. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65860/0.70290. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65753/0.70328. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65676/0.69940. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65274/0.70023. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65463/0.70225. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65279/0.70341. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65173/0.70135. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64792/0.70188. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64833/0.70267. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64853/0.70427. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64401/0.70558. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64098/0.70555. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64045/0.70701. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64366/0.70704. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63930/0.70927. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63554/0.70656. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63628/0.71213. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63181/0.71008. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63035/0.71354. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62745/0.71128. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62295/0.71244. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62227/0.71495. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61993/0.71893. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61912/0.72272. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61977/0.71959. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61484/0.72331. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61466/0.72556. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60719/0.72267. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61193/0.73066. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60365/0.73435. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60120/0.73934. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59868/0.74492. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60134/0.74115. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59987/0.74496. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59627/0.74764. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58744/0.74403. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59080/0.75525. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58665/0.75423. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58591/0.76012. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58026/0.75812. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57760/0.75764. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58316/0.76522. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57713/0.77075. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57651/0.77477. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57168/0.77019. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56782/0.77627. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56982/0.78678. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56317/0.78561. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56660/0.79231. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56110/0.79475. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55932/0.80340. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54908/0.80741. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54786/0.81119. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54072/0.81228. Took 0.08 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69852/0.68589. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69441/0.68841. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69405/0.68929. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69315/0.69007. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69018. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69286/0.69014. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69141/0.68990. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69118/0.68972. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68972/0.68890. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68961/0.68857. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.68816. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.68742. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68852/0.68716. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68682/0.68781. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68799/0.68766. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68548/0.68677. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68498/0.68629. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68561/0.68587. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68409/0.68632. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68315/0.68682. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68355/0.68578. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68309/0.68535. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68287/0.68521. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68252/0.68579. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68159/0.68621. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68179/0.68617. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68127/0.68730. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68137/0.68598. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67945/0.68731. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67903/0.68760. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67798/0.68775. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67830/0.68695. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67752/0.68787. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67835/0.68878. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67674/0.68845. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67663/0.68798. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67636/0.68947. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67429/0.68972. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67353/0.69075. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67212/0.69014. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67275/0.69208. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67355/0.69057. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67167/0.69182. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67120/0.69368. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67083/0.69280. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66782/0.69407. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66969/0.69424. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66741/0.69366. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66642/0.69528. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66739/0.69516. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66619/0.69656. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66027/0.69652. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66138/0.69779. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66082/0.69822. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66192/0.69980. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65819/0.70208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65931/0.70371. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65537/0.70272. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65235/0.70252. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65474/0.70785. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65286/0.70669. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65532/0.70779. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65181/0.70980. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64817/0.71219. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64657/0.71202. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64575/0.71540. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64258/0.71434. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64312/0.71390. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64380/0.71612. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63670/0.71510. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63685/0.71720. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63565/0.72278. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63715/0.72004. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63261/0.72215. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63364/0.72894. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62592/0.73211. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63079/0.73169. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62730/0.73334. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62619/0.73963. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62292/0.73627. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62138/0.73918. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62175/0.74158. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62057/0.74242. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61511/0.74798. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61826/0.74855. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61389/0.75023. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61049/0.75541. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60553/0.75651. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60772/0.75585. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60799/0.76031. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59858/0.76318. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59572/0.76742. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59565/0.76931. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60052/0.76900. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59678/0.77320. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59550/0.77599. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58632/0.77542. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58884/0.77653. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59328/0.77879. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58160/0.78638. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69337/0.69365. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69317. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.69299. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69283/0.69278. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69165/0.69265. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69166/0.69284. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69092/0.69311. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.69339. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69035/0.69377. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69009/0.69385. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68952/0.69403. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68920/0.69450. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68879/0.69518. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68811/0.69615. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68664/0.69718. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68543/0.69771. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68581/0.69783. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68554/0.69873. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68372/0.69912. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68430/0.70031. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68294/0.70034. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68319/0.70134. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68195/0.70134. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68172/0.70160. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67990/0.70205. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67901/0.70350. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67754/0.70461. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67644/0.70434. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67612/0.70390. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67485/0.70473. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67512/0.70380. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67118/0.70386. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67077/0.70485. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67089/0.70388. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66911/0.70388. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66797/0.70413. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66625/0.70345. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66421/0.70371. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66348/0.70294. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66141/0.70441. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65875/0.70385. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65646/0.70286. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65806/0.70229. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65420/0.70226. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65056/0.70062. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64731/0.70174. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64556/0.70204. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64363/0.70293. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64331/0.70041. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64402/0.69984. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63816/0.70169. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63801/0.70295. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63573/0.70300. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63022/0.69987. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63262/0.70131. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63177/0.70155. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62617/0.70122. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62162/0.70070. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61941/0.70136. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61686/0.70148. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61710/0.70233. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61698/0.70121. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62002/0.70315. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61201/0.70718. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61299/0.70858. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.61127/0.70441. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60727/0.70427. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60791/0.70634. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60445/0.70966. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59991/0.70572. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59838/0.70864. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59306/0.71388. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59731/0.71552. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59182/0.71116. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59310/0.71303. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58400/0.71587. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58833/0.71942. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57892/0.71381. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58246/0.71653. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57249/0.71619. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58037/0.71854. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.57665/0.72651. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56849/0.72396. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57458/0.72583. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56150/0.72660. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56911/0.72639. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56503/0.72859. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56062/0.72912. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56048/0.72159. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55302/0.72865. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55500/0.73522. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55266/0.73769. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55059/0.74042. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55184/0.74504. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54689/0.74261. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54131/0.74607. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54190/0.74413. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53786/0.74221. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54080/0.74249. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54193/0.73716. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69008. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69131/0.68898. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69090/0.68929. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69011/0.68935. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68950/0.68933. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68894/0.68946. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68859/0.68943. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68885/0.68964. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68793/0.68939. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68769/0.68940. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68710/0.68945. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68711/0.68936. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.68943. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68683/0.68930. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68648/0.68929. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68636/0.68925. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68598/0.68922. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68530/0.68925. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68535/0.68924. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68461/0.68932. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68404/0.68914. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68433/0.68913. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68334/0.68877. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68368/0.68870. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68336/0.68867. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68352/0.68874. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68236/0.68861. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68237/0.68889. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68192/0.68861. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68145/0.68858. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68152/0.68869. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68121/0.68852. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68006/0.68874. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68048/0.68908. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67931/0.68869. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67801/0.68888. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67878/0.68889. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67709/0.68911. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67598/0.68930. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67615/0.68949. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67575/0.68997. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67471/0.69007. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67432/0.69000. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67298/0.69007. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67257/0.69077. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67142/0.69105. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67090/0.69165. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66953/0.69227. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66809/0.69246. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66811/0.69259. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66664/0.69276. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66505/0.69412. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66626/0.69516. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66444/0.69589. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66376/0.69695. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66276/0.69771. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66233/0.69973. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66177/0.69957. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66108/0.69979. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65938/0.70123. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65772/0.70267. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65705/0.70404. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65869/0.70389. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65462/0.70531. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65427/0.70652. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65122/0.70753. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65078/0.70965. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65215/0.71044. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65073/0.71236. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64674/0.71383. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64931/0.71538. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64645/0.71724. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64578/0.71655. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64226/0.71881. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64378/0.71951. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63965/0.72140. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64530/0.72357. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63997/0.72516. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64009/0.72522. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63673/0.72650. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63598/0.72979. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63675/0.72939. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63401/0.73263. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63446/0.73442. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63145/0.73540. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63455/0.73526. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62824/0.73767. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62935/0.73993. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62764/0.74321. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62261/0.74375. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62387/0.74647. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62218/0.74755. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62284/0.75087. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62089/0.75355. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61865/0.75570. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61803/0.75621. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61642/0.75938. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61532/0.75989. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61256/0.76341. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60882/0.76474. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69393/0.69299. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69225/0.69543. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69127/0.69644. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69156/0.69765. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69192/0.69735. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69058/0.69766. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69009/0.69850. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69030/0.69833. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68909/0.69822. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68841/0.69911. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68704/0.69940. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68717/0.69914. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68598/0.69958. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68642/0.70060. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68527/0.70065. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68428/0.70118. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68393/0.70114. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68407/0.70053. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68361/0.70140. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68185/0.70192. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68243/0.70204. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68228/0.70167. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68049/0.70218. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68040/0.70254. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67885/0.70237. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67882/0.70188. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67756/0.70189. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67883/0.70131. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67745/0.70154. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67784/0.70185. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67408/0.70201. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67523/0.70064. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67167/0.69972. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67462/0.70159. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67323/0.70145. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67067/0.70146. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66918/0.70169. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67089/0.70181. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67113/0.70271. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66818/0.70272. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66921/0.70360. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66826/0.70341. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66490/0.70396. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66570/0.70493. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66591/0.70579. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66353/0.70619. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66152/0.70808. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66145/0.70826. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66038/0.71055. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66083/0.70961. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65581/0.71187. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65910/0.71137. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65815/0.71477. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65409/0.71538. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65701/0.71647. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65407/0.71689. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65287/0.71896. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65191/0.71769. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65096/0.71997. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64855/0.71893. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64858/0.72160. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64804/0.72529. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64658/0.72483. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64221/0.72613. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64116/0.72902. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64453/0.72848. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63918/0.73314. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63743/0.73379. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64022/0.73714. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63723/0.73871. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63617/0.73879. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63346/0.74146. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63502/0.74368. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63290/0.74330. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63146/0.74762. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63011/0.75119. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62858/0.75191. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62679/0.75497. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62361/0.75524. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61739/0.75635. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61724/0.75938. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61977/0.76295. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61627/0.76418. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61231/0.76756. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61502/0.76907. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60954/0.76717. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61163/0.77512. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61237/0.76951. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60970/0.77201. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60564/0.76832. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60374/0.77152. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60251/0.78022. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59830/0.78502. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60123/0.78424. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59590/0.78619. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59684/0.79002. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59145/0.79185. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59401/0.79287. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59214/0.79325. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58370/0.79660. Took 0.08 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.69215/0.70226. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.70259. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69175/0.70279. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69162/0.70332. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.70366. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69052/0.70402. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69152/0.70433. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69074/0.70436. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69012/0.70442. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69019/0.70432. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68989/0.70447. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68984/0.70476. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68899/0.70461. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68886/0.70468. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68861/0.70461. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68725/0.70444. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68810/0.70402. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68703/0.70469. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68707/0.70483. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68788/0.70549. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68643/0.70485. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68528/0.70553. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68427/0.70434. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68274/0.70410. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68194/0.70514. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68241/0.70400. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68075/0.70285. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68107/0.70376. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68023/0.70343. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67910/0.70507. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67781/0.70495. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67976/0.70501. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67899/0.70491. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67679/0.70383. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67799/0.70668. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67818/0.70529. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67597/0.70589. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67263/0.70325. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67224/0.70495. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67355/0.70424. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67121/0.70395. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66987/0.70523. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66742/0.70492. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66917/0.70457. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66811/0.70302. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66702/0.70162. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66519/0.70237. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66483/0.70069. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66318/0.70176. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66292/0.70242. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66117/0.70060. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66009/0.70265. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66045/0.70368. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65791/0.69961. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65611/0.70337. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65564/0.70091. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65368/0.69868. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65520/0.69682. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65098/0.70276. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64885/0.69853. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64713/0.70097. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64583/0.70178. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64698/0.69540. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64244/0.69419. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64314/0.69639. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64084/0.69762. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63722/0.69366. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63970/0.69890. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63426/0.69823. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63480/0.70217. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63304/0.70001. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62434/0.69836. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62813/0.69846. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63075/0.69643. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62171/0.69426. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62518/0.70325. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62278/0.69841. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61866/0.69462. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61645/0.69569. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61436/0.70016. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61441/0.70238. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61074/0.69988. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60895/0.69838. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60852/0.69341. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60088/0.69309. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60368/0.69536. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59934/0.70018. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59448/0.69685. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59631/0.69983. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60185/0.70061. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58663/0.69375. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59366/0.69626. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58985/0.69574. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58758/0.70046. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57829/0.69909. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57440/0.70440. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58413/0.69047. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57549/0.69845. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57375/0.71238. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56870/0.70307. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69451/0.68595. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.68774. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69272/0.68811. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69193/0.68859. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69127/0.68907. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69085/0.68989. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69064/0.69031. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68971/0.69084. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68924/0.69181. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68896/0.69274. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68896/0.69307. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68766/0.69372. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68700/0.69506. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68686/0.69522. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68605/0.69635. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68640/0.69659. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68595/0.69712. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68492/0.69822. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68467/0.69836. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68467/0.69877. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68476/0.69906. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68273/0.70067. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68287/0.70089. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68237/0.70145. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68178/0.70185. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68173/0.70225. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68154/0.70260. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67943/0.70379. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68040/0.70434. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67835/0.70490. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67845/0.70589. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67762/0.70696. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67728/0.70736. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67633/0.70876. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67593/0.70951. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67416/0.71050. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67425/0.71126. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67337/0.71118. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67084/0.71317. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67009/0.71366. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67057/0.71358. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66976/0.71626. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66797/0.71701. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66814/0.71906. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66812/0.71964. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66498/0.71919. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66304/0.72138. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66367/0.72361. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66295/0.72445. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65907/0.72514. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66124/0.72400. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65849/0.72554. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65858/0.72646. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65743/0.72876. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65181/0.73061. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65277/0.73080. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65431/0.73442. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65048/0.73475. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65048/0.73309. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64675/0.73494. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64538/0.73817. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64417/0.73918. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64191/0.74112. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64185/0.74319. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64000/0.74465. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63843/0.74790. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63337/0.74519. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63381/0.74716. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63380/0.75024. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63350/0.74976. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62633/0.75398. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62650/0.75658. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63025/0.75543. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62331/0.75790. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62524/0.76198. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62154/0.76232. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61831/0.76627. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61645/0.76700. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61541/0.76841. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61469/0.77203. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61490/0.77314. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60708/0.77525. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60968/0.77588. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60298/0.78099. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59955/0.78174. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60408/0.78369. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59813/0.78608. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59573/0.78871. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59285/0.79202. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59735/0.79214. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58918/0.79518. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58775/0.79542. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58289/0.80032. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58281/0.80204. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.58139/0.80464. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57364/0.80917. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58237/0.81072. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57982/0.81457. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57039/0.81521. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56947/0.81714. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69428/0.69581. Took 0.15 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.69663. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69153/0.69736. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 0.69213/0.69803. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69039/0.69870. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68910/0.69962. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68948/0.70056. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68847/0.70138. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68823/0.70206. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68732/0.70257. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68603/0.70299. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68641/0.70365. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68656/0.70403. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68605/0.70441. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68623/0.70450. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68524/0.70460. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68408/0.70475. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68436/0.70464. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68322/0.70478. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68290/0.70486. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68210/0.70475. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68206/0.70435. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68121/0.70398. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68061/0.70365. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68049/0.70323. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67894/0.70293. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67985/0.70272. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67803/0.70238. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67692/0.70222. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67680/0.70173. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67720/0.70067. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67578/0.70056. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67313/0.70016. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67296/0.69940. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67190/0.69968. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66993/0.69856. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67111/0.69799. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66868/0.69788. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66534/0.69737. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66613/0.69689. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66527/0.69662. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66262/0.69543. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65849/0.69583. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65899/0.69650. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65828/0.69604. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65591/0.69570. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65645/0.69584. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64959/0.69617. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64735/0.69572. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64914/0.69851. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64485/0.69672. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64064/0.69657. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63949/0.69655. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63915/0.69889. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63309/0.69924. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63361/0.70068. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62948/0.70345. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63189/0.70490. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62776/0.70534. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62863/0.70660. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61951/0.70756. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62042/0.70885. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62033/0.71399. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61420/0.71524. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61116/0.71594. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60644/0.72055. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60887/0.72138. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60854/0.72323. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60963/0.72536. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60195/0.72754. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60718/0.72676. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60010/0.72783. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59625/0.73325. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59325/0.73643. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58774/0.73571. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58530/0.73992. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59024/0.74173. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58451/0.74472. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58004/0.74706. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57636/0.75082. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57876/0.75143. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57029/0.75471. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56851/0.76019. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56683/0.76329. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56606/0.76481. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56276/0.76716. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55735/0.77179. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55220/0.77585. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55273/0.77880. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54509/0.78521. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55288/0.78537. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54914/0.79327. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54265/0.79485. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54335/0.79092. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53300/0.79694. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53535/0.80236. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53441/0.80185. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52695/0.81086. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53422/0.81530. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52800/0.81320. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69418/0.69402. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69286/0.69374. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69405/0.69338. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69392/0.69305. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69343/0.69284. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69318/0.69260. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69308/0.69233. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69279/0.69201. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69290/0.69176. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69252/0.69159. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69206/0.69131. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69220/0.69094. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69197/0.69057. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69170/0.69033. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69098/0.68997. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69183/0.68960. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69022/0.68919. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69000/0.68895. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69028/0.68862. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68945/0.68829. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68780/0.68783. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68690/0.68775. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68733/0.68759. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68550/0.68756. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68463/0.68750. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68427/0.68748. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68539/0.68773. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68463/0.68849. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68321/0.68831. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68290/0.68820. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67946/0.68861. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68142/0.68876. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68203/0.68950. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68072/0.68925. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67892/0.69030. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68012/0.69056. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67780/0.69067. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67775/0.69111. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67669/0.69089. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67430/0.69135. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67531/0.69195. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67280/0.69111. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67233/0.69186. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67435/0.69043. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67104/0.69136. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67714/0.69192. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67228/0.69034. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67075/0.69088. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66858/0.69200. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66600/0.68954. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66859/0.68988. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66620/0.68830. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66294/0.68873. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66427/0.68914. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66443/0.68742. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66367/0.68689. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66444/0.68829. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66204/0.68706. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65743/0.68716. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65643/0.68729. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65408/0.68728. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65719/0.68893. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65409/0.68780. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64980/0.68538. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65147/0.68733. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65068/0.68416. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64711/0.68646. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64575/0.68854. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64616/0.68554. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63972/0.68782. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64392/0.68747. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63874/0.68630. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63614/0.68941. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63791/0.68725. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63234/0.68800. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63383/0.69007. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63441/0.69150. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62933/0.68979. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62580/0.68870. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62035/0.69182. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62164/0.69012. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62260/0.69476. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61383/0.69251. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61481/0.69753. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61466/0.70007. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61059/0.70116. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60462/0.69883. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60328/0.70867. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60428/0.70722. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60837/0.71023. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59847/0.70730. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59972/0.71035. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59912/0.71165. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58738/0.71298. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59210/0.71803. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58308/0.72107. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59036/0.72326. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58394/0.72079. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57508/0.72219. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57849/0.72934. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69486/0.69489. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69351/0.69845. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69811. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69090/0.69806. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69041/0.69749. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69023/0.69778. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68943/0.69832. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68929/0.69829. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68952/0.69878. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68890/0.69847. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68879/0.69832. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68857/0.69853. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68824/0.69841. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68813/0.69845. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68881/0.69883. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68772/0.69808. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68750/0.69848. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68797/0.69857. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68756/0.69839. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68675/0.69819. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68664/0.69867. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68636/0.69852. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68700/0.69789. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68612/0.69860. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68652/0.69805. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68610/0.69800. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68487/0.69769. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68494/0.69810. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68538/0.69750. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68546/0.69708. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68506/0.69740. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68507/0.69727. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68358/0.69715. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68364/0.69712. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68385/0.69683. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68266/0.69684. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68217/0.69584. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68283/0.69498. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68244/0.69488. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68253/0.69492. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68161/0.69472. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67976/0.69388. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68123/0.69423. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68064/0.69412. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67979/0.69392. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67930/0.69322. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68118/0.69377. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67957/0.69310. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67848/0.69386. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67748/0.69377. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67679/0.69346. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67628/0.69320. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67542/0.69267. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67534/0.69226. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67556/0.69218. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67479/0.69284. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67376/0.69192. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67244/0.69127. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67481/0.69262. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.67552/0.69195. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67161/0.69199. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67252/0.69168. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.67156/0.69245. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66925/0.69126. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67124/0.69201. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66873/0.69219. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66943/0.69219. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66768/0.69074. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66747/0.69115. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66696/0.69323. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66672/0.69185. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.66514/0.69272. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66304/0.69255. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.66571/0.69203. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66494/0.69190. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.66269/0.69254. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66191/0.69386. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.66129/0.69334. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.65861/0.69287. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65830/0.69289. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65868/0.69160. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65658/0.69196. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.65736/0.69351. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.65822/0.69422. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.65409/0.69494. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.65565/0.69523. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.65204/0.69520. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.65443/0.69563. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.65020/0.69548. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.65219/0.69482. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.65133/0.69629. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65062/0.69804. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64712/0.69669. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.64660/0.69695. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64809/0.69827. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.64689/0.69837. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.64315/0.70138. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.64278/0.70245. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63774/0.70250. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.63810/0.70330. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69574/0.69306. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69492/0.69297. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69385/0.69319. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69248/0.69354. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69394. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69160/0.69438. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69104/0.69481. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.69482. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68977/0.69509. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69027/0.69553. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68963/0.69598. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68961/0.69606. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68886/0.69610. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68893/0.69570. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68843/0.69567. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68780/0.69610. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68775/0.69553. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68740/0.69548. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68734/0.69556. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68668/0.69482. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68619/0.69446. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68508/0.69394. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68602/0.69337. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68609/0.69367. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68514/0.69320. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68463/0.69306. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68417/0.69265. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68335/0.69241. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68166/0.69275. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68090/0.69137. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68139/0.69136. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68191/0.69002. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.68889. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68064/0.68946. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67802/0.68916. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67795/0.68875. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67723/0.68894. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67634/0.68979. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67542/0.68848. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67542/0.68905. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67435/0.68902. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67347/0.68881. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67349/0.68776. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67109/0.68716. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67105/0.68815. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66773/0.68843. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66947/0.68701. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66595/0.68745. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66597/0.68956. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66553/0.68828. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66644/0.68878. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66292/0.68737. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65995/0.68964. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66041/0.69139. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66022/0.69084. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66032/0.69100. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66039/0.68944. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65637/0.69094. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65395/0.69239. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65457/0.69435. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65121/0.69555. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65049/0.69704. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64998/0.69774. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65137/0.69775. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64688/0.69748. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64682/0.69985. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64240/0.69879. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64326/0.70160. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64396/0.69886. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63823/0.70341. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63904/0.70416. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64374/0.70499. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63837/0.70479. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63682/0.70335. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63488/0.70273. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63132/0.70515. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63294/0.70800. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62813/0.70832. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62902/0.70817. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62576/0.71168. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62716/0.71197. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62302/0.71301. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62103/0.71523. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62319/0.71414. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61895/0.71695. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62037/0.71757. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61698/0.71743. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61628/0.71468. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61309/0.71939. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61600/0.71907. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61320/0.72047. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60792/0.71941. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60673/0.71768. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60835/0.71939. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60215/0.72579. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60447/0.72037. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59497/0.72253. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59523/0.72369. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.59969/0.72565. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59503/0.72378. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69443/0.69482. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69420. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69169/0.69391. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69371. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69116/0.69327. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69299. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.69282. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68952/0.69270. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68820/0.69239. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68850/0.69231. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68891/0.69219. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68810/0.69218. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68742/0.69231. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68682/0.69271. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68784/0.69285. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68592/0.69292. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68544/0.69325. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68606/0.69355. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68532/0.69373. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68435/0.69399. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68442/0.69406. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68299/0.69463. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68368/0.69528. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68341/0.69546. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68297/0.69556. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68316/0.69577. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68227/0.69582. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68204/0.69610. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68183/0.69635. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68135/0.69637. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67985/0.69646. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68008/0.69627. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68065/0.69700. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67857/0.69734. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67824/0.69807. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67704/0.69808. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67791/0.69785. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67617/0.69824. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67558/0.69864. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67548/0.69854. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67570/0.69865. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67211/0.69964. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67184/0.70005. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67048/0.70092. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67088/0.70149. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67342/0.70094. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67234/0.70165. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67103/0.70207. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66796/0.70273. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66682/0.70165. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66448/0.70241. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66638/0.70325. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66548/0.70374. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66440/0.70480. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66308/0.70640. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66325/0.70737. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66071/0.70703. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66187/0.70793. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66082/0.70790. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65954/0.70992. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65727/0.70998. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65636/0.71156. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65401/0.71206. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65573/0.71278. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64986/0.71337. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65254/0.71487. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65006/0.71575. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64960/0.71665. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64823/0.71910. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64623/0.72352. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64397/0.72382. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64313/0.72397. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64513/0.72435. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64316/0.72706. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64187/0.72769. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63652/0.72954. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63620/0.73330. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63767/0.73396. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63441/0.73350. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63244/0.73613. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63279/0.73694. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63347/0.73915. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62768/0.74245. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63036/0.74349. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62699/0.74696. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62478/0.75010. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62251/0.75461. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62342/0.75546. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62334/0.75610. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62052/0.75606. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61679/0.76031. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61843/0.76226. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61113/0.76712. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61221/0.76988. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61018/0.77036. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60821/0.77499. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60536/0.78110. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60172/0.78648. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60220/0.78787. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60078/0.78995. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69552/0.69231. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69375/0.69350. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69274/0.69440. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69201/0.69506. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69141/0.69525. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69045/0.69542. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68956/0.69554. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68907/0.69543. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68794/0.69553. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68737/0.69585. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68637/0.69585. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68507/0.69511. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68454/0.69460. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68366/0.69410. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68288/0.69426. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68234/0.69413. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68152/0.69314. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68057/0.69219. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67958/0.69310. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67868/0.69239. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67928/0.69204. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67697/0.69050. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67694/0.69140. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67645/0.69009. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67506/0.68904. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67412/0.69014. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67597/0.69138. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67278/0.68955. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67274/0.69100. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67035/0.68966. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67103/0.69005. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66816/0.68942. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66923/0.69089. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66804/0.69007. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66606/0.69013. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66588/0.69001. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66734/0.68817. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66265/0.68972. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66307/0.68976. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66315/0.68887. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65962/0.69037. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66283/0.69010. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65990/0.69127. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65819/0.68977. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65682/0.69173. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65691/0.68944. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65389/0.68941. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65348/0.68930. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65477/0.69039. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65484/0.69108. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65197/0.68970. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64847/0.69007. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64786/0.69040. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64742/0.69154. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64591/0.68937. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64624/0.68928. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64476/0.68990. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64485/0.69055. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64227/0.69104. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64072/0.69123. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64047/0.69253. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64089/0.69213. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63837/0.69360. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63660/0.69245. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63454/0.69123. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63580/0.69242. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63408/0.68748. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62887/0.68799. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62663/0.68706. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62409/0.68957. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62814/0.68962. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62127/0.69044. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62065/0.68905. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61835/0.69287. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61939/0.69172. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61989/0.68920. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61749/0.68945. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61169/0.68620. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60874/0.68381. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60914/0.68757. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60555/0.68375. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60665/0.68969. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60563/0.69231. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60148/0.69241. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59880/0.69256. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59819/0.69188. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59722/0.69033. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59246/0.68956. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58961/0.69459. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58947/0.68951. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58665/0.69019. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58371/0.69384. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58027/0.69323. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58138/0.69802. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57967/0.69865. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57539/0.69893. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57650/0.69852. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56045/0.70056. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57071/0.70322. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56515/0.70303. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69293/0.68860. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69211/0.68731. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69099/0.68619. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68994/0.68549. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68907/0.68516. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68791/0.68484. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68741/0.68459. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68545/0.68471. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68486/0.68398. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68563/0.68437. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68433/0.68511. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68410/0.68558. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68348/0.68617. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68420/0.68606. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68268/0.68656. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68276/0.68741. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68277/0.68728. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68219/0.68841. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68119/0.68723. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68069/0.68847. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68113/0.68869. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67970/0.68890. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68107/0.68936. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67964/0.68944. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67938/0.69132. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67826/0.69116. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67610/0.69196. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67784/0.69210. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67682/0.69274. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67632/0.69270. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67393/0.69312. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67490/0.69403. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67281/0.69442. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67496/0.69536. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67335/0.69555. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67149/0.69570. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67359/0.69640. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66976/0.69716. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67120/0.69806. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67003/0.69903. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67026/0.69756. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66730/0.70049. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66722/0.70005. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66624/0.70120. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66290/0.70265. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66430/0.70328. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66389/0.70409. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66452/0.70595. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66061/0.70852. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66123/0.70803. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65760/0.70791. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65755/0.70968. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65525/0.71336. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65628/0.71325. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65527/0.71477. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65265/0.71635. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65487/0.71591. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65065/0.71569. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65194/0.71668. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64892/0.72101. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64936/0.72254. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64691/0.72306. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64526/0.72539. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64403/0.72343. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64555/0.72683. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63867/0.72747. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64147/0.73269. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63777/0.73386. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63692/0.73512. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63600/0.73257. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63174/0.73828. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62927/0.73670. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62974/0.73964. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62659/0.74148. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62826/0.74169. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62737/0.73918. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62133/0.74351. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62389/0.74602. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61678/0.74731. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61892/0.74847. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61977/0.75172. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61775/0.74984. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61637/0.75676. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61527/0.75587. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61107/0.75646. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61040/0.76112. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60764/0.76259. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60604/0.75844. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60169/0.76637. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60032/0.76851. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60655/0.76922. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59527/0.77080. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59020/0.76943. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59739/0.77668. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59377/0.78009. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59297/0.77333. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59315/0.77717. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58668/0.78449. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58808/0.77909. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58744/0.78571. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69364/0.69086. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69272/0.69138. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69158/0.69176. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69068/0.69218. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68990/0.69278. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68917/0.69335. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68824/0.69424. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68808/0.69530. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68669/0.69640. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68582/0.69751. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68450/0.69869. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68510/0.70000. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68339/0.70137. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68291/0.70226. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68280/0.70321. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68287/0.70407. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68094/0.70517. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68052/0.70643. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68085/0.70735. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68060/0.70769. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67870/0.70869. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67806/0.70960. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67947/0.71019. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67762/0.71071. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67699/0.71123. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67562/0.71197. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67639/0.71248. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67515/0.71275. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67400/0.71345. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67444/0.71417. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67232/0.71525. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67379/0.71611. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67240/0.71674. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67247/0.71697. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66936/0.71792. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66969/0.71871. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66787/0.71919. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66670/0.71979. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66489/0.72209. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66535/0.72300. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66449/0.72427. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66454/0.72531. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66377/0.72475. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66063/0.72599. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66111/0.72671. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65824/0.72788. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65922/0.72889. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65509/0.73104. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65649/0.73201. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65583/0.73188. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65430/0.73378. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65284/0.73400. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64891/0.73543. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65048/0.73643. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64745/0.73907. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64862/0.74026. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64683/0.74118. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64207/0.74189. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64746/0.74217. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64196/0.74274. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64064/0.74411. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64169/0.74425. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64128/0.74556. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63830/0.74650. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63478/0.74814. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63417/0.74883. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63295/0.74914. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63442/0.75158. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63180/0.75353. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63146/0.75342. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62952/0.75371. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62707/0.75499. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62379/0.75402. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62176/0.75568. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62227/0.75841. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61874/0.75529. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61805/0.75819. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61576/0.76165. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61370/0.76271. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61579/0.76595. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61078/0.76485. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60766/0.76264. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60746/0.76277. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60875/0.76500. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60342/0.76564. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60600/0.76621. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60325/0.76999. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60004/0.77148. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59756/0.77069. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59345/0.77354. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59620/0.77450. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59173/0.77879. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59329/0.77674. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58965/0.77692. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59219/0.77522. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58688/0.78301. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58053/0.78414. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58341/0.79109. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58139/0.78893. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57343/0.78829. Took 0.08 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69509/0.69237. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69422/0.69238. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69352/0.69253. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69268/0.69296. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69270/0.69333. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69191/0.69380. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69437. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69060/0.69486. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69006/0.69525. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68928/0.69590. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68976/0.69644. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68867/0.69704. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68851/0.69746. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68726/0.69808. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68679/0.69910. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68509/0.69971. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68607/0.70028. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68422/0.70072. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68278/0.70153. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68317/0.70187. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68077/0.70201. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68000/0.70223. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67929/0.70288. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67999/0.70239. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67874/0.70198. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67606/0.70298. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67632/0.70243. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67572/0.70312. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67500/0.70358. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67357/0.70328. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67308/0.70397. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67311/0.70453. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67058/0.70493. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67125/0.70538. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66998/0.70689. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66842/0.70634. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66850/0.70891. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66603/0.70898. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66381/0.71059. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66436/0.71033. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66214/0.71054. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66179/0.71177. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66050/0.71278. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65781/0.71321. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65948/0.71345. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65591/0.71742. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65475/0.71905. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65772/0.72089. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65180/0.72032. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65259/0.72280. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65089/0.72438. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64909/0.72610. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64676/0.72738. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64613/0.72889. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64666/0.73082. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64498/0.72922. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64431/0.72922. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64349/0.73153. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64425/0.73176. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64053/0.73411. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63658/0.73462. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63550/0.73860. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63667/0.73786. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63262/0.74062. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63177/0.74564. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62859/0.74343. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62952/0.74645. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62577/0.74401. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62941/0.74740. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62423/0.75260. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62279/0.75265. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62109/0.75576. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62053/0.75551. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62281/0.75793. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61793/0.75884. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61818/0.75731. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61470/0.76106. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61526/0.75983. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61068/0.76581. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61024/0.76272. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61007/0.76399. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60281/0.76940. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60451/0.76858. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60231/0.76930. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60136/0.77131. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59868/0.77328. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59746/0.77663. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59734/0.78007. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59536/0.77570. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59045/0.78114. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59194/0.78427. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58460/0.78130. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58702/0.78960. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57798/0.79235. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58216/0.78976. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57981/0.78962. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57333/0.78697. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57753/0.79499. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57254/0.79747. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57413/0.80157. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69504/0.69482. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69408/0.69425. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69285/0.69382. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69191/0.69363. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69192/0.69361. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69146/0.69373. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69037/0.69392. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.69422. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.69453. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68886/0.69494. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.69557. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68737/0.69639. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68628/0.69710. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68507/0.69818. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68582/0.69918. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68409/0.69994. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68388/0.70059. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68490/0.70138. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68325/0.70183. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68296/0.70222. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68266/0.70265. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68167/0.70375. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68128/0.70436. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68101/0.70524. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68000/0.70601. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67887/0.70686. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67839/0.70730. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67788/0.70831. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67971/0.70870. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67662/0.70970. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67567/0.71093. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67472/0.71171. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67426/0.71261. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67559/0.71282. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67426/0.71329. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67132/0.71364. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67364/0.71338. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67006/0.71390. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67073/0.71490. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66892/0.71612. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66664/0.71695. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66602/0.71774. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66296/0.71973. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66555/0.72039. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66423/0.72033. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66263/0.72072. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66237/0.72224. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66257/0.72228. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66123/0.72181. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66044/0.72218. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65916/0.72373. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65683/0.72505. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65460/0.72628. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65351/0.72688. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65113/0.72706. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64938/0.72803. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65099/0.72857. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64823/0.73069. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64430/0.73043. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64536/0.73185. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64440/0.73275. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63848/0.73303. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63894/0.73535. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63975/0.73489. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63852/0.73552. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63631/0.73664. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63278/0.73884. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63067/0.74117. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62858/0.74182. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62739/0.74242. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63014/0.74181. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62486/0.74316. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61753/0.74414. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62527/0.74321. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61679/0.74787. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61585/0.74808. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61995/0.75025. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61077/0.75155. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60933/0.75113. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61152/0.75467. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60347/0.75566. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60689/0.76005. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60048/0.75896. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59693/0.76235. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59932/0.76221. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59903/0.76229. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59633/0.76192. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58989/0.76561. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58970/0.76883. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58610/0.76580. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58324/0.76954. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57542/0.77297. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58557/0.77317. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58199/0.77695. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57229/0.77727. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57096/0.78154. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56670/0.78523. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56683/0.78845. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56387/0.78639. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57096/0.79313. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69446/0.69275. Took 0.15 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69352. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69290/0.69425. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 0.69219/0.69505. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69129/0.69609. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69071/0.69737. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.69873. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.70029. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68907/0.70214. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68746/0.70436. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68709/0.70693. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68589/0.70950. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.71217. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68580/0.71432. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68436/0.71656. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68329/0.71889. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68324/0.72137. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68241/0.72310. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68097/0.72485. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68145/0.72637. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68043/0.72754. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67995/0.72950. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67934/0.73072. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67818/0.73175. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67772/0.73265. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67668/0.73387. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67504/0.73474. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67538/0.73583. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67645/0.73573. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67501/0.73562. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67382/0.73628. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67181/0.73708. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67389/0.73745. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67196/0.73741. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66947/0.73817. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66924/0.73877. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66731/0.73994. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66721/0.73988. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66635/0.73984. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66526/0.73960. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66280/0.73875. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66032/0.73881. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66132/0.73954. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65904/0.73926. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65861/0.73940. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65411/0.74019. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65343/0.73997. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65196/0.73972. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65166/0.73867. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64895/0.73968. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64730/0.73992. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64387/0.73979. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64487/0.74043. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64255/0.73954. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63959/0.74105. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64131/0.74152. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63545/0.74076. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63712/0.74153. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63406/0.74359. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63248/0.74397. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.62858/0.74311. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62434/0.74383. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62629/0.74469. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62578/0.74266. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62039/0.74347. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62059/0.74440. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61596/0.74497. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61888/0.74687. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61251/0.74549. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61035/0.74549. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60797/0.74705. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61202/0.74574. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60329/0.75059. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60136/0.74960. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60079/0.74933. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59794/0.75292. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59494/0.75331. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59193/0.75022. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59094/0.75282. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59153/0.75605. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58776/0.75318. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59000/0.75508. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58176/0.75754. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58066/0.75451. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57523/0.76102. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57597/0.75613. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57151/0.76153. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56852/0.76768. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56364/0.76626. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55815/0.76795. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56067/0.76579. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56249/0.76891. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55403/0.77047. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55417/0.77860. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55152/0.78144. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54580/0.77943. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54739/0.78291. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54080/0.78860. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54352/0.78496. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54010/0.79239. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69492/0.69309. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69309/0.69075. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.68993. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.68963. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.68976. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69085/0.68949. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68976/0.68918. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68901/0.68914. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68840/0.68886. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68817/0.68846. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68722/0.68902. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68650/0.68831. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68559/0.68860. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68454/0.68966. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68388/0.69018. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68250/0.68989. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68293/0.68982. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68077/0.68926. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67858/0.68954. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67733/0.69030. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67632/0.69015. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67418/0.68931. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67336/0.69134. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67001/0.68888. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66712/0.68776. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66376/0.68818. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66461/0.68641. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66311/0.68563. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66099/0.68517. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65582/0.68523. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65403/0.68799. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65178/0.68676. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65095/0.68911. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64783/0.69155. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64544/0.69053. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64483/0.69331. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64428/0.69207. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64129/0.69349. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63655/0.69467. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63649/0.69445. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63742/0.69621. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63224/0.69778. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63142/0.69853. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62971/0.69930. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62986/0.70077. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62231/0.70275. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61973/0.70394. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62101/0.70384. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61921/0.70266. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.61906/0.70280. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61975/0.70322. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61459/0.70354. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61146/0.70488. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61097/0.70522. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60877/0.70753. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60592/0.70909. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60487/0.71179. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60153/0.71232. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59581/0.71607. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59688/0.71849. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59190/0.72061. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59631/0.71674. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58488/0.72161. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58663/0.72368. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58370/0.72545. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58712/0.72374. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58137/0.72624. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57784/0.72802. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57296/0.73121. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57261/0.73427. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57596/0.73757. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56491/0.73784. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56452/0.74262. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56631/0.74810. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56065/0.74950. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56134/0.75157. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55847/0.75239. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55034/0.75716. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54898/0.76498. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54353/0.76600. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.54997/0.77142. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54542/0.77231. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54409/0.77063. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54234/0.77719. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54332/0.77965. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52970/0.78526. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.52992/0.78629. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52997/0.79812. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52085/0.79960. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52443/0.80541. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51931/0.81065. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52336/0.80715. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.51461/0.81033. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50381/0.81694. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.50329/0.81748. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50944/0.83493. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50318/0.83478. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49718/0.84087. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49848/0.83667. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.49860/0.84536. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69358/0.69651. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69145/0.69626. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69107/0.69715. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69106/0.69829. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69933. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69057/0.70033. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69017/0.70127. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68970/0.70235. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68886/0.70346. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68861/0.70433. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68896/0.70567. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68832/0.70671. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68797/0.70781. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68819/0.70856. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68745/0.70957. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68653/0.71062. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68540/0.71166. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68562/0.71231. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68420/0.71364. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68409/0.71482. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68197/0.71594. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68258/0.71672. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68228/0.71704. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68197/0.71709. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68145/0.71804. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67911/0.72009. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67844/0.72046. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67774/0.72155. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67770/0.72210. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67637/0.72250. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67534/0.72279. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67449/0.72353. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67167/0.72381. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67089/0.72399. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66798/0.72524. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66658/0.72627. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66532/0.72637. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66421/0.72764. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66119/0.72851. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65995/0.72987. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65718/0.73156. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65599/0.73489. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65009/0.73546. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64825/0.73783. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65033/0.74076. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64702/0.74082. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64533/0.74177. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63952/0.74574. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64210/0.74651. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63949/0.74509. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63940/0.74707. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63136/0.75124. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63227/0.75411. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63125/0.75468. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62706/0.75459. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62449/0.75750. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61893/0.75918. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61332/0.76479. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61390/0.76711. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61112/0.76772. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61190/0.76658. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60701/0.76905. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60353/0.77714. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60260/0.77636. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60112/0.78068. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59768/0.77851. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59636/0.78119. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59561/0.78869. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59370/0.78794. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58539/0.79225. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58324/0.79043. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58158/0.79644. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57897/0.80844. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57549/0.80660. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57491/0.80508. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57127/0.80816. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57626/0.80804. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56332/0.81407. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56665/0.81918. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56330/0.82290. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55865/0.82759. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56188/0.82847. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55402/0.83649. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55061/0.83394. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55255/0.84415. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54996/0.85395. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53732/0.84663. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54514/0.84748. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53964/0.86879. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54009/0.85892. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53453/0.86611. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53456/0.87713. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.52620/0.87301. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52399/0.87796. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52889/0.88620. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52167/0.88754. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51112/0.89840. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51097/0.89277. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50789/0.89817. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51835/0.90976. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69390/0.69576. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69276/0.69604. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69224/0.69593. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69180/0.69565. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69196/0.69556. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.69545. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69093/0.69514. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69003/0.69496. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69068/0.69455. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68972/0.69455. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68933/0.69403. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.69389. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68871/0.69360. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68756/0.69410. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68827/0.69408. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68729/0.69448. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68647/0.69465. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68616/0.69538. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68629/0.69549. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68581/0.69677. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68534/0.69721. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68461/0.69821. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68364/0.69835. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68328/0.69999. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68264/0.70019. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68158/0.70178. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68188/0.70240. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68167/0.70371. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68064/0.70474. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68007/0.70497. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67873/0.70743. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67811/0.70843. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67709/0.70837. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67603/0.71039. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67574/0.71275. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67458/0.71174. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67427/0.71347. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67117/0.71588. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67215/0.71727. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67004/0.71904. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66780/0.71914. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66689/0.72003. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66694/0.72486. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66443/0.72443. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66180/0.72646. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66237/0.72819. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65952/0.73452. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65731/0.73087. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65570/0.73543. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65476/0.73657. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65307/0.73868. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65078/0.74085. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64603/0.74133. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64779/0.74655. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64259/0.74385. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64364/0.74654. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63994/0.74952. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63792/0.75051. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63284/0.74927. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63059/0.75426. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62834/0.75104. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62761/0.75222. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62697/0.74862. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62393/0.75047. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61812/0.75221. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61704/0.75907. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61808/0.76036. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61705/0.75313. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61385/0.76364. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60929/0.75899. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61031/0.76468. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60541/0.76585. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60876/0.76220. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59881/0.76541. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60143/0.76826. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59704/0.77013. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59110/0.76902. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59191/0.76734. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58989/0.76732. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58321/0.77917. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58658/0.77511. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58227/0.77582. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58339/0.78150. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57633/0.78137. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57929/0.77962. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57125/0.77507. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56967/0.78100. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56590/0.77945. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56509/0.78338. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56084/0.78361. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56194/0.78294. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56088/0.79250. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55435/0.79275. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55310/0.78948. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54929/0.78372. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55079/0.79559. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54688/0.78915. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53824/0.78272. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53829/0.78413. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53286/0.80761. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69389/0.69323. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69218. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69147/0.69177. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69090/0.69163. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69039/0.69140. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69075/0.69132. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.69113. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68937/0.69102. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68904/0.69081. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68840/0.69089. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68806/0.69095. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68828/0.69106. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68740/0.69134. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68617/0.69156. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68633/0.69206. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68618/0.69219. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68636/0.69228. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68530/0.69293. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68390/0.69325. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68344/0.69361. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68459/0.69394. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68380/0.69414. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68270/0.69477. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68334/0.69521. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68146/0.69533. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68034/0.69533. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68042/0.69584. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67913/0.69660. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67746/0.69686. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67873/0.69752. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67866/0.69774. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67654/0.69794. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67397/0.69847. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67452/0.69977. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67295/0.70043. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67161/0.70009. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67225/0.70146. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66982/0.70258. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66864/0.70417. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66646/0.70578. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66695/0.70613. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66468/0.70591. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66299/0.70756. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66217/0.71039. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65893/0.71084. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65773/0.71102. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65900/0.71306. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65575/0.71604. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65382/0.71776. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65131/0.71814. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64906/0.72004. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64752/0.72264. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64961/0.72464. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64275/0.72868. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64312/0.73098. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64236/0.73093. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63954/0.73407. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63446/0.73740. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63290/0.73822. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63336/0.73998. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63112/0.74416. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62993/0.74713. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63096/0.74832. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62572/0.75100. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62619/0.75321. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62231/0.75550. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62169/0.75776. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61800/0.76256. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61714/0.76207. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61487/0.76454. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61050/0.76622. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61712/0.76700. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60930/0.76848. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60586/0.77352. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60499/0.77740. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60457/0.77745. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59766/0.77947. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59824/0.78083. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59770/0.78572. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59336/0.78958. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58830/0.79321. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58835/0.80070. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58509/0.80209. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58464/0.80338. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58448/0.80071. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57778/0.80789. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58081/0.81104. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57345/0.81248. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57125/0.81603. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56853/0.81699. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56471/0.82500. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56108/0.82955. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55947/0.83205. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55730/0.83310. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56026/0.83615. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55742/0.84090. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54755/0.84018. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54730/0.84436. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54538/0.84682. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53815/0.85736. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69289/0.68327. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69091/0.68418. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69043/0.68530. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68995/0.68594. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68955/0.68657. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68885/0.68770. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68858/0.68878. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68731/0.68964. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68724/0.69013. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68698/0.69098. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68766/0.69191. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68643/0.69277. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68577/0.69337. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68544/0.69420. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68568/0.69490. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68508/0.69541. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68503/0.69555. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68575/0.69598. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68398/0.69609. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68381/0.69679. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68444/0.69733. Took 0.13 sec\n",
      "Epoch 21, Loss(train/val) 0.68375/0.69767. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68244/0.69811. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68368/0.69860. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68247/0.69896. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68193/0.69914. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68133/0.69985. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68157/0.69981. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68159/0.70104. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68025/0.69956. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68031/0.70013. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67839/0.70131. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67958/0.70086. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67901/0.70216. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67835/0.70131. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67805/0.70209. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67727/0.70185. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67616/0.70188. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67529/0.70245. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67577/0.70239. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67538/0.70367. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67562/0.70281. Took 0.12 sec\n",
      "Epoch 42, Loss(train/val) 0.67407/0.70397. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67319/0.70505. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67166/0.70465. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67244/0.70510. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67111/0.70604. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67239/0.70636. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67086/0.70516. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66964/0.70614. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66724/0.70652. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66727/0.70681. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66765/0.70725. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66609/0.70671. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66551/0.70612. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66191/0.70646. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66385/0.70585. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66206/0.70815. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66299/0.70723. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66053/0.70789. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65873/0.70730. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65883/0.70889. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65472/0.71039. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65590/0.70989. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65158/0.71046. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65380/0.71193. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65182/0.71197. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64816/0.71072. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64821/0.71097. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64913/0.71281. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64493/0.71483. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64642/0.71545. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64364/0.71524. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64398/0.71496. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64001/0.71548. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63908/0.71517. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63668/0.71719. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63510/0.71694. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63189/0.72037. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63224/0.71747. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62632/0.71966. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62776/0.72287. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62419/0.72200. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62792/0.72187. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62253/0.72456. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62069/0.72346. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61995/0.72463. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61650/0.72441. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61251/0.72899. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60806/0.72914. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60633/0.73099. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61061/0.73061. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60506/0.73188. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60556/0.73531. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59900/0.73671. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59707/0.73211. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59154/0.73295. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58867/0.73733. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58468/0.74421. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59629/0.73950. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69553/0.69503. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69172/0.69058. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68994/0.68788. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68947/0.68661. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68849/0.68568. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68856/0.68494. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68799/0.68436. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68678/0.68381. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68725/0.68305. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68753/0.68221. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68538/0.68173. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68509/0.68101. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68560/0.68044. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68315/0.67948. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68428/0.67849. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68397/0.67762. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68385/0.67715. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68156/0.67620. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67966/0.67525. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67891/0.67480. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67937/0.67393. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67775/0.67357. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67713/0.67313. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67532/0.67257. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67566/0.67173. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67431/0.67119. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67514/0.67050. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67221/0.67042. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67024/0.66943. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66991/0.66888. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66869/0.66886. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66782/0.66818. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66674/0.66869. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66688/0.66855. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66657/0.66714. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66507/0.66711. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66428/0.66734. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66573/0.66747. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66431/0.66760. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66260/0.66699. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66067/0.66639. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65918/0.66584. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65870/0.66634. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65711/0.66751. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65696/0.66769. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65532/0.66696. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65139/0.66639. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65241/0.66674. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65104/0.66597. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65191/0.66611. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64994/0.66573. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64943/0.66462. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64847/0.66473. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64468/0.66606. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64403/0.66484. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64141/0.66489. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64167/0.66380. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63899/0.66391. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63941/0.66316. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.63618/0.66180. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63391/0.66244. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 0.63610/0.66387. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63076/0.66455. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63252/0.66274. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62993/0.66095. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62568/0.66161. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62529/0.65930. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62219/0.66160. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.62439/0.66124. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61939/0.66203. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61449/0.65989. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61616/0.65738. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61180/0.65933. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60954/0.66111. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61119/0.66173. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60946/0.66215. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60704/0.66189. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60718/0.66280. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59957/0.66001. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59727/0.65937. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59703/0.65747. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.59228/0.66013. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59145/0.66314. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58903/0.66024. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59011/0.66259. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58211/0.66345. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58275/0.66206. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.57418/0.66510. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57366/0.66187. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57203/0.65722. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.56980/0.66253. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57391/0.66194. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56760/0.66572. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56453/0.66664. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55857/0.66883. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55652/0.67243. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56047/0.67185. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56364/0.67266. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.54832/0.67068. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.55075/0.67199. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69003/0.69748. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68880/0.69824. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68786/0.69926. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68748/0.70060. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68650/0.70202. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68553/0.70393. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.68574/0.70539. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68462/0.70697. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68340/0.70885. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68365/0.71044. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68304/0.71173. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68213/0.71310. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68208/0.71390. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68125/0.71479. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68117/0.71565. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68074/0.71588. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68001/0.71647. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67887/0.71730. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67962/0.71814. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67946/0.71863. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67932/0.71815. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67850/0.71807. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67892/0.71799. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67839/0.71840. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67791/0.71871. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67640/0.71887. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67589/0.71869. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67567/0.71882. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67600/0.71911. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67517/0.71894. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67624/0.71850. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67408/0.71830. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67525/0.71843. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67311/0.71871. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67375/0.71901. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67124/0.71921. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67078/0.71939. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67196/0.71929. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66855/0.71967. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66971/0.72060. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66795/0.72062. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66707/0.72177. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66600/0.72235. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66408/0.72238. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66536/0.72242. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66442/0.72276. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66288/0.72328. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66198/0.72446. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65897/0.72588. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66005/0.72559. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65767/0.72613. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65649/0.72677. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65637/0.72756. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65368/0.73034. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65363/0.73293. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65088/0.73307. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64925/0.73444. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65054/0.73718. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64710/0.73922. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64512/0.74215. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64547/0.74371. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64364/0.74484. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64140/0.74705. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63889/0.75058. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63484/0.75195. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63586/0.75629. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63299/0.75639. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63156/0.75769. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62887/0.76435. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62764/0.76560. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62420/0.76902. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62153/0.77371. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62140/0.77860. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61705/0.78177. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61384/0.78360. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61462/0.78756. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60976/0.79137. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61046/0.79435. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60935/0.80021. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60535/0.80465. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60505/0.80942. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60256/0.81314. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59433/0.81878. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59820/0.82172. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.59424/0.82607. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.59579/0.83075. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.58654/0.83138. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.58954/0.83498. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58497/0.83773. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58956/0.84744. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57819/0.84706. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58016/0.85350. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57849/0.85919. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56996/0.86252. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56864/0.87102. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56975/0.87585. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56744/0.87653. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56079/0.88750. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56060/0.88894. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56168/0.89072. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69300/0.69378. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69263/0.69353. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69157/0.69310. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69074/0.69258. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69022/0.69206. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68900/0.69170. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68851/0.69143. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68795/0.69117. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68770/0.69085. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68757/0.69026. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68693/0.68935. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68585/0.68841. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68662/0.68752. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68528/0.68657. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68427/0.68559. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68301/0.68477. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68224/0.68338. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68048/0.68225. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67993/0.68169. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67928/0.68047. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67792/0.67942. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67721/0.67889. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67593/0.67796. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67432/0.67818. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67305/0.67650. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67100/0.67553. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66997/0.67610. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66925/0.67417. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66815/0.67386. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66899/0.67254. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66556/0.67108. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66372/0.67201. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66234/0.67026. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66249/0.66742. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65963/0.66793. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65772/0.66595. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65637/0.66484. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65393/0.66440. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65286/0.66379. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65168/0.66187. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65147/0.66221. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64765/0.65963. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64960/0.65832. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64411/0.65812. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64367/0.66041. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64241/0.65813. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64218/0.65817. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63779/0.65977. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63908/0.65770. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63758/0.65973. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63441/0.65920. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62979/0.66109. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63343/0.65831. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63062/0.65850. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62869/0.65934. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62677/0.65699. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62637/0.65922. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62487/0.65739. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62653/0.65982. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61867/0.65814. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61807/0.65630. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61951/0.65757. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61972/0.65932. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.61333/0.65820. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61402/0.66153. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61365/0.66114. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61171/0.65960. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61062/0.66203. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61093/0.65643. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60575/0.65467. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60468/0.65664. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.60441/0.65851. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59780/0.65817. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60115/0.65894. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.59746/0.66373. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59845/0.66249. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59301/0.66308. Took 0.12 sec\n",
      "Epoch 77, Loss(train/val) 0.59040/0.66313. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 0.59391/0.66195. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.58904/0.66534. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.59245/0.66121. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58657/0.66981. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58251/0.66108. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58159/0.66167. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58007/0.65968. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57656/0.66649. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57829/0.67465. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57404/0.67479. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.56794/0.66936. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56854/0.67904. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56605/0.67064. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56428/0.67086. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56826/0.66707. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55816/0.66906. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56176/0.66695. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55653/0.67452. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.55597/0.66250. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55929/0.67538. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55128/0.67259. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55525/0.67727. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69315/0.69083. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69080/0.68695. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68868/0.68465. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68909/0.68395. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68792/0.68328. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68800/0.68288. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68667/0.68262. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68691/0.68244. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68688/0.68229. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68670/0.68217. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68629/0.68191. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68568/0.68155. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68581/0.68153. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68552/0.68126. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68397/0.68136. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68504/0.68127. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68355/0.68155. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68352/0.68137. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68293/0.68161. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68219/0.68165. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68186/0.68197. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68248/0.68218. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68117/0.68273. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68112/0.68249. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68114/0.68283. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67982/0.68258. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67879/0.68258. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68028/0.68300. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67953/0.68276. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68095/0.68243. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67888/0.68293. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67762/0.68313. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67636/0.68382. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67699/0.68393. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67630/0.68413. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67661/0.68485. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67518/0.68545. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67603/0.68573. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67313/0.68614. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67329/0.68629. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67165/0.68694. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67150/0.68788. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67040/0.68847. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66904/0.68911. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67177/0.68949. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66814/0.69054. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66623/0.69173. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66543/0.69270. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66433/0.69338. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66647/0.69365. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66314/0.69386. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66351/0.69546. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65912/0.69721. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65801/0.69765. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65852/0.69879. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65909/0.69820. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65634/0.69919. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65522/0.70073. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65193/0.70211. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65230/0.70287. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65116/0.70489. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65549/0.70419. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64832/0.70427. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64867/0.70569. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64150/0.70865. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64620/0.70844. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64344/0.70822. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63765/0.71083. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63658/0.71204. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63755/0.71211. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63169/0.71371. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63546/0.71419. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63212/0.71561. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63097/0.71611. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62663/0.71772. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62370/0.71861. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62093/0.72190. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62126/0.72314. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61801/0.72239. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62024/0.72428. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61570/0.72593. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61081/0.72639. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61170/0.72906. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60855/0.73099. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60828/0.73334. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60931/0.72924. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60072/0.73439. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59718/0.73905. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59700/0.73872. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59667/0.73855. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59391/0.74052. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58823/0.74252. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59115/0.74334. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59101/0.74520. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58371/0.75009. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57919/0.74850. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58203/0.74907. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58049/0.75169. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57503/0.75613. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57243/0.75859. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68829/0.69598. Took 0.13 sec\n",
      "Epoch 1, Loss(train/val) 0.68622/0.69552. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68568/0.69442. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68469/0.69412. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68447/0.69329. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68361/0.69303. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.68315/0.69299. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68309/0.69269. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68255/0.69221. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68155/0.69187. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68203/0.69147. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68134/0.69186. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68071/0.69145. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68041/0.69172. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67942/0.69161. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67928/0.69171. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67798/0.69175. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67858/0.69174. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67838/0.69173. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67740/0.69138. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67773/0.69127. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67742/0.69105. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67637/0.69148. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67643/0.69107. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67526/0.69128. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67610/0.69168. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67431/0.69225. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67588/0.69191. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67463/0.69134. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67502/0.69144. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67284/0.69120. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67295/0.69136. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67175/0.69135. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67184/0.69109. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67107/0.69135. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67076/0.69099. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66998/0.69077. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66981/0.69089. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66934/0.69027. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66749/0.69089. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66746/0.69033. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66595/0.69089. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66712/0.69026. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66506/0.68981. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66649/0.68986. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66464/0.68997. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66152/0.69023. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66259/0.68998. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66005/0.68963. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66277/0.69000. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66024/0.69031. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65909/0.69029. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65864/0.69053. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65821/0.69043. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65656/0.69045. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65571/0.69163. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65743/0.69228. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65403/0.69208. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65520/0.69180. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65383/0.69240. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64941/0.69266. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65071/0.69259. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64756/0.69413. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64741/0.69537. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64819/0.69627. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64393/0.69610. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64469/0.69673. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64254/0.69783. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64192/0.69690. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64179/0.69745. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63832/0.69889. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63775/0.69965. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63674/0.69926. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63365/0.70051. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63288/0.70012. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63001/0.70143. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63155/0.70222. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62560/0.70116. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62249/0.70382. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62583/0.70291. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62523/0.70491. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61864/0.70540. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61621/0.70652. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61484/0.70995. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61792/0.71170. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61170/0.71148. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61594/0.71449. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60946/0.71366. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60948/0.71457. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60281/0.71787. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60207/0.72095. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60849/0.72438. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59755/0.72337. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59747/0.72775. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59187/0.72707. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59478/0.72769. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58562/0.73248. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58812/0.73485. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59045/0.73709. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58706/0.73488. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69336/0.68788. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69124/0.68826. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.68846. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69039/0.68876. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68901/0.68889. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68926/0.68916. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68879/0.68959. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68874/0.68991. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68816/0.69025. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68800/0.69058. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68776/0.69095. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68780/0.69129. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68731/0.69163. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68656/0.69204. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68658/0.69243. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68696/0.69280. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68608/0.69327. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68630/0.69358. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68596/0.69393. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68579/0.69418. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68509/0.69459. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68515/0.69482. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68475/0.69516. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68418/0.69578. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68430/0.69608. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68489/0.69652. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68464/0.69659. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68333/0.69678. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68368/0.69690. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68269/0.69704. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68324/0.69687. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68334/0.69667. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68260/0.69675. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68275/0.69698. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68219/0.69715. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68201/0.69705. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68082/0.69703. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68124/0.69732. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68050/0.69704. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68022/0.69729. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68060/0.69741. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67905/0.69770. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67807/0.69780. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67903/0.69783. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67830/0.69779. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67846/0.69754. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67623/0.69792. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67640/0.69770. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67705/0.69765. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67496/0.69798. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67439/0.69807. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67386/0.69811. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67275/0.69859. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67212/0.69835. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67118/0.69769. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67093/0.69870. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66783/0.69937. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66846/0.70010. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66784/0.69910. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66806/0.69827. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66677/0.69889. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66674/0.69909. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66309/0.69925. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66375/0.69993. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66283/0.70078. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66057/0.70209. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66152/0.70076. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65913/0.70075. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65696/0.70302. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65591/0.70403. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65650/0.70491. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65344/0.70526. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65133/0.70621. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65095/0.70595. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64920/0.70612. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64809/0.70725. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64808/0.70780. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64478/0.70980. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64057/0.70900. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63959/0.70890. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64182/0.70998. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63931/0.71032. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63822/0.71458. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63654/0.71228. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63640/0.71534. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63147/0.71640. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63285/0.71596. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63208/0.71792. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62593/0.71637. Took 0.14 sec\n",
      "Epoch 89, Loss(train/val) 0.62870/0.72006. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62408/0.71884. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61954/0.72400. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.61703/0.72313. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62255/0.72222. Took 0.23 sec\n",
      "Epoch 94, Loss(train/val) 0.61900/0.72235. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.61994/0.72804. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61503/0.72454. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61421/0.72896. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61327/0.72791. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61667/0.72440. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69186. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69239/0.69077. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69156/0.68977. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69062/0.68903. Took 0.12 sec\n",
      "Epoch 4, Loss(train/val) 0.68979/0.68836. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68911/0.68786. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68880/0.68728. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68754/0.68693. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68729/0.68647. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68616/0.68585. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68499/0.68532. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68334/0.68456. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68255/0.68388. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68098/0.68299. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67872/0.68238. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67665/0.68057. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.67246/0.67947. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67279/0.67813. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.66893/0.67773. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.66565/0.67687. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66435/0.67635. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.66287/0.67554. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66337/0.67478. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.65747/0.67475. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65757/0.67509. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65741/0.67522. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65169/0.67439. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.64921/0.67457. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64820/0.67408. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.64593/0.67337. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.64287/0.67239. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.64223/0.67403. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64257/0.67160. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.63675/0.67102. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.63582/0.67036. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63435/0.66901. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63614/0.66917. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.62966/0.66700. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62540/0.66860. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62910/0.66763. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62789/0.66645. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62036/0.66727. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62172/0.66590. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61583/0.66393. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61835/0.66293. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62121/0.66148. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61102/0.66402. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61556/0.66188. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61115/0.66137. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60892/0.65724. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60762/0.66016. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60334/0.65957. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60174/0.65879. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60163/0.65625. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59904/0.65534. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59658/0.65706. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59163/0.65441. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59487/0.65522. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59238/0.65507. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58896/0.65827. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59320/0.65805. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58912/0.65941. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58450/0.66345. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58331/0.66314. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58305/0.66537. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58046/0.66701. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58342/0.66551. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57612/0.67051. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57756/0.66571. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57761/0.66890. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57596/0.66988. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57237/0.66901. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56916/0.66767. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56546/0.67128. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56669/0.67125. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55355/0.67593. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56293/0.67862. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55957/0.68358. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56168/0.67979. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55534/0.68170. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55428/0.67474. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55141/0.67943. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54877/0.67998. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54631/0.68323. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54622/0.68501. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53905/0.69109. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54099/0.68511. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53697/0.69443. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53915/0.69209. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53327/0.68828. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53403/0.69528. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53610/0.69587. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52631/0.70643. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52725/0.70744. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52276/0.70653. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52153/0.71137. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51653/0.71369. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50881/0.72423. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51340/0.72188. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51181/0.72174. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69414/0.68635. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69043/0.68288. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68901/0.68285. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68857/0.68313. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68779/0.68306. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68678/0.68408. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68651/0.68434. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68658/0.68502. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68624/0.68594. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.68613/0.68632. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68505/0.68696. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68593/0.68783. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68555/0.68826. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68576/0.68842. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68438/0.68937. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68316/0.69018. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68279/0.69096. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68287/0.69205. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68261/0.69269. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.69321. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68216/0.69380. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68184/0.69462. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68121/0.69522. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68111/0.69617. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67897/0.69673. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68023/0.69725. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67811/0.69791. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67943/0.69881. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67762/0.69989. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67846/0.70073. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67909/0.70035. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67785/0.70135. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67835/0.70189. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67738/0.70209. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67710/0.70354. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67420/0.70290. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67287/0.70524. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67411/0.70376. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67282/0.70491. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67231/0.70663. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67191/0.70668. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67135/0.70814. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67101/0.70763. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66733/0.70908. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66863/0.71030. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66802/0.71089. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66337/0.71250. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66414/0.71271. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66432/0.71357. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66406/0.71493. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66325/0.71453. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66051/0.71661. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65990/0.71640. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65846/0.71806. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65903/0.71939. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65805/0.72118. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 0.65466/0.72102. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65670/0.72259. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65544/0.72317. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65345/0.72310. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65236/0.72419. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65002/0.72681. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64848/0.72754. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64639/0.72845. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64552/0.73042. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64568/0.73177. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64469/0.73108. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64329/0.73660. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64103/0.73463. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63928/0.73470. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63722/0.73804. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63351/0.73782. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63281/0.74083. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63469/0.74061. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63314/0.74231. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63017/0.74375. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62769/0.74419. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62799/0.75313. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62284/0.75303. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62323/0.75088. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62114/0.75248. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62430/0.75540. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62362/0.75679. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61923/0.75618. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61821/0.76287. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61982/0.75947. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61530/0.76060. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61496/0.76523. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61233/0.76156. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61113/0.76636. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60886/0.77131. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60682/0.76580. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60407/0.77314. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60376/0.77531. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59995/0.77571. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60156/0.78121. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59570/0.77776. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59841/0.77928. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59850/0.78734. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59027/0.78868. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69273/0.69829. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68843/0.69528. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68852/0.69347. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68747/0.69246. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68665/0.69168. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68600/0.69117. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68665/0.69076. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68470/0.69050. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68548/0.69064. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68369/0.69065. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68368/0.69083. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68419/0.69101. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68349/0.69097. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68377/0.69113. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68254/0.69128. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68294/0.69140. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68258/0.69125. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.68236/0.69158. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68214/0.69190. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68162/0.69216. Took 0.12 sec\n",
      "Epoch 20, Loss(train/val) 0.68182/0.69220. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68054/0.69240. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68150/0.69253. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68126/0.69260. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68014/0.69263. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68005/0.69292. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67988/0.69340. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67928/0.69351. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67921/0.69361. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67924/0.69374. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67774/0.69368. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67913/0.69399. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67820/0.69434. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67800/0.69436. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67697/0.69450. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67704/0.69428. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67696/0.69482. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67601/0.69519. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67543/0.69540. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67549/0.69508. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67449/0.69536. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67471/0.69560. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67342/0.69590. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67252/0.69638. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67255/0.69677. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67261/0.69664. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67173/0.69708. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67188/0.69676. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67073/0.69689. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67194/0.69722. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67046/0.69749. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66901/0.69764. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66958/0.69795. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66907/0.69829. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66816/0.69788. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66779/0.69830. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66620/0.69830. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66637/0.69727. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66532/0.69812. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66558/0.69910. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66612/0.69892. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66412/0.69987. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66325/0.70009. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66207/0.69998. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66224/0.70089. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66061/0.70085. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65955/0.70231. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66186/0.70194. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66066/0.70192. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65627/0.70203. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65597/0.70302. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65580/0.70240. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65733/0.70406. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65322/0.70497. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65548/0.70509. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65162/0.70503. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65223/0.70590. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65204/0.70672. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65107/0.70702. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64846/0.70664. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64932/0.70839. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64449/0.70875. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64674/0.70830. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.64519/0.70892. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64440/0.70965. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64426/0.70836. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64343/0.70984. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64026/0.71229. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63830/0.71266. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63716/0.71424. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63892/0.71237. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.63442/0.71319. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63480/0.71657. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63449/0.71487. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62995/0.71641. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62792/0.72081. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62920/0.72094. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62844/0.71932. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62578/0.72227. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62799/0.72383. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69027/0.68777. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69029/0.68649. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68988/0.68560. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68893/0.68499. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68845/0.68461. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68787/0.68423. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68763/0.68385. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68707/0.68359. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68732/0.68334. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68584/0.68306. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68468/0.68277. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68290/0.68204. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68225/0.68112. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68145/0.68039. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68147/0.67987. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68096/0.67864. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67757/0.67834. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67683/0.67699. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67588/0.67578. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67378/0.67495. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67441/0.67394. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67223/0.67340. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67092/0.67243. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66906/0.67135. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67013/0.67096. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66718/0.67070. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66682/0.67025. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66411/0.66957. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66456/0.66879. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66262/0.66785. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65926/0.66736. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66148/0.66639. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65901/0.66618. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65617/0.66562. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65404/0.66436. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65597/0.66396. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65284/0.66331. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65039/0.66383. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64905/0.66249. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64570/0.66092. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64383/0.66096. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64691/0.65985. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64081/0.65918. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.64095/0.65803. Took 0.12 sec\n",
      "Epoch 44, Loss(train/val) 0.64011/0.65763. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.63825/0.65807. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.63561/0.65837. Took 0.13 sec\n",
      "Epoch 47, Loss(train/val) 0.63655/0.66007. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63529/0.65982. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63747/0.65984. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63071/0.65949. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63036/0.66089. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62819/0.65899. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62797/0.66054. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62378/0.66299. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62357/0.66424. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62369/0.66607. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61880/0.66429. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61553/0.66511. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61554/0.66529. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61583/0.66543. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61628/0.66554. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61396/0.66779. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61082/0.66827. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61125/0.66794. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60734/0.66819. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60381/0.66895. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60079/0.66854. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60208/0.67001. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60289/0.67129. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60154/0.67360. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.60258/0.67326. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.60045/0.67303. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59692/0.67381. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59467/0.67493. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58613/0.67367. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59480/0.67839. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.59028/0.68000. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58579/0.67923. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.58388/0.67996. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58023/0.68299. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58284/0.68769. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57958/0.68948. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57440/0.69022. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57319/0.69124. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56832/0.69191. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57186/0.69362. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56706/0.69441. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56443/0.70039. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56170/0.70247. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56426/0.70600. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55938/0.70838. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55465/0.70714. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55067/0.70768. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55359/0.70989. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54570/0.71205. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54570/0.71488. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54549/0.71829. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54522/0.71647. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54510/0.71952. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69020/0.69859. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68821/0.70046. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68796/0.70127. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68828/0.70106. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68801/0.70120. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68711/0.70135. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68687/0.70150. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68686/0.70133. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68578/0.70174. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68508/0.70217. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68520/0.70196. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68446/0.70224. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68380/0.70258. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68359/0.70207. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68227/0.70243. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68179/0.70271. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68197/0.70250. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68066/0.70231. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67960/0.70234. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67808/0.70255. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67749/0.70228. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67651/0.70192. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67727/0.70086. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67411/0.70009. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67272/0.69961. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67219/0.69974. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66971/0.69904. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66823/0.69807. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66737/0.69670. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66505/0.69551. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66148/0.69493. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66059/0.69346. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65667/0.69291. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65405/0.69113. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65324/0.68982. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65003/0.68913. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64737/0.68862. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64660/0.68983. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64375/0.68808. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63988/0.68716. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63631/0.68702. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63470/0.68895. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63435/0.68726. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63592/0.68621. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63092/0.68596. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62676/0.68794. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62726/0.68772. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62537/0.68742. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62239/0.68691. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62311/0.68978. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62210/0.69015. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62264/0.68950. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61659/0.68809. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61257/0.69056. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61426/0.68780. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61174/0.68757. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61236/0.69148. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60804/0.69152. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60601/0.69167. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60542/0.69286. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59763/0.69177. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59856/0.69379. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59972/0.69217. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59589/0.69419. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59595/0.69569. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59817/0.69651. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58963/0.69269. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59227/0.69851. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58861/0.69844. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58860/0.69480. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58216/0.69753. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57873/0.69995. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57673/0.69987. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57640/0.70226. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57815/0.70286. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57406/0.70536. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57332/0.70947. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57222/0.70885. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56378/0.70549. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57229/0.70394. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56379/0.71008. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56307/0.71589. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55853/0.71480. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55407/0.71796. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55021/0.72187. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55303/0.72075. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55098/0.72257. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54990/0.71787. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54928/0.72519. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54370/0.72057. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54339/0.72697. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54481/0.72953. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53709/0.73363. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53363/0.73325. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53159/0.73292. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52846/0.73700. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53679/0.74006. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52617/0.74087. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53078/0.74671. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52125/0.75487. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69139/0.69746. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69162/0.69838. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.69915. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69023/0.69995. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69035/0.70101. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69019/0.70177. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68937/0.70237. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68923/0.70320. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68848/0.70414. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.70450. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68795/0.70488. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68776/0.70550. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68699/0.70616. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68714/0.70681. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68574/0.70683. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68573/0.70754. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68480/0.70759. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68482/0.70780. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68527/0.70957. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68443/0.71124. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68203/0.71216. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68180/0.71273. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68046/0.71365. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68020/0.71600. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67803/0.71832. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67726/0.71988. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67645/0.72279. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67473/0.72440. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.67311/0.72459. Took 0.11 sec\n",
      "Epoch 29, Loss(train/val) 0.67297/0.72741. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67254/0.72825. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66971/0.73061. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66910/0.73265. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66844/0.73470. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66604/0.73742. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66568/0.73646. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66236/0.73815. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65861/0.74093. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65929/0.74065. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65844/0.74477. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65554/0.74595. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65573/0.74942. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65365/0.75128. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65190/0.74929. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65000/0.74915. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64844/0.75215. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64481/0.75375. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64723/0.75566. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64180/0.76092. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64148/0.76016. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63577/0.75730. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63765/0.76458. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.63364/0.76110. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63247/0.76414. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63011/0.76336. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62339/0.76364. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62871/0.76886. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62850/0.76963. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62549/0.77344. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62473/0.77322. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61829/0.77338. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62091/0.77254. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61898/0.77221. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61828/0.77458. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61302/0.77392. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61012/0.77791. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61080/0.77785. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60186/0.77522. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60637/0.77353. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60963/0.78268. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60242/0.77231. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60406/0.77125. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60630/0.77447. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59835/0.77641. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59576/0.77673. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59192/0.78172. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58605/0.77788. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58676/0.78502. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58432/0.78051. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58408/0.77925. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57718/0.78866. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57684/0.78520. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58038/0.79095. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57605/0.78884. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57723/0.79112. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.57730/0.78874. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57387/0.79026. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57014/0.78554. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57099/0.78516. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56278/0.79024. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56816/0.78695. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56146/0.79282. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55586/0.79850. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55441/0.79876. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55804/0.79486. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55162/0.79746. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54998/0.79041. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54462/0.79462. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.55194/0.80127. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54910/0.80337. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69374/0.69682. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69171/0.69617. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69071/0.69506. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69121/0.69484. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68926/0.69427. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68878/0.69367. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68715/0.69330. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68768/0.69336. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68656/0.69352. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68611/0.69370. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68423/0.69371. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68356/0.69387. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68399/0.69320. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68283/0.69395. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68180/0.69396. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67918/0.69399. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67873/0.69436. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68117/0.69548. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67798/0.69540. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67657/0.69576. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67545/0.69619. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67596/0.69682. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67587/0.69664. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67184/0.69691. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67108/0.69779. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67072/0.69840. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67061/0.69749. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66871/0.69905. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66667/0.69959. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66644/0.69967. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66448/0.69963. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66106/0.69987. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65911/0.69962. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66018/0.70073. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65499/0.70082. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65424/0.70115. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65431/0.70172. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65044/0.70124. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64690/0.70104. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64466/0.70211. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64200/0.70231. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63848/0.70350. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63891/0.70379. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62791/0.70421. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62902/0.70526. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62962/0.70633. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62167/0.70842. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61971/0.71097. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62061/0.71307. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61399/0.71412. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60816/0.71449. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60565/0.71812. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60606/0.71929. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.60081/0.72105. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59818/0.72472. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59466/0.72544. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59719/0.72621. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.59446/0.72813. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.58926/0.73178. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.58694/0.73356. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.58746/0.73319. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.58222/0.73649. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.57806/0.74012. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57847/0.74256. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57538/0.74392. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56806/0.74853. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57582/0.75266. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57058/0.75093. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56828/0.75175. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56174/0.75575. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55807/0.75876. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.55914/0.76023. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55798/0.76068. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.54964/0.76419. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55138/0.77090. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54918/0.77232. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54611/0.77455. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53693/0.77839. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54698/0.77785. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53350/0.78662. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53120/0.79156. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53172/0.79820. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53038/0.80053. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.52617/0.80556. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.52091/0.81377. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51336/0.81644. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51620/0.82387. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51851/0.82527. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51463/0.83049. Took 0.14 sec\n",
      "Epoch 89, Loss(train/val) 0.50771/0.83287. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50695/0.83784. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50582/0.84028. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50017/0.84224. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50489/0.85537. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48941/0.86126. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48509/0.87277. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48785/0.88176. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49400/0.88038. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49230/0.88480. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.48143/0.88629. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69490/0.69225. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69219. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69334/0.69210. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69230/0.69216. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69232/0.69219. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69214/0.69228. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69103/0.69234. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69121/0.69239. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69054/0.69265. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68957/0.69270. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68882/0.69294. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68869/0.69331. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68818/0.69357. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.69418. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68713/0.69476. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68663/0.69537. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68521/0.69574. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68378/0.69633. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68412/0.69709. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68382/0.69774. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68162/0.69840. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68003/0.69915. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68034/0.69915. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68125/0.69975. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67933/0.70060. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67902/0.70117. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67678/0.70205. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67603/0.70239. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67612/0.70294. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67435/0.70369. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67331/0.70529. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67141/0.70627. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67030/0.70740. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66821/0.70763. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66487/0.70810. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66352/0.70923. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66075/0.70930. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66203/0.70917. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65960/0.70924. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65535/0.71104. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65621/0.71104. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65144/0.71231. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65039/0.71287. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64623/0.71326. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64549/0.71313. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64213/0.71402. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63835/0.71481. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63623/0.71612. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63278/0.71479. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62801/0.71639. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63026/0.71806. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62471/0.71991. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62381/0.72028. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61937/0.72292. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61701/0.72505. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61345/0.72691. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61124/0.72705. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60672/0.73028. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60595/0.73614. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60239/0.73746. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59564/0.73885. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59509/0.74027. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59100/0.73965. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58096/0.74582. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58518/0.74681. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57937/0.74764. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57852/0.74881. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56809/0.75058. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57396/0.75497. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.57114/0.75510. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56866/0.75374. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56691/0.75676. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56431/0.75975. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56187/0.76026. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56403/0.76543. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55381/0.76913. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55287/0.77813. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54552/0.77387. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54911/0.77288. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55159/0.77641. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54284/0.77978. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54169/0.77829. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53928/0.77726. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53946/0.78332. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53068/0.78358. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53378/0.78667. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52784/0.79129. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52626/0.78720. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.51732/0.78899. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51735/0.79394. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51368/0.80345. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51669/0.79850. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51520/0.80922. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52000/0.80420. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50405/0.81197. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51133/0.80526. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49464/0.80353. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.50193/0.80640. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49470/0.80737. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49128/0.81317. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70385/0.69129. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69361/0.68972. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69221/0.69160. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69036/0.69329. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68900/0.69548. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68812/0.69763. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68728/0.69956. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68671/0.70173. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68536/0.70392. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68452/0.70636. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68429/0.70820. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68320/0.71078. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68326/0.71277. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68282/0.71389. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68288/0.71503. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68205/0.71594. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68139/0.71711. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68136/0.71845. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68126/0.71967. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68187/0.71951. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68063/0.72052. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67900/0.72159. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67866/0.72164. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67868/0.72242. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67787/0.72319. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67934/0.72371. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67804/0.72374. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67641/0.72489. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67659/0.72544. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67739/0.72485. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67702/0.72516. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67412/0.72723. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67451/0.72751. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67514/0.72837. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67364/0.72856. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67316/0.72823. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67322/0.72979. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67329/0.72855. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67212/0.72991. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67165/0.72927. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67343/0.72955. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67087/0.73102. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67044/0.73145. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67171/0.73069. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67007/0.73277. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67008/0.73144. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66954/0.73218. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66695/0.73354. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66912/0.73387. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66571/0.73624. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66643/0.73760. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66574/0.73589. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66308/0.73725. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66690/0.73918. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66335/0.73913. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66289/0.73991. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66318/0.74104. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66313/0.74309. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66144/0.74302. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66121/0.74350. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65938/0.74606. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66029/0.74558. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65824/0.74398. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65660/0.74636. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65508/0.74718. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65594/0.74846. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65712/0.74803. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65294/0.74834. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65341/0.75187. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65420/0.75071. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65273/0.75130. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64962/0.75456. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65141/0.75504. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64785/0.75782. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64908/0.75806. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64630/0.75994. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64594/0.75921. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64620/0.76193. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64936/0.76282. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64431/0.76286. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64568/0.76457. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64388/0.76504. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64233/0.76579. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64082/0.76896. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63870/0.77024. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63998/0.77144. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63737/0.77490. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63465/0.77419. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63499/0.77757. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63193/0.77828. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62893/0.78459. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62985/0.78395. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63011/0.78685. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62873/0.78746. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62989/0.78717. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62736/0.78937. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62351/0.79094. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62795/0.79273. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62380/0.79507. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62329/0.79332. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69208/0.69409. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69178/0.69443. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69159/0.69481. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.69525. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69133/0.69562. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.69620. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69047/0.69678. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69001/0.69731. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68959/0.69768. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68889/0.69823. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68903/0.69865. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.69890. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68795/0.69911. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68710/0.69926. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68631/0.69935. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68547/0.69984. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68502/0.70057. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68506/0.70065. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68442/0.70034. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68287/0.70121. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68230/0.70229. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68021/0.70230. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67914/0.70258. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67887/0.70193. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67828/0.70180. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67731/0.70275. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67488/0.70287. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67619/0.70249. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67517/0.70365. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67324/0.70245. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67343/0.70249. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67523/0.70112. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67405/0.70156. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67187/0.70230. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67258/0.70121. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66731/0.70168. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66825/0.70345. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66598/0.70134. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66570/0.70299. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66832/0.70403. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66543/0.70407. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66493/0.70345. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66344/0.70224. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66290/0.70215. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66155/0.70214. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65721/0.70181. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65757/0.70427. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65622/0.70468. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65643/0.70284. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65518/0.70368. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65387/0.69869. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65218/0.70187. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65084/0.70239. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64853/0.70555. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64887/0.70212. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64451/0.70443. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64579/0.70463. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64605/0.70130. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64121/0.70024. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64083/0.69895. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64617/0.69903. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63856/0.70356. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63433/0.70174. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63272/0.70010. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63364/0.70330. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63139/0.70371. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62757/0.70072. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62938/0.70057. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62458/0.70093. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62660/0.69587. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62222/0.69836. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61758/0.69690. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61770/0.69710. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61906/0.69797. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61547/0.69871. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61421/0.69765. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61302/0.69811. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60762/0.69668. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60866/0.69772. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60731/0.69849. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60117/0.69798. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59879/0.70057. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59877/0.70314. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59985/0.70500. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58938/0.70551. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59425/0.70759. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59185/0.70826. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59210/0.70838. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58793/0.71044. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58290/0.71644. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58721/0.71358. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58010/0.71336. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57984/0.71453. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57509/0.71644. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57415/0.72182. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57168/0.72520. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56646/0.72778. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55941/0.72648. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56808/0.72848. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56607/0.72906. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69222. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69242/0.69263. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69122/0.69340. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68886/0.69426. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68808/0.69554. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68762/0.69678. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68548/0.69787. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68467/0.69906. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68419/0.70039. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68314/0.70141. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68301/0.70282. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68225/0.70408. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68090/0.70525. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68046/0.70567. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68028/0.70631. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67891/0.70667. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67809/0.70743. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67720/0.70796. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67729/0.70861. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67784/0.70857. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67700/0.70940. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67652/0.71003. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67524/0.71142. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67448/0.71236. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67391/0.71275. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67535/0.71287. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67270/0.71372. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67178/0.71360. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67227/0.71324. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66952/0.71410. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67007/0.71433. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66961/0.71391. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67083/0.71422. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66878/0.71454. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66757/0.71508. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66769/0.71588. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66477/0.71753. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66753/0.71736. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66489/0.71630. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66537/0.71713. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66299/0.71679. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66203/0.71778. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65992/0.71877. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66271/0.71856. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66137/0.71848. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65702/0.71810. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65571/0.71853. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65598/0.71899. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65323/0.71971. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65448/0.72161. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65500/0.72087. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65245/0.72031. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64947/0.71984. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64749/0.71992. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64816/0.72141. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64634/0.72244. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64306/0.72171. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63945/0.72233. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64026/0.72244. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64116/0.72239. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63759/0.72323. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63744/0.72419. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63426/0.72447. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63069/0.72649. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63074/0.72817. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62659/0.72707. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62606/0.72664. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62825/0.72717. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62556/0.72750. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62657/0.72851. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62003/0.72764. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62015/0.72920. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61854/0.73142. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61918/0.73242. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61524/0.73069. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61469/0.73020. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61438/0.73174. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60955/0.73089. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60771/0.73054. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60606/0.73337. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60234/0.73466. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60418/0.73653. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59952/0.73816. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59682/0.73509. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59873/0.73443. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.59677/0.73588. Took 0.12 sec\n",
      "Epoch 86, Loss(train/val) 0.59202/0.73611. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.59080/0.73809. Took 0.13 sec\n",
      "Epoch 88, Loss(train/val) 0.59577/0.73707. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.58304/0.73523. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58380/0.73718. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58141/0.74360. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57818/0.74311. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57833/0.74627. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57766/0.74663. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57576/0.74056. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57378/0.74394. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56612/0.74923. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.56499/0.74743. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56098/0.75085. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69315/0.69205. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.69068. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69158/0.68952. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69048/0.68907. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68967/0.68894. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68802/0.68899. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68794/0.68937. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.69028. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68623/0.69113. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68430/0.69212. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68311/0.69275. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68181/0.69383. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68152/0.69465. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68132/0.69470. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68054/0.69584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67867/0.69583. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67933/0.69664. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67751/0.69653. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67726/0.69644. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67597/0.69635. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67586/0.69615. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67487/0.69543. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67467/0.69600. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67427/0.69487. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67380/0.69432. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67257/0.69344. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67221/0.69263. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67265/0.69234. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67143/0.69130. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67134/0.69073. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66936/0.69055. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66994/0.68989. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66971/0.68876. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66997/0.68825. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66671/0.68762. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66867/0.68714. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66733/0.68652. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66689/0.68531. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66555/0.68476. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66473/0.68344. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66525/0.68348. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66481/0.68267. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66533/0.68214. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66224/0.68214. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66219/0.68107. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66419/0.68010. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66219/0.68002. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66116/0.67952. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65990/0.67903. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66012/0.67793. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65825/0.67723. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65700/0.67738. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65718/0.67657. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65502/0.67571. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65560/0.67510. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65486/0.67415. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65460/0.67368. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65170/0.67297. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65339/0.67297. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65031/0.67160. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64977/0.67151. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64959/0.67119. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64833/0.67012. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64753/0.66881. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64572/0.66895. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64501/0.66843. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64047/0.66798. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64190/0.66752. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64273/0.66787. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63805/0.66779. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63982/0.66654. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64401/0.66780. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63883/0.66789. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63904/0.66757. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63826/0.66698. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63537/0.66631. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63509/0.66640. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62988/0.66695. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63301/0.66754. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63009/0.66558. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62577/0.66708. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62505/0.66596. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62241/0.66635. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62136/0.66571. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62270/0.66560. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62196/0.66509. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61811/0.66702. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61910/0.66701. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61226/0.66619. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61677/0.66537. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61604/0.66608. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61022/0.66783. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61031/0.66806. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60781/0.67043. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60213/0.66784. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60277/0.66716. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60032/0.66957. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59939/0.66662. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60015/0.66768. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59579/0.66976. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69028/0.68678. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68882/0.68772. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68781/0.68824. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68781/0.68925. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68706/0.69056. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68597/0.69192. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68526/0.69336. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68448/0.69510. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68428/0.69727. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68361/0.69893. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68288/0.70107. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68191/0.70296. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68234/0.70437. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68122/0.70630. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68230/0.70702. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68122/0.70801. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68057/0.70980. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67864/0.71159. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68004/0.71234. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67890/0.71333. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68017/0.71377. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67784/0.71458. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67811/0.71540. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67779/0.71594. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67684/0.71629. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67840/0.71625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67668/0.71664. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67522/0.71742. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67553/0.71841. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67596/0.71867. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67529/0.71862. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67526/0.71845. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67422/0.71918. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67273/0.72002. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67179/0.71986. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67303/0.72006. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67083/0.72010. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66928/0.72064. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67033/0.72099. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66873/0.72106. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66784/0.72081. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66816/0.72067. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66681/0.72057. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66663/0.72082. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66637/0.72052. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66551/0.72080. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66521/0.72031. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66412/0.72034. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66191/0.72116. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66227/0.72058. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65927/0.72224. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66039/0.72185. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66006/0.72130. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65913/0.72184. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65746/0.72146. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65795/0.72227. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65680/0.72173. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65516/0.72226. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65559/0.72195. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65437/0.72270. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64924/0.72245. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64994/0.72498. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65084/0.72387. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64973/0.72292. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64573/0.72163. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64766/0.72287. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64599/0.72223. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64326/0.72432. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64154/0.72203. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64156/0.72501. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64067/0.72379. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63621/0.72498. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63451/0.72537. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63047/0.72787. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63346/0.72735. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62907/0.72681. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62970/0.72783. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62436/0.72884. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62563/0.72765. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62486/0.72586. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62311/0.72813. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62138/0.72862. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62014/0.72455. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61579/0.72707. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61120/0.72970. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61238/0.72865. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61330/0.73111. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60619/0.72742. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60052/0.72954. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60030/0.73393. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59896/0.73583. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59947/0.73205. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59757/0.73419. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59624/0.73319. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59522/0.73478. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59117/0.73847. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58496/0.73820. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58923/0.73217. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58884/0.73665. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57734/0.73974. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69069/0.69899. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68769/0.69574. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68676/0.69325. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68611/0.69118. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68487/0.68954. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68410/0.68824. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68309/0.68779. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68224/0.68690. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68281/0.68677. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68257/0.68672. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68109/0.68615. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67997/0.68648. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68064/0.68699. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67961/0.68652. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68012/0.68680. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68016/0.68725. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67858/0.68748. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67876/0.68727. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67818/0.68671. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67850/0.68700. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67727/0.68707. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67757/0.68722. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67764/0.68729. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67656/0.68759. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67694/0.68757. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67612/0.68724. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67624/0.68774. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67579/0.68768. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67563/0.68778. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67503/0.68747. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67472/0.68806. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67467/0.68679. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67425/0.68733. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67500/0.68702. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67333/0.68722. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67271/0.68733. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67206/0.68684. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67073/0.68684. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67232/0.68696. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67117/0.68688. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67039/0.68631. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66977/0.68655. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66895/0.68620. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67141/0.68647. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66886/0.68649. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66893/0.68584. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66791/0.68505. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66650/0.68681. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66725/0.68631. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66637/0.68656. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66563/0.68522. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66525/0.68563. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66453/0.68536. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66417/0.68477. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66188/0.68484. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66269/0.68466. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66391/0.68579. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66440/0.68484. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66171/0.68642. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66078/0.68519. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66085/0.68493. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65935/0.68535. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65726/0.68569. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65649/0.68476. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65611/0.68532. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65645/0.68611. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65541/0.68689. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65507/0.68678. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65572/0.68683. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65523/0.68705. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65192/0.68786. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65226/0.68876. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65057/0.68975. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65066/0.68903. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.65082/0.69072. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65155/0.68743. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65084/0.69004. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65015/0.68861. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64575/0.69020. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64617/0.69019. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.64563/0.69157. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64482/0.69272. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64426/0.69024. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64310/0.69101. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64013/0.69327. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64192/0.69310. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64022/0.69503. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64103/0.69374. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63775/0.69269. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64002/0.69596. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63637/0.69557. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.63956/0.69704. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63959/0.69521. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63510/0.69761. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63516/0.69841. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63415/0.69829. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63283/0.69939. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63173/0.69996. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62898/0.70048. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62930/0.70050. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69464/0.69797. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69095/0.69858. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69066/0.69791. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68923/0.69819. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68788/0.69828. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68813/0.69889. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68672/0.69995. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68666/0.70066. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68582/0.70111. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68496/0.70159. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68546/0.70246. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68475/0.70336. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68380/0.70352. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68322/0.70417. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68196/0.70416. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68386/0.70480. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68192/0.70472. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68272/0.70456. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68104/0.70486. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68041/0.70459. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68107/0.70519. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67877/0.70484. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67966/0.70446. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67914/0.70415. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67875/0.70386. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67589/0.70342. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67679/0.70395. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67607/0.70421. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67697/0.70376. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67430/0.70405. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67504/0.70436. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67344/0.70305. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67258/0.70314. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67334/0.70299. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67045/0.70447. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66879/0.70438. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67169/0.70372. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66965/0.70316. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66823/0.70307. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66769/0.70379. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66636/0.70442. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66464/0.70485. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66579/0.70627. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66415/0.70705. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66476/0.70737. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66463/0.70759. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66130/0.70628. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66060/0.70775. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66122/0.70781. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65852/0.70890. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65888/0.70950. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65825/0.70845. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65556/0.70832. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65297/0.70936. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65435/0.71078. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65579/0.71241. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65349/0.71091. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65128/0.71272. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65257/0.71347. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65101/0.71376. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65060/0.71558. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64865/0.71659. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64512/0.71752. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64418/0.71919. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64760/0.71834. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64276/0.71858. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64477/0.71949. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64571/0.71910. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63977/0.71964. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63982/0.72134. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63975/0.72447. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64053/0.72495. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63322/0.72732. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63860/0.72604. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63474/0.72704. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63230/0.72898. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63274/0.73004. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62936/0.72844. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63342/0.73140. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62428/0.73226. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62725/0.73267. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62535/0.73074. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62306/0.73261. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62087/0.73459. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62520/0.73337. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62104/0.73675. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62190/0.73550. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61748/0.73913. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62193/0.74234. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61770/0.74101. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61063/0.74079. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61744/0.74319. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60851/0.74446. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61433/0.74127. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60622/0.74882. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.60656/0.75184. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60642/0.75030. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61019/0.75342. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60567/0.74935. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59925/0.75587. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69058/0.68942. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68904/0.68994. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68794/0.69016. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68644/0.69033. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68557/0.69125. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68518/0.69173. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68313/0.69239. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68418/0.69309. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68345/0.69381. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68223/0.69436. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68184/0.69527. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68128/0.69604. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68053/0.69633. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67956/0.69704. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67941/0.69742. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67872/0.69810. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67765/0.69895. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67732/0.69909. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67656/0.69976. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67597/0.70028. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67523/0.70079. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67555/0.70154. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67507/0.70216. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67356/0.70249. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67306/0.70320. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67062/0.70396. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67248/0.70420. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67248/0.70434. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67021/0.70496. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66867/0.70543. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66997/0.70698. Took 0.15 sec\n",
      "Epoch 31, Loss(train/val) 0.66744/0.70663. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66726/0.70726. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66567/0.70789. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66483/0.70861. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66479/0.70969. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66262/0.71070. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66347/0.71222. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66222/0.71303. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66021/0.71360. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65969/0.71432. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65791/0.71604. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65948/0.71671. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65814/0.71769. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65648/0.71918. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65704/0.72069. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65867/0.72144. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65318/0.72251. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65328/0.72433. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65294/0.72493. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64977/0.72579. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65119/0.72825. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64962/0.73011. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65006/0.73174. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64919/0.73133. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64995/0.73239. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64805/0.73413. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64471/0.73602. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64424/0.73751. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63940/0.73870. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64232/0.73972. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63795/0.74294. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64110/0.74328. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63587/0.74413. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63691/0.74588. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63245/0.74695. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63263/0.74844. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63179/0.74951. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63229/0.75192. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62884/0.75338. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63148/0.75641. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62829/0.75706. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62425/0.75912. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62382/0.75969. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62151/0.76134. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62440/0.76232. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.62134/0.76351. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61948/0.76447. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61785/0.76593. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61398/0.76748. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61474/0.76830. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60598/0.77108. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61264/0.77389. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61263/0.77407. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60411/0.77494. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60502/0.77700. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61020/0.78017. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60329/0.77861. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60305/0.78132. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60277/0.78229. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60100/0.78381. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59779/0.78613. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59099/0.78866. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58992/0.78931. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59044/0.79402. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58963/0.79266. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58809/0.79576. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58812/0.79730. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58600/0.79950. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58044/0.80082. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69821/0.70755. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69721/0.70695. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69725/0.70577. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69584/0.70279. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69367/0.69768. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.69182. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69116/0.68699. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69024/0.68362. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68938/0.68117. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68998/0.67974. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68799/0.67861. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68835/0.67774. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68811/0.67691. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68724/0.67636. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68690/0.67589. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68655/0.67435. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68473/0.67416. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68293/0.67340. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68400/0.67187. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68272/0.67110. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68178/0.67160. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68129/0.67122. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68115/0.67025. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67970/0.67000. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67840/0.67165. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67778/0.67046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67684/0.67086. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67450/0.67344. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67402/0.67262. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67178/0.67362. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67063/0.67245. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66931/0.67617. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66948/0.67637. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66722/0.67687. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66523/0.67670. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66458/0.68006. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66357/0.68051. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66294/0.67992. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66165/0.67941. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65810/0.68413. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65635/0.68045. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65389/0.68252. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65275/0.68533. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65292/0.68431. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64742/0.68500. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64820/0.68732. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64364/0.68661. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64269/0.68664. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64266/0.68907. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63449/0.68898. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63537/0.68807. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63439/0.69249. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.63304/0.69340. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62969/0.68991. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62806/0.69697. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62531/0.69287. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62511/0.69886. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61939/0.69936. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61909/0.70077. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61639/0.70086. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61853/0.70549. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61581/0.70526. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60904/0.70917. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61055/0.71143. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60539/0.71682. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60466/0.71625. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59907/0.71386. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60336/0.71564. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59543/0.71582. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59536/0.72271. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59432/0.72211. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58941/0.72413. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58714/0.72976. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58815/0.72437. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58634/0.73397. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58052/0.73741. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57840/0.74086. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57891/0.74166. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57206/0.74662. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57519/0.74369. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57603/0.74813. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57314/0.75736. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.56333/0.75781. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56622/0.75754. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56093/0.75927. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56137/0.76283. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55536/0.76658. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55604/0.76513. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55434/0.77363. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55754/0.78345. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55258/0.78116. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55066/0.78418. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54259/0.78303. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54824/0.78821. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54103/0.78923. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53673/0.79856. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53421/0.79958. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53233/0.80556. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53311/0.80284. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52999/0.80864. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69498/0.69603. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69435/0.69487. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69368/0.69398. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69286/0.69300. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69256/0.69229. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69145/0.69186. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69178/0.69155. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69064/0.69150. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68987/0.69161. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69015/0.69164. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68945/0.69185. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68891/0.69218. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68696/0.69286. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68669/0.69379. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68632/0.69479. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68402/0.69612. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68453/0.69809. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68390/0.69928. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68165/0.70183. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68179/0.70309. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68024/0.70529. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67750/0.70733. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67779/0.71000. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67404/0.71180. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67371/0.71445. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67328/0.71546. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67300/0.71697. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67257/0.71914. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66981/0.71991. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66946/0.72176. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66889/0.72271. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66577/0.72438. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66884/0.72466. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66490/0.72509. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66493/0.72611. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66293/0.72652. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65972/0.72612. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66117/0.72656. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65963/0.72715. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65895/0.72675. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65693/0.72614. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65733/0.72649. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65270/0.72614. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65241/0.72701. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65235/0.72843. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64992/0.72819. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64849/0.72756. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.64853/0.72785. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64604/0.72824. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64599/0.72800. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64423/0.72902. Took 0.15 sec\n",
      "Epoch 51, Loss(train/val) 0.64189/0.72864. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64209/0.72947. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63990/0.72817. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63707/0.72948. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63646/0.72800. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63844/0.72797. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63319/0.73014. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63116/0.73014. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63051/0.72967. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63075/0.72913. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63097/0.72929. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62787/0.72809. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62383/0.73087. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62263/0.73030. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62384/0.72996. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61709/0.72949. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61752/0.72999. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61895/0.73095. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60890/0.73460. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61076/0.73360. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.61537/0.73567. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.61095/0.73837. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60451/0.73625. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60746/0.73455. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59923/0.73727. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60517/0.73927. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59479/0.73819. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59401/0.74202. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59743/0.74041. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59962/0.74154. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58747/0.74266. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59050/0.74416. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59530/0.74444. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58857/0.74889. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58583/0.75115. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58689/0.74894. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58279/0.74928. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57518/0.75494. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57485/0.75301. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57584/0.75443. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56709/0.75360. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57672/0.75469. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56881/0.75769. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56600/0.75631. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57257/0.75716. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56387/0.75907. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55645/0.76561. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.55813/0.76650. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55932/0.76645. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69318/0.69573. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69228/0.69633. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69191/0.69669. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69196/0.69694. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69741. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.69798. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69008/0.69855. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68875/0.69959. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68801/0.70056. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68772/0.70171. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68609/0.70341. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68481/0.70452. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68473/0.70600. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68205/0.70754. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68157/0.70887. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68109/0.71037. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67920/0.71215. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67909/0.71240. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67678/0.71595. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67579/0.71633. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67503/0.71803. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67388/0.71884. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67208/0.71967. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67177/0.72225. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66980/0.72378. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66793/0.72501. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66666/0.72622. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.66474/0.72703. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66342/0.72951. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66257/0.73157. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65938/0.73093. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65959/0.73287. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65730/0.73414. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65285/0.73435. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65294/0.73687. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64950/0.73802. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64987/0.74170. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64654/0.73930. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64704/0.73855. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64549/0.74205. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64046/0.74649. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64220/0.74828. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63903/0.74886. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63710/0.74957. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63618/0.74850. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63244/0.75218. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63616/0.75335. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62995/0.75749. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63212/0.75612. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62492/0.75529. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62714/0.75805. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62964/0.76011. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62583/0.76066. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62047/0.76373. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61756/0.76234. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62057/0.76744. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61776/0.76529. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61370/0.76781. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61462/0.76724. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61518/0.76989. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61269/0.77143. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60998/0.77304. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60792/0.77216. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60968/0.77417. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60598/0.77299. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59765/0.77782. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59745/0.78117. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59404/0.78308. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59596/0.78302. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59177/0.78535. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59534/0.78584. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58899/0.78671. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58879/0.78581. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58396/0.78617. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58476/0.79245. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58072/0.78921. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58200/0.79715. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57855/0.79678. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57913/0.79765. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57635/0.79560. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56888/0.80022. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56588/0.80043. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56410/0.80456. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57019/0.80079. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56570/0.80161. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56883/0.80428. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55625/0.81051. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55872/0.80487. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55190/0.80869. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55558/0.81791. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54877/0.80895. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53988/0.81317. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54236/0.81483. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54223/0.82075. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53554/0.82167. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54081/0.82550. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53448/0.82977. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53960/0.82202. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52793/0.82917. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52282/0.83335. Took 0.08 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69298/0.69611. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69239/0.69774. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69208/0.69888. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69108/0.69929. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69027/0.70026. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68967/0.70140. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68960/0.70230. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68974/0.70323. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68837/0.70412. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68854/0.70496. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68715/0.70612. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68753/0.70713. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68583/0.70817. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68693/0.70900. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68690/0.70883. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68558/0.70965. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68475/0.71023. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68387/0.71140. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68334/0.71214. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68498/0.71177. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68384/0.71233. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68215/0.71284. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68215/0.71281. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68189/0.71246. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68033/0.71311. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67817/0.71380. Took 0.13 sec\n",
      "Epoch 26, Loss(train/val) 0.67969/0.71385. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67743/0.71377. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67663/0.71520. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67636/0.71363. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67454/0.71457. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67381/0.71344. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67273/0.71301. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67288/0.71255. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66832/0.71360. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66936/0.71255. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66481/0.71417. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66436/0.71304. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66404/0.71360. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66150/0.71368. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65854/0.71461. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65817/0.71591. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65547/0.71556. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65387/0.71547. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65568/0.71592. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65185/0.71762. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65136/0.71747. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.64738/0.71736. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64567/0.71862. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64229/0.72129. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64496/0.72213. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63861/0.72342. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63678/0.72437. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63680/0.72510. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63845/0.72428. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63287/0.72697. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63159/0.72845. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62892/0.72947. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62780/0.72896. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62485/0.73167. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62730/0.73349. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62310/0.73389. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61975/0.73582. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61921/0.73807. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62162/0.73851. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61499/0.73889. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61411/0.73978. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61225/0.74019. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60456/0.74114. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60451/0.74471. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60517/0.74633. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60379/0.74851. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59991/0.75024. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59825/0.74833. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59754/0.75028. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59925/0.75234. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59264/0.75341. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58861/0.75481. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58696/0.75611. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58962/0.75739. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58641/0.75622. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58011/0.75726. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57576/0.75672. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57881/0.75683. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57750/0.75944. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56775/0.76244. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56872/0.76714. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56976/0.76645. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.56435/0.76576. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56710/0.77024. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56084/0.76666. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56166/0.77014. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56030/0.77159. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55009/0.77325. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54609/0.77663. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54991/0.77648. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54112/0.77669. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54348/0.77536. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54213/0.77266. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54336/0.77726. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69273/0.69096. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69226/0.69108. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69182/0.69098. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.69091. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69008/0.69069. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68986/0.69074. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68963/0.69096. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68935/0.69098. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68821/0.69153. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68809/0.69207. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68774/0.69243. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68616/0.69271. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68616/0.69385. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68544/0.69487. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68491/0.69547. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68502/0.69643. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68305/0.69752. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68362/0.69920. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68258/0.70042. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68147/0.70103. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68102/0.70255. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68002/0.70366. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67970/0.70491. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67759/0.70712. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67689/0.70824. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67722/0.70937. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67634/0.71026. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67391/0.71230. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67304/0.71475. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67354/0.71565. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67138/0.71723. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67053/0.71902. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66953/0.72117. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67065/0.72161. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66901/0.72349. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66521/0.72428. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66402/0.72725. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66398/0.72747. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66171/0.72877. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65901/0.73150. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65671/0.73460. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65667/0.73702. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65746/0.73916. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65775/0.74168. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65673/0.74419. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65165/0.74605. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65070/0.74700. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65246/0.75103. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65017/0.75227. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64623/0.75796. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64541/0.75910. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64640/0.76383. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64481/0.76496. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64396/0.76936. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63566/0.77253. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63692/0.77837. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63267/0.78060. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63421/0.78370. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63235/0.78955. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62924/0.79243. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63172/0.79649. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62724/0.79744. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62141/0.79931. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62629/0.80865. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62608/0.80905. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61677/0.81331. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61783/0.81936. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61332/0.82331. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61775/0.82580. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61252/0.83067. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60992/0.82977. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60941/0.83531. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60379/0.84078. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60117/0.84234. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60160/0.85333. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59089/0.85523. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59630/0.86517. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59600/0.86627. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59300/0.87615. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58956/0.87566. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58726/0.89029. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58835/0.88914. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58519/0.89705. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58592/0.89559. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57206/0.90078. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58167/0.90678. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57509/0.90680. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57868/0.92004. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56560/0.92081. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56473/0.93348. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56108/0.93218. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57139/0.92749. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56021/0.94344. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55095/0.95118. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55466/0.95663. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55442/0.95429. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55539/0.95778. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54142/0.97207. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54619/0.97721. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53872/0.97502. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69520/0.69695. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69421/0.69602. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69420/0.69499. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69360/0.69379. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69186/0.69254. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69241/0.69153. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69181/0.69070. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69164/0.69019. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69147/0.68984. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69053/0.68961. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.68953. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68853/0.68906. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68808/0.68826. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68775/0.68750. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68717/0.68663. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68664/0.68628. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.68583. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68680/0.68568. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68545/0.68553. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68324/0.68506. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68313/0.68444. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68181/0.68352. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68039/0.68277. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68027/0.68239. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67868/0.68158. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67914/0.68115. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67539/0.68014. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67430/0.68029. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67279/0.68009. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67265/0.67836. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67043/0.67816. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66686/0.67720. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66790/0.67580. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66656/0.67634. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66427/0.67541. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65908/0.67454. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65962/0.67485. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66274/0.67364. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65486/0.67502. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65385/0.67438. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65416/0.67353. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65212/0.67320. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65167/0.67366. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65070/0.67229. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64515/0.67234. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64473/0.67161. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64574/0.67300. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64112/0.67179. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64047/0.67159. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63450/0.67212. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62795/0.67353. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63614/0.67132. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62901/0.67213. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63182/0.67282. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62920/0.67285. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.62438/0.67103. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62213/0.67132. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62343/0.66966. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61926/0.67104. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61505/0.67121. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61555/0.67228. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61007/0.66994. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61299/0.67191. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60991/0.67511. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60084/0.67436. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60614/0.67778. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60374/0.67453. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60013/0.67615. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60347/0.67549. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59936/0.67859. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59303/0.68484. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58824/0.68507. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59012/0.68355. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58227/0.68798. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58568/0.68832. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58477/0.68963. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57756/0.68736. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57945/0.68970. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56674/0.69875. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.57592/0.69830. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57217/0.69712. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56082/0.70390. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56337/0.69962. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.55151/0.70685. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56113/0.71269. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55360/0.70768. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55311/0.71307. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.54272/0.71879. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54177/0.73188. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54647/0.72324. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54251/0.72152. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53817/0.72661. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54302/0.73537. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53237/0.73672. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51917/0.74795. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52386/0.74505. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52208/0.74426. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52512/0.74446. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51283/0.75258. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51480/0.76339. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69599/0.69299. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69400/0.69240. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69293/0.69208. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69363/0.69202. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69259/0.69199. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69318/0.69195. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69109/0.69198. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69209/0.69202. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69157/0.69197. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69111/0.69190. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69018/0.69202. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69056/0.69221. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.69218. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68829/0.69224. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68779/0.69213. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68759/0.69232. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68652/0.69236. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68493/0.69241. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68447/0.69256. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.69215. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68198/0.69251. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68076/0.69214. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67951/0.69171. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67835/0.69120. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67549/0.69063. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67159/0.68975. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67266/0.68923. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67089/0.68859. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67053/0.68797. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66739/0.68707. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66754/0.68567. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66282/0.68467. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66092/0.68399. Took 0.12 sec\n",
      "Epoch 33, Loss(train/val) 0.66098/0.68375. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65923/0.68257. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65921/0.68150. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.65641/0.68150. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65149/0.68122. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65190/0.68067. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.65238/0.68066. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65007/0.67929. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64496/0.67803. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.64234/0.67992. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64370/0.68107. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64122/0.68031. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63746/0.67939. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63260/0.68179. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63563/0.68032. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63021/0.68030. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.62958/0.68074. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62782/0.67936. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62598/0.68089. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62031/0.67974. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61602/0.68082. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61083/0.67832. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61511/0.68063. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.61082/0.68048. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61048/0.68103. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60788/0.67957. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.60850/0.67942. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59854/0.68385. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60006/0.68116. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.59713/0.68013. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59320/0.68549. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59716/0.68296. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59156/0.68641. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58513/0.68595. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58290/0.68703. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.58285/0.68921. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57769/0.69160. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57492/0.70062. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57839/0.70338. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56600/0.69669. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56718/0.68911. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56832/0.69645. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56691/0.69671. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56254/0.70278. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55549/0.70049. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55814/0.70627. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.55228/0.70559. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54640/0.70287. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54903/0.70180. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54606/0.70801. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.53735/0.71472. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54104/0.71291. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53604/0.70614. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53702/0.72237. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53264/0.71979. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52318/0.71893. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.52648/0.72383. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.52835/0.73292. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.51680/0.72919. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51386/0.73505. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51401/0.74131. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50655/0.75027. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51460/0.73477. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50969/0.73053. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50706/0.75139. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.50028/0.75107. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49859/0.75762. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69485/0.69050. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69082. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69341/0.69153. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69297/0.69178. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69258/0.69186. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69249/0.69171. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69209/0.69195. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69162/0.69254. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69108/0.69223. Took 0.14 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.69220. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69061/0.69240. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68979/0.69265. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68913/0.69256. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68762/0.69257. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68742/0.69290. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68694/0.69257. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68698/0.69295. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68466/0.69271. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68394/0.69302. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68306/0.69257. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68282/0.69350. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68238/0.69372. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68023/0.69227. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67970/0.69259. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67863/0.69126. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67571/0.69244. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67762/0.69246. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67392/0.69202. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67304/0.69175. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67217/0.69212. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66942/0.69202. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66776/0.69281. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66799/0.69483. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66719/0.69414. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66565/0.69263. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66445/0.69545. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66038/0.69395. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65960/0.69682. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65575/0.69638. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65689/0.70117. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65302/0.69806. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65264/0.70286. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64879/0.70340. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64653/0.70340. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64726/0.70610. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64733/0.70544. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64061/0.70427. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63895/0.70980. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63767/0.71290. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63686/0.71637. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63996/0.71549. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63429/0.71884. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63087/0.71778. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63086/0.71941. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62804/0.71871. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62526/0.72295. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62275/0.72362. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62008/0.72746. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61810/0.72932. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61274/0.73644. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61184/0.73739. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61057/0.74040. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60594/0.73998. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60399/0.74755. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60400/0.74714. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60150/0.75267. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60165/0.75607. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59002/0.76301. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59175/0.76317. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59139/0.76267. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59241/0.76460. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58299/0.77193. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58108/0.76701. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58195/0.77316. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57525/0.77352. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57438/0.78205. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56507/0.78720. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56853/0.79186. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56236/0.79839. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55674/0.79842. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56090/0.80514. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55866/0.81428. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56124/0.80730. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.54736/0.81217. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54991/0.82304. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54837/0.82462. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54258/0.82392. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53786/0.83134. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54399/0.83286. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53195/0.84606. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53523/0.84245. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52670/0.84478. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53333/0.84123. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52500/0.84070. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52802/0.85186. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52490/0.85246. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51645/0.86372. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52229/0.86601. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51524/0.86310. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51107/0.85602. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70032/0.68512. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69485/0.68999. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.69004. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69346/0.69048. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69279/0.69040. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69230/0.69037. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69270/0.69058. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69164/0.69063. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69199/0.69087. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69193/0.69109. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69103/0.69074. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69103/0.69118. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69065/0.69115. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68984/0.69082. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69001/0.69128. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68956/0.69091. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68940/0.69154. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68908/0.69133. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68875/0.69190. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68930/0.69232. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68856/0.69257. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68770/0.69233. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.69293. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68707/0.69288. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68606/0.69335. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68658/0.69330. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68628/0.69368. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68513/0.69419. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68408/0.69464. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68315/0.69457. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68347/0.69537. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68339/0.69685. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68184/0.69645. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68165/0.69677. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68115/0.69712. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67854/0.69730. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67767/0.69708. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67909/0.69927. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67851/0.69914. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67657/0.69821. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67720/0.70000. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67525/0.70140. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67264/0.70074. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67237/0.69990. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67291/0.69987. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67148/0.69985. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66883/0.70149. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66974/0.70125. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67000/0.70102. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66869/0.70050. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66520/0.70100. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66415/0.70150. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66410/0.69948. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.66377/0.70185. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66181/0.70281. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65757/0.70343. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65894/0.70207. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65793/0.70406. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65749/0.70335. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65315/0.70154. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65384/0.70356. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65157/0.70142. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65246/0.70298. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65009/0.70368. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64921/0.70500. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65033/0.70171. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64721/0.70282. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64371/0.70104. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64371/0.70204. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64416/0.70317. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64032/0.70339. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64120/0.70238. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63589/0.70200. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63835/0.70211. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63700/0.69943. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63291/0.69742. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63534/0.69623. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62920/0.70380. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62997/0.70143. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63104/0.70115. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62689/0.70448. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62674/0.69632. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62582/0.69632. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62202/0.69964. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61707/0.70062. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61977/0.69969. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62086/0.69701. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61701/0.70223. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61401/0.69623. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61257/0.70479. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60582/0.69894. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61089/0.70359. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60875/0.70480. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60725/0.70613. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60720/0.70287. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60732/0.70371. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60188/0.70057. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60278/0.70785. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59886/0.70577. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59603/0.70055. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69392. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69219/0.69270. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69248/0.69263. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69244/0.69232. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69198/0.69230. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69147/0.69208. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69090/0.69190. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.69195. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69060/0.69188. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69014/0.69158. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69047/0.69197. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69019/0.69168. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68837/0.69163. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68863/0.69145. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68794/0.69163. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68779/0.69147. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68642/0.69117. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68594/0.69136. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68569/0.69123. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68666/0.69089. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68454/0.69102. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68379/0.69075. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68453/0.69121. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68298/0.69129. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68299/0.69095. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68190/0.69107. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68140/0.69055. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67871/0.69027. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67684/0.68986. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67912/0.68989. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67669/0.68986. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67703/0.68886. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67520/0.68849. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67217/0.68790. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67388/0.68730. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67324/0.68746. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67037/0.68729. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66913/0.68795. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66909/0.68679. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66670/0.68555. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.66519/0.68636. Took 0.11 sec\n",
      "Epoch 41, Loss(train/val) 0.66072/0.68580. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65967/0.68442. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66019/0.68431. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65693/0.68356. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.65210/0.68364. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65168/0.68381. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65123/0.68237. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64936/0.68173. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64626/0.68182. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64576/0.68207. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64530/0.68458. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63983/0.68337. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64282/0.68378. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63691/0.68159. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.63342/0.68452. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63098/0.68440. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63171/0.68547. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63108/0.68844. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62578/0.68855. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62382/0.68969. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62374/0.68966. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62243/0.68875. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.61925/0.69175. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62135/0.69248. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61697/0.69556. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61389/0.69811. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61101/0.69907. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61249/0.69864. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60952/0.69996. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60783/0.70061. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60402/0.69758. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60446/0.70183. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59976/0.69789. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59847/0.70273. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59658/0.70374. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58753/0.70192. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58791/0.70088. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58577/0.70179. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58981/0.69924. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58772/0.70184. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58205/0.70569. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57684/0.70771. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57916/0.70942. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57433/0.71393. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56580/0.71500. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57310/0.71620. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56836/0.71668. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55882/0.71672. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55912/0.71934. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55936/0.72084. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55920/0.72411. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55469/0.72362. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55313/0.72570. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54925/0.72533. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54603/0.72595. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54589/0.72863. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53717/0.73327. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54020/0.73589. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53632/0.73561. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69514/0.69014. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69258/0.68990. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69222/0.68985. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69178/0.68979. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69061/0.68971. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.68955. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68805/0.68964. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.68962. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68565/0.68962. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68428/0.68962. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68322/0.68986. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68049/0.68976. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.67824/0.69070. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67608/0.69162. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67528/0.69271. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67320/0.69295. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67171/0.69360. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.66971/0.69465. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.66658/0.69619. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66466/0.69727. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.66100/0.69819. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66190/0.69965. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.65923/0.70093. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.65801/0.70248. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.65385/0.70456. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.65414/0.70616. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.65050/0.70830. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.64967/0.71032. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.64663/0.71177. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64591/0.71335. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64228/0.71518. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.63753/0.71790. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.63741/0.72019. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.63411/0.72254. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.63339/0.72372. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63482/0.72464. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.62826/0.72730. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63113/0.73011. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62636/0.73347. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62533/0.73548. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62374/0.73480. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.62134/0.73771. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62267/0.73832. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62286/0.73796. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61237/0.74121. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61603/0.74411. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61131/0.74621. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61185/0.74692. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.60863/0.74976. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.60320/0.75177. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.60559/0.75244. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60136/0.75335. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.59626/0.75591. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.59661/0.76049. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59641/0.76196. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59235/0.76438. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59388/0.76374. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.58864/0.76792. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.58361/0.77358. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58239/0.76737. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.57799/0.77748. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58205/0.77519. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57137/0.78115. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57393/0.78423. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57270/0.78474. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.56684/0.78518. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56798/0.79081. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56002/0.79688. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56039/0.80089. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56074/0.80535. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55559/0.79924. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54966/0.80504. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55006/0.81396. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.54812/0.81735. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54759/0.81037. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54736/0.81112. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.53866/0.81580. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.54144/0.82020. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.53066/0.82871. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.53253/0.83918. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.53187/0.84532. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.52373/0.85167. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52698/0.84744. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52161/0.85298. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52031/0.85440. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.52127/0.85661. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.51945/0.84597. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51701/0.85504. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.50890/0.85490. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50896/0.84785. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50715/0.86052. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51076/0.84871. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50582/0.84519. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50256/0.84488. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.49522/0.85890. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.50004/0.85907. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49165/0.87546. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.48725/0.88536. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49557/0.88786. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48561/0.88921. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69536/0.69515. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69328/0.69184. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.69107. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69248/0.69066. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69007. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.68974. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69155/0.68922. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69078/0.68881. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69071/0.68823. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69094/0.68773. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68952/0.68733. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68956/0.68697. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68804/0.68651. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68784/0.68601. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68743/0.68591. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68755/0.68589. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68728/0.68612. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68609/0.68568. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68537/0.68641. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68440/0.68619. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68477/0.68584. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68426/0.68729. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68244/0.68762. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68241/0.68739. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68314/0.68879. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68110/0.68864. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68166/0.68971. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68103/0.68909. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68018/0.69066. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68133/0.69049. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68027/0.69172. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67928/0.69183. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67809/0.69243. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67707/0.69375. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67634/0.69430. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67644/0.69610. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67541/0.69542. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67455/0.69583. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67390/0.69601. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67296/0.69637. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67092/0.69838. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66909/0.69690. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66861/0.69808. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66955/0.69935. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66553/0.69859. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66696/0.69891. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66309/0.70020. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66538/0.70154. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66293/0.70219. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65987/0.70566. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65729/0.70371. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65707/0.70401. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65748/0.70367. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65071/0.70690. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65083/0.70884. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64775/0.70814. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65004/0.70754. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64716/0.70792. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64901/0.70812. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64593/0.70926. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64190/0.71137. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63977/0.71022. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63726/0.71122. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63943/0.71186. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63361/0.71410. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63183/0.71516. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62861/0.71558. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63003/0.71751. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62604/0.71766. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62382/0.71934. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62606/0.71661. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62405/0.71818. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61763/0.72019. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61653/0.72327. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61471/0.72379. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61667/0.72253. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61297/0.72640. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61129/0.72649. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61166/0.73117. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60968/0.72927. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60263/0.72623. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60292/0.72908. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59897/0.73075. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59347/0.73568. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59719/0.73785. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59329/0.73945. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58811/0.73921. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58951/0.73950. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58716/0.74922. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58013/0.74833. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58186/0.74924. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57679/0.75220. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57520/0.75198. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56869/0.75856. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57888/0.76302. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57445/0.76646. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56920/0.76445. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56097/0.76807. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55849/0.77255. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55844/0.76828. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69555/0.69293. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69609/0.69278. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69461/0.69258. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.69475/0.69243. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69360/0.69238. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69316/0.69248. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.69262. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69195/0.69277. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69134/0.69290. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69296. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69103/0.69296. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69088/0.69291. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68955/0.69290. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69096/0.69298. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68908/0.69293. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68949/0.69285. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68940/0.69282. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68832/0.69272. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68801/0.69279. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68735/0.69282. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68467/0.69276. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68538/0.69267. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68571/0.69271. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68541/0.69265. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68509/0.69258. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68319/0.69254. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68451/0.69254. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68224/0.69272. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68271/0.69294. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68191/0.69302. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67961/0.69313. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68165/0.69273. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68068/0.69270. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67897/0.69276. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67889/0.69285. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67753/0.69251. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67453/0.69241. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67536/0.69245. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67405/0.69240. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67100/0.69172. Took 0.12 sec\n",
      "Epoch 40, Loss(train/val) 0.67183/0.69152. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67212/0.69091. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66937/0.69025. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66733/0.69039. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66914/0.69044. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66800/0.68963. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66570/0.68841. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66339/0.68847. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66133/0.68845. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66058/0.68807. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66068/0.68755. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65657/0.68828. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65613/0.68699. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65557/0.68758. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65604/0.68646. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64947/0.68611. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65028/0.68630. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64944/0.68586. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64295/0.68654. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63818/0.68667. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64084/0.68596. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63878/0.68582. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63381/0.68865. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63325/0.68768. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62982/0.68834. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62249/0.68937. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62780/0.69073. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62083/0.69266. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61545/0.69301. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62091/0.69231. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61695/0.69497. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61281/0.69793. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60851/0.70025. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60954/0.70058. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60036/0.70200. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60121/0.70230. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60181/0.70426. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59575/0.70634. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59609/0.70956. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59768/0.70979. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58724/0.71198. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58242/0.71459. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57959/0.71937. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57668/0.72045. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57776/0.72611. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57026/0.72763. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56761/0.73087. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56332/0.72908. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57004/0.73303. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56208/0.73784. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56080/0.73648. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55809/0.73971. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55961/0.73656. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55035/0.73932. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54566/0.74198. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54779/0.74070. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54395/0.73962. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53647/0.74296. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53930/0.74746. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53094/0.74646. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69295/0.68969. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69343/0.68903. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.68869. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.68858. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69214/0.68800. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69107/0.68806. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69110/0.68871. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69102/0.68775. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68982/0.68822. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68998/0.68811. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68974/0.68736. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68958/0.68686. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68878/0.68656. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68824/0.68677. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68760/0.68693. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68741/0.68589. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68761/0.68672. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68647/0.68640. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68582/0.68598. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68574/0.68577. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68512/0.68558. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68451/0.68607. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68313/0.68616. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68329/0.68564. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68260/0.68633. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68169/0.68653. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68097/0.68746. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67968/0.68680. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67778/0.68578. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67672/0.68622. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67677/0.68832. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67489/0.68880. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67516/0.68865. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67421/0.68975. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67287/0.69040. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67045/0.69077. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67276/0.69134. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66941/0.69099. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66769/0.69116. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66681/0.69134. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66483/0.69132. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66438/0.69173. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66303/0.69247. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66258/0.69201. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66040/0.69217. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66160/0.69115. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65542/0.69273. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65860/0.69277. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65555/0.69241. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65581/0.69542. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65286/0.69416. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64713/0.69461. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64731/0.69443. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64417/0.69379. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64450/0.69534. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64239/0.69450. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63622/0.69383. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63649/0.69632. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63444/0.69697. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63403/0.69913. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63045/0.69661. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62640/0.70009. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63020/0.69785. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62348/0.69716. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62311/0.69641. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61769/0.69908. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61640/0.70025. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61799/0.70350. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61270/0.70446. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61170/0.70463. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60959/0.71015. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60769/0.70955. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60424/0.71286. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60439/0.71352. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60057/0.71552. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60001/0.71905. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59544/0.71638. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59571/0.71588. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59482/0.71892. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58900/0.71680. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58610/0.71761. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58504/0.71992. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58867/0.72238. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57900/0.72202. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58393/0.72125. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57970/0.72673. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57737/0.72523. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58185/0.73531. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56583/0.73598. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57247/0.73997. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56719/0.74256. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56216/0.74392. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56332/0.74411. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56371/0.73994. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55834/0.74427. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55751/0.75032. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55592/0.74945. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55037/0.74624. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54649/0.75257. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54353/0.75949. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69535/0.68965. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69252/0.68747. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69170/0.68668. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69064/0.68565. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69062/0.68446. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68992/0.68374. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68952/0.68343. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68921/0.68290. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68893/0.68177. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68831/0.68177. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68733/0.68055. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68783/0.68015. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68768/0.67968. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68773/0.67952. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68633/0.67916. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68620/0.67869. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68618/0.67869. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68565/0.67846. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68553/0.67761. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68623/0.67728. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68541/0.67736. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68503/0.67690. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68465/0.67743. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68449/0.67629. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68356/0.67688. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68258/0.67710. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68250/0.67698. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68295/0.67759. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68069/0.67674. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68053/0.67594. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68178/0.67697. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68049/0.67638. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67993/0.67658. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67948/0.67696. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67851/0.67673. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67915/0.67765. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67724/0.67729. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67717/0.67741. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67723/0.67771. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67648/0.67754. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67629/0.67875. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67586/0.67919. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67411/0.67885. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67327/0.68003. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67286/0.67989. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67148/0.67935. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67202/0.68093. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66944/0.68135. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66899/0.68062. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66945/0.68358. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66752/0.68349. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66682/0.68302. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66470/0.68350. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66369/0.68487. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66487/0.68435. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66314/0.68632. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66079/0.68838. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66126/0.68999. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65990/0.69196. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65744/0.69215. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65934/0.69237. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65532/0.69385. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65527/0.69435. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65709/0.69690. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65254/0.69965. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65318/0.70031. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65098/0.70235. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65097/0.70430. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64907/0.70576. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64485/0.70910. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64815/0.71027. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64862/0.70831. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64569/0.71001. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64605/0.71518. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64186/0.71633. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64226/0.71620. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63890/0.72100. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64112/0.71693. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64253/0.72137. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63860/0.71986. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63744/0.72275. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63909/0.72345. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63749/0.72929. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63132/0.73062. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63441/0.73341. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63313/0.73007. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62764/0.73566. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62929/0.73807. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62803/0.74173. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62471/0.73741. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62679/0.74236. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62680/0.74101. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62564/0.74872. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62269/0.74505. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62326/0.75080. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61987/0.74843. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62235/0.75215. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62009/0.75422. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61729/0.75466. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61400/0.75360. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69281/0.69066. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69184/0.68807. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69079/0.68556. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.68404. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69007/0.68305. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68877/0.68178. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68917/0.68094. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68789/0.68096. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68753/0.68000. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68718/0.67949. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68661/0.67941. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68576/0.67841. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68372/0.67793. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68388/0.67778. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68223/0.67681. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68150/0.67610. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68197/0.67597. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68100/0.67430. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67943/0.67535. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67793/0.67566. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67712/0.67506. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67713/0.67539. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67839/0.67410. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67417/0.67504. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67227/0.67488. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67317/0.67524. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67113/0.67572. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67271/0.67553. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66873/0.67483. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66807/0.67596. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66922/0.67648. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66494/0.67632. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66320/0.67702. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66306/0.67740. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65937/0.67905. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65965/0.67889. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65514/0.67987. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65783/0.68030. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65452/0.67948. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65301/0.68211. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64902/0.68191. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64983/0.68523. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64702/0.68481. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64341/0.68564. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64359/0.68339. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64170/0.68341. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64085/0.68729. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63616/0.68560. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63615/0.68698. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63769/0.68820. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63403/0.68952. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63178/0.68974. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62919/0.69485. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62737/0.69417. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62362/0.69684. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62191/0.69833. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62282/0.69985. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61873/0.69949. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62196/0.69758. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61725/0.70171. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61701/0.70456. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61204/0.70483. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61165/0.70634. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60858/0.70869. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60886/0.70874. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60076/0.71261. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60107/0.71568. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59832/0.71811. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59714/0.72230. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59824/0.72256. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59344/0.72653. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58784/0.73178. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59250/0.73042. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58090/0.73612. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58282/0.73870. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58296/0.74213. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57947/0.74367. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56779/0.74884. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57118/0.75411. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56968/0.76015. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56911/0.76106. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56653/0.76872. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56408/0.77149. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55397/0.78003. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55801/0.78421. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55265/0.79069. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55035/0.79625. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55625/0.79279. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54415/0.80438. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55068/0.80616. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54264/0.81109. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53942/0.81985. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53980/0.82326. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53298/0.82861. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53093/0.83128. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52855/0.83482. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52930/0.83710. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52086/0.83934. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51914/0.84660. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52002/0.85564. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69536/0.69516. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69448/0.69396. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69231. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69151/0.69076. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.68953. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68931/0.68873. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68920/0.68838. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68831/0.68839. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68785/0.68852. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68742/0.68891. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68724/0.68948. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68630/0.69000. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68610/0.69041. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68623/0.69121. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68503/0.69193. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68342/0.69261. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68250/0.69384. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67987/0.69511. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67896/0.69689. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67952/0.69804. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67662/0.69918. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67705/0.70044. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67400/0.70270. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67508/0.70300. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67260/0.70454. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67177/0.70531. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66801/0.70671. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66728/0.70754. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66650/0.70877. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66455/0.71059. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66254/0.71186. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65943/0.71177. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65777/0.71319. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65804/0.71292. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65493/0.71334. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65645/0.71505. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65164/0.71576. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64933/0.71450. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64643/0.71455. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64550/0.71392. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64311/0.71516. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64077/0.71547. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63884/0.71601. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63650/0.71648. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63551/0.71913. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63271/0.71967. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62776/0.71861. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63117/0.71884. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62551/0.72097. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62386/0.71988. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62259/0.71848. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62412/0.71999. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61846/0.72060. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61730/0.71972. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61380/0.71838. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60948/0.72009. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60893/0.72247. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60629/0.72155. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60964/0.72230. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60922/0.72394. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60370/0.72299. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59711/0.72522. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59520/0.72647. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59226/0.72656. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59856/0.72672. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59244/0.72548. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59496/0.72890. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58731/0.72635. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58483/0.72603. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58403/0.73012. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58294/0.73178. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58842/0.73522. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58027/0.73688. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57154/0.74277. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57533/0.74081. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57207/0.74306. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57233/0.74064. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56816/0.74534. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56785/0.74654. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57219/0.74949. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56277/0.75149. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57045/0.75212. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56057/0.75087. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54935/0.75357. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55337/0.75398. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54764/0.76226. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55149/0.75939. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54787/0.76080. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54487/0.76929. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54689/0.77087. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53985/0.76870. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54091/0.77168. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53371/0.77597. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53088/0.78230. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52904/0.78647. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52876/0.79346. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52403/0.79369. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52328/0.79481. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51968/0.80142. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51496/0.80379. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69454/0.69005. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69150/0.69036. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68901/0.69022. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68854/0.69008. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68872/0.68990. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68786/0.69010. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68771/0.69037. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68626/0.69045. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68638/0.69051. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68664/0.69075. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68583/0.69097. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68448/0.69106. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68477/0.69144. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68534/0.69118. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68473/0.69115. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68281/0.69138. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68366/0.69189. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68278/0.69212. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68367/0.69257. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68207/0.69255. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68079/0.69246. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68166/0.69293. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68147/0.69327. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67978/0.69381. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68010/0.69433. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67910/0.69423. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67779/0.69456. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67951/0.69499. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67813/0.69570. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67769/0.69563. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67833/0.69615. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67739/0.69653. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67589/0.69701. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67755/0.69724. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67521/0.69800. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67380/0.69855. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67365/0.69877. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67258/0.69929. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67203/0.70000. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67152/0.70048. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67128/0.70067. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66988/0.70048. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67043/0.70106. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66771/0.70095. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66934/0.70100. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66470/0.70287. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66715/0.70394. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66210/0.70315. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66494/0.70365. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66552/0.70358. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66463/0.70432. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66192/0.70477. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66337/0.70371. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66340/0.70385. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66030/0.70346. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65783/0.70453. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65690/0.70449. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65465/0.70651. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65502/0.70660. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65338/0.70803. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65706/0.70687. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65326/0.70811. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65038/0.71011. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65119/0.71144. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64925/0.70941. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64765/0.71025. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64794/0.71033. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64245/0.71265. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64186/0.71219. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64396/0.71104. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.64047/0.71373. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64081/0.71394. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64089/0.71419. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63554/0.71380. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63786/0.71575. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63424/0.71572. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63561/0.71703. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63457/0.71857. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63042/0.71765. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63196/0.71786. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63003/0.71635. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62531/0.71848. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62704/0.72043. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62551/0.72214. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62346/0.72258. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62140/0.72255. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61973/0.72618. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61905/0.72183. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61615/0.72360. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61679/0.72485. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61610/0.72789. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61381/0.73009. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60940/0.72870. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61053/0.73013. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60875/0.72887. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60614/0.73250. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60822/0.73090. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60479/0.73099. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60082/0.73079. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60339/0.73324. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69083/0.69355. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68869/0.69462. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68823/0.69522. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68754/0.69577. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68764/0.69614. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68726/0.69690. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68643/0.69766. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68622/0.69856. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68523/0.69967. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68517/0.70055. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68399/0.70165. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68400/0.70260. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68385/0.70349. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68263/0.70450. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68182/0.70547. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68149/0.70670. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68032/0.70825. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67963/0.70968. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67915/0.71073. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67834/0.71200. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67780/0.71328. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67785/0.71421. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67730/0.71452. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67592/0.71605. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67445/0.71718. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67399/0.71815. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67280/0.71805. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67176/0.71925. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67039/0.72045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66992/0.72192. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66954/0.72224. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66620/0.72362. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66616/0.72459. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66504/0.72549. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66339/0.72730. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66305/0.72848. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66306/0.72916. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65849/0.72972. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65831/0.73131. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65635/0.73370. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65609/0.73423. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65501/0.73650. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65399/0.73672. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65312/0.73889. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65245/0.73912. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64907/0.74136. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64727/0.74343. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64566/0.74297. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64351/0.74653. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64249/0.74835. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64479/0.74903. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64001/0.74944. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63644/0.75217. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63672/0.75283. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63615/0.75436. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.63460/0.75497. Took 0.12 sec\n",
      "Epoch 56, Loss(train/val) 0.63402/0.75878. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.62936/0.76233. Took 0.23 sec\n",
      "Epoch 58, Loss(train/val) 0.62983/0.76260. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62714/0.76562. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62779/0.76643. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61985/0.76935. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62044/0.77320. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61751/0.77455. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61917/0.77704. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61395/0.77732. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61192/0.78059. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61407/0.78202. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61117/0.78400. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60641/0.79047. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60407/0.79289. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59736/0.79386. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60104/0.79949. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59791/0.80264. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59624/0.80451. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59518/0.80415. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59293/0.81062. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58777/0.81441. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58890/0.81504. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58803/0.81569. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58546/0.81874. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57732/0.82332. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57815/0.82788. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57580/0.83574. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.57051/0.84062. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57823/0.83734. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57024/0.83799. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57193/0.84045. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56615/0.84450. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56499/0.84485. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56255/0.85236. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55561/0.85424. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55875/0.86076. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55662/0.85932. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55603/0.86412. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54961/0.86765. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55413/0.87417. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55010/0.86938. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54286/0.87655. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53888/0.87393. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69535/0.69065. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68865/0.68695. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68712/0.68597. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68625/0.68582. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68742/0.68572. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68632/0.68567. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68511/0.68567. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68607/0.68560. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68543/0.68547. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68534/0.68537. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68519/0.68525. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68427/0.68512. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68369/0.68514. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68423/0.68503. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68282/0.68505. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68362/0.68509. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68127/0.68506. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68188/0.68515. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68126/0.68530. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68049/0.68560. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 0.68026/0.68567. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67965/0.68585. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67915/0.68588. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67891/0.68586. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67758/0.68604. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67584/0.68652. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67785/0.68687. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67495/0.68740. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67376/0.68730. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67311/0.68775. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67339/0.68848. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67316/0.68878. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67145/0.68913. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67026/0.68914. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67176/0.68929. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66942/0.68898. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66731/0.68911. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66557/0.69010. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66582/0.69056. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66341/0.69078. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66584/0.69092. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66340/0.69137. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66220/0.69188. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65960/0.69180. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65619/0.69297. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65741/0.69338. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65687/0.69327. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65399/0.69420. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65160/0.69483. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65205/0.69622. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65205/0.69566. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64926/0.69572. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64720/0.69825. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64883/0.69941. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64711/0.69983. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64539/0.70001. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64577/0.70097. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64318/0.69960. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64244/0.70016. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64535/0.70067. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63646/0.70169. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63634/0.70305. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63700/0.70541. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63417/0.70509. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63349/0.70440. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63294/0.70503. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63084/0.70633. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63160/0.70880. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62821/0.71070. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62616/0.71265. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62505/0.71242. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62660/0.71256. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62292/0.71433. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62149/0.71546. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62521/0.71433. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62103/0.71539. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61297/0.71803. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61544/0.71811. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61536/0.71879. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61455/0.71814. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60828/0.71967. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60213/0.72217. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60781/0.72386. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60982/0.72433. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60409/0.72868. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60568/0.72493. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59960/0.73008. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60152/0.73083. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59655/0.73174. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.58963/0.73206. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59165/0.73580. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59403/0.73718. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58708/0.74065. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58999/0.74127. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58527/0.74316. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58142/0.74582. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58174/0.74636. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57728/0.74788. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57531/0.74964. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57376/0.75271. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69165/0.68062. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68751/0.67787. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68678/0.67739. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68620/0.67718. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68610/0.67689. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68592/0.67674. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68585/0.67649. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68605/0.67621. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68528/0.67614. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68559/0.67587. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68503/0.67572. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68452/0.67548. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68422/0.67491. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68387/0.67465. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68344/0.67430. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68331/0.67393. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68300/0.67371. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68283/0.67320. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68169/0.67275. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68195/0.67267. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68108/0.67249. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67972/0.67220. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67991/0.67159. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67887/0.67106. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68030/0.67068. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67982/0.67034. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67874/0.66970. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67777/0.66911. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67826/0.66887. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67663/0.66860. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67738/0.66818. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67656/0.66756. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67583/0.66684. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67515/0.66681. Took 0.12 sec\n",
      "Epoch 34, Loss(train/val) 0.67375/0.66649. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67429/0.66606. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67276/0.66547. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67089/0.66512. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67145/0.66421. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67150/0.66456. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66814/0.66464. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66856/0.66394. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66401/0.66393. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66797/0.66363. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66483/0.66328. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66286/0.66377. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66327/0.66444. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66083/0.66383. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65938/0.66349. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65847/0.66411. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65723/0.66443. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65627/0.66510. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65437/0.66533. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65531/0.66743. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65465/0.66629. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65154/0.66647. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65198/0.66719. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64843/0.66741. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65071/0.66809. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64749/0.66958. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64502/0.67037. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64407/0.67028. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64124/0.67174. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64079/0.67135. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64210/0.67203. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.64068/0.67276. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63620/0.67604. Took 0.12 sec\n",
      "Epoch 67, Loss(train/val) 0.63627/0.67533. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63494/0.67612. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63311/0.67649. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62832/0.67714. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62533/0.68013. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62576/0.68234. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62548/0.68096. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62322/0.68409. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62058/0.68268. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62040/0.68495. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61957/0.68722. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61106/0.68864. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60894/0.68995. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61209/0.68931. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61064/0.68998. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61311/0.69445. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.60498/0.69651. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.60474/0.69543. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60253/0.69709. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60775/0.69637. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59587/0.70053. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59843/0.70241. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60135/0.70209. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.59253/0.70444. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59498/0.70717. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58510/0.70707. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59204/0.70537. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59172/0.70517. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58729/0.70651. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57966/0.70958. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58080/0.71341. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57902/0.71467. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57175/0.71510. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69179/0.70522. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68364/0.70967. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68247/0.70865. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68325/0.70808. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68325/0.70791. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68256/0.70745. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68217/0.70734. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68204/0.70703. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68217/0.70665. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68158/0.70629. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68108/0.70582. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68080/0.70563. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68064/0.70554. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68022/0.70601. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67993/0.70586. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67984/0.70558. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67895/0.70484. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67924/0.70534. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67882/0.70465. Took 0.12 sec\n",
      "Epoch 19, Loss(train/val) 0.67790/0.70511. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67748/0.70462. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.67757/0.70450. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67731/0.70479. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67674/0.70459. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67617/0.70487. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67685/0.70452. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67548/0.70511. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67453/0.70449. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67408/0.70456. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67396/0.70479. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67281/0.70447. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67194/0.70506. Took 0.11 sec\n",
      "Epoch 32, Loss(train/val) 0.67269/0.70571. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67274/0.70387. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67126/0.70417. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67024/0.70409. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66926/0.70358. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66964/0.70520. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66830/0.70522. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66798/0.70327. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66900/0.70410. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66712/0.70493. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66511/0.70403. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66498/0.70529. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66640/0.70569. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66461/0.70328. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66225/0.70557. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66148/0.70561. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66262/0.70567. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66051/0.70703. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65949/0.70655. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65882/0.70638. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66003/0.70521. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65874/0.70491. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65513/0.70676. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65564/0.70907. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65509/0.70713. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65171/0.70931. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65404/0.70895. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65242/0.70878. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65067/0.71133. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65063/0.70904. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64863/0.71084. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64530/0.71237. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64428/0.71346. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64361/0.71619. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64176/0.71743. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64055/0.71839. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63873/0.72086. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63698/0.72104. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63554/0.72205. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63419/0.72447. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63686/0.72492. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63061/0.72341. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.63096/0.72879. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63111/0.73032. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62532/0.73108. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62522/0.73289. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62161/0.73570. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62064/0.74034. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62270/0.74143. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62042/0.74430. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61019/0.74632. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61545/0.74854. Took 0.14 sec\n",
      "Epoch 84, Loss(train/val) 0.60941/0.75310. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61041/0.75178. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60981/0.75134. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60405/0.75989. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60193/0.76439. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60249/0.76487. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60230/0.76835. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59795/0.77541. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59494/0.77733. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59579/0.78575. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59505/0.78432. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58963/0.78966. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58419/0.79578. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58399/0.79652. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58426/0.81158. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58012/0.80153. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69741/0.69216. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69538/0.69162. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69364/0.69075. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69037. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69239/0.69006. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69173/0.69011. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69008. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69116/0.69014. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69094/0.69034. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69043/0.69034. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68988/0.69067. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68970/0.69089. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68898/0.69120. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68951/0.69146. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68909/0.69144. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68694/0.69187. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68815/0.69237. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68696/0.69235. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68615/0.69266. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68534/0.69324. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68416/0.69395. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68432/0.69409. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68360/0.69488. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68260/0.69491. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68225/0.69527. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68102/0.69548. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67989/0.69615. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67956/0.69673. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67643/0.69693. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67689/0.69730. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67562/0.69838. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67539/0.69917. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67261/0.70023. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67129/0.70085. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67154/0.70144. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66918/0.70266. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66775/0.70325. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66714/0.70356. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66557/0.70493. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66560/0.70548. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66358/0.70564. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66129/0.70705. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65945/0.70881. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65903/0.70913. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65579/0.71081. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65708/0.71112. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.65570/0.71216. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65042/0.71347. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65077/0.71425. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64872/0.71440. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64706/0.71626. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64657/0.71723. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64628/0.71808. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64095/0.71919. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64203/0.71890. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63685/0.72088. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63613/0.72275. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63339/0.72570. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63379/0.72422. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62919/0.72657. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62821/0.72810. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62340/0.72836. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62742/0.72943. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62235/0.73059. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61868/0.73485. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61862/0.73424. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61823/0.73316. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61209/0.73556. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61195/0.73626. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.61142/0.73559. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60750/0.73466. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60583/0.73651. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.60066/0.73750. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.60193/0.73802. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.59990/0.73499. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59999/0.73883. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59470/0.73918. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59212/0.74098. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59574/0.73889. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59083/0.73796. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58901/0.73577. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58282/0.74040. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58073/0.74498. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.57695/0.74521. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57954/0.74372. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57654/0.74565. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57065/0.74791. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56768/0.75114. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56812/0.75372. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56641/0.75470. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56510/0.75730. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55915/0.75567. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56603/0.75527. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55019/0.75998. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56053/0.76242. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55683/0.75967. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54463/0.76922. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.54552/0.76256. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54864/0.76630. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53937/0.77129. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69278/0.69323. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69264/0.69339. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69195/0.69348. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69144/0.69371. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69103/0.69380. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69064/0.69403. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69041/0.69408. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69008/0.69439. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68946/0.69483. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68903/0.69529. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68845/0.69524. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68773/0.69562. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68718/0.69589. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68689/0.69598. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68632/0.69606. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68514/0.69645. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68474/0.69695. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68394/0.69651. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68261/0.69691. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.69662. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68059/0.69632. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67929/0.69774. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67834/0.69685. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67763/0.69663. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67764/0.69757. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67365/0.69506. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67182/0.69830. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67033/0.69740. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66737/0.69760. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66659/0.69614. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66430/0.69687. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66447/0.69727. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66059/0.69715. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65699/0.69847. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65718/0.70004. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65763/0.69699. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65406/0.69816. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64918/0.70190. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64833/0.70012. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64733/0.69950. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64091/0.70289. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64013/0.70494. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63686/0.70497. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63613/0.70307. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63339/0.70661. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63337/0.71280. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62860/0.71271. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62785/0.71841. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62293/0.71537. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62195/0.72129. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.62306/0.71950. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61917/0.71985. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61091/0.73189. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60949/0.73515. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60977/0.73289. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60581/0.73787. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60557/0.73969. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59946/0.74050. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60112/0.74993. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59527/0.75367. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59324/0.76165. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58710/0.75788. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59173/0.76951. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.58332/0.76961. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58162/0.76777. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58011/0.76416. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57705/0.77227. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57381/0.78092. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57241/0.79195. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56844/0.79872. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57409/0.79061. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56200/0.80033. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56468/0.79699. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55422/0.80583. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55379/0.81296. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55935/0.81024. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.55576/0.82107. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.54834/0.81227. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54337/0.81209. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54517/0.81190. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54310/0.81238. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54438/0.82725. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53825/0.83065. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53452/0.83849. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53435/0.83614. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52416/0.82997. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52671/0.83742. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52114/0.84251. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52736/0.85218. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52513/0.84460. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52381/0.81879. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52215/0.83267. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51711/0.83168. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51555/0.84470. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50789/0.84254. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49558/0.86323. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.50679/0.84147. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50066/0.86405. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49599/0.86538. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49459/0.86623. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69431/0.69093. Took 0.13 sec\n",
      "Epoch 1, Loss(train/val) 0.69183/0.69584. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.69718. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.69716. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69178/0.69631. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69064/0.69646. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69107/0.69594. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.69618. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68995/0.69528. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68878/0.69491. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68892/0.69509. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68824/0.69461. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68908/0.69424. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68779/0.69378. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68871/0.69277. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68603/0.69224. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68563/0.69183. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68517/0.69065. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68504/0.69107. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68470/0.69020. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68401/0.69025. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68281/0.68972. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68051/0.68797. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68043/0.68771. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67830/0.68566. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67821/0.68574. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67483/0.68345. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67445/0.68128. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67465/0.68200. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67204/0.67862. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66967/0.67674. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66996/0.67732. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67031/0.67449. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66452/0.67258. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66364/0.66954. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66094/0.66770. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65888/0.66603. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65776/0.66598. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65513/0.66162. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.65477/0.66012. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65274/0.65661. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64697/0.65321. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64434/0.65149. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64222/0.64893. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64193/0.64778. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63894/0.64412. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63576/0.64157. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63182/0.64037. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63596/0.63711. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.62946/0.63670. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.62770/0.63450. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.62881/0.63157. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.62602/0.63211. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62228/0.63032. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.62125/0.62871. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.61593/0.62542. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61421/0.62423. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.61366/0.62275. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.60756/0.62073. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.60862/0.62042. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60492/0.61760. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60291/0.61683. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59810/0.61577. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60060/0.61598. Took 0.13 sec\n",
      "Epoch 64, Loss(train/val) 0.59634/0.61565. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.59338/0.61573. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.59310/0.61468. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.58957/0.61299. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.58769/0.61469. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58818/0.61449. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58331/0.61199. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.58489/0.61394. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57831/0.61370. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57697/0.61324. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56910/0.61276. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56924/0.61488. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56616/0.61867. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56616/0.61946. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57090/0.62080. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55597/0.62307. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.55749/0.62345. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55657/0.62788. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55944/0.62194. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54894/0.62425. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55173/0.62276. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54707/0.62447. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54008/0.63258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54171/0.63208. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54625/0.63357. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53480/0.63016. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53526/0.62974. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.52690/0.63285. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52994/0.63518. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51229/0.64362. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51673/0.63803. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51240/0.64446. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51629/0.64919. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50904/0.64740. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51290/0.64261. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50365/0.64890. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69555/0.70414. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69433/0.70199. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69391/0.70043. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69337/0.69897. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69343/0.69735. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69310/0.69573. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69289/0.69447. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69244/0.69345. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69243/0.69284. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69225/0.69261. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69223/0.69249. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69226/0.69248. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69195/0.69267. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69174/0.69278. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.69171/0.69310. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69148/0.69353. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69072/0.69347. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69086/0.69421. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69038/0.69484. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68975/0.69546. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68905/0.69548. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68951/0.69603. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68915/0.69712. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68854/0.69829. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68801/0.69763. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68702/0.69810. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68605/0.69780. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68700/0.69715. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68594/0.69813. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68598/0.69768. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68478/0.69985. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68454/0.69810. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68450/0.69857. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68322/0.69912. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68279/0.69875. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68224/0.69950. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68209/0.69999. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68094/0.69942. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67847/0.70221. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67993/0.70200. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67880/0.70222. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67720/0.70308. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67620/0.70099. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67645/0.70320. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67525/0.70264. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67360/0.70251. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67321/0.70520. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67201/0.70557. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67131/0.70579. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67035/0.70628. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66927/0.70721. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66636/0.71175. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66744/0.70826. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66680/0.71076. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66574/0.71187. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66136/0.71359. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66095/0.71727. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65798/0.71816. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65769/0.71946. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65457/0.71976. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65236/0.72304. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65017/0.72323. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65173/0.72741. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64754/0.72887. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64672/0.73232. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64273/0.73241. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64248/0.73439. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64036/0.73970. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63734/0.74400. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63469/0.74750. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63008/0.74538. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62790/0.75292. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62474/0.75325. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61945/0.76035. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61986/0.76383. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.61822/0.76517. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61619/0.77292. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61406/0.77398. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60888/0.77840. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60183/0.78449. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60338/0.79027. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.60037/0.78957. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59730/0.79656. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59375/0.79646. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58896/0.80512. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58448/0.81197. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58366/0.81519. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58487/0.81560. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58209/0.81936. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57909/0.82153. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.56855/0.83059. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57148/0.83619. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.56883/0.84247. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.56351/0.85317. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.56258/0.84784. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.55592/0.84967. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55477/0.86111. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.54548/0.86153. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55046/0.86756. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54360/0.87595. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69232/0.70274. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69055/0.70334. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.70338. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 0.69049/0.70319. Took 0.13 sec\n",
      "Epoch 4, Loss(train/val) 0.68997/0.70307. Took 0.13 sec\n",
      "Epoch 5, Loss(train/val) 0.68930/0.70345. Took 0.13 sec\n",
      "Epoch 6, Loss(train/val) 0.68961/0.70378. Took 0.14 sec\n",
      "Epoch 7, Loss(train/val) 0.68933/0.70398. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68858/0.70386. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68797/0.70425. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.70456. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68771/0.70462. Took 0.13 sec\n",
      "Epoch 12, Loss(train/val) 0.68786/0.70438. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68620/0.70480. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68761/0.70480. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68648/0.70530. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68550/0.70526. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68491/0.70583. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68516/0.70633. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68452/0.70635. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68486/0.70604. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68404/0.70561. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68398/0.70503. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68364/0.70499. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68378/0.70513. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68241/0.70478. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68067/0.70508. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68169/0.70514. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68035/0.70531. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68137/0.70495. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67869/0.70362. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67948/0.70346. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67730/0.70312. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67628/0.70408. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67542/0.70370. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67546/0.70410. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67278/0.70424. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67395/0.70373. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67086/0.70418. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66953/0.70698. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66653/0.70540. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66670/0.70495. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66460/0.70585. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66398/0.70536. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66185/0.70714. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66160/0.70593. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65770/0.70698. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65725/0.71073. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65373/0.71075. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64968/0.71120. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65039/0.71456. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64876/0.71623. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64508/0.72266. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64196/0.72372. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63824/0.72647. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63927/0.72589. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63813/0.72777. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63225/0.72928. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63337/0.73295. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63017/0.74097. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62977/0.74343. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.62949/0.74678. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63008/0.74405. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62627/0.75060. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62424/0.74852. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62467/0.74661. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62065/0.74899. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61870/0.75274. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61715/0.75411. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61405/0.76371. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.60955/0.76475. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60916/0.76851. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60895/0.76516. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60889/0.76879. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60233/0.77691. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60792/0.77854. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59577/0.77915. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60075/0.78876. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59394/0.79509. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59932/0.79348. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59533/0.79510. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58904/0.79587. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59025/0.79938. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58788/0.79237. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58769/0.80659. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58700/0.80211. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58181/0.80956. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57848/0.80797. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58084/0.81677. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57954/0.82184. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57878/0.81522. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57385/0.82916. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57047/0.82410. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57750/0.83683. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56864/0.82682. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56734/0.82730. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56025/0.82728. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56066/0.83263. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56193/0.83480. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55579/0.82853. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69404/0.69709. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69619. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69572. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 0.69236/0.69578. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69045/0.69557. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.69524. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69040/0.69514. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.69477. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68968/0.69439. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69000/0.69424. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68925/0.69375. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69020/0.69357. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68928/0.69307. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68996/0.69252. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68850/0.69258. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68758/0.69259. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68830/0.69266. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68770/0.69256. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68669/0.69254. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68675/0.69239. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 0.68672/0.69164. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.68737/0.69169. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68528/0.69187. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68640/0.69164. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68611/0.69129. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68482/0.69150. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68472/0.69127. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68321/0.69062. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68422/0.68960. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68196/0.68963. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68066/0.68871. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68075/0.68804. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67945/0.68776. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67793/0.68738. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67683/0.68715. Took 0.12 sec\n",
      "Epoch 35, Loss(train/val) 0.67707/0.68622. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67705/0.68588. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67516/0.68504. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67393/0.68432. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67146/0.68425. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.66881/0.68287. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67084/0.68247. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66931/0.68074. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66439/0.67997. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66501/0.67992. Took 0.12 sec\n",
      "Epoch 45, Loss(train/val) 0.66349/0.67797. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66275/0.67706. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.65985/0.67642. Took 0.12 sec\n",
      "Epoch 48, Loss(train/val) 0.65498/0.67458. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65434/0.67371. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.64853/0.67127. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65454/0.67125. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64668/0.67104. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.64244/0.66952. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.64178/0.67135. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64225/0.67165. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 0.64353/0.67261. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.63701/0.67129. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.63614/0.67041. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63066/0.67068. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63201/0.67390. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.62788/0.67257. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62598/0.67134. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.62342/0.66836. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61528/0.66542. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62160/0.67212. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.61265/0.67542. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61249/0.67323. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.61391/0.67000. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.60760/0.68255. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.60474/0.67398. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.60259/0.67367. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.60317/0.67238. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59664/0.67553. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59507/0.67421. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58738/0.67342. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58884/0.67646. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57818/0.67142. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58567/0.67191. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58234/0.67486. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.58179/0.67316. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57259/0.68158. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57467/0.67616. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57285/0.67582. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56010/0.67429. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56548/0.68251. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56146/0.66646. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55662/0.67325. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55466/0.68193. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55127/0.68137. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54980/0.67692. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54618/0.67496. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.54288/0.67433. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.54647/0.67046. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53143/0.67197. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53889/0.66468. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.52992/0.68075. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52852/0.67302. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52510/0.67097. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52552/0.67632. Took 0.10 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69135/0.69963. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69090/0.69847. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69041/0.69760. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68987/0.69693. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.68965/0.69599. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68948/0.69551. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68839/0.69532. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68801/0.69497. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.69498. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68748/0.69469. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68690/0.69499. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68682/0.69506. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68615/0.69505. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68657/0.69586. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68498/0.69551. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68436/0.69572. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68585/0.69628. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.69636. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68358/0.69698. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68237/0.69613. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68278/0.69713. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68138/0.69738. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67939/0.69771. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.68088/0.69913. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68105/0.69780. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67824/0.69756. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67899/0.69850. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67647/0.69932. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67677/0.69909. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67618/0.69994. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67281/0.69964. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67303/0.69888. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66928/0.69971. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66964/0.70084. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67024/0.69997. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66946/0.70135. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66950/0.70304. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66538/0.70472. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66542/0.70394. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66272/0.70232. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66088/0.70305. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66372/0.70685. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66129/0.70739. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65700/0.70711. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65750/0.70904. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65573/0.71112. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65498/0.71271. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65125/0.71322. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64690/0.71299. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64938/0.71428. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64685/0.71542. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64881/0.71722. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64280/0.71844. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64174/0.71909. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63689/0.72131. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64130/0.72149. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63367/0.72386. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63185/0.72582. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63389/0.72528. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62866/0.72675. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.62889/0.72587. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.62473/0.72680. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62825/0.72938. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62811/0.73050. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61962/0.73086. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61720/0.73495. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61246/0.73702. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.61126/0.74120. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60779/0.74213. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.60464/0.74382. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60734/0.74668. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60359/0.74272. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60052/0.75062. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59700/0.75033. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59371/0.75383. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59122/0.75379. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58984/0.75714. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59124/0.75788. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.58914/0.76018. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.58035/0.76149. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57805/0.76560. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.57821/0.76493. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.57195/0.76894. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.57126/0.77635. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57206/0.77489. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.56777/0.77516. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.55739/0.78017. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56589/0.77856. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55226/0.78580. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.55200/0.79655. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55650/0.79230. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.55149/0.79569. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.54663/0.80391. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54063/0.80212. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54064/0.80965. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.53667/0.81550. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54009/0.80952. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.53268/0.81807. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52638/0.82293. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.53652/0.82385. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69466/0.69222. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69302/0.69308. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69255/0.69384. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69229/0.69464. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69224/0.69525. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69161/0.69592. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69020/0.69595. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69216/0.69614. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69184/0.69663. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.69636. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.69618. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69027/0.69622. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69026/0.69606. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68913/0.69579. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68939/0.69512. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68852/0.69496. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68837/0.69435. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68678/0.69344. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68633/0.69371. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68589/0.69449. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68573/0.69402. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68449/0.69319. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68457/0.69218. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68500/0.69363. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68143/0.69250. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68076/0.69445. Took 0.12 sec\n",
      "Epoch 26, Loss(train/val) 0.68104/0.69525. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67962/0.69433. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67894/0.69382. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67853/0.69312. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67451/0.69563. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67418/0.69432. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67531/0.69568. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.67422/0.69967. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67241/0.69825. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67234/0.69961. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.66854/0.69840. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66840/0.70117. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66767/0.70565. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.66516/0.70209. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66586/0.70240. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66619/0.70186. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66692/0.69924. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66191/0.70027. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.66133/0.70781. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65739/0.70844. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65311/0.70534. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.65378/0.70703. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65597/0.71008. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65026/0.71010. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64575/0.71353. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64585/0.71196. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64666/0.71219. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64515/0.71265. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64136/0.71476. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63790/0.71219. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63737/0.71311. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63283/0.72293. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.63316/0.72573. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.62943/0.72474. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.62848/0.72623. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 0.62485/0.72995. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.62152/0.72857. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.62548/0.73554. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62055/0.73630. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.61873/0.73440. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.61735/0.74155. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.61428/0.74342. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60757/0.74419. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60831/0.73837. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60535/0.74278. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60586/0.74267. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60086/0.74835. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60121/0.76152. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59797/0.76296. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59519/0.75546. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59384/0.76723. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59074/0.75823. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59703/0.76099. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58733/0.76890. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58372/0.77669. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57805/0.77603. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57912/0.77418. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57448/0.77661. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56962/0.77577. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57520/0.78389. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56959/0.78867. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.56976/0.79039. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56138/0.79495. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56229/0.79159. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56153/0.80057. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56079/0.79732. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54678/0.79335. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55260/0.80960. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55323/0.80557. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55147/0.80410. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.54775/0.79776. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54244/0.80238. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54154/0.81039. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53159/0.81768. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69321/0.69512. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69348/0.69462. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69434. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69242/0.69404. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69214/0.69381. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69225/0.69366. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69176/0.69349. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69337. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69046/0.69303. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68964/0.69275. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68930/0.69248. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.69230. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68709/0.69227. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68671/0.69217. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68633/0.69201. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68508/0.69210. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68513/0.69203. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68411/0.69236. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68347/0.69247. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68353/0.69267. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68347/0.69298. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68230/0.69265. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68156/0.69286. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68149/0.69252. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68040/0.69320. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68170/0.69293. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68023/0.69339. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67988/0.69301. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67743/0.69266. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67880/0.69296. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67835/0.69267. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67731/0.69221. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67508/0.69191. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67559/0.69146. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67582/0.69159. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67517/0.69110. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67310/0.69071. Took 0.12 sec\n",
      "Epoch 37, Loss(train/val) 0.67251/0.69063. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.67314/0.69001. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67286/0.68949. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67153/0.68899. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66807/0.68832. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66992/0.68760. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66807/0.68690. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66671/0.68680. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66493/0.68603. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66631/0.68558. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66506/0.68491. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66209/0.68364. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66003/0.68411. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65869/0.68279. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65998/0.68241. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65760/0.68135. Took 0.12 sec\n",
      "Epoch 53, Loss(train/val) 0.65599/0.68148. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65435/0.68095. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65244/0.68151. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64999/0.68053. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64835/0.68013. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64699/0.68148. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64384/0.68115. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64566/0.67945. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64097/0.67885. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64018/0.67843. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63820/0.67927. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63841/0.67841. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63451/0.67965. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63259/0.67855. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63189/0.67847. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62766/0.67858. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62974/0.67748. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62201/0.67692. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61625/0.67701. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61801/0.67829. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61770/0.67717. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61862/0.67646. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60928/0.67859. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60969/0.67769. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60479/0.67851. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60017/0.67950. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59958/0.67818. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59474/0.67951. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59909/0.67915. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59174/0.67990. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59387/0.68020. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58455/0.68074. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59003/0.68141. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.58317/0.68137. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58451/0.68437. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57664/0.68485. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57301/0.68647. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57782/0.69003. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56759/0.69151. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56678/0.69834. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56924/0.69466. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.56155/0.69420. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55807/0.69858. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55541/0.69820. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55529/0.70473. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54731/0.70829. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55192/0.70980. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69796/0.69838. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69418/0.70049. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69171/0.70307. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.70564. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68893/0.70832. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68778/0.71122. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68780/0.71362. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68685/0.71592. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68642/0.71824. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68564/0.72015. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68657/0.72149. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68438/0.72323. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68563/0.72435. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68486/0.72528. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68578/0.72566. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68437/0.72624. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68426/0.72688. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68372/0.72750. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68422/0.72798. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68360/0.72782. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68331/0.72819. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68344/0.72795. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68341/0.72815. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68317/0.72828. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68264/0.72843. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68251/0.72842. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68235/0.72858. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68286/0.72878. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68228/0.72833. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68202/0.72876. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68122/0.72850. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68156/0.72878. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68104/0.72876. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68166/0.72899. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68088/0.72908. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68100/0.72869. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68101/0.72854. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68022/0.72853. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67983/0.72847. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67936/0.72878. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68012/0.72899. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68013/0.72869. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67797/0.72915. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67869/0.72890. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67746/0.72923. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67866/0.72926. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67842/0.72889. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67748/0.72917. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67818/0.72924. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67566/0.72938. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67675/0.72946. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67585/0.72925. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67582/0.72888. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67526/0.72899. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67422/0.72931. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67506/0.72914. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67347/0.72906. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67515/0.72860. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67287/0.72877. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67274/0.72869. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67308/0.72881. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67244/0.72870. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67065/0.72870. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67017/0.72825. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66947/0.72879. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67064/0.72891. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67091/0.72859. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66639/0.72970. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66714/0.72949. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66665/0.72974. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66547/0.72994. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66428/0.72989. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66605/0.72967. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66192/0.72990. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66426/0.72978. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66542/0.72977. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66104/0.72929. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66133/0.73019. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66108/0.72984. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66071/0.72940. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65905/0.73030. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65962/0.72929. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65630/0.72975. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65809/0.73055. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65692/0.73040. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65546/0.73039. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65449/0.73269. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65480/0.73281. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65399/0.73291. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65152/0.73339. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65179/0.73369. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64831/0.73382. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64737/0.73508. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64844/0.73586. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64750/0.73661. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64523/0.73590. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64482/0.73635. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64391/0.73844. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64353/0.73873. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64076/0.73951. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69353/0.69113. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69165/0.69124. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69101/0.69125. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69065/0.69118. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69006/0.69113. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68935/0.69093. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68920/0.69082. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68898/0.69082. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68870/0.69087. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68738/0.69103. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68672/0.69099. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68709/0.69110. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68624/0.69120. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68538/0.69125. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68548/0.69128. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68605/0.69123. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68469/0.69149. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68344/0.69148. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68314/0.69202. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68309/0.69206. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68241/0.69215. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68107/0.69195. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67928/0.69212. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68000/0.69212. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67835/0.69229. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67954/0.69233. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67941/0.69203. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67665/0.69170. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67687/0.69188. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67675/0.69231. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67596/0.69251. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67457/0.69264. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67272/0.69271. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67312/0.69282. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67276/0.69302. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67295/0.69254. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66895/0.69267. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66988/0.69319. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66794/0.69332. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66891/0.69332. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66505/0.69344. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66790/0.69359. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66488/0.69315. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66309/0.69315. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66447/0.69380. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66263/0.69394. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66108/0.69426. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66220/0.69416. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65775/0.69418. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65869/0.69449. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65586/0.69417. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65570/0.69424. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65210/0.69436. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65439/0.69535. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65234/0.69536. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65210/0.69537. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65257/0.69480. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65225/0.69583. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64997/0.69506. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64909/0.69471. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64401/0.69441. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64383/0.69545. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64208/0.69570. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64146/0.69636. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64491/0.69606. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64054/0.69500. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64157/0.69534. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63636/0.69581. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64012/0.69501. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63544/0.69632. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63592/0.69644. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63401/0.69390. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63030/0.69651. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63112/0.69662. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62939/0.69856. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62747/0.69629. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62334/0.69829. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62592/0.69688. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62553/0.70009. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61863/0.69827. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61824/0.69732. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62332/0.69759. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61515/0.69749. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61604/0.69694. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61532/0.69761. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61450/0.69911. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61341/0.69865. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60729/0.69836. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60337/0.70140. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60463/0.70331. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60512/0.70344. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60090/0.70447. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59805/0.70785. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59958/0.70824. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59205/0.70423. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60235/0.70557. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.59660/0.70358. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.59252/0.70599. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.59338/0.70460. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57924/0.70618. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69110/0.69356. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68990/0.69344. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68961/0.69335. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68953/0.69331. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68889/0.69325. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68847/0.69332. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68773/0.69317. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68726/0.69288. Took 0.14 sec\n",
      "Epoch 8, Loss(train/val) 0.68629/0.69253. Took 0.14 sec\n",
      "Epoch 9, Loss(train/val) 0.68537/0.69211. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68491/0.69183. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68311/0.69138. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68229/0.69129. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68004/0.69142. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67929/0.69158. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67869/0.69164. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67540/0.69178. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67474/0.69332. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67334/0.69464. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.66883/0.69491. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66897/0.69703. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66612/0.69693. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.66460/0.69549. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66435/0.70017. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.66092/0.70001. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.65874/0.70152. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.65627/0.70346. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.65490/0.70283. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65469/0.70565. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.64751/0.70574. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.64571/0.70923. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.64221/0.71265. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.63965/0.71063. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.64003/0.71055. Took 0.12 sec\n",
      "Epoch 34, Loss(train/val) 0.63631/0.71217. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.63681/0.71303. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.63425/0.71839. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.62958/0.72048. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.62618/0.72107. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.62334/0.72450. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.61734/0.72909. Took 0.11 sec\n",
      "Epoch 41, Loss(train/val) 0.61240/0.72999. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.61030/0.73176. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.60821/0.73473. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.60635/0.74045. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.59707/0.74093. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.59717/0.74393. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.59328/0.74776. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.58992/0.75130. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.58499/0.75742. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.58343/0.76321. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.57208/0.76892. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.57503/0.77213. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.57009/0.77782. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.57036/0.78010. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.56928/0.78409. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.56241/0.78752. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.55891/0.79062. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.55254/0.79876. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.55433/0.80250. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.54328/0.80948. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.54311/0.81256. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.53974/0.81690. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.53888/0.82398. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.53683/0.83247. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.52851/0.83314. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.52155/0.83670. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.52359/0.84378. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.52091/0.84267. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.52069/0.85331. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.50681/0.86248. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.50751/0.87421. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.50413/0.87990. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.50900/0.87702. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.50322/0.89639. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.49769/0.89093. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.49190/0.90388. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.48377/0.91041. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.48503/0.91142. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.48896/0.92526. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.47637/0.92728. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.47720/0.93385. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.47164/0.94903. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.47051/0.94685. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.46547/0.95577. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.47019/0.95530. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.47220/0.95829. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.45921/0.96645. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.45392/0.98597. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.46072/0.98546. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.45098/0.98850. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.45325/0.99619. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.44515/1.01197. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.43668/1.02413. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.44042/1.01975. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.43053/1.04569. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.43727/1.03863. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.44214/1.02985. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.42608/1.04282. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.43712/1.05026. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69373/0.69886. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.69866. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.69877. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69011/0.69890. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68990/0.69901. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68913/0.69930. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68765/0.69985. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68742/0.70060. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68666/0.70121. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68586/0.70244. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68471/0.70371. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68477/0.70464. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68404/0.70561. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68244/0.70653. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68248/0.70800. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68129/0.70873. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67887/0.71016. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67972/0.71102. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67881/0.71140. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67773/0.71167. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67714/0.71167. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67806/0.71219. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67664/0.71214. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67569/0.71290. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67566/0.71314. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67364/0.71261. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67511/0.71252. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67329/0.71330. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67440/0.71292. Took 0.12 sec\n",
      "Epoch 29, Loss(train/val) 0.67247/0.71404. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67246/0.71415. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67194/0.71467. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67208/0.71457. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67256/0.71435. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66854/0.71504. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67008/0.71676. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66869/0.71785. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66769/0.71789. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66778/0.71806. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66732/0.71934. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66553/0.71882. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66529/0.71937. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66633/0.71962. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66503/0.72015. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66467/0.72069. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66377/0.72044. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66452/0.72027. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66209/0.72047. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66302/0.72033. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66019/0.72089. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66295/0.72129. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65905/0.72090. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65681/0.72157. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65625/0.72297. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65627/0.72276. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65603/0.72263. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65394/0.72346. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65636/0.72239. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65362/0.72349. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65259/0.72466. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65422/0.72358. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.65133/0.72394. Took 0.15 sec\n",
      "Epoch 62, Loss(train/val) 0.65184/0.72300. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65082/0.72343. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64813/0.72508. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64846/0.72378. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64317/0.72208. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64420/0.72193. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.64234/0.72308. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63890/0.72248. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64158/0.72304. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63792/0.72291. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63881/0.72263. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63292/0.72125. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63696/0.72372. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63479/0.72186. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63047/0.71983. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62903/0.72099. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.62639/0.72144. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62372/0.72372. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.62674/0.72391. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62181/0.71876. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62314/0.71928. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61925/0.71873. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61880/0.72089. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61572/0.72001. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62074/0.71441. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61474/0.71799. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60659/0.72074. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60704/0.72235. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60316/0.71656. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60519/0.71910. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59998/0.71672. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59884/0.71538. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59375/0.71421. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59415/0.71153. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59216/0.72319. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58959/0.71633. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58833/0.71140. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58509/0.71601. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69284/0.68418. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69124/0.68416. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69106/0.68377. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.68393. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69021/0.68381. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69002/0.68370. Took 0.12 sec\n",
      "Epoch 6, Loss(train/val) 0.68957/0.68302. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68931/0.68280. Took 0.16 sec\n",
      "Epoch 8, Loss(train/val) 0.68762/0.68239. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68767/0.68189. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68659/0.68115. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68624/0.68079. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68399/0.68023. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68273/0.68040. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68067/0.67974. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.67833/0.67928. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.67814/0.67999. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67650/0.68010. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67453/0.68006. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67448/0.67908. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67304/0.67780. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67019/0.67958. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67009/0.67917. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66700/0.67715. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66644/0.67511. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66512/0.67664. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66379/0.67694. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66263/0.67457. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66055/0.67233. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65968/0.67421. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65490/0.67113. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65592/0.67019. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.65467/0.66631. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65513/0.66796. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65187/0.66596. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64750/0.66353. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65028/0.66392. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.64725/0.66136. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.64523/0.65847. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.64606/0.65772. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.64007/0.65618. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63767/0.65862. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63443/0.65659. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63457/0.65511. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63220/0.65597. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62852/0.65531. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62859/0.65342. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62683/0.65133. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.62344/0.65253. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.62547/0.65035. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.62049/0.65042. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.61660/0.65083. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61412/0.65270. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61598/0.65196. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60908/0.65009. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.60903/0.65092. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60627/0.64996. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60856/0.64847. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60012/0.64805. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60405/0.64857. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59853/0.64578. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.60036/0.64975. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59153/0.64945. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59062/0.65118. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59162/0.64909. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59030/0.64976. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58325/0.65052. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58500/0.65070. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58164/0.65118. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57833/0.65417. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57390/0.65400. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57315/0.65194. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56967/0.65584. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56905/0.65706. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56173/0.65472. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55733/0.65674. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56379/0.65723. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55315/0.65788. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55374/0.66096. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54963/0.66201. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55155/0.66087. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54633/0.66276. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54445/0.66301. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53459/0.67197. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.53626/0.66977. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53515/0.67061. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53341/0.67177. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52996/0.67783. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52859/0.67704. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52644/0.67807. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52300/0.67703. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52847/0.67885. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51174/0.68353. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50472/0.68571. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51477/0.69114. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51092/0.68616. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50982/0.69056. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49947/0.69361. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49626/0.69468. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.49768/0.69419. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69128/0.69167. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68964/0.69185. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68927/0.69228. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68907/0.69273. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68780/0.69336. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68584/0.69409. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68729/0.69467. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68658/0.69525. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68559/0.69577. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68499/0.69621. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68469/0.69661. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68474/0.69688. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68418/0.69757. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68447/0.69767. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68407/0.69766. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68236/0.69787. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68263/0.69812. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68279/0.69760. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68193/0.69717. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68162/0.69725. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68142/0.69743. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68066/0.69803. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67989/0.69757. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67999/0.69728. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68078/0.69628. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67785/0.69614. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67789/0.69616. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67892/0.69613. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67689/0.69569. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67793/0.69543. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67656/0.69545. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67763/0.69479. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67523/0.69442. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67574/0.69405. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67521/0.69403. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67360/0.69481. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67188/0.69480. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67307/0.69440. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67218/0.69319. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66995/0.69288. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67006/0.69363. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66977/0.69365. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66819/0.69368. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66899/0.69308. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66836/0.69305. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66664/0.69394. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66727/0.69285. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66691/0.69286. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66517/0.69462. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66638/0.69329. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66398/0.69326. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66451/0.69304. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66435/0.69296. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66240/0.69283. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66325/0.69401. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66056/0.69385. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66002/0.69270. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66011/0.69344. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65898/0.69463. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65871/0.69581. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65725/0.69722. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65529/0.69560. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65587/0.69681. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65686/0.69638. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65140/0.69847. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65255/0.69922. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65222/0.69915. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65095/0.69870. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65082/0.69879. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65301/0.69972. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65118/0.70260. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64957/0.70172. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64704/0.70178. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64862/0.70283. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64678/0.70358. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64692/0.70176. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64412/0.70473. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64437/0.70713. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63963/0.70562. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63708/0.71022. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63881/0.70953. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63913/0.71089. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63859/0.71047. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63799/0.71110. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63594/0.71087. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63291/0.71343. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63272/0.71392. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62944/0.71778. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63315/0.71761. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62848/0.71878. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62554/0.72062. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63128/0.72104. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62778/0.72323. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62353/0.72453. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62378/0.72142. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61856/0.72381. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61748/0.72858. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61503/0.73041. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61449/0.72961. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61635/0.73062. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69560/0.69626. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68948/0.69893. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68921/0.70058. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68836/0.70254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68746/0.70430. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68628/0.70610. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68631/0.70750. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68548/0.70864. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68551/0.70940. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68456/0.71076. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68430/0.71131. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68409/0.71135. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68322/0.71158. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68198/0.71172. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68172/0.71117. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68075/0.71077. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68066/0.71079. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68043/0.70983. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67933/0.70917. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67901/0.70786. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67881/0.70701. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67630/0.70587. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67720/0.70488. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67614/0.70319. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67533/0.70273. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67375/0.70180. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67336/0.70133. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67134/0.70005. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67204/0.69861. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67206/0.69738. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67005/0.69659. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66840/0.69506. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66800/0.69413. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66762/0.69349. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.66544/0.69324. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66450/0.69084. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66586/0.69117. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66228/0.69004. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66002/0.69026. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66262/0.68887. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66008/0.68755. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65913/0.68707. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65639/0.68752. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65843/0.68528. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65499/0.68575. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65227/0.68483. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65294/0.68463. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65224/0.68587. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64774/0.68415. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64690/0.68499. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64772/0.68578. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64248/0.68638. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64240/0.68662. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64168/0.68616. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63981/0.68637. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63701/0.68901. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63803/0.68992. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63862/0.69154. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63502/0.69200. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63230/0.69271. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63062/0.69318. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62953/0.69720. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62606/0.69849. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.62613/0.69635. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62199/0.70130. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62095/0.70168. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62113/0.70285. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61751/0.70643. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61602/0.70960. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61238/0.71261. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61277/0.71502. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60690/0.72246. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60663/0.71697. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60428/0.72508. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60009/0.72794. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.60158/0.73098. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59687/0.73687. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.59703/0.74165. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59737/0.74227. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59363/0.74224. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59325/0.74991. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.59031/0.75646. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58969/0.75456. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58579/0.75762. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58112/0.76487. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58158/0.76836. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57798/0.77169. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57520/0.77270. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57173/0.79146. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57445/0.78216. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.56754/0.78518. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57269/0.78753. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56471/0.80000. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57024/0.79827. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56387/0.81020. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55688/0.80904. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55559/0.82106. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56221/0.82284. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55271/0.81811. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55294/0.83018. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69341/0.68840. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69040/0.68654. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68935/0.68622. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68907/0.68603. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68866/0.68585. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68784/0.68589. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68805/0.68587. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68764/0.68585. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.68573. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68683/0.68537. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68654/0.68520. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68611/0.68522. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68525/0.68497. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68514/0.68489. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68386/0.68414. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68339/0.68408. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68315/0.68347. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68211/0.68273. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68091/0.68275. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68021/0.68209. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68004/0.68102. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67899/0.68058. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67836/0.67950. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67829/0.67885. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67568/0.67822. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67733/0.67788. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67537/0.67642. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67478/0.67545. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67298/0.67384. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67332/0.67308. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67264/0.67324. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67099/0.67219. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67163/0.67185. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67039/0.67101. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66806/0.67098. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66675/0.67002. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66669/0.67003. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66522/0.67047. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66505/0.67000. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66662/0.67120. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66415/0.67086. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66162/0.67090. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66241/0.66983. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65979/0.66964. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65913/0.66969. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65911/0.67088. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65746/0.67040. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65701/0.67119. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65835/0.67135. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65385/0.67153. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65504/0.67178. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65440/0.67129. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65104/0.67179. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65048/0.67073. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65221/0.67130. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64963/0.67239. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64701/0.67226. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64728/0.67195. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64790/0.67151. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64229/0.67326. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64405/0.67256. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64310/0.67203. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64077/0.67086. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64006/0.67025. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63752/0.67127. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63907/0.67071. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63790/0.67008. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63387/0.66977. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63513/0.67077. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63504/0.67036. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63088/0.67139. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63188/0.67006. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62796/0.66901. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62841/0.66860. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62911/0.66768. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62691/0.66899. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62401/0.66736. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62371/0.66689. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62397/0.66510. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61703/0.66535. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61848/0.66296. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61545/0.66567. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61453/0.66494. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61097/0.66221. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61514/0.66264. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61035/0.66164. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.60798/0.66313. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60547/0.66401. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60402/0.66103. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.60190/0.65857. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59825/0.65924. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59948/0.65828. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59874/0.65745. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.59395/0.65639. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59164/0.65814. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59358/0.65641. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59424/0.65734. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59461/0.65572. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58860/0.65472. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.58740/0.65068. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69353/0.68917. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69087/0.68742. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69056/0.68657. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68910/0.68564. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68845/0.68476. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68797/0.68382. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68733/0.68280. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68611/0.68159. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68610/0.68078. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68492/0.67968. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68400/0.67858. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68275/0.67775. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68282/0.67711. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68034/0.67624. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67991/0.67545. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67851/0.67487. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67685/0.67437. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67861/0.67444. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67728/0.67415. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.67533/0.67411. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67529/0.67427. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67388/0.67454. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67227/0.67457. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67181/0.67511. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67016/0.67520. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66992/0.67613. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67078/0.67643. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66745/0.67755. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66576/0.67794. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66568/0.67952. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66198/0.68123. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66259/0.68237. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65953/0.68333. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65870/0.68420. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65861/0.68524. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65461/0.68662. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65610/0.68722. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65253/0.68832. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65348/0.68985. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65251/0.69299. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64590/0.69515. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64858/0.69560. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64512/0.69783. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64296/0.70174. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64030/0.70350. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64116/0.70516. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63946/0.70682. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63746/0.71019. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63560/0.71288. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63667/0.71451. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63401/0.71545. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63480/0.71677. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63039/0.71888. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63295/0.71993. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62741/0.72341. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62563/0.72556. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62477/0.72711. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62450/0.72682. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62420/0.72842. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62179/0.73016. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61778/0.73147. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61777/0.73428. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61515/0.73631. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61624/0.73493. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61054/0.73617. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60960/0.74015. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60860/0.74130. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60586/0.74312. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60703/0.74449. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60503/0.74482. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60519/0.74480. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60132/0.74489. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59937/0.74476. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59620/0.74492. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59602/0.74828. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59068/0.74803. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59252/0.75294. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58924/0.75059. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58727/0.75065. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59036/0.75502. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57868/0.75320. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58068/0.75414. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57883/0.75681. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57824/0.75496. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57303/0.75784. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57634/0.75769. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57175/0.75775. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56570/0.75822. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56303/0.75921. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55785/0.76552. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.56408/0.76522. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55900/0.76669. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55908/0.76553. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55783/0.76775. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55006/0.76952. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54613/0.77185. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55105/0.77689. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55039/0.78109. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53770/0.78357. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.54217/0.78886. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69378/0.69161. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69309/0.69181. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.69221. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69066/0.69283. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68987/0.69362. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68971/0.69423. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68895/0.69481. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68876/0.69528. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68852/0.69546. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68894/0.69576. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68837/0.69591. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68827/0.69606. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68784/0.69624. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68837/0.69611. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68672/0.69615. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68637/0.69643. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68707/0.69667. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68722/0.69683. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68609/0.69702. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68534/0.69722. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68398/0.69733. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68362/0.69771. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68364/0.69819. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68400/0.69844. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68206/0.69874. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68187/0.69889. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67890/0.69958. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68104/0.70032. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67989/0.70009. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67905/0.70048. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67763/0.70067. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67841/0.70056. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67614/0.70097. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67678/0.70152. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67350/0.70169. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67355/0.70173. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67296/0.70213. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67165/0.70230. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67061/0.70351. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67001/0.70351. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67010/0.70316. Took 0.13 sec\n",
      "Epoch 41, Loss(train/val) 0.66705/0.70421. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66659/0.70526. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66671/0.70506. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66459/0.70497. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66198/0.70487. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66178/0.70615. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66306/0.70587. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66066/0.70683. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65866/0.70797. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65711/0.70823. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65630/0.70893. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65416/0.71027. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.65141/0.71095. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.65038/0.71117. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.65210/0.71074. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64376/0.71217. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64758/0.71432. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64266/0.71552. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64307/0.71807. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64095/0.71981. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63648/0.72119. Took 0.12 sec\n",
      "Epoch 62, Loss(train/val) 0.63706/0.72230. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.63403/0.72592. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.63192/0.72742. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.62956/0.72832. Took 0.12 sec\n",
      "Epoch 66, Loss(train/val) 0.62634/0.73148. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62519/0.73443. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.62313/0.73588. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62028/0.73875. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62001/0.73903. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61942/0.74199. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62105/0.74389. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61062/0.74759. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60965/0.74924. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60938/0.74946. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60514/0.75524. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.60630/0.75940. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.59766/0.76341. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.59629/0.76644. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.59320/0.77102. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.59797/0.77304. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58935/0.77518. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.59625/0.77814. Took 0.15 sec\n",
      "Epoch 84, Loss(train/val) 0.58779/0.78328. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.58626/0.78372. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.57551/0.79309. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.57897/0.79529. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.57457/0.79521. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.57575/0.79966. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.57163/0.80470. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.56118/0.80640. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.56271/0.80518. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.55834/0.81386. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.56167/0.81954. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55824/0.82652. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55625/0.82387. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.55784/0.83109. Took 0.12 sec\n",
      "Epoch 98, Loss(train/val) 0.54570/0.83530. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.54659/0.83923. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69841/0.70175. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69656/0.70003. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69448/0.69784. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69306/0.69516. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69047/0.69243. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68890/0.69051. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68887/0.68968. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68799/0.68924. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68615/0.68915. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68577/0.68911. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68482/0.68917. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68484/0.68965. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68290/0.68998. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68261/0.69027. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.68089/0.69034. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68040/0.69128. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67853/0.69172. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67660/0.69202. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67739/0.69291. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67460/0.69398. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67211/0.69526. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67253/0.69545. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67071/0.69638. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66809/0.69597. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66699/0.69755. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66497/0.69868. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66855/0.69869. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66575/0.69867. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66031/0.70069. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66294/0.70125. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65873/0.70205. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65895/0.70308. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.65704/0.70417. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65284/0.70458. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65298/0.70647. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65242/0.70687. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65051/0.70784. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65196/0.70952. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.64958/0.71094. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64909/0.71141. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.64590/0.71067. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64657/0.71207. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.64621/0.71215. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64132/0.71260. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63997/0.71276. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63848/0.71426. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64043/0.71495. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63517/0.71722. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63603/0.71680. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63491/0.71454. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63299/0.71441. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62982/0.71717. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62983/0.71632. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62485/0.71733. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62315/0.71757. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62306/0.71832. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62263/0.71769. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61871/0.72047. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.61732/0.72186. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61967/0.71891. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.61581/0.71792. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61247/0.71838. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61612/0.72115. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.61264/0.72136. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60898/0.72182. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60826/0.72545. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60703/0.72792. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60903/0.72284. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59981/0.72691. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.59793/0.72609. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59336/0.73074. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59724/0.72569. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.59632/0.72837. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59224/0.72921. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.59297/0.73163. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59395/0.73260. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58915/0.73314. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58592/0.73278. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.58308/0.73422. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57944/0.73680. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58661/0.73645. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58292/0.73733. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57898/0.73494. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.57694/0.73774. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57256/0.74099. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56884/0.74108. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.56825/0.74360. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55840/0.74386. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.56314/0.74563. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.55826/0.74373. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55591/0.74517. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56142/0.74927. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.55594/0.74874. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55435/0.75121. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.55511/0.75333. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54326/0.75199. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.54384/0.75571. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54093/0.75489. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54161/0.75245. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52844/0.76230. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69497/0.69625. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69415/0.69475. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69228/0.69511. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.69166/0.69561. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69068/0.69576. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69017/0.69629. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68917/0.69671. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68807/0.69654. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68804/0.69687. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68721/0.69742. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68531/0.69795. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68411/0.69898. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68365/0.69879. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68316/0.69866. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68236/0.70078. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68226/0.70087. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67958/0.70127. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68029/0.70141. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67699/0.70371. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67660/0.70432. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 0.67670/0.70409. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67410/0.70646. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67362/0.70595. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67264/0.70795. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67017/0.70951. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66892/0.71070. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66833/0.71332. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66631/0.71396. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66716/0.71437. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66274/0.71514. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66327/0.71763. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66352/0.71932. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66317/0.72052. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66014/0.72058. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65687/0.72387. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65804/0.72456. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65792/0.72490. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65215/0.72714. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65310/0.72893. Took 0.14 sec\n",
      "Epoch 39, Loss(train/val) 0.65105/0.73132. Took 0.12 sec\n",
      "Epoch 40, Loss(train/val) 0.65236/0.72934. Took 0.12 sec\n",
      "Epoch 41, Loss(train/val) 0.65166/0.73152. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64449/0.73226. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64120/0.73239. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64281/0.73611. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.64047/0.73463. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64177/0.73597. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63952/0.73575. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63260/0.73991. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63436/0.73863. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63320/0.73937. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.63170/0.74149. Took 0.14 sec\n",
      "Epoch 52, Loss(train/val) 0.62932/0.74142. Took 0.16 sec\n",
      "Epoch 53, Loss(train/val) 0.62860/0.74106. Took 0.14 sec\n",
      "Epoch 54, Loss(train/val) 0.62344/0.74413. Took 0.14 sec\n",
      "Epoch 55, Loss(train/val) 0.62368/0.74184. Took 0.12 sec\n",
      "Epoch 56, Loss(train/val) 0.62193/0.74575. Took 0.17 sec\n",
      "Epoch 57, Loss(train/val) 0.62135/0.74381. Took 0.16 sec\n",
      "Epoch 58, Loss(train/val) 0.61193/0.74499. Took 0.16 sec\n",
      "Epoch 59, Loss(train/val) 0.61358/0.74530. Took 0.14 sec\n",
      "Epoch 60, Loss(train/val) 0.60882/0.75007. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61332/0.75388. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.61043/0.75495. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60945/0.75965. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.60569/0.76060. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.60055/0.75966. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60496/0.76387. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.59792/0.76422. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.59312/0.76513. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.59504/0.76825. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.58904/0.77145. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58442/0.77322. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58317/0.77656. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.58177/0.77712. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.57646/0.78538. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58068/0.78413. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.57479/0.78485. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57708/0.78598. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.57568/0.78531. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.56827/0.78924. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.56288/0.78565. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.55924/0.79114. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.56521/0.79464. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.55977/0.79189. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.56078/0.79296. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.54997/0.79899. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.55034/0.80406. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.54944/0.81542. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.55015/0.81038. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.55009/0.80958. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.53519/0.81565. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.53932/0.82227. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.53436/0.82546. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.53121/0.82922. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.51868/0.83825. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.52547/0.84271. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.52422/0.84726. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.51534/0.85047. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.51278/0.84889. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.50591/0.84679. Took 0.10 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69439/0.69814. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69325/0.69633. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69238/0.69574. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69302/0.69565. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69266/0.69581. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69122/0.69568. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69221/0.69584. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69127/0.69580. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69070/0.69681. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69159/0.69698. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69071/0.69754. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68933/0.69764. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68789/0.69868. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68994/0.69939. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68810/0.69985. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68733/0.70066. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68797/0.70166. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68624/0.70176. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68654/0.70229. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68537/0.70239. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68528/0.70311. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68315/0.70301. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68201/0.70400. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68344/0.70613. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68150/0.70476. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68059/0.70511. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68019/0.70664. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67866/0.70725. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67790/0.70902. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67664/0.70616. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67514/0.70807. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67630/0.70767. Took 0.11 sec\n",
      "Epoch 32, Loss(train/val) 0.67616/0.71227. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67322/0.71110. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67263/0.71146. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66930/0.70997. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66808/0.71407. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66786/0.71238. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66739/0.71778. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66696/0.71693. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66695/0.71854. Took 0.11 sec\n",
      "Epoch 41, Loss(train/val) 0.66187/0.71616. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66283/0.72175. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66027/0.72486. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65953/0.72276. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65908/0.72511. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65671/0.72619. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65265/0.72720. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65201/0.73185. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64854/0.72830. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64619/0.73660. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64466/0.73586. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64834/0.74060. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64178/0.74326. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64412/0.73975. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.63733/0.74343. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63817/0.74888. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64027/0.75350. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63799/0.75586. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.63517/0.75512. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62826/0.76021. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63630/0.76984. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.62512/0.76571. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62849/0.76537. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62713/0.76835. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62032/0.77034. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62526/0.77256. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61746/0.78569. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61525/0.77914. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61779/0.77388. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61832/0.77573. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61627/0.78583. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61616/0.78978. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61115/0.78363. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61118/0.78725. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.60579/0.78036. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60651/0.78834. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60541/0.78785. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60573/0.79163. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60222/0.79029. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59835/0.79394. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59841/0.79689. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59827/0.79262. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59464/0.79695. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58740/0.80508. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59531/0.80133. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.58547/0.80062. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58648/0.81211. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58320/0.80869. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58126/0.80574. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57585/0.80453. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.58049/0.80299. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.56778/0.81234. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56745/0.81920. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57231/0.81160. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56878/0.81520. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56244/0.82298. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57231/0.81795. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55512/0.82271. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55705/0.82579. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69242/0.69650. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69215/0.69694. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69112/0.69760. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69112/0.69829. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69140/0.69916. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69076/0.69991. Took 0.14 sec\n",
      "Epoch 6, Loss(train/val) 0.68877/0.70112. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68994/0.70230. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68891/0.70347. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68788/0.70487. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68815/0.70593. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68699/0.70701. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68731/0.70808. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68739/0.70889. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68631/0.70964. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68545/0.71081. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68659/0.71174. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68551/0.71231. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68472/0.71268. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68427/0.71308. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68378/0.71342. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68291/0.71450. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68318/0.71472. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68231/0.71487. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68198/0.71476. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68115/0.71500. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68203/0.71428. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68069/0.71381. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68079/0.71332. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67929/0.71291. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67983/0.71275. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67825/0.71195. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67638/0.71147. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67588/0.71143. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67642/0.71101. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67462/0.71009. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67371/0.70901. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67465/0.70944. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67278/0.70848. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67242/0.70873. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67018/0.70891. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67128/0.70727. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66742/0.70638. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66765/0.70490. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66558/0.70376. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66539/0.70121. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66383/0.70142. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66284/0.70034. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65991/0.69851. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65964/0.69849. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65749/0.69867. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65574/0.69892. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65582/0.69908. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65358/0.69753. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65314/0.69801. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64922/0.69511. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64889/0.69492. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64750/0.69580. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64763/0.69676. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64625/0.69686. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64393/0.69864. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64505/0.69796. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64059/0.69694. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.64272/0.69438. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.63561/0.69680. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63492/0.69870. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.63668/0.69723. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62892/0.70178. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63086/0.70533. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62665/0.70520. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62634/0.70427. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62580/0.70777. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62282/0.70815. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62519/0.70775. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61807/0.71091. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61687/0.71197. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61468/0.71395. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61025/0.70904. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61261/0.71448. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60695/0.71519. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60527/0.71802. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61143/0.71971. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60213/0.71943. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59813/0.72022. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59878/0.72341. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59613/0.72770. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59582/0.73138. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59249/0.73339. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59223/0.73154. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59097/0.73718. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58834/0.73640. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.58504/0.74499. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58372/0.74497. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58163/0.74143. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58480/0.74417. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57481/0.74209. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57456/0.74860. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57557/0.75519. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57733/0.75554. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56734/0.76326. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69202/0.69243. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69018/0.69271. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68961/0.69259. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68782/0.69197. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68747/0.69019. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68519/0.68880. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68327/0.68821. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68555/0.68556. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68261/0.68254. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68185/0.68143. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.67925/0.67949. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.67835/0.67847. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.67825/0.67628. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.67775/0.67522. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67447/0.67298. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67406/0.67513. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67468/0.67484. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67225/0.67218. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67038/0.67168. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67034/0.67016. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67064/0.66785. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66879/0.66664. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.66614/0.66627. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66517/0.66377. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66405/0.66166. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66427/0.66328. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.65756/0.66037. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.66120/0.65975. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65688/0.65823. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65676/0.65525. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65053/0.65285. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65186/0.65230. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.64988/0.64905. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.64825/0.64454. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.64640/0.64471. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.64280/0.64365. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64326/0.64146. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64002/0.63615. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.64202/0.63574. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63633/0.63066. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.63506/0.63280. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.62818/0.63203. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63096/0.62551. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.63147/0.62695. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62906/0.62332. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62472/0.62108. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62087/0.62222. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62038/0.62246. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61870/0.61691. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.61715/0.61768. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61621/0.61258. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.61414/0.61353. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61198/0.61691. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60790/0.61740. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60494/0.61907. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60594/0.61565. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60297/0.61616. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60226/0.61263. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60192/0.61378. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59672/0.60730. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59129/0.60265. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.58751/0.60710. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58450/0.60682. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58336/0.60520. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57807/0.60546. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.57480/0.60227. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.57574/0.60338. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.57237/0.59924. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.56759/0.59782. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.57000/0.59440. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.56504/0.59815. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.56699/0.59603. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56921/0.60034. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55421/0.60430. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56297/0.59776. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55359/0.59601. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55496/0.59537. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54267/0.59841. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54101/0.59805. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53364/0.60137. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54352/0.59782. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53397/0.60413. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52447/0.60565. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53070/0.60914. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52172/0.61391. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52636/0.60729. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52039/0.61150. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51254/0.59869. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51196/0.60859. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52011/0.61303. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.51093/0.61372. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.50468/0.62118. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 0.49859/0.62360. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.49478/0.62562. Took 0.24 sec\n",
      "Epoch 94, Loss(train/val) 0.49289/0.63019. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.48999/0.62900. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.49232/0.63305. Took 0.12 sec\n",
      "Epoch 97, Loss(train/val) 0.49187/0.63106. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.48782/0.63746. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.47905/0.63563. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69383/0.69254. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69217/0.69236. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69341/0.69211. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.69187. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69107/0.69171. Took 0.12 sec\n",
      "Epoch 5, Loss(train/val) 0.69111/0.69147. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69144/0.69123. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68961/0.69106. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69004/0.69090. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68853/0.69065. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.69032. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68835/0.69011. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68721/0.68975. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68654/0.68939. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68421/0.68892. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68464/0.68865. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68317/0.68828. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68077/0.68783. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67893/0.68723. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67443/0.68680. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67608/0.68658. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67460/0.68668. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67126/0.68683. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66856/0.68704. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66824/0.68762. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66580/0.68713. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66421/0.68833. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66234/0.69041. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66025/0.69138. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65738/0.69346. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65745/0.69333. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65295/0.69671. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65574/0.69698. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65233/0.69796. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.64716/0.69871. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65005/0.70093. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.64474/0.70092. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64205/0.70072. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64352/0.70084. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64029/0.70275. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63948/0.70323. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63566/0.70510. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63321/0.70694. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63219/0.70688. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63016/0.70846. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62943/0.70829. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.62133/0.70833. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62415/0.71238. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61521/0.71358. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61873/0.71205. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61667/0.71234. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61615/0.71054. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61341/0.71066. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60591/0.71637. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60260/0.71915. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60630/0.71881. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60158/0.72537. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59988/0.72703. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59938/0.72445. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59744/0.72528. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59496/0.72784. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59188/0.72510. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58705/0.72507. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58150/0.73658. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58729/0.73406. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57883/0.73567. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58243/0.73566. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57240/0.74045. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57446/0.74410. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57099/0.73921. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56819/0.74069. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56876/0.74282. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56244/0.74849. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56434/0.74292. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56162/0.75229. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55323/0.75422. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55949/0.75332. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55368/0.76084. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.54560/0.76198. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54306/0.76545. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54161/0.76791. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53276/0.77523. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53298/0.78348. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54039/0.77244. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.52828/0.77643. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53732/0.77374. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52529/0.78712. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.51521/0.79913. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51449/0.80748. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51320/0.81924. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51001/0.82004. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51859/0.81913. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50591/0.82719. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50417/0.82466. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.50124/0.82838. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50319/0.82660. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.49620/0.83360. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48622/0.83852. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49876/0.84635. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49846/0.84322. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69365/0.69359. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69395. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69274/0.69435. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69170/0.69456. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69230/0.69480. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69068/0.69514. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69081/0.69541. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69024/0.69575. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68988/0.69627. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68924/0.69674. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68751/0.69699. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68817/0.69731. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68683/0.69779. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68613/0.69874. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68462/0.69925. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68395/0.69965. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68247/0.69997. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68270/0.70077. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68045/0.70139. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68013/0.70229. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67723/0.70272. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67692/0.70320. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67686/0.70338. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67489/0.70362. Took 0.12 sec\n",
      "Epoch 24, Loss(train/val) 0.67438/0.70374. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67216/0.70374. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67259/0.70372. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66895/0.70450. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66964/0.70405. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66681/0.70438. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66661/0.70380. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66525/0.70494. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66497/0.70506. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66289/0.70566. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66164/0.70562. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66075/0.70527. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66049/0.70548. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65678/0.70574. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65589/0.70624. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65610/0.70781. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65435/0.70860. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65138/0.70829. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65105/0.70979. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64833/0.71073. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64571/0.71155. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64587/0.71130. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64188/0.71304. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64060/0.71412. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64007/0.71635. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63859/0.71704. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63359/0.71893. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63519/0.72049. Took 0.12 sec\n",
      "Epoch 52, Loss(train/val) 0.63295/0.72111. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62956/0.72355. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.62262/0.72569. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.62420/0.72781. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62426/0.73046. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62094/0.72924. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62036/0.73172. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62360/0.73275. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.61729/0.73338. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61563/0.73478. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61372/0.74015. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.60793/0.74000. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61115/0.74048. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60555/0.74258. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60064/0.74636. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60473/0.74935. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60342/0.75162. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59688/0.75399. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59470/0.75364. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59449/0.75758. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59794/0.75845. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58791/0.76189. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58690/0.76406. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.58515/0.76573. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.57999/0.76507. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57984/0.77105. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.58354/0.77289. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57266/0.77659. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57773/0.77751. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.57689/0.78033. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57448/0.78071. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57437/0.78167. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57007/0.78642. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.56515/0.78807. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56090/0.79322. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.56234/0.79312. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55949/0.79132. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56795/0.79604. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.55547/0.79727. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55086/0.80496. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55624/0.80768. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.55266/0.80682. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.54753/0.81120. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.55054/0.81110. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.54835/0.81552. Took 0.23 sec\n",
      "Epoch 97, Loss(train/val) 0.54001/0.82326. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.54314/0.82136. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.53913/0.82622. Took 0.20 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69476/0.69432. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69306/0.69413. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.69528. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69207/0.69637. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 0.69094/0.69719. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.69824. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69041/0.69974. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69019/0.70103. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68978/0.70204. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68890/0.70399. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68815/0.70581. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68809/0.70812. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68570/0.71014. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68579/0.71279. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68531/0.71545. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68485/0.71743. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68436/0.72018. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68272/0.72280. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68261/0.72518. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68149/0.72732. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67978/0.72949. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67934/0.73147. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67789/0.73315. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67866/0.73490. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67763/0.73637. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67530/0.73884. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67471/0.74177. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67476/0.74263. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67293/0.74430. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67259/0.74556. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67246/0.74660. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67049/0.74903. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66775/0.75095. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66767/0.75389. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66810/0.75519. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66833/0.75557. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66452/0.75757. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66369/0.75908. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66242/0.76146. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66168/0.76246. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66120/0.76458. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66068/0.76574. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66030/0.76677. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66028/0.76907. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65767/0.76923. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65536/0.77162. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65722/0.77216. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65433/0.77358. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65232/0.77746. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65263/0.77859. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65251/0.77978. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64960/0.77970. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64960/0.78010. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64891/0.78222. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64746/0.78337. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64321/0.78569. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64232/0.78812. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64024/0.78976. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64301/0.79098. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63931/0.79366. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63751/0.79381. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63756/0.79412. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63502/0.79590. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63547/0.79719. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63449/0.79963. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62958/0.80061. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63112/0.80155. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62783/0.80277. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.62557/0.80513. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62393/0.80765. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62097/0.81022. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61815/0.81166. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61739/0.81419. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61865/0.81520. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61513/0.81638. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61260/0.81995. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61098/0.82499. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61194/0.83029. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60923/0.82980. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60752/0.83450. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60907/0.83525. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60563/0.83683. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60153/0.83575. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.60532/0.84089. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59972/0.84120. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59721/0.84479. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.59821/0.84797. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59096/0.85558. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58950/0.85773. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59169/0.85457. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58703/0.86220. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.58517/0.86464. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58785/0.86800. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58056/0.87177. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58203/0.87474. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58099/0.87673. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.57619/0.88253. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57515/0.88396. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57323/0.88836. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57184/0.88849. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69509/0.69662. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69371/0.69600. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.69539. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69492. Took 0.12 sec\n",
      "Epoch 4, Loss(train/val) 0.69171/0.69468. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69024/0.69436. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.69400. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68980/0.69367. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68820/0.69329. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68857/0.69287. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68728/0.69251. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68621/0.69217. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68540/0.69189. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68522/0.69148. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68423/0.69090. Took 0.15 sec\n",
      "Epoch 15, Loss(train/val) 0.68188/0.69061. Took 0.13 sec\n",
      "Epoch 16, Loss(train/val) 0.68240/0.69067. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68108/0.69032. Took 0.17 sec\n",
      "Epoch 18, Loss(train/val) 0.67956/0.69015. Took 0.13 sec\n",
      "Epoch 19, Loss(train/val) 0.67700/0.69005. Took 0.14 sec\n",
      "Epoch 20, Loss(train/val) 0.67682/0.69037. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67594/0.69010. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67410/0.69058. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67459/0.69132. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67141/0.69259. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67229/0.69322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66880/0.69320. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67085/0.69436. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66850/0.69484. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66687/0.69699. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66726/0.69734. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66753/0.69809. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66474/0.69871. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66264/0.69942. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66020/0.70200. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65913/0.70282. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65664/0.70477. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65700/0.70654. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65574/0.70799. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65683/0.70993. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65132/0.71114. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65357/0.71295. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.65041/0.71495. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64852/0.71667. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64764/0.71879. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.64615/0.72148. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64545/0.72280. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.64292/0.72461. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64112/0.72709. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63995/0.73099. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63739/0.73261. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63522/0.73495. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63521/0.73777. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63278/0.74132. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.62906/0.74264. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63212/0.74365. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62522/0.74543. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62513/0.74799. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62196/0.75167. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62312/0.75404. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61821/0.75561. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61524/0.75800. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61683/0.75914. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61569/0.76100. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61084/0.76335. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60656/0.76714. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60477/0.77057. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60727/0.77461. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60359/0.77764. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59864/0.77879. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59961/0.78145. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59479/0.78451. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59368/0.78743. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59118/0.79052. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58953/0.79326. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59133/0.79634. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58661/0.79928. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58785/0.80065. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57899/0.80163. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58033/0.80786. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57365/0.81083. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57752/0.81306. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57331/0.81558. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57162/0.81886. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57121/0.82156. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56723/0.82287. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56390/0.82534. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56044/0.82859. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56221/0.83282. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56186/0.83362. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.54949/0.83902. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55449/0.84066. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55400/0.84074. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55000/0.84482. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54336/0.84845. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54615/0.85610. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55036/0.85323. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54196/0.85594. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53831/0.86158. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53025/0.86779. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69250/0.69311. Took 0.17 sec\n",
      "Epoch 1, Loss(train/val) 0.69321/0.69297. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69280. Took 0.13 sec\n",
      "Epoch 3, Loss(train/val) 0.69200/0.69267. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69146/0.69240. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69187/0.69230. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69061/0.69230. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.69225. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69114/0.69218. Took 0.17 sec\n",
      "Epoch 9, Loss(train/val) 0.68961/0.69216. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68964/0.69218. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.69223. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68871/0.69226. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68811/0.69198. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68701/0.69170. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68697/0.69158. Took 0.17 sec\n",
      "Epoch 16, Loss(train/val) 0.68557/0.69173. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68374/0.69168. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68230/0.69161. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68176/0.69172. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68029/0.69191. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67992/0.69231. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67869/0.69277. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67753/0.69347. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67759/0.69308. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67663/0.69395. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67621/0.69382. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67400/0.69442. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67461/0.69509. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67193/0.69582. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67126/0.69698. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66927/0.69791. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66754/0.70011. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66418/0.70213. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66474/0.70353. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66182/0.70530. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66274/0.70733. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65974/0.71057. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66091/0.70936. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66058/0.71066. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65739/0.71199. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65534/0.71526. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65144/0.71597. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65257/0.71877. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65005/0.71902. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64840/0.72132. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64548/0.72353. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64248/0.72674. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64017/0.73047. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64024/0.73236. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63950/0.73606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64000/0.74014. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63823/0.74268. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63377/0.74168. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.62988/0.74498. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63412/0.74857. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62930/0.75207. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62973/0.75209. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63134/0.75556. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62344/0.75986. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62610/0.76142. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62333/0.76175. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62125/0.76591. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61792/0.76715. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61640/0.77121. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61152/0.77290. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61128/0.77844. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61589/0.77510. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.61288/0.77838. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60341/0.78519. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.60663/0.78267. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60249/0.78863. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60518/0.78613. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59720/0.78711. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60057/0.79328. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59449/0.79360. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60063/0.79824. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59968/0.79835. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59430/0.79587. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.59301/0.79872. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59105/0.79807. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58836/0.79944. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.58864/0.80338. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58592/0.80784. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.58460/0.80822. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58320/0.80839. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58016/0.81126. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57547/0.81394. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57884/0.81868. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57410/0.82179. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57363/0.82250. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57050/0.82523. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57001/0.82841. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56619/0.82581. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56127/0.82700. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56584/0.82959. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56302/0.82874. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56021/0.83981. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56108/0.84109. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55626/0.84009. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69356/0.69751. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69220/0.69726. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69697. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.69680. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69034/0.69666. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68987/0.69673. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69012/0.69680. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68798/0.69736. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68876/0.69709. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68536/0.69711. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68365/0.69797. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68162/0.69845. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68084/0.69953. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67933/0.69968. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67674/0.69876. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67503/0.70007. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67460/0.70244. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.66939/0.70152. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.66502/0.70202. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66530/0.70196. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66332/0.70396. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66197/0.70085. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.65634/0.70411. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65640/0.69972. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.65375/0.69988. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65219/0.70319. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65154/0.70492. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.64541/0.70216. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64874/0.70336. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.64849/0.70545. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.64475/0.70442. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64130/0.70573. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.63795/0.70647. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64100/0.70705. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63882/0.70410. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63553/0.70470. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.63397/0.70549. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63182/0.70621. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62741/0.70681. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.63092/0.70435. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62420/0.70685. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62548/0.70629. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62009/0.70635. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.61542/0.71108. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61923/0.71007. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61010/0.71456. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.61123/0.71261. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60808/0.71888. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60749/0.71840. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60548/0.72051. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60283/0.72713. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.59971/0.72845. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59626/0.73114. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59731/0.73400. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59097/0.73872. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.58944/0.74518. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59057/0.73823. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59144/0.73641. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58651/0.74415. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.59026/0.73623. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.58335/0.74082. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58076/0.74675. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.57930/0.75144. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57459/0.76157. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57054/0.77132. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57128/0.77034. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.56772/0.77929. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56756/0.77489. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56808/0.77766. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.55903/0.78433. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55525/0.78444. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.54912/0.79966. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55702/0.79701. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55284/0.79084. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.55218/0.80640. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54812/0.81251. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54576/0.81555. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54623/0.81528. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54277/0.81946. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53845/0.81451. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53971/0.82083. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53317/0.83415. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52815/0.83907. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.53014/0.84908. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.52118/0.84212. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.52461/0.84496. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51993/0.85472. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51254/0.87127. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51813/0.87178. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50826/0.87520. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51575/0.89027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51486/0.87104. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50798/0.87729. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50544/0.90407. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.50277/0.87365. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50539/0.90442. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49406/0.89891. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49386/0.91354. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.48961/0.91749. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50114/0.92577. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69548/0.69555. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.69442. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69191/0.69414. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.69431. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69035/0.69467. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68983/0.69520. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68933/0.69579. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68907/0.69638. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68950/0.69705. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68889/0.69787. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68807/0.69855. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68742/0.69933. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68719/0.70002. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68672/0.70062. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68646/0.70168. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68675/0.70224. Took 0.13 sec\n",
      "Epoch 16, Loss(train/val) 0.68639/0.70311. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68593/0.70363. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68567/0.70433. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68473/0.70525. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68587/0.70622. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68485/0.70699. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68391/0.70749. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68271/0.70847. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68292/0.70926. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68318/0.71016. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68306/0.71090. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68241/0.71156. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68222/0.71231. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68077/0.71298. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68010/0.71391. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68070/0.71461. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68021/0.71562. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67946/0.71641. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68012/0.71714. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67843/0.71785. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67992/0.71842. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67777/0.71921. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67809/0.71992. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67815/0.72051. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67658/0.72125. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67705/0.72220. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67765/0.72263. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67556/0.72314. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67490/0.72389. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67411/0.72508. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67347/0.72537. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67302/0.72643. Took 0.14 sec\n",
      "Epoch 48, Loss(train/val) 0.67226/0.72754. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67267/0.72817. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67158/0.72885. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67154/0.72989. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66919/0.73127. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66845/0.73241. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66671/0.73399. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66781/0.73423. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66808/0.73536. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66735/0.73577. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66556/0.73659. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66458/0.73723. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66281/0.73761. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66469/0.73803. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66321/0.73944. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66321/0.73966. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66131/0.74050. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65976/0.74113. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65835/0.74235. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65996/0.74210. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66062/0.74289. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65706/0.74419. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65581/0.74489. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65134/0.74683. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65228/0.74760. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65261/0.74985. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65170/0.75010. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64919/0.75094. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64966/0.75142. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64747/0.75297. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64552/0.75286. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64371/0.75291. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64390/0.75582. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63995/0.75736. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64265/0.75650. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63935/0.75760. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63826/0.75770. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63733/0.75891. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63613/0.76151. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63327/0.76331. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63517/0.76371. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63356/0.76547. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62944/0.76458. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62759/0.76447. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62699/0.76716. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62677/0.76756. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62747/0.76604. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62297/0.76571. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62091/0.77011. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61961/0.76974. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62167/0.77038. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61756/0.77103. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69541/0.69293. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69395/0.69190. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69295/0.69147. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69236/0.69118. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69088. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69214/0.69069. Took 0.16 sec\n",
      "Epoch 6, Loss(train/val) 0.69178/0.69067. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69111/0.69052. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69085/0.69044. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69048/0.69050. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68995/0.69018. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68940/0.69014. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68925/0.69003. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.69005. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.68970. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68646/0.68996. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68504/0.68990. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68394/0.69011. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68373/0.69023. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68243/0.69072. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68290/0.69150. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68103/0.69184. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67932/0.69253. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68028/0.69355. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68036/0.69372. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67900/0.69547. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67742/0.69561. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67526/0.69738. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67479/0.69774. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67260/0.69866. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67174/0.69927. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67147/0.70043. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66997/0.70097. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66930/0.70286. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66995/0.70349. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66545/0.70396. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66598/0.70436. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66506/0.70603. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66470/0.70807. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66225/0.71030. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66146/0.71157. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65993/0.71305. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66007/0.71359. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65657/0.71610. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65546/0.71888. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65316/0.72015. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65549/0.72124. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65375/0.72313. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64844/0.72459. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64763/0.72677. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64647/0.72776. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64444/0.72943. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63971/0.73286. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64416/0.73518. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63678/0.73978. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63668/0.74144. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63565/0.74288. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63102/0.74608. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63013/0.74904. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62971/0.75226. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63015/0.75428. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63015/0.75427. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62485/0.75841. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62495/0.76158. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62359/0.76532. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62509/0.76598. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61548/0.77022. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61655/0.77212. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62055/0.77645. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61226/0.77770. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60971/0.78198. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61063/0.78596. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61404/0.78557. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60371/0.78818. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59871/0.79479. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60355/0.79940. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59889/0.80238. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59906/0.80336. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59814/0.80607. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59911/0.81019. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59345/0.81273. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59055/0.81695. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58846/0.81999. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57837/0.82744. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58711/0.82994. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57531/0.83423. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57895/0.83800. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.57648/0.84170. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56811/0.85009. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57576/0.85668. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56473/0.86150. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57851/0.86276. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56386/0.86801. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.56174/0.87017. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56082/0.87063. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55973/0.87921. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56140/0.87698. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55795/0.88720. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55694/0.88795. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54241/0.89501. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69394/0.69135. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69203/0.69006. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69238/0.68953. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69086/0.68902. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69067/0.68865. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69021/0.68815. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69002/0.68775. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68903/0.68729. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69014/0.68703. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68933/0.68668. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68959/0.68638. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.68612. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68805/0.68586. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68830/0.68559. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68885/0.68538. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68770/0.68520. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68732/0.68509. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68728/0.68497. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68661/0.68457. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68818/0.68428. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68579/0.68392. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68755/0.68400. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68694/0.68380. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68679/0.68356. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68702/0.68340. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68466/0.68321. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68579/0.68328. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68580/0.68319. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68403/0.68333. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68470/0.68340. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68341/0.68329. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68232/0.68308. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68337/0.68262. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68172/0.68248. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68182/0.68284. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68231/0.68295. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68075/0.68295. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68135/0.68346. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67959/0.68343. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68030/0.68388. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67936/0.68389. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67862/0.68345. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67883/0.68380. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67966/0.68417. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67785/0.68418. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67629/0.68472. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67689/0.68554. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67515/0.68574. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67439/0.68612. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67385/0.68649. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67198/0.68750. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67025/0.68805. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67052/0.68841. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66768/0.68879. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66899/0.68981. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66552/0.68999. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66680/0.69215. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66375/0.69355. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66482/0.69519. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66327/0.69544. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66183/0.69613. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66188/0.69768. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66003/0.70011. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65806/0.70079. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65404/0.70246. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65514/0.70471. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65247/0.70593. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65375/0.70668. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65128/0.70885. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64712/0.71159. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64847/0.71467. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64583/0.71717. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64252/0.71575. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64158/0.71877. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63623/0.72510. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64025/0.72531. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63380/0.72945. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63264/0.72957. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62895/0.73536. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62338/0.73538. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62594/0.73905. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62418/0.74119. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62162/0.74397. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62372/0.74642. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61686/0.75204. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61210/0.75265. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61189/0.75548. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61727/0.75664. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60869/0.75584. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60604/0.76170. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60331/0.76331. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59884/0.77134. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59729/0.77510. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58959/0.77966. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.59275/0.78164. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59019/0.78903. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59109/0.78961. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58949/0.78805. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58193/0.79462. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57916/0.79819. Took 0.09 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69245/0.68450. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69045/0.68277. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68991/0.68214. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68914/0.68181. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68882/0.68107. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68879/0.68055. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68781/0.68031. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68756/0.68036. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68668/0.68030. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68720/0.68042. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68650/0.68053. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68672/0.68015. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68547/0.68024. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68566/0.68045. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68521/0.68040. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68484/0.68050. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68403/0.68068. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68366/0.68091. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68319/0.68077. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68309/0.68059. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68347/0.68077. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68256/0.68067. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68182/0.68049. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68237/0.68012. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68177/0.67999. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68130/0.68022. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68079/0.67983. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68046/0.67929. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67944/0.67986. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67955/0.67907. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67923/0.67893. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67767/0.67923. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67620/0.67875. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67723/0.67736. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67631/0.67737. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67514/0.67657. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67475/0.67690. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67483/0.67671. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67504/0.67606. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67126/0.67523. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67184/0.67484. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67085/0.67428. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66985/0.67395. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66929/0.67349. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66812/0.67335. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66562/0.67331. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66565/0.67282. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66640/0.67246. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66381/0.67221. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66164/0.67189. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66236/0.67165. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66010/0.67073. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65809/0.67118. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65650/0.67081. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65523/0.67215. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65450/0.67249. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65186/0.67208. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65006/0.67303. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64749/0.67301. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64896/0.67338. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64280/0.67396. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64331/0.67558. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64328/0.67621. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64038/0.67669. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63694/0.67817. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63204/0.67774. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63398/0.67997. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63486/0.68120. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63094/0.68409. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62712/0.68479. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62503/0.68619. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62518/0.68657. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62098/0.68821. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61957/0.69007. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61975/0.69222. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61252/0.69254. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61664/0.69484. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60974/0.69850. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61155/0.69769. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60476/0.70153. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60555/0.70326. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60513/0.70506. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59905/0.70698. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60211/0.71073. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.59559/0.70982. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59443/0.71292. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58771/0.71691. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59438/0.71728. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58728/0.71851. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58792/0.72042. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58265/0.72442. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58313/0.72634. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58310/0.72617. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57721/0.72915. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57015/0.73459. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57128/0.73319. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57492/0.73596. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57496/0.74004. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56750/0.74089. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56417/0.74221. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69078/0.69416. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68946/0.69448. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68939/0.69464. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68915/0.69486. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68955/0.69489. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68836/0.69496. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68759/0.69519. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68712/0.69553. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68687/0.69588. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68630/0.69616. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68552/0.69668. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68536/0.69674. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68568/0.69709. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68399/0.69749. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68465/0.69804. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68385/0.69830. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68349/0.69865. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68273/0.69885. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68203/0.69914. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68214/0.69956. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68100/0.70024. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68128/0.70088. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68082/0.70125. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68097/0.70134. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67903/0.70168. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67972/0.70218. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67974/0.70253. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67799/0.70285. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67760/0.70335. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67665/0.70371. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67534/0.70414. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67486/0.70483. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67340/0.70586. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67504/0.70602. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67248/0.70668. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67309/0.70655. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67237/0.70653. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66926/0.70776. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66811/0.70835. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66770/0.70891. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66711/0.71023. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66570/0.71114. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66493/0.71223. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66391/0.71305. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66105/0.71344. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65984/0.71491. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65869/0.71585. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65623/0.71786. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65606/0.71842. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65413/0.71857. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65187/0.72116. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64928/0.72201. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65142/0.72320. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64927/0.72467. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64700/0.72631. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64566/0.72646. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64216/0.72730. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64300/0.72905. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63889/0.73020. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63708/0.73105. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63722/0.73148. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63632/0.73245. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62985/0.73251. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63224/0.73381. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62841/0.73613. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63029/0.73545. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62281/0.73750. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62252/0.73725. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61946/0.73838. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62263/0.73924. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61650/0.73870. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61606/0.73806. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61593/0.73877. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.60894/0.73949. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.60825/0.74040. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60612/0.74084. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60206/0.74292. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60156/0.74621. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60127/0.74700. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59983/0.74584. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59540/0.74628. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59682/0.74663. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59451/0.74587. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58397/0.75028. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59226/0.74573. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58724/0.75089. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58859/0.74910. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58294/0.75352. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57880/0.75685. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56816/0.75684. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57844/0.75801. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57216/0.76098. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56876/0.76108. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56040/0.76150. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56372/0.76671. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56199/0.76774. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56409/0.76708. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55453/0.77353. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.55373/0.78157. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55367/0.78178. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69203/0.69534. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68883/0.69837. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68817/0.70047. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68741/0.70231. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68744/0.70339. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68685/0.70455. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68612/0.70599. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68609/0.70675. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68546/0.70773. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68501/0.70811. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68458/0.70855. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68438/0.70884. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68370/0.70855. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68370/0.70904. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68394/0.70882. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68210/0.70899. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68301/0.70953. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68208/0.70956. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68187/0.71000. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68161/0.70975. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68166/0.70988. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68081/0.70987. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68015/0.71010. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67877/0.71028. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67849/0.71061. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67841/0.71004. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67772/0.71020. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67752/0.71103. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67647/0.71208. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67705/0.71121. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67567/0.71125. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67398/0.71294. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67379/0.71361. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67396/0.71448. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67105/0.71382. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67102/0.71566. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66890/0.71674. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66982/0.71665. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66875/0.71821. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66899/0.71727. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66728/0.71831. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66666/0.71856. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66300/0.72145. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66308/0.72202. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66083/0.72173. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66273/0.72466. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66226/0.72543. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65959/0.72374. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65875/0.72484. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65737/0.72741. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65781/0.72501. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65746/0.72616. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65534/0.72700. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65273/0.72918. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65079/0.73013. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64987/0.73080. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65003/0.73221. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64729/0.73295. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64718/0.73268. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64542/0.73444. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64544/0.73429. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64112/0.73874. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64004/0.73819. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63909/0.73805. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63632/0.74043. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63151/0.74189. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63579/0.74407. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63302/0.74548. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63495/0.74800. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62887/0.75055. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62342/0.74748. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62484/0.75322. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62382/0.75579. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62366/0.75807. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62362/0.75633. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62021/0.75925. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61789/0.76195. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61537/0.76564. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61221/0.76594. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61765/0.76810. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60909/0.76259. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60972/0.77350. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60499/0.77149. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60579/0.77188. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60271/0.77468. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60597/0.77542. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59848/0.78025. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60089/0.78276. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59411/0.79117. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59867/0.78827. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59404/0.78720. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59166/0.79099. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58725/0.79257. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58967/0.79515. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58893/0.79874. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58024/0.80160. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58127/0.80105. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58223/0.80256. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57404/0.80686. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57560/0.81359. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69167/0.69225. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69078/0.69220. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69002/0.69219. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68970/0.69217. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68925/0.69238. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68905/0.69245. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68861/0.69249. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68778/0.69265. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68738/0.69300. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68608/0.69308. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68576/0.69356. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68423/0.69418. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68388/0.69495. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68370/0.69540. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68224/0.69589. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68103/0.69662. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68101/0.69761. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67954/0.69831. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67794/0.69897. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67838/0.69945. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67709/0.70024. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67707/0.70091. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67557/0.70139. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67620/0.70137. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67431/0.70159. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67095/0.70167. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67280/0.70150. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67075/0.70094. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66876/0.70052. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66818/0.70115. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66905/0.70120. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66339/0.70103. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66409/0.70177. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66199/0.70115. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66185/0.69994. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65970/0.69945. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65990/0.69948. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65950/0.69948. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65820/0.69932. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65662/0.69993. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65428/0.69963. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65163/0.69900. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65136/0.69836. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64972/0.69796. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64813/0.69863. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65075/0.69711. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64619/0.69545. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64763/0.69593. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64140/0.69546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64171/0.69695. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64287/0.69550. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64028/0.69527. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63907/0.69439. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64099/0.69442. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63666/0.69442. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63707/0.69456. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63452/0.69509. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63143/0.69523. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62952/0.69538. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63026/0.69618. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62922/0.69462. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62675/0.69628. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63007/0.69657. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62308/0.69787. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62660/0.69838. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62050/0.70001. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62024/0.69908. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61584/0.69926. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.61695/0.69880. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61326/0.70188. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61036/0.70464. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61312/0.70170. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61031/0.70431. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60291/0.70348. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60758/0.70351. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59758/0.70222. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60116/0.70091. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60768/0.70352. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59486/0.70524. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59640/0.70777. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59369/0.70983. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59461/0.70913. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.58937/0.70802. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58812/0.70800. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58872/0.71217. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58306/0.71216. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59029/0.71519. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57889/0.71159. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57624/0.71250. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57793/0.71435. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57804/0.71620. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57219/0.71502. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57674/0.71489. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57108/0.71134. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.56599/0.71253. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.56043/0.71534. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.55851/0.71938. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56113/0.71600. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54885/0.72261. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55330/0.71449. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69218/0.69535. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68992/0.69581. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68849/0.69619. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68859/0.69616. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68713/0.69612. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68702/0.69626. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68534/0.69612. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68603/0.69595. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68532/0.69595. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68460/0.69579. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68367/0.69567. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68462/0.69531. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68329/0.69510. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68321/0.69493. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68211/0.69432. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68029/0.69406. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68130/0.69393. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68109/0.69433. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68019/0.69488. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67882/0.69509. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67895/0.69509. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67707/0.69539. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67697/0.69551. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67592/0.69589. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67289/0.69622. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67478/0.69655. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67447/0.69619. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67247/0.69630. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67254/0.69705. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67087/0.69750. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66916/0.69852. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67025/0.69848. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66778/0.69878. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66953/0.69893. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66474/0.69841. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66676/0.69800. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66501/0.69865. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66305/0.69918. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66204/0.69957. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66055/0.69917. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65986/0.69940. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66039/0.69814. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65838/0.69726. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65822/0.69748. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65683/0.69831. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65519/0.69796. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65542/0.69988. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65393/0.69842. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65224/0.69768. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65118/0.69761. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64878/0.69611. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64856/0.69816. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64497/0.70000. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64659/0.69764. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64249/0.69723. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64203/0.69446. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64010/0.69446. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64113/0.69524. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63701/0.69443. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64035/0.69540. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63552/0.69412. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63307/0.69507. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63341/0.69314. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63179/0.69510. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62872/0.69119. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62578/0.69094. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62964/0.69184. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62762/0.68983. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62524/0.69256. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62480/0.68915. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62271/0.69005. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62033/0.69233. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61514/0.68955. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61793/0.69249. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61370/0.68845. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61555/0.68922. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61219/0.68797. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.60210/0.68903. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60445/0.69130. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60869/0.68869. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59844/0.68631. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60061/0.68436. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60147/0.68662. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59307/0.68903. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59345/0.68934. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58565/0.68928. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58714/0.69155. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58598/0.68757. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58404/0.68613. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.58151/0.69046. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57713/0.68673. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57791/0.69128. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57029/0.69431. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56952/0.68910. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56428/0.68919. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56301/0.68899. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56525/0.69606. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56219/0.68941. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55525/0.69540. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55388/0.69197. Took 0.11 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69535/0.69020. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69286/0.68888. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69236/0.68830. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69167/0.68812. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69109/0.68796. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69156/0.68803. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68974/0.68810. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69037/0.68813. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69025/0.68809. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68990/0.68818. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.68831. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68862/0.68855. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68741/0.68905. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68704/0.68929. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68645/0.68962. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68583/0.68980. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68425/0.69027. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.69072. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68420/0.69080. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68283/0.69188. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68280/0.69182. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68068/0.69208. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67982/0.69294. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67842/0.69371. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67664/0.69274. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67387/0.69323. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67391/0.69320. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67228/0.69285. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67048/0.69256. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66919/0.69169. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66585/0.69193. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66473/0.69136. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65991/0.69280. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66034/0.69153. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65642/0.69455. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65323/0.69466. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65240/0.69392. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64895/0.69219. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64447/0.69315. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64419/0.69425. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64059/0.69433. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63747/0.69276. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63515/0.69363. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63433/0.69132. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63290/0.68992. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62925/0.68897. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.62260/0.69015. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62042/0.69036. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61900/0.69118. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61452/0.69307. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61426/0.69362. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61056/0.69059. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60851/0.69210. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60606/0.69162. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.60313/0.69391. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60232/0.69394. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60217/0.69389. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58916/0.69467. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59380/0.69286. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.59153/0.69339. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58799/0.69620. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58766/0.69784. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58499/0.69899. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58489/0.69933. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.58040/0.69895. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57595/0.70036. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57797/0.70326. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57641/0.70452. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57334/0.70402. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56867/0.70431. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56164/0.70724. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56263/0.70649. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.56234/0.71009. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.55647/0.71040. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.55180/0.71427. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.55585/0.71481. Took 0.15 sec\n",
      "Epoch 76, Loss(train/val) 0.55569/0.71537. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.55295/0.71885. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55282/0.72062. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54335/0.72381. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54597/0.72302. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54128/0.72559. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53478/0.72480. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.53320/0.72841. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53546/0.72998. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52828/0.73396. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53227/0.73428. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.52910/0.73849. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52988/0.73955. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52411/0.74102. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52055/0.74119. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51621/0.74703. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51336/0.74952. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51442/0.75405. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51602/0.75317. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49968/0.75256. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50346/0.75987. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50399/0.76195. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.49694/0.76202. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49473/0.76281. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69257/0.69123. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69155/0.69096. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.69069. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69135/0.69050. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69155/0.69031. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.69095/0.69011. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69059/0.68994. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69075/0.68980. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69047/0.68966. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69056/0.68952. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.68930. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69010/0.68912. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68974/0.68902. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68897/0.68884. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68848/0.68876. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68851/0.68889. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68762/0.68902. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68665/0.68926. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68583/0.68977. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68631/0.69020. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68442/0.69082. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68454/0.69156. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68303/0.69246. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68144/0.69355. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68291/0.69392. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68076/0.69380. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67922/0.69425. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67833/0.69404. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68027/0.69414. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67734/0.69435. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67669/0.69443. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67586/0.69449. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67514/0.69416. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67435/0.69391. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.67203/0.69368. Took 0.14 sec\n",
      "Epoch 35, Loss(train/val) 0.67162/0.69323. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67096/0.69305. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67191/0.69194. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66603/0.69141. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66881/0.69052. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66791/0.68945. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66384/0.68948. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66679/0.68983. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66206/0.68910. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66181/0.68853. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65862/0.68729. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66169/0.68728. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.65707/0.68711. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65706/0.68605. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65336/0.68723. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65086/0.68618. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65461/0.68531. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64996/0.68568. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64935/0.68512. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64708/0.68536. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64686/0.68522. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64368/0.68546. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64289/0.68418. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64166/0.68247. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.63924/0.68283. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63718/0.68400. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63483/0.68431. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63158/0.68446. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63081/0.68436. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62801/0.68592. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62396/0.68372. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62229/0.68347. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62175/0.68629. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61458/0.68481. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.61921/0.68493. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61836/0.68425. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61141/0.68406. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61006/0.68604. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61042/0.68833. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.60511/0.68589. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60106/0.68479. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59339/0.68474. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60158/0.68619. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59842/0.68954. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59642/0.69012. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59151/0.69036. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58302/0.69296. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58742/0.69173. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57979/0.69171. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58572/0.69398. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58082/0.69761. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.56764/0.69880. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56650/0.70129. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.57244/0.70133. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56834/0.70186. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56530/0.70585. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56146/0.70199. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55998/0.69989. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55161/0.70265. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55438/0.70483. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53976/0.70662. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54644/0.70970. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53749/0.71277. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54160/0.70900. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53688/0.71010. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69584/0.69297. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69254/0.69274. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69255/0.69237. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69232/0.69197. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69140/0.69160. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69138/0.69118. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69089/0.69095. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69079/0.69076. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68979/0.69049. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68919/0.69040. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68900/0.69016. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68900/0.68982. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68823/0.68952. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68839/0.68921. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68887/0.68904. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68734/0.68888. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68756/0.68864. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68763/0.68841. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68654/0.68826. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68654/0.68801. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68605/0.68757. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.68740. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68554/0.68713. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68478/0.68676. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68342/0.68614. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68305/0.68597. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68260/0.68563. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68300/0.68526. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68184/0.68469. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68137/0.68416. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67962/0.68352. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67947/0.68293. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67847/0.68268. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67742/0.68221. Took 0.15 sec\n",
      "Epoch 34, Loss(train/val) 0.67546/0.68156. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67634/0.68109. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67642/0.68063. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67500/0.68027. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67359/0.67968. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67284/0.67922. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67076/0.67894. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66980/0.67860. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67004/0.67886. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66711/0.67864. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66239/0.67941. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66337/0.67875. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66322/0.67952. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65961/0.68059. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65939/0.68245. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65783/0.68369. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65650/0.68573. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65447/0.68820. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65471/0.69116. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64973/0.69474. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64774/0.69777. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64789/0.70008. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64497/0.69940. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64465/0.70490. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64055/0.70684. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63760/0.71073. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64055/0.71395. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63878/0.71779. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63579/0.72369. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63592/0.72390. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63387/0.72659. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63114/0.72791. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63012/0.73335. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62329/0.73697. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.62828/0.74116. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62352/0.73993. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62056/0.74679. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62086/0.74578. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61872/0.74847. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61719/0.75563. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61679/0.75647. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61124/0.75543. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61614/0.75972. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61060/0.76027. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61369/0.76656. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60906/0.76672. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60501/0.76199. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60211/0.77601. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60473/0.77808. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59716/0.78326. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59898/0.78691. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59384/0.78654. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59647/0.79179. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59691/0.78958. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58993/0.79833. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59143/0.79665. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58922/0.78764. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58712/0.79925. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58422/0.80444. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58060/0.80728. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57636/0.81194. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58090/0.80661. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57403/0.81339. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57328/0.81503. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.57305/0.82117. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.57070/0.81780. Took 0.10 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69557/0.69330. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69228. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69333/0.69187. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69263/0.69169. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69253/0.69153. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69200/0.69154. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69150/0.69161. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69095/0.69137. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.69119/0.69127. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68998/0.69112. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.69131. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68869/0.69132. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68918/0.69125. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68883/0.69129. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68757/0.69142. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68727/0.69161. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68628/0.69189. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68667/0.69210. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68545/0.69243. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68425/0.69267. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68286/0.69319. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68278/0.69335. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68165/0.69388. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68038/0.69439. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67915/0.69511. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67681/0.69583. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67516/0.69647. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67388/0.69761. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67355/0.69898. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67032/0.69981. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66781/0.70093. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66758/0.70158. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66695/0.70242. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66294/0.70429. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66060/0.70492. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65961/0.70669. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65576/0.70884. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65898/0.70945. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65321/0.71084. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65172/0.71325. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65065/0.71545. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65036/0.71531. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64662/0.71672. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64292/0.71746. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64441/0.71987. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64288/0.71952. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63980/0.71932. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64132/0.72066. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63691/0.72142. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63324/0.72066. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63464/0.72068. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62946/0.72289. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62956/0.72407. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63018/0.72368. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62399/0.72719. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62354/0.72696. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62158/0.72915. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62243/0.72950. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61924/0.73161. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62005/0.73149. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61474/0.73354. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61393/0.73233. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61156/0.73092. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61577/0.73426. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61283/0.73548. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60889/0.73672. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60687/0.73570. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60675/0.73702. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60180/0.73817. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60082/0.73861. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.59686/0.74051. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59206/0.74528. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59359/0.74584. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59143/0.74885. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59279/0.75014. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59133/0.75020. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58520/0.74973. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58702/0.75724. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58423/0.75524. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.58207/0.75452. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57792/0.75652. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57811/0.75713. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57664/0.76077. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.57415/0.76186. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57292/0.76187. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56922/0.76484. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56953/0.76941. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56504/0.77031. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56445/0.77295. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56120/0.77450. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56186/0.77519. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56151/0.77785. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55648/0.78366. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55267/0.78375. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.54913/0.78941. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54545/0.78710. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53971/0.79322. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54563/0.79845. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54285/0.79693. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53870/0.79737. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69387/0.69354. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69261/0.69431. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69160/0.69504. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69198/0.69560. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69070/0.69614. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69053/0.69691. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68952/0.69767. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68926/0.69857. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68921/0.69945. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68777/0.70047. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68796/0.70156. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68841/0.70279. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68718/0.70425. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68549/0.70574. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68632/0.70715. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68496/0.70850. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68549/0.70952. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68488/0.71086. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68288/0.71225. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68279/0.71359. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68332/0.71519. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68134/0.71652. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68129/0.71791. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67994/0.71973. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68070/0.72083. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68114/0.72197. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67907/0.72329. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67943/0.72408. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67802/0.72443. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67624/0.72573. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67377/0.72710. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67459/0.72854. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67322/0.72999. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67199/0.73151. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67374/0.73222. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67160/0.73312. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67035/0.73429. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66993/0.73621. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66931/0.73722. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66863/0.73831. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66746/0.73939. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66680/0.73943. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66674/0.74036. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66373/0.74232. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66016/0.74392. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66242/0.74564. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66039/0.74560. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66080/0.74691. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65473/0.74968. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65858/0.75224. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65786/0.75092. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65285/0.75174. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65617/0.75394. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65226/0.75514. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65146/0.75541. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65128/0.75610. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64821/0.75716. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64907/0.75891. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64293/0.76120. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64231/0.76317. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64176/0.76405. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64239/0.76534. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63484/0.76611. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63395/0.76820. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63037/0.77039. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63312/0.77501. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62800/0.77613. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62373/0.77761. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62451/0.77961. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62398/0.78287. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61757/0.78616. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62177/0.78981. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61568/0.79240. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61640/0.79240. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61488/0.79225. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61269/0.79723. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60639/0.79902. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60733/0.80030. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60544/0.80665. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.60359/0.80635. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60614/0.81062. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59911/0.81063. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59693/0.81594. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59763/0.81724. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59551/0.82189. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59177/0.82205. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58963/0.82138. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.58432/0.82888. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58633/0.82778. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58527/0.82702. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57798/0.83652. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57763/0.83795. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57038/0.83314. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57550/0.83705. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56401/0.84154. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.56759/0.84556. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.56171/0.84809. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56018/0.84940. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56227/0.84828. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55594/0.85662. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69131/0.69131. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69125/0.69116. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69035/0.69102. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69059/0.69090. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68930/0.69083. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.69071. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68975/0.69058. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68845/0.69057. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68871/0.69048. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68713/0.69048. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68820/0.69047. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68661/0.69044. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68632/0.69034. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68623/0.69034. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68673/0.69027. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68550/0.68993. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68458/0.68970. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68529/0.68968. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68380/0.68977. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68302/0.68950. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68353/0.68923. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68271/0.68942. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68195/0.68944. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68091/0.68913. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68079/0.68907. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68077/0.68941. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67862/0.68937. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67903/0.68962. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67776/0.68994. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67632/0.69034. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67745/0.69067. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67746/0.69086. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67447/0.69098. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67369/0.69114. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67296/0.69228. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67328/0.69355. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67069/0.69457. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66989/0.69508. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66987/0.69514. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66963/0.69617. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66746/0.69710. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66763/0.69840. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66689/0.69908. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66718/0.69983. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66370/0.70013. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66280/0.70087. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66331/0.70066. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66419/0.70125. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66009/0.70313. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65832/0.70448. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65914/0.70600. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66008/0.70593. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65814/0.70723. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65400/0.70767. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65598/0.70795. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65351/0.70921. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65356/0.71094. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64718/0.71200. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64946/0.71415. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64699/0.71752. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64739/0.71849. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64467/0.72051. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64281/0.72152. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64094/0.72349. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64026/0.72533. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63810/0.72760. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63683/0.72643. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63165/0.72868. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63503/0.73005. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63068/0.73228. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63324/0.73495. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62695/0.73773. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62386/0.74078. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62660/0.74142. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62394/0.74413. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62077/0.74592. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61438/0.75006. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61522/0.74953. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61840/0.75060. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61110/0.75232. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61026/0.75679. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61143/0.75923. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60685/0.76288. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60301/0.76251. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.60097/0.76487. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60127/0.76420. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59902/0.76968. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59108/0.77035. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59510/0.77316. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58711/0.77881. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59255/0.77488. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58857/0.77972. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58279/0.78315. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58012/0.78481. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57912/0.78697. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57346/0.79597. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.57177/0.79573. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56966/0.79906. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56813/0.80047. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56141/0.80267. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69348/0.69609. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69141/0.69669. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69064/0.69717. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68937/0.69748. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68908/0.69788. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68788/0.69772. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68715/0.69774. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68579/0.69770. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68538/0.69775. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68422/0.69718. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68225/0.69691. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68202/0.69707. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68051/0.69643. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67840/0.69590. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67575/0.69391. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67459/0.69302. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67256/0.69298. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.66916/0.69191. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.66802/0.69039. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.66493/0.68826. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66387/0.68699. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66097/0.68662. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.65750/0.68276. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.65611/0.68204. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.65609/0.68213. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65244/0.68076. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.64902/0.67722. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.65032/0.67983. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64760/0.67244. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.64519/0.67456. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64270/0.67260. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.63862/0.66739. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64019/0.67136. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.63450/0.66701. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63402/0.66365. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.63162/0.66493. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.62874/0.66867. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.62889/0.66254. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62428/0.66257. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62236/0.66192. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62179/0.65920. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.61990/0.65688. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.61716/0.66001. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61434/0.65588. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61341/0.66077. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61119/0.65110. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.60551/0.64955. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.60548/0.64500. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.60143/0.65403. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.59993/0.64898. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60047/0.65283. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.59849/0.64893. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59188/0.65527. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59256/0.65060. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.58816/0.65588. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.58544/0.64849. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58333/0.63973. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.58241/0.64411. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.57915/0.63845. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.57530/0.63748. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.57019/0.64615. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57046/0.65195. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.56369/0.63464. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.56337/0.63938. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.55986/0.63961. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.55844/0.64082. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.55969/0.63161. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55387/0.63352. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.55490/0.63001. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.54991/0.63448. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.54586/0.64290. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.54327/0.65826. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.53302/0.65538. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.53816/0.66190. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.53406/0.64609. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53210/0.64411. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.52898/0.63439. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.52894/0.63988. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.52032/0.63625. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.51751/0.64052. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.51699/0.64554. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.51230/0.64840. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.51014/0.63465. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.50820/0.65791. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.50741/0.65830. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.50327/0.67453. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.50120/0.66625. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.50275/0.67514. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.49137/0.67103. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.49165/0.65646. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.48068/0.68489. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.49123/0.67691. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.47939/0.68361. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.47258/0.67857. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.47632/0.67568. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.47941/0.68595. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.47708/0.69030. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.47204/0.68480. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.46697/0.68701. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.46318/0.70265. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69512/0.69306. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69296. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69212/0.69288. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69208/0.69284. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69042/0.69289. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69079/0.69312. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.69327. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69075/0.69341. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68978/0.69367. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68987/0.69413. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68852/0.69452. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.69503. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68889/0.69545. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.69589. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68683/0.69636. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68647/0.69677. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68621/0.69714. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68508/0.69773. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68555/0.69814. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68570/0.69845. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68423/0.69883. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68559/0.69907. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68494/0.69931. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68375/0.69958. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68216/0.70011. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68164/0.70070. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68030/0.70090. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68133/0.70100. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67956/0.70090. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67882/0.70084. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67869/0.70126. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67794/0.70181. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67659/0.70159. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67846/0.70180. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67498/0.70166. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67496/0.70236. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67527/0.70226. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67433/0.70231. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67408/0.70224. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66929/0.70349. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66807/0.70341. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66740/0.70429. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66893/0.70332. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66805/0.70248. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66387/0.70159. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66138/0.70235. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66513/0.70154. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66641/0.70286. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65837/0.70368. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65848/0.70386. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65964/0.70420. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65679/0.70481. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65312/0.70420. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65579/0.70342. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65362/0.70484. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65387/0.70452. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64788/0.70506. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64928/0.70566. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.64500/0.70629. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64760/0.70813. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64301/0.70640. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64251/0.70608. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64140/0.70468. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63778/0.70564. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63712/0.70957. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63818/0.70892. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63475/0.70744. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63116/0.70901. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62743/0.71086. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63022/0.71130. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62274/0.71249. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62343/0.71459. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62735/0.71595. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62664/0.71559. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61736/0.71870. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61849/0.71833. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61870/0.71853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61261/0.71878. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61495/0.72077. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60906/0.72065. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61003/0.71929. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.60630/0.72182. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60307/0.72715. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60267/0.72537. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59568/0.72915. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60146/0.72997. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59904/0.73028. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.59410/0.73184. Took 0.12 sec\n",
      "Epoch 88, Loss(train/val) 0.59043/0.73347. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.59427/0.73630. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58879/0.73708. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.58530/0.73568. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 0.58380/0.73761. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58398/0.73829. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57819/0.74169. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.57978/0.74207. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.57289/0.74306. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.57473/0.74316. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57440/0.74437. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56903/0.75109. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69583/0.69556. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69522/0.69576. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 0.69484/0.69592. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69472/0.69609. Took 0.13 sec\n",
      "Epoch 4, Loss(train/val) 0.69392/0.69629. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69346/0.69655. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.69684. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69337/0.69708. Took 0.12 sec\n",
      "Epoch 8, Loss(train/val) 0.69254/0.69731. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69144/0.69745. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69011/0.69760. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68992/0.69776. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68941/0.69835. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68744/0.69977. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68665/0.70141. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68520/0.70321. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68445/0.70511. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68433/0.70672. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68127/0.70874. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68016/0.71124. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67866/0.71334. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67863/0.71527. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67735/0.71672. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67694/0.71902. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67520/0.72107. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67283/0.72326. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67157/0.72577. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67048/0.72790. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67061/0.73001. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66983/0.73157. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66728/0.73322. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66821/0.73497. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66635/0.73644. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66370/0.73967. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66540/0.74141. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66193/0.74360. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66333/0.74460. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65973/0.74619. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65795/0.74910. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65723/0.75153. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65530/0.75325. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65488/0.75605. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65234/0.75778. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65208/0.75883. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65094/0.76047. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64843/0.76362. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.65019/0.76526. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.64766/0.76621. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64757/0.76738. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63994/0.76907. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64053/0.77166. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63970/0.77353. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63837/0.77525. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63541/0.77699. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63320/0.77981. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63141/0.78182. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62541/0.78503. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62550/0.78748. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62488/0.78904. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62212/0.79026. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61772/0.79205. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61460/0.79598. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60981/0.80077. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60808/0.80493. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60439/0.80816. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60327/0.81328. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60080/0.81378. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59628/0.81756. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59515/0.81881. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58826/0.82225. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.58857/0.82505. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58768/0.82652. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58709/0.82943. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57435/0.83946. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57323/0.84275. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57782/0.84265. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57246/0.84919. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57021/0.85029. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56062/0.85477. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55677/0.85911. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56105/0.85872. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55833/0.86479. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55221/0.86888. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54770/0.87188. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54239/0.87788. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54010/0.88284. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53593/0.88549. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53747/0.88835. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52839/0.89167. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52752/0.90154. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52557/0.90893. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53049/0.90736. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51801/0.91322. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.51740/0.92148. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50783/0.92688. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50844/0.93232. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50471/0.93284. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50576/0.93576. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49866/0.93576. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49841/0.94396. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69292/0.69732. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69102/0.69757. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68958/0.69781. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68991/0.69819. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68945/0.69893. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68866/0.69960. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68844/0.70044. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68829/0.70116. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.70205. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68758/0.70286. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68613/0.70354. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68660/0.70431. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68671/0.70518. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68592/0.70574. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68524/0.70661. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68497/0.70711. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68568/0.70769. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68505/0.70797. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68538/0.70831. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68429/0.70875. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68397/0.70957. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68437/0.71027. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68200/0.71094. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68262/0.71148. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68233/0.71194. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68197/0.71228. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68120/0.71263. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68213/0.71309. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68134/0.71277. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67915/0.71389. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67969/0.71495. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67919/0.71551. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67753/0.71595. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67837/0.71602. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67730/0.71603. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67753/0.71590. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67669/0.71628. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67518/0.71640. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67591/0.71682. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67386/0.71781. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67318/0.71842. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67142/0.71902. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67244/0.71991. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67013/0.72008. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66967/0.72107. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66787/0.72187. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66567/0.72235. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66641/0.72270. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66455/0.72213. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66398/0.72325. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66207/0.72333. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65820/0.72464. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65900/0.72520. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65831/0.72609. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65501/0.72692. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65814/0.72818. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65241/0.72922. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65231/0.73058. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64880/0.73138. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64721/0.73363. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64899/0.73472. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64547/0.73634. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64182/0.73897. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64364/0.74044. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64116/0.74317. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63664/0.74562. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63464/0.74664. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63086/0.74982. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62601/0.75165. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62959/0.75292. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62241/0.75812. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62076/0.76157. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62271/0.76120. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61905/0.76457. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61596/0.76749. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61046/0.77367. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60917/0.77586. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60830/0.77799. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60289/0.78430. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60105/0.78930. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59769/0.79229. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59843/0.79528. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59552/0.79811. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58789/0.80501. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59186/0.81337. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58783/0.81359. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58919/0.81612. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58287/0.82038. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57501/0.82605. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57284/0.83154. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57720/0.83383. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57148/0.83942. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57103/0.84344. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56706/0.84253. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55897/0.85169. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55992/0.85953. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55362/0.86039. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55749/0.86930. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54540/0.87796. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55122/0.87941. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69504/0.69226. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69269/0.69213. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.69245. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.69249. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69241. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.69251. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68968/0.69261. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68936/0.69284. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.69325. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68792/0.69357. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68759/0.69354. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68579/0.69360. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68629/0.69363. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68526/0.69363. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68565/0.69292. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68412/0.69266. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68443/0.69256. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68370/0.69225. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68335/0.69209. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68296/0.69145. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68140/0.69100. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68264/0.69084. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68192/0.69088. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68035/0.69144. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68080/0.69097. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68120/0.68979. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68028/0.68968. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68066/0.68967. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67984/0.68913. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67820/0.68890. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67884/0.68845. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67793/0.68863. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67820/0.68890. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67763/0.68846. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67701/0.68890. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67691/0.68851. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67703/0.68830. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67719/0.68750. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67477/0.68734. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67430/0.68683. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67366/0.68657. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67612/0.68605. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67251/0.68562. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67243/0.68438. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67318/0.68528. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67155/0.68525. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66890/0.68413. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67004/0.68406. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66825/0.68453. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66829/0.68444. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66880/0.68448. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66687/0.68334. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66858/0.68253. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66635/0.68357. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66519/0.68561. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66498/0.68460. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66440/0.68344. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66386/0.68385. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66327/0.68330. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66038/0.68289. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66138/0.68396. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66185/0.68355. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65734/0.68323. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65685/0.68204. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65879/0.68300. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65443/0.68324. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65562/0.68309. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65454/0.68486. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65428/0.68659. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65109/0.68530. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65213/0.68475. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64796/0.68353. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64784/0.68475. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64830/0.68573. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64739/0.68308. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64263/0.68262. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64223/0.68357. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64157/0.68284. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64059/0.68274. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64086/0.68345. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64104/0.68410. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63713/0.68485. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63722/0.68592. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63517/0.68252. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63566/0.68730. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62806/0.68731. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63214/0.68734. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62888/0.68681. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62631/0.68676. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62778/0.68504. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62590/0.68723. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62338/0.68432. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62160/0.68568. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61680/0.68849. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62088/0.69039. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61616/0.69197. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61562/0.69450. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61389/0.69218. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60898/0.69303. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60750/0.69217. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69356/0.69420. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69012/0.69439. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68889/0.69493. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68817/0.69574. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68698/0.69678. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68587/0.69777. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68467/0.69925. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68284/0.70078. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68273/0.70189. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68215/0.70300. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67985/0.70489. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68045/0.70658. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67880/0.70797. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67860/0.70948. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67907/0.71060. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67673/0.71158. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67583/0.71291. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67549/0.71432. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67466/0.71539. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67285/0.71679. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67302/0.71741. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67212/0.71774. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67102/0.71801. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67075/0.71846. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66997/0.71843. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66792/0.71907. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66699/0.71937. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66595/0.71972. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66446/0.72034. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66198/0.72070. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66038/0.72168. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65879/0.72277. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65981/0.72323. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65572/0.72372. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65423/0.72469. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65494/0.72500. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.65197/0.72393. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64939/0.72564. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64900/0.72721. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64503/0.72714. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64517/0.72832. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64366/0.72721. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64152/0.72773. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63722/0.72791. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63838/0.73153. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63544/0.73186. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63341/0.73327. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62943/0.73704. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63070/0.73787. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62008/0.73897. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62200/0.74015. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61802/0.74342. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62255/0.74456. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61760/0.74697. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61712/0.74935. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61062/0.75108. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61164/0.75230. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60627/0.75835. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60585/0.76164. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60604/0.76411. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60258/0.76340. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60054/0.76951. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59727/0.77239. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59123/0.77033. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59200/0.76961. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58957/0.77927. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58620/0.78087. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58574/0.78717. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58542/0.78872. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57944/0.78367. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58025/0.78978. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57597/0.78939. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57320/0.79258. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57280/0.78733. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.57075/0.80161. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57089/0.80077. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56388/0.80062. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.56426/0.79969. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55523/0.80732. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55564/0.81306. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55394/0.80927. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.54998/0.82119. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54914/0.82041. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55139/0.82092. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54448/0.82395. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54019/0.83325. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54425/0.83370. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53708/0.82676. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53740/0.83496. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.53917/0.83513. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53363/0.83682. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52776/0.84315. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52670/0.84124. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51645/0.84778. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52100/0.85370. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52005/0.84740. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50976/0.85284. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51804/0.85739. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50423/0.85784. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.50786/0.86471. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69636/0.69510. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69275/0.69733. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69115/0.69886. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.70044. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69134/0.70232. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69019/0.70389. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68945/0.70582. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68903/0.70762. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68743/0.70997. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68820/0.71198. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68741/0.71438. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68674/0.71666. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68659/0.71811. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68559/0.72000. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68542/0.72162. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68460/0.72362. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68290/0.72530. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68378/0.72754. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68304/0.72935. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68205/0.73115. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68220/0.73234. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68280/0.73327. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68228/0.73424. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68202/0.73511. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68075/0.73606. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68071/0.73683. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67966/0.73703. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67871/0.73843. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67848/0.73961. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67794/0.74129. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67820/0.74272. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67754/0.74338. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67673/0.74452. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67631/0.74489. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67471/0.74700. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67508/0.74803. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67106/0.74965. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67176/0.75170. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67380/0.75319. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67182/0.75298. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67114/0.75343. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67287/0.75418. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66939/0.75492. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66960/0.75824. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67000/0.75951. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66718/0.76085. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66682/0.76298. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66709/0.76473. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66583/0.76640. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66536/0.76702. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66332/0.76902. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66321/0.77107. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66246/0.77270. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66317/0.77259. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65821/0.77697. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65858/0.77863. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65649/0.78042. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65641/0.78287. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65295/0.78747. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65454/0.78798. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65282/0.79010. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64847/0.79133. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64823/0.79419. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65227/0.79537. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64822/0.79934. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64888/0.80207. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64749/0.80579. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64854/0.80406. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64276/0.80657. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64342/0.80971. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64228/0.81656. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64406/0.81233. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64314/0.81525. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63767/0.81881. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63989/0.82257. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63583/0.82605. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63632/0.82578. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63275/0.82902. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63272/0.83172. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63198/0.83544. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63160/0.83466. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63016/0.83660. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63055/0.83908. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63064/0.84012. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62586/0.84712. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62333/0.84667. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62793/0.84885. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62051/0.85049. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62041/0.85506. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62132/0.85622. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61868/0.85991. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61984/0.86203. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61504/0.86427. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61835/0.86587. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61287/0.86834. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61285/0.87143. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61468/0.86881. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60988/0.87090. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60398/0.87713. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61003/0.87905. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69460/0.70171. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69367/0.69896. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69358/0.69759. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69287/0.69692. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69242/0.69617. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69238/0.69608. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69582. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69263/0.69532. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69129/0.69532. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69102/0.69604. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69118/0.69579. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69078/0.69547. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69027/0.69558. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68988/0.69630. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68975/0.69622. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68854/0.69745. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68921/0.69776. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68825/0.69764. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68679/0.69861. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68627/0.69759. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68618/0.69990. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68604/0.69846. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68472/0.69986. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68350/0.70079. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68193/0.70079. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68174/0.70240. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68073/0.70369. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68157/0.70374. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67891/0.70539. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67963/0.70768. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67751/0.70742. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67744/0.70971. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67607/0.71035. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67515/0.70939. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67382/0.71124. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67308/0.71273. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67106/0.71302. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67062/0.71382. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66948/0.71418. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66780/0.71470. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66883/0.71535. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66617/0.71722. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66494/0.72014. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66507/0.71584. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66280/0.71793. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66220/0.72065. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66114/0.72092. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66235/0.71919. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65713/0.72019. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65371/0.72497. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65390/0.71984. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65448/0.72071. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65100/0.72321. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65292/0.72092. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65034/0.72289. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65002/0.72333. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64783/0.72550. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64717/0.72563. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64603/0.72224. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64052/0.72847. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64220/0.72548. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64063/0.72825. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64030/0.72883. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63926/0.72794. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63378/0.73086. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63290/0.73176. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63120/0.73115. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63323/0.73202. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62848/0.73393. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62759/0.73364. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63105/0.73281. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62667/0.73303. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62339/0.73493. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61658/0.73546. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62102/0.73823. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61784/0.74249. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61070/0.74323. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61685/0.74308. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61246/0.74392. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60872/0.74705. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60758/0.74981. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60605/0.74976. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60247/0.75137. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59865/0.75598. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60016/0.75499. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59831/0.75776. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59977/0.75740. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58843/0.76039. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58806/0.76146. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59080/0.76443. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58686/0.76784. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58665/0.76872. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58662/0.76762. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58302/0.76735. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57735/0.76769. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57859/0.77165. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57455/0.77499. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56924/0.77320. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56281/0.77620. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56893/0.77712. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69646/0.69147. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69272/0.68884. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69360/0.68826. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.68768. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.68753. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69060/0.68736. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.68734. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68952/0.68727. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68984/0.68744. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68830/0.68737. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68801/0.68769. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68846/0.68800. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68767/0.68847. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68702/0.68894. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68573/0.68926. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68540/0.68971. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68473/0.69002. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68424/0.69038. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68410/0.69088. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68262/0.69146. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68177/0.69215. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68110/0.69279. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67997/0.69370. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68007/0.69416. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67868/0.69470. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67574/0.69540. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67659/0.69603. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67384/0.69672. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67371/0.69726. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67051/0.69793. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66999/0.69893. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66576/0.70024. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66599/0.70055. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66414/0.70123. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66275/0.70231. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66099/0.70432. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65982/0.70471. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65786/0.70533. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65400/0.70682. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65161/0.70834. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65200/0.70957. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65074/0.71046. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64877/0.71048. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64759/0.71122. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64381/0.71341. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64169/0.71420. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63939/0.71541. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64018/0.71685. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63446/0.71956. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63562/0.71954. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63331/0.72000. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63449/0.72149. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63278/0.72390. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62926/0.72543. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63058/0.72503. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62369/0.72624. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62024/0.72795. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62283/0.72900. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62376/0.72902. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61196/0.73111. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61497/0.73311. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61294/0.73540. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.60641/0.73535. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60671/0.73669. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60613/0.73753. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60564/0.73736. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59769/0.73945. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59321/0.74100. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59405/0.74341. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59797/0.74477. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58720/0.74596. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58694/0.75057. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58626/0.75372. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58105/0.75339. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58421/0.75426. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58228/0.75393. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57548/0.75912. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56774/0.75945. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57078/0.76032. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56674/0.76557. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57100/0.76545. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56232/0.76526. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55776/0.76931. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55851/0.76631. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55213/0.77268. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55365/0.77924. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54738/0.78265. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54590/0.78473. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54476/0.78751. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54524/0.78561. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53808/0.79193. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53765/0.79108. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.53472/0.79693. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52955/0.80374. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52533/0.80475. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52246/0.80401. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.52399/0.80733. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51347/0.80917. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51178/0.81465. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51660/0.81792. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69682/0.68960. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69130/0.68759. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.68775. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69223/0.68781. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69053/0.68758. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69090/0.68771. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69024/0.68767. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68946/0.68773. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68862/0.68767. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68949/0.68781. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68910/0.68810. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68935/0.68841. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68802/0.68855. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68800/0.68919. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68701/0.68955. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68760/0.68985. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68628/0.69030. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68587/0.69067. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68505/0.69137. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68412/0.69204. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68366/0.69252. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68410/0.69289. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68372/0.69349. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68320/0.69345. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68286/0.69418. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68088/0.69443. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68204/0.69525. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67946/0.69569. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67943/0.69641. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67972/0.69697. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68015/0.69737. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67938/0.69790. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67694/0.69823. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67715/0.69890. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67568/0.69952. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67625/0.70059. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67429/0.70129. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67521/0.70178. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67387/0.70141. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67320/0.70211. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67408/0.70261. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67135/0.70331. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66959/0.70466. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66922/0.70511. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67001/0.70582. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66998/0.70584. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66765/0.70605. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66932/0.70642. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66626/0.70732. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66647/0.70977. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66449/0.71044. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66369/0.71128. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66421/0.71191. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66033/0.71268. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66151/0.71410. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66144/0.71398. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66058/0.71518. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65950/0.71628. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65748/0.71755. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65640/0.71898. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65757/0.72094. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65254/0.72220. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65442/0.72283. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65369/0.72383. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65264/0.72611. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65042/0.72579. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65188/0.72669. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64642/0.72890. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65056/0.72919. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64835/0.73132. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64742/0.73186. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64458/0.73236. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64563/0.73213. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64554/0.73405. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63889/0.73498. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64195/0.73777. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64328/0.73917. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63991/0.73907. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63657/0.74024. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63656/0.74150. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63395/0.74638. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63497/0.74432. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62895/0.74611. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62924/0.75004. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63304/0.74959. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62668/0.74831. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62680/0.74938. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62604/0.75365. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62200/0.75524. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62307/0.75556. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62250/0.75835. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62148/0.75750. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61706/0.75942. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61511/0.76287. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61404/0.76533. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60699/0.76497. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60917/0.76495. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60843/0.76922. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60576/0.76888. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60419/0.77222. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69632/0.69365. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69431/0.69273. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69353/0.69207. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69132. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69193/0.69059. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69003/0.68983. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69025/0.68931. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68973/0.68902. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68959/0.68886. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68717/0.68855. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68825/0.68855. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68809/0.68857. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68577/0.68860. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68496/0.68851. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68360/0.68810. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68226/0.68803. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68192/0.68809. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67864/0.68787. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67663/0.68792. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67463/0.68801. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67213/0.68896. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66931/0.69032. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66710/0.69006. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66324/0.69111. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66240/0.69420. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66315/0.69578. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65955/0.69396. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65325/0.69722. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65494/0.69590. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65179/0.69554. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64845/0.69918. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64836/0.70219. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64538/0.70202. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64214/0.70724. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64170/0.70720. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63732/0.71010. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63547/0.70847. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63597/0.71210. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63256/0.71411. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63064/0.71442. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62781/0.71564. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62612/0.71807. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62293/0.72476. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.62504/0.72087. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62196/0.72324. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61259/0.71720. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61173/0.72520. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61295/0.72404. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60810/0.72565. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60862/0.72366. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60815/0.72862. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60249/0.73035. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60017/0.73035. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59534/0.73763. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59275/0.74044. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59684/0.74254. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58489/0.73856. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58926/0.74006. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59035/0.74073. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58662/0.74000. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.57702/0.74293. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58223/0.74832. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.57794/0.74853. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57123/0.75167. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57470/0.75204. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56908/0.75892. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56519/0.76692. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56348/0.75996. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56274/0.75823. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.55974/0.76128. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56066/0.76098. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56323/0.76439. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56198/0.76656. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55160/0.76670. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.54734/0.78301. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54439/0.78252. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54644/0.77623. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54451/0.78557. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54258/0.78601. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54132/0.79041. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53875/0.79618. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53114/0.79478. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53210/0.80406. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.52946/0.79073. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52709/0.79013. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53054/0.79106. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53044/0.80588. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52229/0.79392. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51915/0.80523. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.51226/0.80972. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51092/0.80218. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52556/0.81571. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51335/0.81552. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51170/0.81699. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51133/0.82796. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50400/0.83416. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50638/0.83946. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49434/0.83693. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49712/0.84309. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49373/0.83588. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69169/0.69057. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69111/0.69036. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69109/0.69012. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69069/0.68992. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68994/0.68975. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68962/0.68941. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68919/0.68912. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68887/0.68898. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68877/0.68861. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68788/0.68855. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68686/0.68845. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68596/0.68860. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68565/0.68867. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68477/0.68845. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68405/0.68841. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68310/0.68829. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68136/0.68803. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68090/0.68780. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67842/0.68790. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67686/0.68730. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67587/0.68746. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67387/0.68874. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67052/0.68829. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67007/0.68730. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66924/0.68734. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66562/0.68768. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66366/0.69120. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66175/0.69145. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66184/0.68578. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65765/0.68895. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65827/0.68898. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65573/0.69072. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65435/0.69106. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65022/0.69218. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64971/0.69303. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64857/0.69004. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64705/0.69296. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64308/0.68984. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.63919/0.69268. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64113/0.68988. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63789/0.69056. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63785/0.68677. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63530/0.68939. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63331/0.68780. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62978/0.68985. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62893/0.69178. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62678/0.69670. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62668/0.68904. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61825/0.69222. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62772/0.69510. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62402/0.68606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61558/0.69277. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.61569/0.69245. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61324/0.69321. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61110/0.69038. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61010/0.68830. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60650/0.69038. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60242/0.69438. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60283/0.69017. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60688/0.69762. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60307/0.69325. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59988/0.69152. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59495/0.69040. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59452/0.69036. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58963/0.69122. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59019/0.69180. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59218/0.68794. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59040/0.69114. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58440/0.69596. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58334/0.69032. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57969/0.69192. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57902/0.68998. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57470/0.69475. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57939/0.69614. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56803/0.69845. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57441/0.70391. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57391/0.69595. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56624/0.70069. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56708/0.70522. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56148/0.68833. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56465/0.70005. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56476/0.69370. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55852/0.69539. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55366/0.69855. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55198/0.69796. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55401/0.69676. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54966/0.70204. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54762/0.69554. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54742/0.70159. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54651/0.70708. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54073/0.69761. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53888/0.70626. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53612/0.70085. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53858/0.70537. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52769/0.70752. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52971/0.70879. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52952/0.72107. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52723/0.71580. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52623/0.71708. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52424/0.71355. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69205/0.68921. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69057/0.68706. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69018/0.68644. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68966/0.68592. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68991/0.68551. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68883/0.68515. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68884/0.68473. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68851/0.68459. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68729/0.68408. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68745/0.68367. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68729/0.68343. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68634/0.68299. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68574/0.68269. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68508/0.68216. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68498/0.68201. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68497/0.68250. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68386/0.68147. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68327/0.68171. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68357/0.68124. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68145/0.68132. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68109/0.68126. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67980/0.68120. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67928/0.68100. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67826/0.68139. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67924/0.68190. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67768/0.68211. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67660/0.68235. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67512/0.68295. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67374/0.68285. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67304/0.68337. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67195/0.68387. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67030/0.68522. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67081/0.68499. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66873/0.68635. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66662/0.68610. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66585/0.68731. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66357/0.68867. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66234/0.68847. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66300/0.68991. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66049/0.68971. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65764/0.69105. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65722/0.69155. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65608/0.69273. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65354/0.69262. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65175/0.69424. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64710/0.69726. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64658/0.69822. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64391/0.69854. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64642/0.69980. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64285/0.70018. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63967/0.70322. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64087/0.70137. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63644/0.70308. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63594/0.70362. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63266/0.70613. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63250/0.70734. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62675/0.70892. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62578/0.71093. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62944/0.71186. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62454/0.71173. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62312/0.71329. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62068/0.71376. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62118/0.71489. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61548/0.71404. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61872/0.71418. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61314/0.71684. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61256/0.71844. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61105/0.71849. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60856/0.71954. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60866/0.72160. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60592/0.72276. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60169/0.72364. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59837/0.72599. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60165/0.72678. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59643/0.72746. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59986/0.72795. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59630/0.73055. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59151/0.73087. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58885/0.73115. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58918/0.73390. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58366/0.73821. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58115/0.73997. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58097/0.74002. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57927/0.74290. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57893/0.74330. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57717/0.74329. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57291/0.74588. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56842/0.74580. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.56587/0.75238. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56684/0.75269. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56658/0.75297. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56600/0.75383. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55940/0.75597. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56127/0.75380. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55961/0.76384. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55190/0.76127. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54866/0.76538. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54931/0.76082. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54773/0.76839. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54727/0.77052. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69021/0.69395. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68954/0.69447. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68871/0.69516. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68802/0.69559. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68852/0.69588. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68778/0.69603. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68711/0.69605. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68610/0.69618. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68620/0.69671. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68468/0.69650. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68394/0.69655. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68357/0.69675. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68350/0.69744. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68395/0.69780. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68136/0.69864. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68123/0.69941. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67929/0.69999. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67896/0.70071. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67782/0.70177. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67635/0.70294. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67751/0.70362. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67449/0.70495. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67413/0.70597. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67213/0.70735. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67192/0.70886. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67028/0.71141. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66963/0.71357. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66841/0.71513. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66537/0.71746. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66524/0.71940. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66319/0.72007. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65930/0.72160. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66282/0.72397. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65853/0.72630. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65555/0.72801. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65310/0.73090. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65446/0.73355. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65148/0.73459. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64843/0.73756. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64920/0.74142. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64462/0.74358. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64615/0.74521. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64193/0.74834. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64180/0.75062. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63656/0.75249. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63460/0.75570. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63574/0.75816. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63191/0.76218. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62869/0.76682. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62264/0.77335. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62549/0.77539. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62131/0.77639. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62192/0.77966. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61611/0.78494. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61573/0.78561. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61698/0.78735. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61403/0.79103. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60146/0.79678. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60665/0.79831. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60708/0.79991. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60754/0.80427. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60503/0.80355. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60064/0.80854. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60222/0.81262. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59670/0.81468. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58758/0.82381. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58971/0.82803. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59164/0.82932. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.59133/0.83482. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58547/0.83013. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58440/0.83810. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57821/0.84107. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57713/0.83817. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57943/0.84064. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57632/0.84667. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56867/0.85394. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57012/0.85290. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.56761/0.85931. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56814/0.85676. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56230/0.86798. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55528/0.87233. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55264/0.87702. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55472/0.88639. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56216/0.88478. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55053/0.88334. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54949/0.88902. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.54507/0.89258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54804/0.89617. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53542/0.89989. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55054/0.90213. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53427/0.90583. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53735/0.90542. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53114/0.91008. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53359/0.90980. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52840/0.91729. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52368/0.91679. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52316/0.92096. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52430/0.92262. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51976/0.92999. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51297/0.93561. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69526/0.67568. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68978/0.67919. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68838/0.68046. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68771/0.68168. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68705/0.68287. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68666/0.68444. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68526/0.68526. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68420/0.68679. Took 0.13 sec\n",
      "Epoch 8, Loss(train/val) 0.68464/0.68793. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68438/0.68834. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68336/0.68898. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68305/0.68965. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68243/0.69030. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68283/0.69007. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68147/0.68970. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68143/0.69000. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68029/0.69007. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68204/0.69024. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68033/0.69001. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68035/0.68924. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67995/0.68920. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67932/0.68877. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67934/0.68814. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67879/0.68820. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67822/0.68872. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67852/0.68831. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67752/0.68718. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67693/0.68772. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67670/0.68714. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67648/0.68608. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67607/0.68685. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67703/0.68631. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67580/0.68571. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67560/0.68538. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67368/0.68504. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67371/0.68485. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67424/0.68451. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67433/0.68376. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67308/0.68411. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67290/0.68254. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67188/0.68348. Took 0.14 sec\n",
      "Epoch 41, Loss(train/val) 0.67176/0.68399. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67122/0.68332. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66994/0.68365. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66936/0.68413. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66864/0.68349. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66868/0.68402. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66605/0.68383. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66695/0.68281. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66431/0.68226. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66618/0.68219. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66604/0.68382. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66363/0.68297. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66333/0.68263. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66172/0.68422. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66306/0.68268. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65915/0.68204. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66131/0.68368. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65975/0.68153. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65929/0.68360. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65801/0.68155. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65646/0.68190. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65296/0.68403. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65572/0.68375. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65087/0.68183. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64973/0.68196. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65127/0.68281. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65040/0.68167. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64818/0.68137. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64802/0.68426. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64694/0.68582. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64592/0.68747. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64511/0.68626. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64395/0.68633. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64425/0.68674. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63784/0.68460. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63794/0.68543. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63943/0.68485. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63302/0.68503. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63509/0.68695. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63335/0.68709. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63176/0.68829. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62815/0.69010. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.63174/0.69074. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62635/0.68853. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62779/0.69044. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62287/0.68909. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62455/0.69086. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62091/0.69446. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62171/0.68972. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61578/0.69081. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61628/0.69440. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61591/0.69345. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61297/0.69403. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60918/0.70018. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61287/0.69531. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61390/0.69887. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60831/0.70222. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60057/0.70091. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60381/0.70342. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69294/0.68898. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68872/0.69115. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68663/0.69388. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68549/0.69599. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68504/0.69862. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68279/0.70109. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68191/0.70346. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68180/0.70557. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68196/0.70735. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68032/0.70967. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67944/0.71145. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67927/0.71342. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67900/0.71502. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67843/0.71661. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67811/0.71771. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67689/0.71860. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67597/0.71991. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67599/0.72107. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67496/0.72194. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67502/0.72326. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67496/0.72421. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67291/0.72484. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67245/0.72597. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67157/0.72709. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67116/0.72768. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67096/0.72860. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66978/0.72945. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66946/0.73032. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66828/0.73234. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66833/0.73307. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66624/0.73433. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66620/0.73539. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66622/0.73574. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66603/0.73732. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66420/0.73963. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66218/0.74000. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66433/0.74022. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66157/0.74216. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66182/0.74421. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66013/0.74453. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66027/0.74486. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66122/0.74491. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65861/0.74772. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65513/0.75032. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65804/0.75217. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65720/0.75092. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65656/0.75176. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65242/0.75642. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65512/0.75791. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65099/0.75991. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65138/0.76119. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65160/0.76161. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64960/0.76249. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65406/0.76324. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64936/0.76437. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64836/0.76747. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64569/0.76945. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64922/0.76981. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64615/0.77242. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64705/0.77352. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64463/0.77331. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64490/0.77340. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64672/0.77345. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64247/0.77649. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64118/0.77820. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63801/0.78000. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64065/0.78245. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64081/0.78106. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63690/0.78451. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63577/0.78471. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63893/0.78496. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63553/0.78807. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63661/0.78999. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63596/0.79072. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63280/0.78949. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63244/0.79315. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63150/0.79649. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.63207/0.79493. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62885/0.79906. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62802/0.79929. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62802/0.80242. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62545/0.80313. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62812/0.80305. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62500/0.80827. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62834/0.80779. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62121/0.80860. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61829/0.80822. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62111/0.81625. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62035/0.80918. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61983/0.81370. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61571/0.81485. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61569/0.81920. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61457/0.82357. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61328/0.82176. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61113/0.82636. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61687/0.82577. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60976/0.82875. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60498/0.83451. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60480/0.83448. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60416/0.83482. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69508/0.69465. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69606. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69125/0.69724. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69013/0.69819. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68983/0.69935. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68902/0.70016. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68908/0.70112. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68805/0.70211. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.70299. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68680/0.70387. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.70428. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68674/0.70501. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68594/0.70561. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68640/0.70587. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68563/0.70659. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68586/0.70701. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68534/0.70710. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68470/0.70708. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68493/0.70702. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68438/0.70747. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68318/0.70752. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68306/0.70809. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68241/0.70833. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68157/0.70825. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68169/0.70829. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68119/0.70843. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68008/0.70833. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68060/0.70839. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67990/0.70839. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67946/0.70845. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67759/0.70860. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67844/0.70871. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67551/0.70898. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67577/0.70903. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67415/0.70932. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67400/0.70977. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67256/0.71011. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67117/0.71086. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67064/0.71145. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67003/0.71254. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66813/0.71414. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66674/0.71482. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66651/0.71583. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66557/0.71623. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66380/0.71780. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66241/0.71912. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66222/0.72069. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65973/0.72187. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65884/0.72241. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65855/0.72479. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65979/0.72759. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65669/0.72901. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65527/0.72994. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65116/0.73292. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65330/0.73420. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65130/0.73729. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65170/0.73813. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64873/0.74064. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64881/0.74141. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64589/0.74205. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64883/0.74552. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64414/0.74683. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64204/0.74806. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64369/0.74991. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64595/0.75145. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63616/0.75476. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63915/0.75589. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63244/0.75969. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63695/0.76243. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63257/0.76913. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63402/0.76918. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63294/0.77019. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63075/0.77271. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62690/0.77625. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62551/0.78019. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62594/0.77899. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62512/0.78199. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62628/0.78633. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62350/0.79006. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61999/0.79084. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61499/0.79497. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61706/0.79789. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61526/0.80043. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61368/0.80278. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.61346/0.80422. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60890/0.81457. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61220/0.81460. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60286/0.81807. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60218/0.82286. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60086/0.83253. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60138/0.83019. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60405/0.83656. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59966/0.84045. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59297/0.84706. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59685/0.84674. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59314/0.85233. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58801/0.85594. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58804/0.86044. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58383/0.86760. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.59023/0.87086. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69492/0.69131. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69248/0.69376. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69164/0.69511. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.69633. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69081/0.69737. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69077/0.69834. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69057/0.69903. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69004/0.69969. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68958/0.70023. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.70059. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68898/0.70110. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68955/0.70143. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68852/0.70185. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68872/0.70227. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68851/0.70239. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68797/0.70283. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68789/0.70345. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.70385. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68677/0.70418. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68729/0.70455. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68587/0.70481. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68598/0.70529. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68550/0.70614. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68533/0.70669. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68433/0.70715. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68502/0.70753. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68415/0.70775. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68343/0.70832. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68278/0.70859. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68271/0.70949. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68262/0.71077. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68248/0.71089. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68331/0.71115. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68153/0.71131. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68147/0.71206. Took 0.12 sec\n",
      "Epoch 35, Loss(train/val) 0.68041/0.71213. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68120/0.71225. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67972/0.71247. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68006/0.71279. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67960/0.71328. Took 0.17 sec\n",
      "Epoch 40, Loss(train/val) 0.67956/0.71374. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67795/0.71456. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67820/0.71521. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67751/0.71515. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67640/0.71541. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67580/0.71535. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67522/0.71579. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67442/0.71684. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67425/0.71723. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67370/0.71687. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67241/0.71761. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67422/0.71749. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67287/0.71735. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67165/0.71872. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67203/0.71831. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67128/0.71959. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66906/0.72100. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67087/0.72015. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66908/0.71938. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66708/0.71990. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66738/0.72007. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66660/0.72056. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66534/0.72246. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66355/0.72206. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66455/0.72337. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66599/0.72394. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66266/0.72567. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65968/0.72557. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66202/0.72612. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66136/0.72640. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65832/0.72736. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65832/0.72896. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65690/0.72772. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65706/0.72984. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65598/0.73033. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65467/0.73242. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65434/0.73466. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65337/0.73332. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65068/0.73664. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65300/0.73546. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64917/0.73621. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65000/0.73802. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65031/0.73779. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64927/0.74034. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64222/0.73972. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64427/0.74227. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64341/0.74455. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64305/0.74578. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64287/0.74870. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63977/0.74841. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64126/0.74939. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63768/0.75139. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63691/0.75235. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63644/0.75751. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63475/0.75406. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63382/0.75810. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63361/0.76042. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63230/0.76233. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63203/0.76514. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63004/0.75866. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69332/0.69389. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69477. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69240/0.69601. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.69726. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69086/0.69866. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69047/0.70044. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69030/0.70198. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68980/0.70349. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68899/0.70519. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68855/0.70674. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68859/0.70881. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.71110. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68775/0.71315. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68611/0.71539. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68607/0.71817. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68501/0.72066. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68428/0.72414. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68383/0.72655. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68345/0.72856. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68210/0.73106. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67993/0.73328. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67843/0.73521. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67826/0.73841. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67742/0.74006. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67666/0.74147. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67395/0.74268. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67366/0.74387. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67085/0.74581. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66960/0.74788. Took 0.11 sec\n",
      "Epoch 29, Loss(train/val) 0.66882/0.74871. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66447/0.75193. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66266/0.75572. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66108/0.75920. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65884/0.76014. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66009/0.76029. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65508/0.76370. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65375/0.76470. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65423/0.76656. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65085/0.76814. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64964/0.77208. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64853/0.77453. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64334/0.77709. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64184/0.77898. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63938/0.77829. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63804/0.78159. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.63173/0.78557. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63341/0.78737. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62684/0.78969. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62893/0.79148. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62902/0.79461. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62195/0.79449. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62283/0.79531. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61897/0.79697. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61731/0.79792. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60840/0.80207. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61443/0.80593. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60939/0.80592. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60977/0.80997. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.60101/0.81003. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60574/0.81199. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59788/0.81035. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60027/0.81248. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59611/0.81166. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59230/0.81415. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59026/0.81707. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58758/0.81642. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58611/0.81649. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58570/0.81825. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58467/0.82102. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58321/0.81924. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58387/0.82030. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57456/0.82028. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57279/0.82011. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56423/0.82598. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56516/0.83078. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56074/0.83452. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56455/0.82858. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56163/0.83177. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55239/0.83012. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55606/0.83638. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55235/0.83981. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54795/0.83872. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54733/0.84051. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54069/0.84081. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.53890/0.85174. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52930/0.84933. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53662/0.85106. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52939/0.85295. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52629/0.85907. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52439/0.84224. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52224/0.85763. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.51328/0.86047. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51463/0.86566. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51852/0.86578. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51080/0.87446. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50447/0.86715. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50911/0.87547. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50633/0.87569. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49939/0.87194. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49161/0.87555. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69601/0.68817. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69299/0.68736. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.68660. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69122/0.68594. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69053/0.68544. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69026/0.68475. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68823/0.68399. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68881/0.68332. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68852/0.68292. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68717/0.68252. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68704/0.68237. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68648/0.68253. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68601/0.68252. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68424/0.68232. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68502/0.68244. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68363/0.68228. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68462/0.68239. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68504/0.68268. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68315/0.68244. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68257/0.68256. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68207/0.68267. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68134/0.68267. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68212/0.68277. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68185/0.68264. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68192/0.68240. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68065/0.68233. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68017/0.68264. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68062/0.68214. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67972/0.68155. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67878/0.68115. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67930/0.68063. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67959/0.67971. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67889/0.67983. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67867/0.67916. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67730/0.67959. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67681/0.67903. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67636/0.67936. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67625/0.67949. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67403/0.67879. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67488/0.67842. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67447/0.67826. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67092/0.67745. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67326/0.67741. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67210/0.67782. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67180/0.67711. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67067/0.67762. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66845/0.67757. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66849/0.67723. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66900/0.67742. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66723/0.67753. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66667/0.67811. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66731/0.67762. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66493/0.67607. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66239/0.67727. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66112/0.67697. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66033/0.67684. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65781/0.67667. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65905/0.67686. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65795/0.67710. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65586/0.67882. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65368/0.67773. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65137/0.67929. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64939/0.67989. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65046/0.67880. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64825/0.68172. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64360/0.68207. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64431/0.68374. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63948/0.68450. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63683/0.68509. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63459/0.68717. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63675/0.68927. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.63696/0.69020. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63200/0.69043. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62889/0.68865. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62840/0.69130. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62667/0.69087. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62425/0.69340. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61995/0.69439. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62168/0.69333. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61971/0.70273. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61560/0.69714. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61623/0.70110. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61428/0.70533. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61637/0.70199. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61267/0.70830. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60951/0.70694. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60599/0.70861. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60562/0.70788. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60525/0.71243. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60058/0.70394. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59895/0.70618. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59851/0.71269. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59803/0.70718. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59755/0.70966. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59271/0.71463. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58536/0.71607. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59010/0.71697. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58655/0.71841. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58526/0.71954. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58598/0.72760. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69196/0.69380. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69116/0.69376. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.69365. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69358. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69079/0.69353. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69081/0.69352. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.69350. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.69351. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68986/0.69341. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68897/0.69331. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68820/0.69316. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68730/0.69317. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68690/0.69319. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68556/0.69323. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68459/0.69350. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68363/0.69391. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68062/0.69405. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68095/0.69423. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67689/0.69504. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67511/0.69603. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67440/0.69707. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66948/0.69823. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66816/0.70002. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66437/0.70198. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66055/0.70419. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66050/0.70582. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65644/0.70773. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65500/0.70914. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65394/0.71042. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65186/0.71287. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64839/0.71441. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64720/0.71581. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64279/0.71775. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64209/0.71785. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63741/0.71978. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63692/0.71973. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63125/0.72179. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63238/0.72208. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62875/0.72378. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62510/0.72560. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62311/0.72771. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.62174/0.72734. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62356/0.72718. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61737/0.72768. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61670/0.72947. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61197/0.73131. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61060/0.73199. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60883/0.73602. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60769/0.73721. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60847/0.73670. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60165/0.73796. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60540/0.73660. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59734/0.73899. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59374/0.74148. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.59158/0.74228. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59313/0.74397. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58986/0.74381. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58308/0.75184. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58381/0.75171. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.58178/0.75564. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.57483/0.75594. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57468/0.75875. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.57555/0.76147. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57420/0.76015. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.56729/0.76328. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56628/0.76534. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.55934/0.76570. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56330/0.76732. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.55840/0.77342. Took 0.14 sec\n",
      "Epoch 69, Loss(train/val) 0.56106/0.77507. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55311/0.77674. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.55249/0.78007. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.54708/0.77891. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55286/0.78438. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54079/0.78580. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53862/0.79030. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54107/0.79129. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.53526/0.79996. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.53251/0.79417. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53404/0.79983. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53000/0.80306. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52148/0.80744. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.51676/0.80919. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.52026/0.81411. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51537/0.81526. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51655/0.82353. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51061/0.82641. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.50082/0.82840. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50549/0.83218. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51156/0.83369. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50216/0.83749. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50033/0.84058. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.49975/0.84162. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.48926/0.84476. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.49606/0.85070. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49057/0.85268. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48085/0.86330. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48076/0.86991. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.47662/0.87104. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47510/0.87674. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69376/0.69429. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69241/0.69400. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.69390. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69394. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69062/0.69423. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68990/0.69474. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68994/0.69528. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68855/0.69586. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68776/0.69651. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68880/0.69728. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68567/0.69796. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68556/0.69901. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68480/0.69960. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68382/0.69985. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68325/0.69984. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68125/0.70002. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68036/0.70013. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67840/0.70081. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67601/0.70090. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67510/0.70160. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67156/0.70179. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66952/0.70066. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66948/0.70128. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66656/0.70257. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66456/0.70211. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66029/0.70166. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66191/0.70532. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65687/0.70793. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65619/0.70817. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65371/0.71200. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65390/0.71387. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65297/0.71503. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64750/0.71976. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64450/0.71664. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64765/0.71984. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64155/0.72212. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64114/0.72629. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63773/0.73061. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.63874/0.73716. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.63712/0.73299. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63289/0.73629. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63225/0.73402. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63097/0.73951. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62887/0.74075. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62462/0.74546. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62361/0.74529. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62506/0.73953. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.61963/0.74544. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62241/0.75366. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61527/0.75971. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61473/0.76021. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60979/0.76372. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60765/0.76822. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.60622/0.76916. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60667/0.76421. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60575/0.77484. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60382/0.77200. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59952/0.78090. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59810/0.78658. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59389/0.78841. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59602/0.79282. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59217/0.78926. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59061/0.78800. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58899/0.79608. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58559/0.79898. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58639/0.80646. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58526/0.80719. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58092/0.80575. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57270/0.80777. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57879/0.81687. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57396/0.81839. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57963/0.81189. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56750/0.81046. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56785/0.81667. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56429/0.81745. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56851/0.81243. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.56578/0.81925. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55484/0.82064. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55421/0.82929. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55404/0.83582. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55315/0.83753. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54848/0.83173. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54804/0.84071. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55232/0.83476. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.54409/0.84263. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.53567/0.84442. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54026/0.83661. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54354/0.83735. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53668/0.85652. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.53463/0.85267. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52683/0.85264. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52759/0.85270. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52446/0.86313. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52335/0.86396. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51828/0.85754. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52463/0.86436. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51923/0.86123. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51334/0.85734. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51563/0.86675. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.50291/0.86577. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69647/0.70081. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69611/0.69920. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69519/0.69666. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69417/0.69376. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69232/0.69122. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69180/0.68944. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69132/0.68857. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69088/0.68785. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69044/0.68737. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.68732. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68923/0.68737. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68864/0.68758. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68784/0.68807. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68786/0.68836. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68631/0.68862. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68654/0.68913. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68507/0.68945. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68448/0.69021. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68502/0.69087. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68343/0.69084. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68145/0.69185. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68289/0.69253. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68119/0.69321. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68087/0.69420. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67987/0.69402. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67968/0.69480. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67803/0.69514. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67929/0.69658. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67864/0.69580. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67716/0.69679. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67673/0.69718. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67532/0.69758. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67379/0.69933. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67327/0.69965. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67173/0.70010. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67055/0.70010. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66895/0.70208. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66851/0.70115. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66700/0.70290. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66705/0.70418. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66366/0.70420. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66410/0.70686. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66400/0.70776. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66192/0.70830. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66011/0.71039. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65770/0.71283. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65313/0.71307. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65416/0.71578. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65362/0.71681. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65147/0.71710. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64914/0.71829. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64752/0.72007. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64352/0.72324. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64530/0.72339. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64563/0.72510. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64133/0.72956. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64053/0.72992. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63676/0.73045. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63595/0.73094. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63210/0.73306. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63287/0.73698. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62593/0.73856. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63080/0.74025. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62549/0.74409. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62487/0.74534. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62089/0.75053. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62183/0.75049. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61815/0.74985. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61607/0.75672. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61432/0.75983. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61257/0.76196. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61093/0.76039. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60847/0.76921. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60853/0.76641. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60764/0.76997. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60024/0.77263. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60593/0.77062. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59999/0.77768. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60029/0.77955. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59586/0.77833. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59192/0.78188. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59047/0.79209. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59147/0.78451. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.58866/0.78655. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58961/0.80161. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59264/0.79174. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59058/0.79201. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58769/0.79187. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57887/0.80104. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.57624/0.79492. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57535/0.80385. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57385/0.81184. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56796/0.80922. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57158/0.81387. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56378/0.81110. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56559/0.81408. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56120/0.81634. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56386/0.81762. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56090/0.83060. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55868/0.82471. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69098/0.69518. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69079/0.69576. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69043/0.69642. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69004/0.69709. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68963/0.69758. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68973/0.69794. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68904/0.69825. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68825/0.69894. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68827/0.69937. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68712/0.69906. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68636/0.69955. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68702/0.69986. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68455/0.70032. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68377/0.70059. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68364/0.70177. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68387/0.70193. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68129/0.70255. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67914/0.70282. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67875/0.70267. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67832/0.70225. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67543/0.70326. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67532/0.70353. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67421/0.70464. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67267/0.70324. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67308/0.70290. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67123/0.70477. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66733/0.70428. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66451/0.70414. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66525/0.70436. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66416/0.70363. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66266/0.70421. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66184/0.70275. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66201/0.70242. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65782/0.70271. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65842/0.70324. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65286/0.70216. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65533/0.70184. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65446/0.70097. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64908/0.70042. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64994/0.70111. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64614/0.70139. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64437/0.70152. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64765/0.70229. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64222/0.70165. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63821/0.70229. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63889/0.70208. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63199/0.70091. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63515/0.70126. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63038/0.70219. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62328/0.70275. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62988/0.70226. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62431/0.70003. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.61653/0.70086. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62075/0.70067. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61463/0.69923. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61206/0.69943. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60952/0.69858. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60951/0.69812. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60620/0.69910. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60612/0.69810. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59880/0.69651. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60259/0.69626. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59724/0.69447. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59715/0.69200. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58897/0.69475. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59605/0.69421. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58784/0.69277. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58231/0.69085. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57624/0.69297. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58308/0.69607. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.58189/0.69748. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57596/0.69491. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57783/0.69645. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56874/0.69549. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56488/0.69061. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56438/0.69419. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56085/0.69290. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55325/0.68795. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55868/0.69468. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54922/0.69447. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55114/0.69626. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54407/0.69752. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54975/0.69771. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54005/0.69760. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53632/0.69429. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54158/0.69265. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53103/0.69595. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53213/0.69755. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51919/0.69788. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51868/0.70114. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52216/0.69301. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51985/0.70712. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51538/0.69581. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51373/0.70367. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51123/0.70360. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51104/0.69890. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50027/0.70783. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50303/0.71280. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50111/0.71211. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50148/0.70423. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69346. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69222/0.69247. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69143/0.69191. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69000/0.69174. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68960/0.69133. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69049/0.69088. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68985/0.69043. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68920/0.68968. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68904/0.68863. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68813/0.68748. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68815/0.68654. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68802/0.68572. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68738/0.68465. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68639/0.68324. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68506/0.68154. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68507/0.68034. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68308/0.67871. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68222/0.67698. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68157/0.67546. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68247/0.67411. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68069/0.67283. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68082/0.67238. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67985/0.67224. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68028/0.67086. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67806/0.66993. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67742/0.66915. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67727/0.66812. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67459/0.66757. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.67420/0.66672. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67138/0.66617. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67253/0.66493. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67055/0.66485. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67172/0.66482. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66872/0.66513. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66764/0.66477. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66663/0.66466. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66423/0.66517. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66610/0.66345. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66279/0.66273. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66423/0.66162. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66466/0.66359. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66528/0.66409. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.65990/0.66212. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66013/0.66305. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65938/0.66275. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65627/0.66287. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65539/0.66301. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65485/0.66149. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65589/0.66088. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65062/0.66068. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65137/0.66063. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64741/0.66041. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65071/0.66082. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64666/0.66077. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.64480/0.66090. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64358/0.66078. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64156/0.66134. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63919/0.66162. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64002/0.66209. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63608/0.66073. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63306/0.66036. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63142/0.66025. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63256/0.66076. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.62672/0.65887. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62880/0.66358. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62637/0.66034. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62329/0.65938. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62090/0.66016. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61864/0.66114. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61963/0.65852. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61313/0.65789. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61314/0.65918. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61422/0.65771. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61566/0.65834. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60548/0.65987. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60234/0.65997. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60005/0.65761. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59683/0.65812. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59783/0.66095. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59324/0.65889. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59226/0.65563. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.58541/0.65865. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58487/0.65762. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58787/0.65748. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58467/0.66014. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58194/0.65926. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57363/0.66005. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57380/0.66115. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56175/0.65921. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57175/0.66221. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56234/0.65932. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55985/0.66131. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55415/0.66550. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55368/0.65855. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55494/0.66536. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54820/0.66359. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55019/0.66855. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54400/0.67477. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.53674/0.67464. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54831/0.67121. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69646/0.69084. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69155/0.69144. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69084/0.69358. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69007/0.69544. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69697. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69001/0.69789. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68857/0.69915. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68680/0.70004. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68819/0.70101. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68796/0.70175. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68638/0.70266. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68639/0.70321. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68664/0.70346. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68666/0.70350. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68604/0.70437. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68534/0.70457. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68589/0.70482. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.68555/0.70479. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68469/0.70499. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68402/0.70558. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68469/0.70576. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68359/0.70582. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68280/0.70562. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68277/0.70575. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68250/0.70576. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68321/0.70567. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68110/0.70563. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68110/0.70536. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67988/0.70510. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68043/0.70545. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67889/0.70518. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67844/0.70490. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67891/0.70485. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67856/0.70431. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67540/0.70411. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67658/0.70447. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67504/0.70467. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67666/0.70407. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67450/0.70503. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67403/0.70457. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67375/0.70505. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67400/0.70478. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67239/0.70571. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67016/0.70527. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67041/0.70625. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66988/0.70641. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67122/0.70601. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66625/0.70613. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66733/0.70832. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66579/0.70908. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66471/0.71013. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66427/0.71039. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66400/0.71126. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66450/0.71175. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66429/0.71289. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66104/0.71374. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65915/0.71464. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66105/0.71511. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65870/0.71552. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65781/0.71693. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65678/0.71806. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65689/0.72096. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65630/0.72003. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65582/0.72135. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65193/0.72359. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65147/0.72566. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65032/0.72431. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64892/0.72583. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64558/0.72715. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64887/0.72848. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64678/0.72796. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64901/0.72924. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64396/0.72943. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64390/0.73269. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64192/0.73446. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64257/0.73383. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64061/0.73609. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64253/0.73724. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63882/0.73899. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64036/0.73863. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63698/0.73898. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63352/0.74043. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63383/0.74285. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63217/0.74298. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63093/0.74437. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62877/0.74700. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62856/0.74856. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62582/0.74879. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62365/0.75239. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62532/0.75171. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62472/0.75310. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62026/0.75650. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61404/0.75737. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62068/0.76009. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61610/0.75908. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61653/0.75808. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61635/0.76365. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60519/0.76591. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60892/0.76723. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60711/0.76814. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69363/0.69329. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 0.69207/0.69199. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69159/0.69127. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69119/0.69083. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69059/0.69054. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69032/0.69021. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69038/0.68981. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68895/0.68941. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68863/0.68895. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68885/0.68841. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68835/0.68809. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.68778. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68699/0.68748. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68644/0.68714. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68586/0.68681. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68514/0.68642. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68353/0.68596. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68336/0.68544. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68090/0.68497. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68238/0.68441. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68031/0.68430. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68077/0.68368. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67896/0.68358. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67680/0.68262. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67615/0.68203. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67404/0.68158. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67429/0.68095. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67111/0.68006. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67067/0.67891. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66872/0.67843. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66643/0.67831. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66533/0.67717. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66494/0.67574. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66366/0.67452. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66158/0.67433. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66201/0.67300. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66054/0.67263. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65521/0.67047. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65375/0.66918. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.65222/0.66854. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64733/0.66732. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64830/0.66674. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64836/0.66559. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64366/0.66390. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64436/0.66262. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64160/0.66211. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64012/0.66051. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63354/0.66082. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63621/0.65857. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62971/0.65835. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63212/0.65774. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.62864/0.65640. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62594/0.65707. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62339/0.65646. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62104/0.65635. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62073/0.65725. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61658/0.65828. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61608/0.65904. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61326/0.65908. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61020/0.65751. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60384/0.65897. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60648/0.66043. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61076/0.65958. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60477/0.66164. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59667/0.66523. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59812/0.66595. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59890/0.66592. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59686/0.66518. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59407/0.66991. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59327/0.67134. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58544/0.67192. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59028/0.67036. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58525/0.67410. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58378/0.67725. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57696/0.67944. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58376/0.67664. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57920/0.68146. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57709/0.68021. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57287/0.67862. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57364/0.68453. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56895/0.68357. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56738/0.69019. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57226/0.68601. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56493/0.68775. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56033/0.68753. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55883/0.68867. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55353/0.68827. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55047/0.69116. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55013/0.69205. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55011/0.69571. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54478/0.69703. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54266/0.69659. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54434/0.69929. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54039/0.70169. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53729/0.70060. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53359/0.69959. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52993/0.70903. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53438/0.70892. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53110/0.71328. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52672/0.70987. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69535/0.69325. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69277/0.69351. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69349/0.69379. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69262/0.69403. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69245/0.69438. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69204/0.69458. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.69481. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69173/0.69518. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69115/0.69546. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69142/0.69581. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69039/0.69612. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69105/0.69654. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69081/0.69699. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69008/0.69733. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68950/0.69778. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68990/0.69829. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68885/0.69890. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68829/0.69945. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68824/0.70007. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68643/0.70097. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68595/0.70190. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68432/0.70254. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68392/0.70365. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68336/0.70453. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68215/0.70583. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68143/0.70688. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68059/0.70796. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67857/0.70903. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67841/0.71027. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67765/0.71101. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67386/0.71250. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67225/0.71329. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67256/0.71504. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66910/0.71613. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66718/0.71730. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66517/0.71963. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66316/0.72083. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65986/0.72268. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66038/0.72536. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65741/0.72682. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65482/0.72939. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65115/0.73050. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64907/0.73248. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64953/0.73208. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64917/0.73287. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64541/0.73773. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64617/0.73926. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64178/0.74164. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63989/0.74458. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63390/0.74547. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63772/0.74614. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63440/0.74942. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63089/0.75334. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63034/0.75386. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62886/0.75749. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62301/0.76174. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62264/0.76275. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62043/0.76613. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61947/0.76436. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62092/0.77012. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61603/0.76970. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61244/0.77427. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61135/0.78039. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60884/0.78172. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60744/0.78229. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60802/0.78539. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61029/0.78448. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60462/0.78841. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60714/0.78739. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59637/0.79192. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60037/0.79619. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59359/0.80412. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59601/0.80678. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58644/0.81018. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59540/0.81010. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59132/0.81365. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58503/0.81161. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58394/0.81879. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58187/0.82355. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.58353/0.82723. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57537/0.82926. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57875/0.82885. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57807/0.83614. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57575/0.83578. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56770/0.84393. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56754/0.85359. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57072/0.85045. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56585/0.85429. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56222/0.86166. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55439/0.86055. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55610/0.86393. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55108/0.86613. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55293/0.86783. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56280/0.86749. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54717/0.87612. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55032/0.87677. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54287/0.88066. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54616/0.88211. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54512/0.88447. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54114/0.88925. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69667. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69615. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69237/0.69604. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.69618. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69083/0.69692. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.69774. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68905/0.69832. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68855/0.69903. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68700/0.69974. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68586/0.70109. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68437/0.70182. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68271/0.70257. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68043/0.70295. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68022/0.70206. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67783/0.70196. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67502/0.70190. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67252/0.70404. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67100/0.70183. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.66967/0.70220. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.66518/0.69985. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66035/0.70185. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66209/0.70109. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66179/0.70187. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65825/0.70319. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65603/0.70302. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65390/0.70359. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65268/0.70479. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65175/0.70324. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.64988/0.70659. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.64754/0.70773. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64468/0.70833. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64220/0.70850. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64349/0.70863. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.63905/0.70780. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.63809/0.70833. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63767/0.70999. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63670/0.71082. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63444/0.71387. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63157/0.71420. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62922/0.71491. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62908/0.71574. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62846/0.71619. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62785/0.71642. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62135/0.71694. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62108/0.72102. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61874/0.72072. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.61597/0.71985. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60978/0.72524. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61375/0.72784. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61477/0.72623. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60825/0.72846. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60715/0.73104. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60758/0.73335. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60596/0.73423. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60397/0.73260. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59908/0.73376. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59715/0.73596. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59656/0.73898. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59310/0.73901. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58461/0.74255. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58630/0.74839. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58829/0.74833. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58162/0.75252. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57884/0.75698. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57399/0.75357. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57832/0.76170. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57524/0.75944. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57510/0.76107. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57416/0.76802. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56789/0.77022. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.56222/0.77448. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56398/0.77793. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55914/0.78647. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56133/0.79552. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55847/0.79219. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55462/0.79811. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54623/0.80338. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54922/0.79943. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54307/0.81284. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54115/0.82486. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54025/0.81219. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.54312/0.81949. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.53217/0.82011. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53504/0.83107. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53197/0.82900. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52191/0.83679. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52565/0.84323. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52103/0.85147. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51568/0.85049. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51258/0.85074. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51854/0.86432. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51342/0.86349. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51037/0.87697. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51132/0.88089. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51136/0.87399. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50542/0.88902. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50586/0.88778. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49844/0.89320. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49989/0.90615. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49862/0.91826. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69448/0.70044. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69193/0.69758. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69145/0.69620. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69006/0.69550. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69045/0.69479. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68996/0.69473. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69033/0.69445. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68952/0.69432. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68914/0.69442. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68940/0.69482. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68820/0.69498. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68785/0.69570. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68733/0.69584. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68710/0.69656. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68773/0.69678. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68568/0.69789. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.69810. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68636/0.69858. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68624/0.69844. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68513/0.69964. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68480/0.69949. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68427/0.69982. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68331/0.70074. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68289/0.70086. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68249/0.70222. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68267/0.70226. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68216/0.70237. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68127/0.70304. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68080/0.70360. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68006/0.70435. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67822/0.70533. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67862/0.70775. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67743/0.70714. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67551/0.70965. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67502/0.70970. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67447/0.71112. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67493/0.71285. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67218/0.71290. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67074/0.71565. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67128/0.71836. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67017/0.71808. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66768/0.72141. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66785/0.72240. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66715/0.72415. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66526/0.72551. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66415/0.72482. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66286/0.72688. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66328/0.72995. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66238/0.73049. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65936/0.73048. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66126/0.73216. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65817/0.73281. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65663/0.73366. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65594/0.73530. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65469/0.73559. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65393/0.73964. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65148/0.73932. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65140/0.74105. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64685/0.74083. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64701/0.74602. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64356/0.74635. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64371/0.74830. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64320/0.74948. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64142/0.75231. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64337/0.75302. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63902/0.75162. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63960/0.75057. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63471/0.75069. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63468/0.75282. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63304/0.75283. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62829/0.75462. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63177/0.75486. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62678/0.75442. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62735/0.75578. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62249/0.75916. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62702/0.75500. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62018/0.75566. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61938/0.75846. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61715/0.75649. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61922/0.75668. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61503/0.75785. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61434/0.75872. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60750/0.75349. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61083/0.75652. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60703/0.75703. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60258/0.75576. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60275/0.75980. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59776/0.75625. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.60033/0.75579. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59765/0.76103. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59503/0.75494. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59311/0.75342. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59108/0.75454. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58944/0.75569. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58712/0.75507. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59258/0.75904. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58221/0.75440. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58250/0.75382. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58237/0.75686. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57099/0.75661. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69429/0.68420. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69147/0.68488. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69037/0.68510. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68981/0.68498. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68976/0.68528. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68915/0.68570. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68945/0.68631. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.68607. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68839/0.68642. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68776/0.68666. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68762/0.68705. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68711/0.68748. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68715/0.68786. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68629/0.68856. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68599/0.68941. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68633/0.68975. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68547/0.69017. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68541/0.69089. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68549/0.69154. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68457/0.69204. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68427/0.69254. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68470/0.69386. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68423/0.69439. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68292/0.69474. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68262/0.69572. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68216/0.69657. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68246/0.69813. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68143/0.69960. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68135/0.70018. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68119/0.70169. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67911/0.70214. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68011/0.70370. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67982/0.70532. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67898/0.70639. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67902/0.70788. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67804/0.70804. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67813/0.70945. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67696/0.71105. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67658/0.71305. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67617/0.71433. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67562/0.71539. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67418/0.71692. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67542/0.71872. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67423/0.72018. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67370/0.72122. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67358/0.72221. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67328/0.72286. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67277/0.72502. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67169/0.72682. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67061/0.72651. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67000/0.72894. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66935/0.73027. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66908/0.73080. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66782/0.73305. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66845/0.73331. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66791/0.73272. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66574/0.73585. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66566/0.73624. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66432/0.73793. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66361/0.73841. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66304/0.73903. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66234/0.73974. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66186/0.74037. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66095/0.74086. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66018/0.74008. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65936/0.74117. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65669/0.74091. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65658/0.74265. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65809/0.74299. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65402/0.74498. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65319/0.74214. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65413/0.74299. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65263/0.73910. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64776/0.73758. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65011/0.73802. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64514/0.73748. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64663/0.73716. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64297/0.73646. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64137/0.73483. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64064/0.73463. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63885/0.73447. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63811/0.73366. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63644/0.73233. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63521/0.73247. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63338/0.73020. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63067/0.72952. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63075/0.73140. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63039/0.73294. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62566/0.73123. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62626/0.73226. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62283/0.72672. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62331/0.72997. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61633/0.72885. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61828/0.73148. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61283/0.73430. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61484/0.73131. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61151/0.73498. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61343/0.73292. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61146/0.73580. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60983/0.73347. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69397/0.69162. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69194/0.68920. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69031/0.68766. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68981/0.68664. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68951/0.68590. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68936/0.68553. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68808/0.68512. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68761/0.68475. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68845/0.68457. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68792/0.68415. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68674/0.68362. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68696/0.68317. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68495/0.68255. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68520/0.68203. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68501/0.68147. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68465/0.68106. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68349/0.68054. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68163/0.67997. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68066/0.67917. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67930/0.67830. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67775/0.67745. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67850/0.67668. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67703/0.67622. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67403/0.67527. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67310/0.67460. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67052/0.67261. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66954/0.67196. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66626/0.67053. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66477/0.66973. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66349/0.66985. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66106/0.66665. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66098/0.66720. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65894/0.66702. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65549/0.66721. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65395/0.66533. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65410/0.66413. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64994/0.66450. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64507/0.66490. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64281/0.66480. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64162/0.66766. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63946/0.66566. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63561/0.66902. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63538/0.66861. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63629/0.66997. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63301/0.66900. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62891/0.67336. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63172/0.66927. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62596/0.66851. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62336/0.67105. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62360/0.67449. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62540/0.66996. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61981/0.67574. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61818/0.67545. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61993/0.67475. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61360/0.67660. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61619/0.67993. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61117/0.67666. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61104/0.67699. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60842/0.67614. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60441/0.67606. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60335/0.67269. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60143/0.68116. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60228/0.68321. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59489/0.68064. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59788/0.67911. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59654/0.68551. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58850/0.68408. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59069/0.69140. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58711/0.68402. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58949/0.68778. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58189/0.68735. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.58162/0.69028. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57701/0.69609. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57896/0.69464. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57314/0.70039. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57915/0.69854. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56911/0.70104. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57109/0.69691. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56980/0.69990. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55810/0.70080. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56117/0.69510. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56088/0.70646. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56094/0.71347. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55360/0.71269. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55594/0.71896. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55220/0.71509. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54754/0.72153. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54767/0.72217. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54517/0.72121. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54506/0.72713. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54827/0.72786. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54284/0.72937. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53453/0.72709. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53425/0.73437. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53464/0.73618. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52607/0.73747. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52298/0.74815. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52906/0.73906. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.52846/0.74433. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52125/0.74593. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69728/0.69430. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69577/0.69350. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69475/0.69263. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69354/0.69163. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69194/0.69079. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69104/0.69043. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69027/0.69024. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68905/0.69021. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69013/0.69029. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68886/0.69043. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68762/0.69061. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68867/0.69086. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68804/0.69116. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68741/0.69146. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68684/0.69186. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68519/0.69211. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68442/0.69232. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68348/0.69298. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68237/0.69375. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68170/0.69422. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67997/0.69519. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67853/0.69616. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67642/0.69643. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67241/0.69717. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67121/0.69813. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67031/0.69925. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66842/0.70063. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66643/0.70182. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66623/0.70346. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66349/0.70324. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66161/0.70567. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66066/0.70626. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66030/0.70993. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65446/0.70985. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65408/0.71262. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65544/0.71583. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65274/0.71850. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64688/0.71966. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64912/0.72190. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64730/0.72617. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64480/0.72647. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64239/0.72979. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64149/0.73525. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63943/0.73669. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63995/0.74079. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63864/0.74326. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63367/0.74435. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62980/0.74612. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63228/0.74947. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62998/0.75083. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.62644/0.75486. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.62499/0.76014. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62395/0.76333. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61849/0.76489. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61599/0.77047. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61619/0.77360. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61461/0.77791. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61286/0.77931. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61284/0.78374. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60784/0.78749. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60941/0.79157. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60432/0.79455. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60407/0.79527. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59868/0.79804. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60125/0.80074. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59682/0.80257. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59452/0.80355. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59295/0.80527. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.59052/0.80823. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58735/0.80960. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58343/0.81462. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58105/0.81759. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58319/0.81969. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57372/0.82548. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57715/0.82906. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57470/0.82999. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57364/0.83393. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.56680/0.83601. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56903/0.83908. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56254/0.84276. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55987/0.84426. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56606/0.84397. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55335/0.84968. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54879/0.85512. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55591/0.85469. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55063/0.85680. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54001/0.86494. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54500/0.86726. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.54799/0.86718. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54056/0.86445. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53170/0.87439. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53387/0.88335. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53170/0.88518. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51536/0.88907. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.52169/0.88631. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53404/0.88386. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52224/0.88830. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51396/0.90392. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51981/0.90136. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52254/0.90054. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69142/0.69661. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68913/0.69624. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68862/0.69569. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68777/0.69534. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68717/0.69496. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68643/0.69481. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68612/0.69464. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68540/0.69455. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68459/0.69440. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68364/0.69426. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68314/0.69434. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68248/0.69442. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68103/0.69457. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68143/0.69496. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68080/0.69522. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67876/0.69528. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67869/0.69552. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67589/0.69565. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67515/0.69602. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67461/0.69660. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67356/0.69723. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67274/0.69810. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66923/0.69871. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67049/0.69920. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66777/0.69970. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66613/0.70013. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66542/0.70093. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66401/0.70184. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66221/0.70316. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65981/0.70411. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65847/0.70550. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65781/0.70674. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65459/0.70785. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65373/0.70872. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65057/0.71061. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64719/0.71291. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64656/0.71530. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64682/0.71712. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64646/0.72012. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64372/0.72367. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64096/0.72462. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63902/0.72757. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63947/0.72763. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63672/0.73084. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63722/0.73367. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63410/0.73676. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63147/0.73934. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62832/0.74118. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62866/0.74512. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62981/0.75041. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62594/0.74868. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62493/0.74510. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62020/0.74751. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61865/0.75193. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61830/0.75094. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61733/0.75021. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61572/0.75047. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60722/0.75960. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60703/0.76445. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61050/0.76136. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60388/0.76508. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60257/0.77002. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59859/0.76582. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59381/0.77386. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59684/0.77953. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59542/0.78601. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59070/0.78444. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58831/0.78741. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.58469/0.78670. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58566/0.79921. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58670/0.79999. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57865/0.80003. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57457/0.80081. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57661/0.80537. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57403/0.81348. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57070/0.81384. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57191/0.80709. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56576/0.81816. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56199/0.82267. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56172/0.82291. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56440/0.82506. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54802/0.83133. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55319/0.83380. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54983/0.84554. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55165/0.83923. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54240/0.84388. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.54178/0.85165. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54123/0.85464. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53275/0.86735. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53452/0.86942. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52633/0.86270. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53045/0.86889. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.53189/0.87111. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52004/0.88099. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51745/0.89499. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51478/0.89146. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51439/0.88339. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50977/0.91709. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50873/0.90914. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49949/0.91928. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69572/0.69183. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69446/0.69219. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69353/0.69264. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69298. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69210/0.69332. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69144/0.69378. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69058/0.69414. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68994/0.69453. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.69509. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68773/0.69558. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68713/0.69626. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68434/0.69703. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68232/0.69767. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68156/0.69807. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67999/0.69900. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67720/0.69946. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67485/0.69955. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67440/0.69945. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67121/0.69919. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66842/0.69960. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66748/0.69955. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66380/0.69931. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66250/0.69882. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66067/0.69821. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65927/0.69714. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65593/0.69639. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.65393/0.69596. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65360/0.69590. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64946/0.69585. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65187/0.69504. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64665/0.69540. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64401/0.69525. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64500/0.69485. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64644/0.69376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63712/0.69495. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63720/0.69483. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63661/0.69489. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63339/0.69517. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63487/0.69630. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62992/0.69708. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63074/0.69779. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63082/0.69847. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62737/0.69780. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62756/0.69773. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62684/0.69757. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62575/0.69845. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62177/0.69972. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61551/0.70009. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61546/0.70124. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61631/0.70226. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61381/0.70280. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60991/0.70557. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60854/0.70437. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60662/0.70752. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60342/0.71163. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59937/0.71321. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59986/0.71244. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59888/0.71135. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60180/0.71346. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59293/0.71608. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59179/0.71492. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59456/0.72181. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59418/0.71639. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58720/0.71896. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58813/0.71769. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58342/0.71900. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58278/0.71780. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57608/0.72214. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57111/0.72292. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57280/0.72749. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57343/0.73020. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.56957/0.73127. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56796/0.73014. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56421/0.73251. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56516/0.73682. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55909/0.74065. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55764/0.74518. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55918/0.73949. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55521/0.74453. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55324/0.74558. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54733/0.74910. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55095/0.74941. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54290/0.74893. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54263/0.75710. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53197/0.76061. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53418/0.76546. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.54147/0.76654. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53303/0.76355. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53216/0.76903. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52427/0.77137. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52061/0.78035. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52569/0.78355. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51944/0.78738. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52206/0.79338. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51522/0.79997. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51945/0.80005. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50597/0.79127. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50695/0.79667. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50771/0.78951. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49996/0.80174. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69078/0.69864. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68938/0.69975. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68857/0.70011. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68878/0.69993. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68771/0.70022. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68821/0.70004. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68773/0.70016. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68699/0.70041. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68677/0.70074. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68629/0.70115. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68625/0.70137. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68600/0.70131. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68587/0.70179. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68517/0.70222. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68534/0.70242. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68387/0.70261. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68390/0.70304. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68379/0.70325. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68389/0.70327. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68351/0.70383. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68324/0.70354. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68254/0.70394. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68213/0.70379. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68143/0.70404. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68078/0.70458. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67969/0.70534. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67976/0.70538. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67827/0.70501. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67835/0.70521. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67796/0.70522. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67734/0.70450. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67574/0.70539. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67512/0.70541. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67488/0.70573. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67394/0.70545. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67384/0.70595. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67213/0.70594. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67154/0.70706. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67068/0.70630. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67001/0.70737. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66884/0.70728. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66792/0.70617. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66609/0.70707. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66412/0.70773. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66416/0.70812. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66066/0.70880. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66151/0.70872. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65905/0.71115. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65800/0.71022. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65535/0.71073. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65548/0.71292. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65627/0.71227. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65250/0.71241. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65019/0.71114. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64751/0.71226. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64483/0.71252. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64362/0.71398. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64427/0.71495. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64144/0.71385. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63703/0.71617. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63555/0.72066. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63414/0.72063. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63251/0.71983. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62782/0.72162. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62862/0.71775. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63070/0.72466. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62638/0.72464. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62480/0.72472. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62108/0.72686. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62340/0.72784. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61679/0.73175. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61628/0.73625. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61514/0.74073. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61258/0.73554. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61063/0.73874. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61339/0.74186. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60858/0.74261. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.60443/0.74251. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60096/0.74849. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60039/0.75000. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60308/0.75543. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59896/0.75076. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59329/0.75658. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59476/0.76119. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58872/0.76611. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59267/0.76822. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58558/0.76808. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58464/0.77204. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58182/0.77809. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58742/0.78296. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58251/0.78477. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57529/0.79370. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57271/0.79606. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57140/0.79850. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56940/0.79010. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57045/0.80378. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57031/0.81139. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56806/0.80493. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55997/0.81611. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55839/0.82090. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69237/0.68413. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.68436. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69170/0.68521. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69153/0.68570. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69149/0.68637. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69109/0.68655. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69053/0.68650. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68982/0.68605. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.68637. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68902/0.68605. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68793/0.68564. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68794/0.68494. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68597/0.68469. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68759/0.68424. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68323/0.68322. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68262/0.68292. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68054/0.68276. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68207/0.68164. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68087/0.68007. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67866/0.67786. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67658/0.67734. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67433/0.67566. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67559/0.67634. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67272/0.67372. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66960/0.67334. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66865/0.67043. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66514/0.66917. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.66514/0.66655. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66474/0.66466. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65981/0.66470. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65875/0.65702. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65217/0.65706. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65424/0.65236. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64831/0.65293. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64574/0.64970. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64669/0.65205. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64000/0.64968. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63844/0.64798. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63478/0.64617. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.63139/0.64651. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63397/0.64781. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62269/0.64417. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62364/0.64624. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62204/0.64494. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61530/0.64583. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61220/0.64486. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61742/0.64649. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60923/0.64972. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60627/0.64951. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60530/0.65195. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60043/0.65377. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.59806/0.64891. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59366/0.65104. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59647/0.65211. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.59125/0.65363. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.58972/0.65727. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59224/0.65983. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58592/0.65298. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58493/0.65940. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.57623/0.65823. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.57926/0.65975. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.56927/0.65782. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.56795/0.65830. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.56631/0.66299. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.56129/0.66564. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56877/0.66086. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.55984/0.66468. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56247/0.66773. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.55177/0.66946. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.54860/0.67374. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.54269/0.67904. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.54171/0.67757. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.54755/0.67402. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.53829/0.68216. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.53651/0.68039. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53268/0.67687. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.52930/0.67943. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.52928/0.68154. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.53173/0.68886. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53204/0.69179. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.51958/0.69086. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.51321/0.69970. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.51469/0.69752. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51458/0.70537. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.50585/0.71001. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51236/0.71039. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.50786/0.71984. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.49955/0.72167. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50278/0.71664. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50383/0.72472. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50293/0.71920. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.48912/0.72772. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.48860/0.72528. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.48919/0.72909. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48489/0.73222. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48121/0.74372. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.46982/0.74772. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.47583/0.75416. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.46968/0.75018. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47140/0.75080. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69537/0.70156. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69233/0.69964. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69919. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69926. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.69867. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.69836. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69038/0.69841. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69145/0.69802. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69102/0.69765. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69079/0.69762. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.69771. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.69710. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68892/0.69663. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69029/0.69696. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69016/0.69667. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68844/0.69621. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68833/0.69607. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68702/0.69629. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68708/0.69574. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68703/0.69554. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68675/0.69546. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68710/0.69542. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68526/0.69545. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68484/0.69518. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68544/0.69434. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68498/0.69423. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68490/0.69408. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68381/0.69301. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68379/0.69312. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68361/0.69337. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68178/0.69331. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68186/0.69365. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67989/0.69357. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68197/0.69334. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68073/0.69332. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67954/0.69243. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67813/0.69288. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67795/0.69206. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67689/0.69264. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67738/0.69204. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67567/0.69116. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67643/0.69214. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67353/0.69171. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67196/0.69145. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67311/0.69117. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67049/0.69129. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67046/0.69123. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66989/0.69027. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66791/0.69018. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66896/0.69005. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.66692/0.68922. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66366/0.68836. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66379/0.68810. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66190/0.68849. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66143/0.68862. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65971/0.68897. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66144/0.68922. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65867/0.68682. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65660/0.68692. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65656/0.68709. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65415/0.68598. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65650/0.68667. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65149/0.68722. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65353/0.68648. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65061/0.68584. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64724/0.68705. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64529/0.68801. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64860/0.68678. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64282/0.68650. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64373/0.68623. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64701/0.68567. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64126/0.68788. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64110/0.68737. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63848/0.68697. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63488/0.68887. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63109/0.68907. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63240/0.69109. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63019/0.68901. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62715/0.68914. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62589/0.69197. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62629/0.69311. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62050/0.68915. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62515/0.69474. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62026/0.69673. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62196/0.69567. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61177/0.69220. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61425/0.69682. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61353/0.69868. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61284/0.69756. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61206/0.69419. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61179/0.69619. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60978/0.69958. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60462/0.70500. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60179/0.70854. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59772/0.70808. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59466/0.70573. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60118/0.71100. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59821/0.71222. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59450/0.70585. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59004/0.70756. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69307/0.69544. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69229/0.69553. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69187/0.69574. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69119/0.69593. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69139/0.69617. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69126/0.69630. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69055/0.69644. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69046/0.69669. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68998/0.69708. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68899/0.69767. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68864/0.69830. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68902/0.69887. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68689/0.69966. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68630/0.70060. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68631/0.70121. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68466/0.70175. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68375/0.70238. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68439/0.70344. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68226/0.70404. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68083/0.70518. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68149/0.70524. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67929/0.70632. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67888/0.70775. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67762/0.70891. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67726/0.71024. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67700/0.71023. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67515/0.71119. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67485/0.71085. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67446/0.71152. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67144/0.71187. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67264/0.71195. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66948/0.71291. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66900/0.71416. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67097/0.71289. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66667/0.71326. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66675/0.71224. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66532/0.71497. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66176/0.71514. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66309/0.71451. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66274/0.71480. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66159/0.71426. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66199/0.71618. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65833/0.71699. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65997/0.71495. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65939/0.71571. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65915/0.71268. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65710/0.71363. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65663/0.71295. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65360/0.71351. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65023/0.71435. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65388/0.71515. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64901/0.71393. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64900/0.71066. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64519/0.71218. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64111/0.71159. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64386/0.71347. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64262/0.71165. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64119/0.70949. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64252/0.70824. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63617/0.70824. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63815/0.70854. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63350/0.71113. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62872/0.70876. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.62626/0.70619. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63324/0.70907. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62739/0.70806. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62721/0.70687. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62437/0.70731. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.62215/0.70482. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.61739/0.70976. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61408/0.70722. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61376/0.70611. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61173/0.70876. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60847/0.70607. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61088/0.70996. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60804/0.71004. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60600/0.71051. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60309/0.70780. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.60046/0.70923. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60152/0.71060. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59504/0.71250. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59137/0.71602. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59432/0.71513. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59294/0.71809. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58196/0.71744. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58084/0.72402. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58666/0.72226. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58359/0.72100. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58063/0.72854. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57780/0.72569. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57071/0.73178. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57248/0.72796. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57432/0.73385. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56712/0.73785. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56480/0.72915. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55507/0.73761. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55269/0.74445. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56189/0.74067. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55335/0.74330. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55436/0.74155. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69381/0.69235. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69363/0.69281. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69335/0.69339. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69308/0.69342. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69284/0.69342. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69265/0.69328. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69205/0.69307. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69240/0.69303. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69156/0.69304. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69274. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69105/0.69234. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.69058/0.69210. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69141/0.69208. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69017/0.69198. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69000/0.69157. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68895/0.69089. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68883/0.69101. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68937/0.69048. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68784/0.69018. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68794/0.69001. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68782/0.68969. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68753/0.68970. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68565/0.68899. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68475/0.68925. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68388/0.68921. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68440/0.68913. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68293/0.68966. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68184/0.68955. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68165/0.68951. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68180/0.69051. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68038/0.69020. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67839/0.69107. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67909/0.69148. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67676/0.69171. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67707/0.69249. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67804/0.69372. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67497/0.69447. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67407/0.69520. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67188/0.69644. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67462/0.69877. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66984/0.69915. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66934/0.70080. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67052/0.70147. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66629/0.70338. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66718/0.70513. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66726/0.70723. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66524/0.70674. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.66297/0.70842. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66340/0.71101. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66331/0.70984. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66052/0.71244. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66235/0.71396. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65893/0.71670. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65476/0.71724. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65524/0.71892. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65293/0.72155. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65570/0.72395. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65154/0.72598. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65070/0.72703. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64864/0.72875. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64561/0.73053. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64877/0.73255. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64455/0.73402. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64519/0.73702. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64657/0.74066. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64330/0.74106. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64183/0.74241. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64001/0.74319. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64421/0.74476. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63566/0.74793. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63735/0.74872. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63566/0.74977. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63419/0.75269. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63137/0.75603. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63150/0.75718. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62839/0.76174. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62740/0.76190. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62485/0.76343. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62241/0.76488. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62329/0.76539. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62241/0.76474. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62524/0.76413. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61985/0.76884. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61572/0.77148. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61521/0.77006. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61287/0.77048. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61427/0.77270. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61167/0.77554. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61229/0.77615. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60934/0.77689. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60911/0.77848. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60563/0.77948. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60513/0.78188. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60234/0.78367. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60644/0.78115. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60201/0.78335. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59997/0.78413. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60061/0.78569. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59760/0.78713. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59603/0.78830. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69380/0.69326. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69264/0.69258. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69190. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69299/0.69129. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69196/0.69053. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69240/0.68997. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69171/0.68952. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69189/0.68869. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69034/0.68781. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69185/0.68727. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69054/0.68657. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68990/0.68581. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68957/0.68480. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68934/0.68450. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.68993/0.68362. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68878/0.68280. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68874/0.68217. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68807/0.68202. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68774/0.68167. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68703/0.68124. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68547/0.68034. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68640/0.68001. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68507/0.67979. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.68466/0.67994. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68290/0.67940. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68405/0.67985. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68358/0.67983. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68204/0.67985. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68152/0.68019. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67978/0.68099. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67953/0.68202. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67826/0.68235. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67858/0.68259. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67786/0.68256. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67538/0.68184. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67560/0.68219. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67652/0.68391. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67541/0.68350. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67206/0.68418. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67123/0.68583. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67223/0.68554. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66937/0.68669. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66829/0.68708. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66863/0.68779. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66775/0.68903. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66553/0.69011. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66538/0.68952. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66582/0.69087. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66260/0.69019. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66330/0.68944. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66156/0.69311. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65931/0.69456. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65648/0.69541. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65921/0.69468. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65513/0.69629. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65810/0.69927. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65808/0.69829. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65271/0.69835. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65482/0.69787. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64777/0.70027. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65069/0.70035. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65073/0.70170. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65004/0.70005. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64739/0.70274. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64544/0.70385. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64206/0.70497. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64571/0.70484. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64052/0.70747. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64090/0.70985. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63950/0.71302. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63837/0.71178. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.63796/0.71016. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63972/0.71611. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63401/0.71366. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63493/0.71670. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63292/0.71554. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63508/0.71911. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63422/0.71266. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62992/0.72176. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62532/0.72054. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62579/0.72439. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62450/0.72782. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62582/0.72772. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62449/0.72418. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62381/0.73278. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61680/0.73324. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.62064/0.73143. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61739/0.73146. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62064/0.72879. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61505/0.73145. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61480/0.73325. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61162/0.72884. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61258/0.73762. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60754/0.73593. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60628/0.73457. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60695/0.73242. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60216/0.73688. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60655/0.74233. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59912/0.74091. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60108/0.74251. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69549/0.70226. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69265/0.69662. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69257/0.69462. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69147/0.69406. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69152/0.69382. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69144/0.69487. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69086/0.69488. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69035/0.69551. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69512. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69036/0.69586. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69012/0.69695. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68861/0.69841. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68877/0.69899. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68827/0.69966. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68750/0.70048. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68801/0.70164. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68616/0.70220. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68466/0.70334. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68612/0.70493. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68426/0.70372. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68385/0.70591. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68387/0.70552. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68113/0.70805. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67953/0.70692. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68026/0.70635. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68007/0.70952. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67771/0.70780. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67551/0.70832. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67407/0.71045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67286/0.70918. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67610/0.71001. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67480/0.71287. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67216/0.71457. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66738/0.71616. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67028/0.71589. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66640/0.71612. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66557/0.71206. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66492/0.71406. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66270/0.71947. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66357/0.71813. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66388/0.71676. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65733/0.71780. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65915/0.71861. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65702/0.71928. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65345/0.72026. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65200/0.72332. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64642/0.72188. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64706/0.72294. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64621/0.72060. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64458/0.71937. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63923/0.71999. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63958/0.71731. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63727/0.72047. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63522/0.71675. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63897/0.71543. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63216/0.71620. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63116/0.71847. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62816/0.71471. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62833/0.71397. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62331/0.72112. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62181/0.72601. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61732/0.71901. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61798/0.71769. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61350/0.72069. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61249/0.72595. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.60797/0.71644. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60780/0.71896. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59896/0.71875. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60041/0.72798. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59388/0.72290. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59787/0.72953. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59463/0.72575. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59035/0.73119. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58534/0.73396. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.59161/0.73291. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57691/0.73026. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57652/0.73822. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57365/0.73653. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57067/0.73809. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56999/0.73574. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56499/0.75013. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56123/0.74877. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56020/0.75592. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56183/0.75556. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55261/0.76088. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55241/0.75402. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54990/0.76604. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55297/0.75747. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54275/0.76791. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54255/0.76470. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53296/0.76769. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53772/0.77759. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53498/0.77417. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53417/0.76873. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52921/0.78959. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52956/0.78043. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52973/0.78686. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51570/0.78021. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.51825/0.78862. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51613/0.79779. Took 0.09 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69430/0.68536. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69403/0.68580. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69294/0.68585. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69309/0.68633. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69210/0.68630. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69237/0.68567. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69210/0.68515. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69146/0.68461. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69161/0.68448. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69068/0.68418. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69077/0.68412. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69026/0.68381. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69030/0.68300. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69053/0.68287. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69017/0.68288. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68959/0.68326. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69023/0.68282. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68936/0.68284. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68898/0.68286. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68886/0.68145. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68885/0.68190. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68858/0.68253. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68842/0.68218. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68783/0.68216. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68807/0.68274. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68762/0.68265. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68652/0.68211. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68656/0.68253. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68687/0.68355. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68586/0.68319. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68494/0.68377. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68486/0.68406. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68456/0.68526. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68453/0.68367. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68310/0.68263. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68507/0.68559. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68314/0.68349. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68200/0.68482. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68219/0.68446. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68086/0.68464. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68169/0.68483. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68091/0.68670. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67909/0.68439. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67791/0.68468. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67950/0.68508. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67921/0.68719. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67612/0.68541. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67512/0.68760. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67312/0.68508. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67413/0.68625. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67427/0.68432. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67459/0.68558. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67317/0.68745. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67093/0.68657. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67066/0.68490. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66919/0.68852. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66701/0.68554. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66632/0.68795. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66418/0.68794. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66373/0.68859. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66147/0.68923. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66144/0.69109. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66166/0.68926. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65876/0.69111. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65851/0.69309. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65420/0.69214. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65431/0.69573. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65400/0.69578. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65353/0.69439. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65054/0.69573. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64874/0.69713. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64645/0.69976. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64558/0.70235. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64347/0.70085. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64584/0.70395. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64166/0.70391. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64051/0.71009. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64106/0.71153. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64006/0.71049. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63563/0.71282. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63487/0.71389. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62938/0.71798. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63255/0.71827. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63256/0.72129. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62659/0.72321. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62612/0.72318. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62106/0.72917. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62277/0.72986. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61988/0.73090. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61969/0.73274. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61596/0.73362. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61381/0.73543. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60987/0.73595. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60510/0.74072. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60610/0.74261. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60868/0.74518. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59740/0.75029. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60459/0.74949. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59883/0.75413. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59720/0.75678. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69374/0.68851. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.68696. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.68722. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.68779. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.68791. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69116/0.68822. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.68857. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.68890. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69020/0.68879. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69034/0.68935. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68984/0.68947. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.68972. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68946/0.68986. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68887/0.69011. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68911/0.69051. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68818/0.69109. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68739/0.69132. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68816/0.69133. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68668/0.69182. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68654/0.69202. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68758/0.69221. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68570/0.69240. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68585/0.69249. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68474/0.69323. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68349/0.69354. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68288/0.69289. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68203/0.69354. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68206/0.69347. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67962/0.69363. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67998/0.69390. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67789/0.69440. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67694/0.69477. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67526/0.69567. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67333/0.69658. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67306/0.69794. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67221/0.69821. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66960/0.69872. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66952/0.69914. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66888/0.70074. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66415/0.70158. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66222/0.70300. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66302/0.70463. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66282/0.70509. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66094/0.70749. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65798/0.70846. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65849/0.70771. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65473/0.70972. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65384/0.71051. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65236/0.71332. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65357/0.71443. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64987/0.71535. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64639/0.71947. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64478/0.72089. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64643/0.72349. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64599/0.72483. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64306/0.72579. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64158/0.72535. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63965/0.72581. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63625/0.72941. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63686/0.73261. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63373/0.73251. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63549/0.73490. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63156/0.73775. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63030/0.73869. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62627/0.74190. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62365/0.74342. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62181/0.74672. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62233/0.74646. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62044/0.75001. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61803/0.74878. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62024/0.75012. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61213/0.75415. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61405/0.75042. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61253/0.75389. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61257/0.75526. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60595/0.75589. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60569/0.75895. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60362/0.76118. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60110/0.76014. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59981/0.76515. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.59422/0.76822. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59679/0.76482. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59282/0.76498. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59502/0.75994. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58929/0.76782. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58717/0.76712. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58728/0.76975. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58141/0.76956. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58286/0.76579. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58002/0.76821. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58322/0.76853. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57226/0.77040. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57156/0.77450. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56730/0.77559. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56825/0.78128. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56800/0.76607. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56262/0.76893. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55613/0.77759. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55577/0.77542. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55206/0.78302. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69371/0.69597. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69203/0.69682. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69176/0.69703. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.69718. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69114/0.69726. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69067/0.69728. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69110/0.69741. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.69721. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69035/0.69696. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68920/0.69707. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.69698. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68855/0.69694. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68749/0.69704. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68813/0.69669. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68779/0.69672. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68736/0.69617. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68657/0.69581. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68471/0.69547. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68470/0.69567. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68508/0.69538. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68242/0.69519. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.69416. Took 0.14 sec\n",
      "Epoch 22, Loss(train/val) 0.68241/0.69348. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68128/0.69256. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67918/0.69151. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67875/0.69058. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67532/0.68958. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67382/0.68849. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67250/0.68779. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67053/0.68617. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66750/0.68573. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66719/0.68439. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66492/0.68467. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66648/0.68337. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66243/0.68214. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66241/0.67974. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65806/0.67909. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65754/0.67727. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65560/0.67750. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65338/0.67856. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65005/0.67771. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65047/0.67553. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64723/0.67619. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64453/0.67629. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64155/0.67437. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.64261/0.67512. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64096/0.67545. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63692/0.67607. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63901/0.67768. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63482/0.67717. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63366/0.67623. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63139/0.67813. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62717/0.67883. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63179/0.67745. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62459/0.67675. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62308/0.67747. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62015/0.67807. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.61995/0.68017. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61786/0.68096. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61574/0.68275. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61229/0.68162. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61237/0.68242. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60566/0.68332. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60991/0.68090. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.60784/0.68397. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60299/0.68270. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60203/0.68672. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60148/0.68657. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59968/0.68659. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59597/0.68696. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59599/0.68965. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59017/0.68801. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58597/0.68905. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58257/0.69013. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58083/0.69200. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.58307/0.68854. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57623/0.69284. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58335/0.69353. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57828/0.68859. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56971/0.68820. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56821/0.69443. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56682/0.69513. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56048/0.69573. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56483/0.69162. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56217/0.69571. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.55183/0.69957. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55320/0.70229. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55189/0.70316. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54682/0.70823. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53776/0.70806. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53843/0.69971. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53989/0.70982. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53564/0.70395. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53580/0.70730. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52842/0.70997. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52632/0.71266. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52287/0.71422. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51961/0.72596. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52237/0.72390. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.52145/0.72702. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69024/0.69759. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69023/0.69775. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69046/0.69788. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68990/0.69803. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69044/0.69807. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69062/0.69821. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68967/0.69840. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68990/0.69858. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68993/0.69876. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69032/0.69895. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68984/0.69928. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68925/0.69964. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68932/0.70001. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68899/0.70036. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68902/0.70065. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68830/0.70105. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68878/0.70143. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68811/0.70192. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68778/0.70237. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68750/0.70283. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68734/0.70340. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68632/0.70398. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68657/0.70475. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68677/0.70523. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68588/0.70583. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68476/0.70658. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68477/0.70714. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68311/0.70804. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68377/0.70915. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68282/0.70989. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68036/0.71056. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68205/0.71086. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68038/0.71188. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67953/0.71227. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67930/0.71281. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67988/0.71305. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67797/0.71328. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67729/0.71357. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67627/0.71408. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67556/0.71450. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67553/0.71451. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67276/0.71525. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67370/0.71565. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67164/0.71559. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66924/0.71587. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66941/0.71607. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66859/0.71610. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66813/0.71546. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66586/0.71579. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66589/0.71622. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66833/0.71659. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66262/0.71611. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66386/0.71684. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65983/0.71602. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65872/0.71584. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65533/0.71515. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65425/0.71578. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65227/0.71560. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65198/0.71493. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65217/0.71521. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65013/0.71514. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64637/0.71559. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64520/0.71694. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64562/0.71622. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63855/0.71772. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64145/0.71884. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64331/0.71756. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63735/0.71790. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63312/0.71653. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63798/0.71741. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62941/0.71924. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62905/0.72131. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62941/0.72027. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62627/0.71972. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62774/0.72163. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62372/0.71957. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62200/0.72218. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61963/0.72093. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61209/0.72291. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61339/0.72435. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61323/0.72535. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60972/0.72680. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61021/0.72912. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60736/0.72914. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60491/0.72956. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60107/0.72954. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59975/0.73145. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59974/0.73199. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60159/0.73413. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59670/0.73595. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.58962/0.73534. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58273/0.73595. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58634/0.73933. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58423/0.74206. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58263/0.73595. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58164/0.74011. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57994/0.74416. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57826/0.74413. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56875/0.74690. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57082/0.74743. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.68997/0.69311. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68793/0.69427. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68716/0.69508. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68727/0.69545. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68627/0.69578. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68649/0.69599. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68590/0.69616. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68594/0.69629. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68506/0.69633. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68438/0.69654. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68379/0.69685. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68318/0.69668. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68250/0.69686. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68216/0.69675. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68125/0.69688. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68093/0.69675. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67936/0.69665. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67955/0.69682. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67825/0.69734. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67796/0.69716. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67635/0.69646. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67538/0.69628. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67447/0.69698. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67392/0.69679. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67142/0.69760. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67021/0.69825. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67011/0.69868. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66909/0.69905. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66865/0.69973. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66699/0.70149. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66311/0.70154. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66174/0.70349. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66364/0.70385. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66106/0.70365. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66004/0.70500. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65689/0.70518. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65756/0.70480. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65414/0.70562. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65399/0.70484. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65023/0.70528. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.64940/0.70781. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64593/0.70813. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64428/0.70837. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64304/0.71012. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64182/0.71048. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64169/0.70854. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64220/0.70988. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63856/0.70734. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63287/0.70984. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63178/0.71295. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63156/0.71197. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62875/0.71367. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62700/0.71049. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62582/0.71453. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62153/0.71263. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62066/0.71446. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62026/0.71506. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61230/0.71740. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61223/0.72082. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61511/0.71917. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61038/0.72354. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60780/0.72245. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60287/0.72239. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59882/0.72616. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60003/0.72743. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60003/0.72782. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59542/0.72841. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59187/0.73092. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58673/0.73605. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58831/0.73354. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58462/0.73832. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58381/0.73994. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57843/0.73608. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57678/0.73645. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57403/0.74086. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56701/0.74319. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.57063/0.74712. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56726/0.74901. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56703/0.74717. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55943/0.74854. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55990/0.75285. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55850/0.75918. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.55448/0.75442. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55347/0.75778. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55005/0.76534. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54667/0.76445. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53923/0.77112. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53856/0.76754. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53683/0.77696. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52850/0.77925. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53029/0.78820. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.52187/0.78476. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53569/0.77740. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52285/0.78067. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52414/0.78297. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51794/0.78630. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51266/0.79435. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51020/0.80198. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50391/0.80265. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50830/0.80268. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69502/0.69999. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68934/0.70102. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68846/0.70067. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68742/0.70116. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68714/0.70174. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68638/0.70246. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68595/0.70306. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68559/0.70419. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68599/0.70477. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68493/0.70562. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68455/0.70610. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68479/0.70660. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68333/0.70732. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68365/0.70815. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68276/0.70907. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68289/0.70944. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68239/0.70986. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68236/0.71079. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68297/0.71116. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68166/0.71144. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68105/0.71173. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68052/0.71229. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67980/0.71301. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67965/0.71331. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67972/0.71368. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67889/0.71465. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67828/0.71531. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67661/0.71571. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67792/0.71642. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67606/0.71706. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67536/0.71845. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67633/0.71870. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67604/0.71841. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67480/0.71899. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67380/0.71940. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67319/0.72010. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67301/0.72012. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67353/0.72053. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67161/0.72092. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67159/0.72112. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67097/0.72172. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67045/0.72221. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66895/0.72240. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66788/0.72388. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66797/0.72430. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66791/0.72457. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66703/0.72477. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66612/0.72435. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66521/0.72570. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66452/0.72549. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66466/0.72600. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66264/0.72665. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66293/0.72706. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66334/0.72765. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66335/0.72750. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66109/0.72744. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65749/0.72871. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65848/0.72984. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65827/0.72920. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65733/0.73164. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65642/0.73184. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65513/0.73253. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65616/0.73311. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65343/0.73376. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65313/0.73415. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65378/0.73287. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65103/0.73739. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65119/0.73928. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65021/0.73865. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64936/0.73719. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64740/0.74098. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64907/0.74087. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64732/0.74128. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64547/0.74173. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64613/0.74018. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64274/0.74639. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64295/0.74381. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64091/0.74570. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63957/0.74638. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64001/0.75221. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64052/0.74970. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63729/0.75215. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63469/0.75376. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63739/0.75706. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63746/0.75546. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63537/0.75455. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63380/0.75660. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63011/0.75776. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62855/0.76446. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63251/0.76269. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62744/0.75861. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62411/0.76288. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62844/0.76627. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62114/0.76719. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62227/0.77195. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62434/0.77166. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61847/0.77154. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61916/0.77625. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61580/0.76912. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61506/0.76963. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69087/0.69995. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69013/0.70206. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68898/0.70359. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68946/0.70428. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68866/0.70494. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68876/0.70548. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68863/0.70588. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68751/0.70592. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68753/0.70667. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68740/0.70649. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68611/0.70752. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68629/0.70759. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68510/0.70834. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68536/0.70923. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68493/0.70984. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68363/0.71035. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68320/0.71136. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68224/0.71182. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68144/0.71278. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68231/0.71253. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68056/0.71227. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68017/0.71329. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67887/0.71418. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67664/0.71454. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67622/0.71518. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67660/0.71425. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67378/0.71574. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67461/0.71792. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67342/0.71590. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67199/0.71744. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67111/0.71800. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67077/0.72075. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66985/0.71730. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66551/0.72147. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66381/0.71815. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66385/0.71915. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66462/0.72297. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66340/0.72036. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66022/0.72071. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66013/0.72523. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65738/0.72128. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65764/0.72510. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.65623/0.72649. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65394/0.72751. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65199/0.72295. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65061/0.72180. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64851/0.72539. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64966/0.72928. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64711/0.72947. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64666/0.72787. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64407/0.72759. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64134/0.73156. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64235/0.73321. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63897/0.73105. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63448/0.73272. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63373/0.73349. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63088/0.73585. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63190/0.73789. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62934/0.73556. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63074/0.73860. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63006/0.74379. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62657/0.73457. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62407/0.74407. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62011/0.74291. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61918/0.74766. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61868/0.74705. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61436/0.74345. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60970/0.74506. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61128/0.74444. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60979/0.75496. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60904/0.75199. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60627/0.75151. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60151/0.75090. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60512/0.76112. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59752/0.76017. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60072/0.75882. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60221/0.75746. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60027/0.75977. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.59893/0.76691. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58840/0.76808. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58994/0.77526. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58638/0.77487. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58824/0.77119. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58885/0.77502. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58388/0.77328. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57567/0.78379. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58341/0.77860. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57242/0.79654. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56627/0.78944. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57344/0.79161. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56464/0.80456. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56583/0.79644. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56382/0.81029. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.56211/0.79586. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56178/0.81443. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56181/0.81009. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55576/0.80902. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55413/0.80824. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55634/0.81801. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54832/0.82411. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69628/0.69066. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69186/0.68853. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69015/0.68878. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68975/0.68908. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68893/0.68965. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68790/0.69016. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68818/0.69092. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68723/0.69144. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68698/0.69210. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68599/0.69275. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68584/0.69338. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68590/0.69401. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68432/0.69466. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68482/0.69541. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68378/0.69590. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68346/0.69659. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68336/0.69708. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68303/0.69752. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68321/0.69795. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68266/0.69826. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68275/0.69854. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68123/0.69869. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68047/0.69934. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68124/0.69956. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67986/0.69958. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67986/0.69981. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67929/0.69984. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67822/0.69990. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67764/0.70003. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67672/0.70065. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67731/0.70068. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67614/0.70021. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67556/0.69998. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67424/0.70004. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67281/0.70027. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67234/0.70057. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67232/0.69977. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67124/0.69997. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66984/0.70119. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66811/0.70169. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66922/0.70148. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66807/0.70128. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66623/0.70097. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66446/0.70093. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66268/0.70198. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66375/0.70220. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65970/0.70267. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66203/0.70224. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66072/0.70230. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65847/0.70396. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65717/0.70405. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65428/0.70482. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65523/0.70479. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65382/0.70469. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65176/0.70620. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65214/0.70884. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64766/0.71015. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64741/0.71144. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64708/0.71245. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64328/0.71241. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64261/0.71428. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64255/0.71472. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64184/0.71594. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63940/0.71677. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64026/0.71946. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63762/0.72062. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63330/0.72408. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63284/0.72498. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63349/0.72457. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63013/0.72812. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62864/0.73075. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62778/0.72941. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62440/0.73069. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62500/0.73405. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62449/0.73639. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62508/0.73655. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62051/0.73531. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62055/0.73808. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61791/0.73952. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61766/0.74387. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61596/0.74378. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61561/0.74508. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61529/0.74485. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61385/0.74720. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61203/0.74732. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60994/0.74691. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60600/0.75038. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60399/0.75041. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60332/0.75387. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60129/0.75763. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60122/0.75737. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59783/0.75781. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60129/0.76080. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59991/0.75945. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59276/0.76589. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59436/0.76874. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59352/0.76520. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59120/0.76465. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59361/0.76771. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58864/0.77177. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69005/0.70019. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68818/0.69867. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68722/0.69874. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68778/0.69838. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68654/0.69828. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68590/0.69805. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68540/0.69817. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68410/0.69798. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68271/0.69831. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68321/0.69825. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68247/0.69961. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68143/0.69921. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68132/0.70007. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68039/0.69946. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68054/0.69985. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67828/0.69978. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67970/0.70084. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67994/0.70070. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67898/0.70071. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67815/0.70080. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67797/0.70095. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67778/0.70078. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67793/0.70120. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67597/0.70083. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67641/0.70036. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67457/0.70049. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67561/0.70131. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67522/0.70134. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67497/0.70110. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67451/0.70132. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67280/0.70066. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67305/0.70092. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67397/0.70197. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67059/0.70170. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67127/0.70177. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67099/0.70188. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67031/0.70288. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66970/0.70143. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66959/0.70252. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66914/0.70292. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66740/0.70138. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66675/0.70302. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66602/0.70351. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66552/0.70380. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66623/0.70431. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66568/0.70436. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66452/0.70434. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66248/0.70587. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66267/0.70492. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66118/0.70644. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65878/0.70524. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65993/0.70743. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65780/0.70777. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65828/0.70881. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65780/0.70817. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65533/0.70914. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65559/0.70894. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65442/0.71047. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65268/0.71010. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65118/0.71146. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64964/0.71402. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64861/0.71403. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65170/0.71442. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64798/0.71515. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64472/0.71719. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64396/0.71726. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64504/0.71766. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64245/0.72022. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64178/0.71995. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64075/0.72045. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63563/0.72136. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63468/0.72308. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63694/0.72498. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63473/0.72496. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63049/0.72629. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62838/0.72800. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63082/0.72824. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63001/0.72974. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62859/0.72942. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62650/0.73123. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61956/0.73213. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62191/0.73490. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62056/0.73572. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61830/0.73683. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61853/0.73849. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61544/0.74186. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61068/0.74449. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61595/0.74418. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.60696/0.74848. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60849/0.75002. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60697/0.75081. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60728/0.75209. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60834/0.75184. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60208/0.75426. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59962/0.75681. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59796/0.76113. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59250/0.76322. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59781/0.76273. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59421/0.76359. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59430/0.76553. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69111/0.68418. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68933/0.68364. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68918/0.68335. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68838/0.68309. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68757/0.68287. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68700/0.68257. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68645/0.68231. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68616/0.68193. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68551/0.68186. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68419/0.68194. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68471/0.68178. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68349/0.68199. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68264/0.68220. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68242/0.68238. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68188/0.68299. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68136/0.68295. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68006/0.68372. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67970/0.68401. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67983/0.68428. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67993/0.68464. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67861/0.68532. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67829/0.68560. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67952/0.68557. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67869/0.68618. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67783/0.68660. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67792/0.68640. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67713/0.68675. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67752/0.68736. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67622/0.68786. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67492/0.68793. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67625/0.68768. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67480/0.68802. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67484/0.68852. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67567/0.68800. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67323/0.68909. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67401/0.68947. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67213/0.68965. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67238/0.69042. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67117/0.69097. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67044/0.69188. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67043/0.69116. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67114/0.69192. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66999/0.69202. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66910/0.69286. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67036/0.69318. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66737/0.69405. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66642/0.69359. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66652/0.69374. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66680/0.69454. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66738/0.69306. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66447/0.69462. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66363/0.69362. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66099/0.69560. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66296/0.69617. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66106/0.69343. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66100/0.69768. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66226/0.69820. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65578/0.69767. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66013/0.69769. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65627/0.70059. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65690/0.69665. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65304/0.69909. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65521/0.70000. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65381/0.70047. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65442/0.69790. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65196/0.69940. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64977/0.70219. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64990/0.70288. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64534/0.70315. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64744/0.70377. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64809/0.70360. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64573/0.70451. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64627/0.70604. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64427/0.70801. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64253/0.70926. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64359/0.70480. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63783/0.70675. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63882/0.70831. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63780/0.70965. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63648/0.71059. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63439/0.71112. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63249/0.71244. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63436/0.71886. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63366/0.71484. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63275/0.71594. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62856/0.71877. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63193/0.71706. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62833/0.71880. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62323/0.71614. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62815/0.71578. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62690/0.72030. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62329/0.72201. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62252/0.72634. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62079/0.72671. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61882/0.72434. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62106/0.72579. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62280/0.73237. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61608/0.72510. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61048/0.73016. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61568/0.72973. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69463/0.69334. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.68688. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69095/0.68259. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68978/0.68005. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68984/0.67890. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68913/0.67837. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68813/0.67774. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68821/0.67754. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68718/0.67790. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68641/0.67796. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68504/0.67787. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68482/0.67770. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68379/0.67770. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68367/0.67687. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68231/0.67715. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68070/0.67676. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67895/0.67652. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67846/0.67564. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67780/0.67504. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67687/0.67395. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67400/0.67418. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67198/0.67466. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66951/0.67103. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67004/0.67262. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66603/0.67207. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66491/0.67038. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66374/0.67278. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66095/0.67189. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65787/0.67357. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65556/0.67390. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65384/0.67558. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64866/0.67606. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64764/0.67553. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64723/0.67540. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64328/0.67850. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63903/0.67908. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64083/0.68222. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63936/0.68204. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63363/0.68568. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63152/0.68785. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63057/0.68962. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62614/0.68845. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62197/0.68762. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62070/0.68783. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62126/0.69190. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61482/0.68936. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61262/0.69492. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61183/0.69175. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.60658/0.69912. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.60662/0.70189. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.60092/0.69831. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.60038/0.69952. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59917/0.70147. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59226/0.70362. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59319/0.70658. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59109/0.70864. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59161/0.71457. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58352/0.70894. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58172/0.71491. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58275/0.71290. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.57267/0.72214. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57635/0.72392. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.57411/0.73444. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57394/0.73163. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57375/0.73561. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56896/0.74024. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.56633/0.73540. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55935/0.73262. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56169/0.73513. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.55260/0.73914. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55556/0.74874. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.55156/0.74690. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55288/0.74660. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.54459/0.74775. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.54552/0.74905. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.54500/0.76551. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.53274/0.76112. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53605/0.75865. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.52761/0.76233. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53398/0.76748. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.52823/0.76923. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.52635/0.77459. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52477/0.77915. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.52268/0.78208. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51714/0.77827. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51308/0.78410. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51004/0.78672. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.50890/0.79275. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50739/0.79321. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50023/0.79153. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50372/0.79414. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.49496/0.79184. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.49012/0.79694. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.49235/0.80301. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48947/0.80702. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48994/0.80358. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49144/0.81344. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48755/0.79988. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.46917/0.81600. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47715/0.83571. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69277/0.70262. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68983/0.70922. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69010/0.71148. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68900/0.71220. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68862/0.71140. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68887/0.71150. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68828/0.71176. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68803/0.71255. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68712/0.71291. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68736/0.71330. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68684/0.71404. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68668/0.71509. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68551/0.71460. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68497/0.71547. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68525/0.71678. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68411/0.71745. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68518/0.71719. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68393/0.71759. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68302/0.71983. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68323/0.72037. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68121/0.72004. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68224/0.72197. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68075/0.72187. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68263/0.72291. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68107/0.72500. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68077/0.72393. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67926/0.72401. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67938/0.72686. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67922/0.72616. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67870/0.72631. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67716/0.72988. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67720/0.72938. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67534/0.73028. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67385/0.73193. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67337/0.73168. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67243/0.73222. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67173/0.73445. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67127/0.73521. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67125/0.73709. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66762/0.73824. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66867/0.74006. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66521/0.73963. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66699/0.74033. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66389/0.74350. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66518/0.74578. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66472/0.74691. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66342/0.75223. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66040/0.75038. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66212/0.75155. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65872/0.75632. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65772/0.75575. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65556/0.75839. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65588/0.75818. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65740/0.75661. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65387/0.76039. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65081/0.76210. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65001/0.76391. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64815/0.76656. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64567/0.77154. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64190/0.77192. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64522/0.76980. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64238/0.77150. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63900/0.77869. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63957/0.78333. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63891/0.77710. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63178/0.77878. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63436/0.78487. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62970/0.78105. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62444/0.78835. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62349/0.79331. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62225/0.79586. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61984/0.79299. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62146/0.79495. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61338/0.79855. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61465/0.80180. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61198/0.80768. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61322/0.80600. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60371/0.80217. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60334/0.80400. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60422/0.80958. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60210/0.80460. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60259/0.80639. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59729/0.81312. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59328/0.80569. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59537/0.80257. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58219/0.81366. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58097/0.80976. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57817/0.81953. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58360/0.81835. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57572/0.81614. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57482/0.81131. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56902/0.82646. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56766/0.81943. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56841/0.81878. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55892/0.82308. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56062/0.82893. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55689/0.83062. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55431/0.83479. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54886/0.83449. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54333/0.83424. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69310/0.69090. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69096. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.69108. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69182/0.69106. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.69102. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69125/0.69102. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.69089. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69084. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69021/0.69068. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68991/0.69047. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69000/0.69044. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68906/0.69038. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68918/0.69044. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68819/0.69037. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68780/0.69055. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68788/0.69094. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68680/0.69151. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68633/0.69189. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68622/0.69210. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68575/0.69258. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68496/0.69337. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68489/0.69353. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68479/0.69405. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68361/0.69444. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68343/0.69501. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68239/0.69545. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68246/0.69555. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68208/0.69573. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68220/0.69627. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68075/0.69658. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67997/0.69688. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67957/0.69688. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67909/0.69788. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67800/0.69873. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67825/0.69933. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67582/0.69998. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67491/0.70101. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67325/0.70151. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67255/0.70276. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66919/0.70473. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67061/0.70598. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66833/0.70636. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66703/0.70725. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66618/0.70672. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66613/0.70770. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66291/0.70974. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66318/0.71141. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65914/0.71423. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65744/0.71631. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65506/0.71851. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65382/0.72041. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65239/0.72158. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65156/0.72528. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65018/0.72596. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64785/0.72632. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64481/0.72939. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64220/0.72937. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64211/0.72939. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63966/0.73139. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63789/0.73279. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63430/0.73620. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63648/0.73990. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62765/0.74295. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63127/0.73949. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62500/0.74688. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62698/0.74859. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62042/0.75186. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.61656/0.75103. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61876/0.74883. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61182/0.75186. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61125/0.75420. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60982/0.75703. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60724/0.75908. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60714/0.76031. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.60005/0.76278. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60458/0.76267. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59415/0.76708. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59381/0.76894. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59321/0.76730. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58874/0.76736. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58283/0.77908. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58811/0.77530. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58130/0.77189. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57637/0.77435. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57924/0.77969. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57570/0.78110. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57526/0.78006. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57545/0.78315. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56573/0.78127. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56667/0.79578. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56136/0.79974. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55682/0.79317. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55624/0.79528. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55505/0.79969. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55344/0.78794. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54716/0.80311. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54965/0.80019. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54727/0.80859. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54484/0.81228. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54623/0.82037. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69534/0.68679. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.68817. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.68924. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69195/0.68971. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69133/0.69008. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69069/0.69080. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.69140. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68982/0.69206. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69024/0.69268. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68947/0.69302. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68870/0.69310. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68895/0.69352. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68834/0.69427. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.69485. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68731/0.69474. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68825/0.69478. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68791/0.69479. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68736/0.69498. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68713/0.69492. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68761/0.69523. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69512. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68617/0.69606. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68662/0.69619. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68591/0.69650. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68588/0.69702. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68461/0.69725. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68434/0.69865. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68410/0.69861. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68479/0.69879. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68328/0.69920. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68369/0.70022. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68279/0.70100. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68095/0.70078. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68127/0.70193. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68095/0.70263. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68079/0.70234. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68096/0.70242. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67958/0.70301. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67995/0.70404. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67853/0.70542. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67730/0.70641. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67735/0.70670. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67772/0.70761. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67595/0.70866. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67319/0.70927. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67165/0.71215. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67454/0.71178. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67219/0.71133. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66999/0.71359. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67067/0.71377. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67021/0.71472. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66943/0.71596. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66841/0.71593. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66652/0.71754. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66513/0.71844. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66287/0.72014. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65895/0.72214. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65964/0.72179. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65994/0.72495. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65903/0.72483. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65634/0.72846. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65310/0.72912. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65442/0.73304. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65069/0.73421. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64733/0.73388. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65030/0.73546. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64382/0.73630. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64207/0.73954. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63727/0.74452. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63868/0.74553. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63526/0.75094. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63432/0.75083. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62907/0.75202. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62406/0.75950. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61803/0.75630. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61967/0.75845. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62060/0.76119. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61503/0.76771. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61025/0.76544. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60996/0.77442. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60482/0.76790. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60182/0.77634. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60315/0.77875. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60114/0.77982. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59659/0.78056. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58874/0.78132. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59037/0.79261. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58586/0.79036. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58291/0.79910. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57543/0.79511. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57872/0.80043. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57161/0.80008. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57257/0.80764. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56283/0.81610. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56838/0.81298. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55957/0.81353. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55699/0.82281. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56335/0.82493. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55233/0.82601. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55665/0.83517. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69658/0.69038. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69376/0.69017. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69307/0.69000. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69252/0.68952. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69265/0.68930. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69115/0.68899. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.68874. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69047/0.68826. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68996/0.68796. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.68769. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68868/0.68731. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68801/0.68729. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68706/0.68723. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68669/0.68674. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68666/0.68663. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68663/0.68663. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68574/0.68641. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68551/0.68643. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68537/0.68617. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68253/0.68627. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68301/0.68613. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68122/0.68619. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68330/0.68609. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68196/0.68590. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68152/0.68565. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68039/0.68549. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68056/0.68542. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68015/0.68538. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68020/0.68536. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67675/0.68606. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67828/0.68578. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67423/0.68624. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67548/0.68561. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67579/0.68557. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67400/0.68614. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67428/0.68583. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67357/0.68581. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67305/0.68599. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67100/0.68623. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67055/0.68617. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66760/0.68620. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66841/0.68584. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66699/0.68595. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66462/0.68665. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66647/0.68722. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66404/0.68749. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66401/0.68788. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66233/0.68831. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66223/0.68831. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65924/0.68920. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66119/0.68932. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65752/0.68986. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65893/0.69085. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65609/0.69185. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65674/0.69212. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65415/0.69295. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65296/0.69351. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64939/0.69399. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65097/0.69451. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65082/0.69525. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64960/0.69612. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64526/0.69708. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64584/0.69837. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64458/0.69802. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64475/0.69893. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64228/0.70049. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64160/0.70107. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63876/0.70064. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63830/0.70056. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63412/0.70200. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63683/0.70366. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63471/0.70524. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63001/0.70654. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62910/0.70782. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62915/0.70829. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62914/0.70901. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62536/0.71177. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62835/0.71307. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62406/0.71315. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61883/0.71563. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61960/0.71704. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61710/0.71739. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61385/0.72091. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61582/0.72104. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61743/0.72132. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61265/0.72324. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60900/0.72538. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61363/0.72731. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.60269/0.72903. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60840/0.73237. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60372/0.73428. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.59717/0.73727. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60158/0.73767. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59356/0.74235. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59319/0.74458. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59621/0.74453. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58874/0.74786. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58531/0.74854. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58604/0.75448. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58110/0.75551. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69288/0.69426. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69282. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69167/0.69252. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69116/0.69294. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69103/0.69352. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68987/0.69404. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68943/0.69479. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68851/0.69556. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68755/0.69725. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68736/0.69785. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68689/0.69878. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68494/0.69970. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68444/0.70080. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68373/0.70222. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68338/0.70298. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68337/0.70337. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68185/0.70298. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68130/0.70316. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68094/0.70420. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68022/0.70407. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67894/0.70472. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67852/0.70564. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67717/0.70634. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67673/0.70698. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67680/0.70764. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67492/0.70788. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67594/0.70847. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67362/0.70880. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67403/0.70932. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67260/0.71008. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67402/0.71005. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67087/0.71101. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67151/0.71082. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67055/0.71062. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66853/0.71008. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66671/0.71161. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66757/0.71140. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66572/0.71199. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66481/0.71167. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66463/0.71062. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66164/0.71201. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66302/0.71178. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66178/0.71200. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65970/0.71253. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65808/0.71208. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65797/0.71204. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65820/0.71399. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65785/0.71531. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65354/0.71527. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65357/0.71534. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65379/0.71687. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65450/0.71540. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65169/0.71629. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64856/0.71664. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65121/0.71574. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65071/0.71751. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64767/0.71862. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64838/0.72046. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64635/0.71961. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64645/0.71945. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64431/0.72088. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64349/0.72287. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64043/0.72344. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63943/0.72599. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63948/0.72625. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63718/0.72853. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63588/0.72751. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63898/0.72914. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63819/0.73056. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63950/0.73076. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63553/0.73324. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63430/0.73332. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63525/0.73403. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62886/0.73820. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63002/0.73721. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62583/0.73881. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62671/0.73762. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62717/0.73699. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62162/0.74187. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62037/0.74354. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62088/0.74629. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61931/0.74665. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61838/0.74941. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61477/0.75137. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61677/0.75367. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61429/0.75389. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60652/0.75798. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61199/0.75820. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60670/0.76271. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60325/0.76396. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60113/0.76394. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60617/0.76964. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60876/0.76756. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60056/0.76878. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59908/0.77250. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59503/0.77283. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.59317/0.77205. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59279/0.77701. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58995/0.77843. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58954/0.78344. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69291/0.69341. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69191/0.69316. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69155/0.69285. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69068/0.69277. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69041/0.69277. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69268. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68856/0.69245. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68883/0.69238. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.69228. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68739/0.69237. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68713/0.69231. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68630/0.69247. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68606/0.69226. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68588/0.69228. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68420/0.69235. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68405/0.69272. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68403/0.69328. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68299/0.69342. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68183/0.69365. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68142/0.69370. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68168/0.69368. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68120/0.69416. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68190/0.69379. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67977/0.69337. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68061/0.69335. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67786/0.69301. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67659/0.69267. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67788/0.69240. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67801/0.69159. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67485/0.69196. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67467/0.69130. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67464/0.69072. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67325/0.68977. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67232/0.68988. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67168/0.68919. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67036/0.68848. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67168/0.68776. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67098/0.68629. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66818/0.68594. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66653/0.68551. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66764/0.68581. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66644/0.68442. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66473/0.68397. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66144/0.68285. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66352/0.68316. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66138/0.68277. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65778/0.68227. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65996/0.68239. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65908/0.68226. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65667/0.68122. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65395/0.68072. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65676/0.67856. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65341/0.67912. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64905/0.67904. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65283/0.67931. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65272/0.67971. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64795/0.67906. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64917/0.67702. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64586/0.67685. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64562/0.67674. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64304/0.67841. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64540/0.67827. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64043/0.67618. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64178/0.67665. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64056/0.67657. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63408/0.67484. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63515/0.67437. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63418/0.67495. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63430/0.67642. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63181/0.67735. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63075/0.67647. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62909/0.67671. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62858/0.67737. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62405/0.67636. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62169/0.67592. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62172/0.67657. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62380/0.67615. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61775/0.67632. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61860/0.67708. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61655/0.67883. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61527/0.67809. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60890/0.67918. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60867/0.68159. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.60977/0.68136. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61254/0.68210. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60444/0.68250. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60716/0.68431. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59997/0.68567. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59785/0.68503. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60001/0.68837. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59510/0.68911. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59273/0.69170. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58978/0.69188. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58614/0.69117. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58643/0.69270. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57930/0.69367. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58189/0.69761. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58009/0.69999. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58136/0.69999. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57726/0.70158. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69636/0.69565. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69333/0.69445. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69276/0.69334. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69243. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69024/0.69173. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69050/0.69122. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68901/0.69069. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68889/0.69023. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68824/0.68968. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68681/0.68926. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68711/0.68881. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68659/0.68819. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68571/0.68776. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68385/0.68742. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68283/0.68731. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68384/0.68712. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68140/0.68690. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68075/0.68732. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67982/0.68761. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67681/0.68836. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67724/0.68861. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67692/0.68885. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67507/0.68922. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67232/0.69049. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67185/0.69120. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67063/0.69224. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66935/0.69364. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66765/0.69433. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.66699/0.69578. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66626/0.69638. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66510/0.69724. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66282/0.69860. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65944/0.70088. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65928/0.70252. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65426/0.70368. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65589/0.70610. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.65287/0.70704. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65276/0.70895. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65218/0.71050. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64851/0.71297. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64631/0.71601. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64216/0.71784. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64206/0.71999. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63706/0.72302. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63937/0.72608. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.63476/0.72860. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63088/0.73107. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62741/0.73479. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.62995/0.73676. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62777/0.74011. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61892/0.74535. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62015/0.74654. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61974/0.74999. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61473/0.75352. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.60726/0.76038. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60776/0.76447. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60706/0.76647. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60551/0.76846. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59712/0.77377. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59587/0.77578. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59424/0.77968. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59171/0.78349. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59230/0.78353. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58647/0.79203. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58292/0.79582. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58445/0.79684. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58177/0.79787. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57393/0.80191. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57181/0.80438. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57217/0.80518. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57026/0.80748. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57363/0.80990. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57030/0.81424. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56700/0.81391. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55824/0.81800. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55800/0.82254. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55261/0.82974. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55392/0.83377. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.55224/0.84144. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54972/0.83405. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54217/0.84428. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54270/0.85049. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53870/0.85362. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54195/0.85073. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53979/0.85065. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53490/0.85083. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53286/0.86467. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52818/0.86691. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53255/0.86679. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.53352/0.87097. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53189/0.86682. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51978/0.88137. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52770/0.87905. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51723/0.88170. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52301/0.88906. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51470/0.88272. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50894/0.89363. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52028/0.89327. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51423/0.90064. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.50396/0.89730. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69181/0.68774. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69085/0.68773. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69051/0.68807. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69022/0.68855. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68976/0.68903. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68871/0.68958. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68802/0.69021. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68820/0.69099. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68696/0.69181. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68563/0.69276. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68501/0.69408. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68445/0.69523. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68362/0.69645. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68286/0.69750. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68343/0.69873. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68122/0.70038. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68116/0.70149. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68176/0.70214. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68157/0.70314. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68080/0.70394. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68022/0.70495. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67882/0.70586. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67837/0.70756. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67874/0.70848. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67682/0.70970. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67679/0.71129. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67540/0.71277. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67473/0.71290. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67582/0.71350. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67529/0.71438. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67601/0.71473. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67191/0.71581. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67079/0.71650. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67150/0.71698. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67029/0.71800. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67075/0.71838. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66899/0.72002. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66526/0.72116. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66606/0.72207. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66297/0.72415. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66488/0.72498. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66238/0.72553. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66075/0.72751. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66018/0.72692. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65854/0.72776. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66060/0.72964. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65611/0.73121. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65793/0.73237. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65589/0.73247. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65513/0.73384. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65257/0.73547. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65041/0.73526. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65172/0.73828. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64656/0.74032. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64640/0.74409. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64379/0.74552. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64347/0.74778. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64060/0.75076. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64061/0.75147. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63934/0.75348. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63784/0.75522. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63862/0.75507. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63264/0.75739. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63370/0.75971. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63233/0.76051. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62835/0.76487. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62913/0.76593. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62652/0.77037. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62141/0.77404. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62352/0.77507. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62045/0.77788. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.62337/0.78015. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61929/0.78348. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62061/0.78660. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61743/0.78789. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61712/0.78873. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61296/0.78989. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60686/0.79577. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61027/0.79919. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60857/0.79974. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60497/0.80496. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60669/0.80382. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60643/0.80718. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60016/0.80966. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59985/0.81169. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59547/0.81322. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.59890/0.82274. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59492/0.82093. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59310/0.82005. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59090/0.82770. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59178/0.82722. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59219/0.82680. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58561/0.82927. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58504/0.83384. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58742/0.83495. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58356/0.83230. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58316/0.83807. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58180/0.84274. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57217/0.84328. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57524/0.84537. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69007/0.68529. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68942/0.68556. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68824/0.68614. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68802/0.68659. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68785/0.68720. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68701/0.68771. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68602/0.68823. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68492/0.68883. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68609/0.68936. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68482/0.69001. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68438/0.69073. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68400/0.69164. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68466/0.69254. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68358/0.69340. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68252/0.69441. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68381/0.69525. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68373/0.69579. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68276/0.69679. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68190/0.69785. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68172/0.69851. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68036/0.69884. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68174/0.69943. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67932/0.70011. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67982/0.70110. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67819/0.70168. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67757/0.70255. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67819/0.70348. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67676/0.70428. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67735/0.70504. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67688/0.70653. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67589/0.70676. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67537/0.70763. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67355/0.70844. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67232/0.70910. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67287/0.71048. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67369/0.71154. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67276/0.71248. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67222/0.71326. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67110/0.71455. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66859/0.71590. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67068/0.71681. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66710/0.71734. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67101/0.71802. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66606/0.71920. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66665/0.72086. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66799/0.72152. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66519/0.72309. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66545/0.72429. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66224/0.72497. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66019/0.72615. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66179/0.72760. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66019/0.72870. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65853/0.73082. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65886/0.73221. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65653/0.73380. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65609/0.73598. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65472/0.73784. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65370/0.73972. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65589/0.74174. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65165/0.74226. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65477/0.74452. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65277/0.74599. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64956/0.74759. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64909/0.74951. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65070/0.75105. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64873/0.75258. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64820/0.75331. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64578/0.75424. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64232/0.75548. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64229/0.75838. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64522/0.76021. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64131/0.76405. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63818/0.76475. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63975/0.76635. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63919/0.76749. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63274/0.77059. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63563/0.77173. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63445/0.77246. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63382/0.77473. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63097/0.77590. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63250/0.77731. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62847/0.77766. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62942/0.78073. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62821/0.78436. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62909/0.78356. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62575/0.78704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62398/0.79033. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62397/0.79065. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62352/0.79209. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62280/0.79500. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61757/0.79415. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61863/0.79625. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61827/0.79766. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61576/0.79920. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61228/0.80183. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61560/0.80382. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61380/0.80619. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61467/0.80842. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61020/0.80996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60579/0.81182. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69171/0.70958. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69023/0.71026. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68853/0.70952. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68868/0.70927. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68765/0.70940. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68670/0.70898. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68615/0.70970. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68698/0.71004. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68603/0.70992. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68525/0.71020. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68560/0.71142. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68472/0.71140. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68425/0.71291. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68360/0.71322. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68380/0.71335. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68325/0.71422. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68368/0.71486. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68279/0.71535. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68324/0.71584. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68290/0.71633. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68287/0.71669. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68210/0.71673. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68175/0.71741. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68118/0.71797. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68029/0.71750. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68037/0.71859. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68015/0.71997. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67999/0.72067. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67991/0.72069. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67965/0.72033. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67740/0.72131. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67721/0.72239. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67826/0.72198. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67571/0.72247. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67757/0.72374. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67613/0.72330. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67681/0.72381. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67569/0.72480. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67485/0.72415. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67399/0.72549. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67232/0.72575. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67115/0.72749. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67180/0.72749. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66963/0.72927. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66980/0.72848. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66911/0.73002. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66915/0.73167. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66627/0.73145. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66732/0.73050. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66571/0.73275. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66334/0.73370. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66377/0.73419. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66371/0.73542. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66097/0.73584. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66209/0.73721. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66047/0.73709. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66323/0.73709. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65972/0.74043. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65933/0.73633. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65731/0.73801. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65463/0.74056. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65497/0.74116. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65558/0.74125. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65126/0.74189. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65119/0.74098. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64890/0.74498. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64835/0.74523. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64652/0.74679. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65195/0.74610. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64641/0.74274. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64503/0.74698. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64381/0.74709. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64781/0.74933. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64398/0.74578. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64161/0.74981. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63815/0.75003. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63865/0.74823. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63871/0.75215. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63772/0.75068. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63522/0.75304. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63363/0.75709. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63045/0.75452. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62763/0.75701. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63336/0.76246. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62846/0.76321. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62966/0.75927. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62647/0.76164. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62692/0.76355. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62221/0.76134. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62459/0.76363. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61966/0.76508. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62077/0.76882. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61772/0.76676. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61740/0.76871. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61586/0.77004. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61144/0.76962. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60640/0.77240. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60740/0.77790. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60906/0.78003. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61330/0.78303. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69465/0.69148. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69204/0.68661. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69028/0.68321. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68981/0.68130. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68866/0.68051. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68865/0.68040. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68755/0.68065. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68735/0.68148. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68703/0.68179. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68685/0.68220. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68584/0.68313. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68514/0.68399. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68593/0.68491. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68477/0.68607. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68464/0.68689. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68282/0.68841. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68256/0.68997. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68183/0.69195. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68138/0.69256. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68098/0.69427. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68086/0.69546. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67850/0.69673. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67839/0.69747. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67853/0.70014. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67903/0.69969. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67743/0.70039. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67768/0.70266. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67755/0.70288. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67624/0.70359. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67517/0.70461. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67476/0.70498. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67553/0.70556. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67348/0.70682. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67170/0.70825. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67202/0.70904. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67038/0.70907. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66979/0.70885. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67118/0.70953. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66774/0.70917. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66751/0.71073. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66764/0.71132. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66702/0.71256. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66595/0.71410. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66361/0.71419. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66239/0.71501. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66142/0.71693. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66074/0.71680. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65990/0.71809. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65968/0.71907. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65610/0.72112. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65347/0.72233. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65414/0.72376. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65503/0.72578. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65157/0.72337. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64942/0.72326. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64866/0.72530. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64771/0.72458. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64429/0.72593. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64472/0.73235. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63740/0.73425. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63576/0.73322. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63413/0.73170. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63241/0.73417. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63219/0.73957. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62761/0.74379. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62658/0.74340. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62065/0.74905. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61893/0.75041. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61956/0.75217. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61413/0.75563. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61117/0.75707. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61143/0.75303. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60687/0.75649. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59760/0.76455. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60197/0.76779. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59738/0.77135. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60026/0.76691. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59481/0.77509. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58636/0.77270. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58999/0.77285. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57829/0.77676. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58460/0.77779. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57682/0.78405. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.57369/0.78467. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56723/0.78000. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56205/0.78685. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56615/0.79263. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55998/0.79408. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56484/0.80052. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55882/0.79469. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54955/0.80403. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55446/0.80424. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55039/0.80144. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54582/0.80777. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54567/0.80306. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53139/0.81256. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52935/0.81242. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53023/0.81686. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.53334/0.81583. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52978/0.82261. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69123/0.68979. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68988/0.68907. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68963/0.68836. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68856/0.68752. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68716/0.68678. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68679/0.68604. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68602/0.68544. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68535/0.68494. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.68425/0.68447. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68398/0.68409. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68252/0.68369. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68180/0.68353. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68055/0.68297. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68101/0.68267. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67849/0.68276. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67877/0.68273. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67815/0.68264. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67604/0.68223. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67599/0.68214. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67470/0.68215. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67491/0.68209. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67223/0.68186. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67136/0.68184. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67061/0.68197. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66853/0.68227. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66832/0.68232. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66813/0.68260. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66626/0.68246. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66612/0.68267. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66530/0.68288. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66420/0.68271. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66323/0.68316. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65961/0.68328. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65918/0.68348. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65763/0.68407. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65687/0.68462. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65593/0.68488. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65371/0.68485. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65306/0.68442. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65058/0.68540. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64807/0.68656. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64402/0.68691. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64141/0.68694. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64141/0.68811. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64107/0.68762. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63716/0.68667. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63951/0.68767. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63114/0.68611. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63469/0.68572. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63193/0.68470. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62532/0.68601. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62377/0.68562. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62661/0.68525. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62047/0.68645. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61596/0.68497. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61390/0.68689. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61537/0.68788. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61244/0.68986. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60707/0.68807. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60605/0.69012. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60549/0.69249. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60862/0.69124. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59985/0.69151. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59684/0.69032. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59727/0.69424. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59017/0.69451. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59078/0.69747. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58847/0.69833. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58377/0.69697. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57581/0.69999. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58147/0.69787. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57339/0.70323. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57429/0.70199. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56788/0.70686. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57054/0.70974. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56710/0.71273. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56961/0.71187. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55438/0.71965. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55564/0.71832. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55590/0.71761. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55052/0.72154. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54156/0.72442. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54090/0.72527. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54298/0.73087. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53464/0.73787. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53389/0.73765. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53425/0.74254. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52773/0.73958. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53304/0.74612. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52592/0.76161. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52093/0.75225. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51600/0.75926. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51539/0.76471. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50580/0.76700. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50865/0.77425. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51371/0.77987. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50855/0.77287. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50346/0.78073. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50612/0.78647. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49653/0.78490. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69778/0.69199. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69097/0.69026. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68903/0.69052. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68798/0.69101. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68671/0.69132. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68709/0.69147. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68716/0.69163. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68640/0.69174. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68526/0.69195. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68621/0.69224. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68463/0.69260. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68445/0.69281. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68415/0.69326. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68322/0.69369. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68223/0.69425. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68208/0.69473. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68085/0.69530. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68097/0.69596. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68007/0.69650. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67957/0.69714. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67932/0.69754. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67766/0.69803. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67765/0.69802. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67750/0.69857. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67698/0.69914. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67607/0.69906. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67506/0.69941. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67498/0.69974. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67449/0.70045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67463/0.70043. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67232/0.70116. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67217/0.70169. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67345/0.70223. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67370/0.70258. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67047/0.70276. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66957/0.70300. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67029/0.70304. Took 0.13 sec\n",
      "Epoch 37, Loss(train/val) 0.66765/0.70330. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66845/0.70347. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66694/0.70292. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.66789/0.70314. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66518/0.70378. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66503/0.70451. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66439/0.70400. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66119/0.70456. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66148/0.70440. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66231/0.70491. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65881/0.70474. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65903/0.70487. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65868/0.70465. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65653/0.70527. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65298/0.70567. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65305/0.70547. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65174/0.70455. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64875/0.70543. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65044/0.70463. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64615/0.70505. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64666/0.70384. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64265/0.70337. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64117/0.70296. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63924/0.70406. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63693/0.70345. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63520/0.70325. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63189/0.70480. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62868/0.70447. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62845/0.70450. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62681/0.70391. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62542/0.70380. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61936/0.70367. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62178/0.70295. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62105/0.70311. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61855/0.70434. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61559/0.70557. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61695/0.70452. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61057/0.70496. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60958/0.70533. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61131/0.70686. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60967/0.70571. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60152/0.70937. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60599/0.71187. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60306/0.71174. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60254/0.71491. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59735/0.71061. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59989/0.71045. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59458/0.71560. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59124/0.71167. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58931/0.71182. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59341/0.71483. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58906/0.71851. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58234/0.71958. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58723/0.71971. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57999/0.71912. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.57789/0.71779. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.57739/0.71522. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.57273/0.72347. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57455/0.72872. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.57331/0.72639. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57348/0.73126. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56746/0.73558. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57148/0.73181. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69832/0.69434. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69749/0.69379. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69607/0.69315. Took 0.17 sec\n",
      "Epoch 3, Loss(train/val) 0.69397/0.69247. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69335/0.69216. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69150/0.69213. Took 0.14 sec\n",
      "Epoch 6, Loss(train/val) 0.69170/0.69217. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69049/0.69222. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68996/0.69222. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68980/0.69243. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68970/0.69253. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68939/0.69247. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68786/0.69240. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68812/0.69225. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68816/0.69184. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.69157. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68654/0.69144. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68621/0.69097. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 0.68643/0.69073. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68551/0.69038. Took 0.14 sec\n",
      "Epoch 20, Loss(train/val) 0.68505/0.69005. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68413/0.68995. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68165/0.68929. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68090/0.68883. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68092/0.68829. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67887/0.68745. Took 0.16 sec\n",
      "Epoch 26, Loss(train/val) 0.67581/0.68663. Took 0.16 sec\n",
      "Epoch 27, Loss(train/val) 0.67640/0.68601. Took 0.26 sec\n",
      "Epoch 28, Loss(train/val) 0.67313/0.68467. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67158/0.68379. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67199/0.68293. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.66799/0.68213. Took 0.15 sec\n",
      "Epoch 32, Loss(train/val) 0.66527/0.68083. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66698/0.67858. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66457/0.67706. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66135/0.67598. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.65940/0.67469. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65644/0.67366. Took 0.14 sec\n",
      "Epoch 38, Loss(train/val) 0.65613/0.67364. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.65300/0.67178. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65188/0.66975. Took 0.17 sec\n",
      "Epoch 41, Loss(train/val) 0.65087/0.66860. Took 0.17 sec\n",
      "Epoch 42, Loss(train/val) 0.65067/0.66833. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64826/0.66798. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64618/0.66798. Took 0.14 sec\n",
      "Epoch 45, Loss(train/val) 0.64617/0.66630. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.63998/0.66662. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63863/0.66575. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63997/0.66767. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63482/0.66547. Took 0.27 sec\n",
      "Epoch 50, Loss(train/val) 0.63363/0.66885. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.63430/0.66465. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63000/0.66762. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62995/0.66741. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62600/0.66786. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62259/0.66693. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62256/0.66714. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61904/0.66561. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61649/0.66579. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61702/0.66207. Took 0.34 sec\n",
      "Epoch 60, Loss(train/val) 0.60694/0.66392. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60792/0.66681. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61134/0.67003. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60189/0.67065. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60071/0.66968. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.60629/0.66663. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60091/0.66977. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58928/0.66772. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.59391/0.66816. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58740/0.67221. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59144/0.67365. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58823/0.66999. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58299/0.67657. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58442/0.67974. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57537/0.68336. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57953/0.67687. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57463/0.67969. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57436/0.68370. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56445/0.68574. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57165/0.68532. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56593/0.68646. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55636/0.68314. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56138/0.68178. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55247/0.68739. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55335/0.68637. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54843/0.68859. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54889/0.69004. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54601/0.68697. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53912/0.69509. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54591/0.69550. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53838/0.69571. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53501/0.69878. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.53374/0.70250. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52357/0.70512. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52775/0.70437. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52047/0.70347. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52234/0.70143. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52089/0.70802. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51748/0.70808. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51335/0.70853. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69250/0.69234. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69193/0.69215. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69211/0.69195. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69206/0.69187. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69162/0.69190. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69169/0.69191. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69114/0.69215. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69130/0.69240. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69245. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69050/0.69271. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69050/0.69290. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68972/0.69309. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68997/0.69327. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68948/0.69371. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68914/0.69415. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68859/0.69449. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68932/0.69491. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68699/0.69523. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.69580. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68735/0.69644. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68744/0.69683. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68565/0.69752. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68542/0.69782. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68517/0.69856. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68355/0.69987. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68361/0.69999. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68242/0.70156. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68139/0.70247. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68160/0.70272. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68221/0.70433. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68074/0.70520. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67912/0.70601. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67845/0.70647. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67752/0.70774. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67594/0.70806. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67498/0.70922. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67689/0.71031. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67471/0.71095. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67291/0.71225. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67353/0.71382. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67178/0.71514. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66887/0.71572. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66645/0.71759. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66638/0.71856. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66507/0.71993. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66500/0.72162. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66460/0.72328. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66333/0.72430. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65959/0.72699. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65895/0.72734. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65865/0.72925. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65491/0.73198. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65708/0.73186. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65657/0.73260. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65131/0.73468. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64839/0.73729. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64839/0.73552. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64583/0.73894. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64689/0.73850. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64234/0.74111. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64244/0.74375. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64427/0.74531. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64119/0.74518. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63646/0.74634. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63547/0.74771. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63467/0.74977. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62624/0.75186. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62475/0.75369. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62340/0.75577. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62389/0.75582. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62497/0.75815. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61793/0.76200. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61374/0.76470. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61159/0.76817. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61127/0.76768. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61034/0.76870. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60732/0.77028. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60281/0.77292. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59059/0.77685. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58861/0.78001. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59873/0.77884. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58466/0.78179. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59304/0.78370. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58498/0.79062. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57339/0.79619. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57331/0.79626. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57337/0.80377. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57624/0.81445. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56866/0.81133. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56688/0.81633. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56357/0.81961. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55324/0.82013. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.55096/0.82917. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55292/0.83637. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54586/0.83568. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54403/0.84486. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54180/0.85324. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54044/0.85753. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52297/0.85389. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52655/0.86401. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69296/0.69902. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69168/0.70005. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69182/0.70059. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69149/0.70118. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69159/0.70138. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69135/0.70190. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69145/0.70211. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69098/0.70223. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69078/0.70254. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69118/0.70246. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69041/0.70249. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69028/0.70284. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68944/0.70313. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68982/0.70324. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68919/0.70348. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68920/0.70413. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68938/0.70499. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.70546. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68745/0.70627. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68701/0.70693. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68629/0.70748. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68621/0.70861. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68590/0.70947. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68574/0.71002. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68568/0.71014. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68453/0.71022. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68449/0.71076. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68375/0.71085. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68293/0.71177. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68410/0.71141. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68262/0.71156. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68158/0.71237. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68127/0.71243. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68104/0.71421. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68030/0.71354. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68042/0.71329. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68039/0.71306. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67922/0.71276. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68024/0.71314. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67955/0.71237. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67747/0.71291. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67689/0.71179. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67791/0.71218. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67508/0.71155. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67583/0.71135. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67529/0.71075. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67616/0.71114. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67341/0.71075. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67351/0.71046. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67365/0.70913. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67272/0.71012. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67111/0.70918. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67040/0.70923. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66866/0.70893. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66855/0.70837. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66930/0.70711. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66801/0.70774. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66473/0.70792. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66747/0.70774. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66757/0.70643. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66352/0.70513. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66268/0.70355. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66154/0.70366. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66160/0.70323. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65829/0.70132. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65675/0.70149. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65551/0.70164. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65444/0.69982. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65239/0.70073. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65067/0.69846. Took 0.16 sec\n",
      "Epoch 70, Loss(train/val) 0.65047/0.69593. Took 0.17 sec\n",
      "Epoch 71, Loss(train/val) 0.64526/0.69395. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.64385/0.69535. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64267/0.69544. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64102/0.69467. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64002/0.69442. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63558/0.69641. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63253/0.69622. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63333/0.69621. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63138/0.69350. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62564/0.69546. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62092/0.69656. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62139/0.69620. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62049/0.69839. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61537/0.69753. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61003/0.69597. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61107/0.69492. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61360/0.69884. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60799/0.70209. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60097/0.69594. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60040/0.70342. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59465/0.70235. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59735/0.70557. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59323/0.70649. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58814/0.71096. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58724/0.70684. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58287/0.70876. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58532/0.71491. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58066/0.71356. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57586/0.70966. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69519/0.68140. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69096/0.68342. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.68516. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68944/0.68662. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68942/0.68798. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68870/0.68945. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68767/0.69131. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68714/0.69337. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68592/0.69602. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68588/0.69820. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68514/0.69964. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68433/0.70180. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68408/0.70363. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68411/0.70488. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68280/0.70631. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68262/0.70755. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68280/0.70875. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68163/0.71067. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67997/0.71120. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68124/0.71143. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68084/0.71202. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67922/0.71233. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67995/0.71235. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67910/0.71345. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67768/0.71490. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67792/0.71644. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67660/0.71623. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67647/0.71742. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67700/0.71859. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67572/0.71742. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67463/0.71956. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67535/0.71870. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67538/0.71925. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67483/0.72022. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67340/0.72246. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67282/0.72196. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67327/0.72108. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67036/0.72581. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66896/0.72444. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67105/0.72664. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66994/0.72756. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66642/0.72942. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66762/0.72908. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66842/0.72879. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66596/0.72975. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66653/0.73119. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66346/0.73224. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66533/0.73317. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66456/0.73346. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66369/0.73497. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66040/0.73699. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66291/0.73775. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66284/0.73582. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65980/0.73966. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65948/0.73914. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65883/0.73815. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65812/0.74174. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65450/0.74405. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65882/0.74253. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65684/0.74135. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65581/0.74401. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65260/0.74720. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65190/0.74614. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65263/0.74770. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65044/0.75056. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65045/0.75047. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64840/0.75254. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64777/0.75335. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64552/0.75568. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64498/0.76013. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64511/0.75806. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64431/0.75932. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64517/0.75560. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64087/0.75900. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63979/0.75761. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63559/0.76296. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63421/0.76605. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63464/0.76685. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63367/0.76762. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62940/0.77222. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63134/0.77239. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62816/0.77476. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62555/0.77347. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62537/0.77822. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62616/0.77684. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62291/0.77640. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61689/0.78276. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61774/0.78597. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61508/0.78412. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61233/0.78909. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60945/0.78996. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61109/0.78850. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60926/0.78950. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60658/0.79183. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59963/0.79119. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60079/0.79612. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60120/0.79255. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59720/0.79847. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59389/0.79802. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59323/0.80149. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69602/0.68562. Took 0.31 sec\n",
      "Epoch 1, Loss(train/val) 0.69100/0.68097. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68953/0.67919. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69002/0.67853. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68977/0.67803. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68812/0.67744. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68752/0.67708. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.67694. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68699/0.67647. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68757/0.67634. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68687/0.67593. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68386/0.67558. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68411/0.67501. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68312/0.67507. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68292/0.67453. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68233/0.67377. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68008/0.67462. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68069/0.67447. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68062/0.67467. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67924/0.67457. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67899/0.67504. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67736/0.67579. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67813/0.67669. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67805/0.67537. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67641/0.67639. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67757/0.67787. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67542/0.67783. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67709/0.67803. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67477/0.67893. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67464/0.67854. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67462/0.67894. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67358/0.67965. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67509/0.67969. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67357/0.68130. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67277/0.68045. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67253/0.68066. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67241/0.68091. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67286/0.68256. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67200/0.68263. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67116/0.68192. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66989/0.68156. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67081/0.68153. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66835/0.68274. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66966/0.68357. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66738/0.68394. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66820/0.68394. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66857/0.68298. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66695/0.68331. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66746/0.68282. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66666/0.68424. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66575/0.68455. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66605/0.68497. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66368/0.68473. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66411/0.68659. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66430/0.68608. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66484/0.68442. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66247/0.68518. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66136/0.68707. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66195/0.68774. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66062/0.68757. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65875/0.68844. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65697/0.68856. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65875/0.68937. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65861/0.69071. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65713/0.69027. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65621/0.68884. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65557/0.69206. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65479/0.69128. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65229/0.69164. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65512/0.69213. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64916/0.69286. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65377/0.69247. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65118/0.69279. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65033/0.69347. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64886/0.69576. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64846/0.69508. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64432/0.69636. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64326/0.69317. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64816/0.69724. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64460/0.69934. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64419/0.69544. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64422/0.69867. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63907/0.69614. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64320/0.69808. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64000/0.69755. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63714/0.69910. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63623/0.70351. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63459/0.69690. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63452/0.70076. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63277/0.70106. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63075/0.70299. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63201/0.70361. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62613/0.70134. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62853/0.70455. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62784/0.70212. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62418/0.70628. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62345/0.70530. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62137/0.70608. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61955/0.70737. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61931/0.70766. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69822/0.70198. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69767/0.70060. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69621/0.69817. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69432/0.69458. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69282/0.69067. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69088/0.68774. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68964/0.68586. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68998/0.68486. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68919/0.68434. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68836/0.68368. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68764/0.68307. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.68274. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68614/0.68224. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68507/0.68208. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68434/0.68189. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68394/0.68180. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68223/0.68180. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68205/0.68095. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67952/0.68094. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68015/0.68171. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67848/0.68207. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67801/0.68213. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67668/0.68344. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67532/0.68384. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67434/0.68420. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67341/0.68535. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67171/0.68546. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67170/0.68653. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66977/0.68807. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67035/0.68769. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67104/0.68728. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66983/0.68731. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66762/0.68740. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66593/0.69029. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66621/0.69116. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66556/0.69077. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66572/0.69099. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66386/0.69220. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66444/0.69267. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66138/0.69252. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65930/0.69182. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66196/0.69356. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66030/0.69389. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65883/0.69341. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65924/0.69542. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65662/0.69523. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65352/0.69503. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65686/0.69526. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65531/0.69729. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65426/0.69421. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64919/0.69533. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65020/0.69526. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64779/0.69614. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64811/0.69605. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64896/0.69992. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64857/0.69805. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64221/0.69588. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64165/0.69647. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63994/0.69555. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63931/0.69526. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64049/0.69322. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63656/0.69322. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63320/0.69483. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63397/0.69698. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62941/0.69213. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62843/0.69028. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62826/0.69258. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62868/0.69426. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62100/0.69398. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62370/0.69615. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61778/0.69643. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61980/0.69376. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61654/0.68574. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61070/0.68965. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61437/0.68847. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61121/0.68719. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60615/0.68524. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59958/0.68720. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59793/0.69007. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59872/0.69187. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59660/0.68766. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59535/0.68749. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59196/0.69069. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58417/0.68658. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58518/0.68862. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58218/0.69231. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.58023/0.69221. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57810/0.69080. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56922/0.68539. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57693/0.68773. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57063/0.69164. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57001/0.69169. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56722/0.69711. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56707/0.69895. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56119/0.69924. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56224/0.70424. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55130/0.70317. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55755/0.69951. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54735/0.70484. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55254/0.71024. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.68822/0.69185. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68705/0.69235. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68546/0.69208. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68472/0.69141. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68320/0.69092. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68200/0.68973. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68038/0.68946. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.67969/0.68883. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.67805/0.68852. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67838/0.68907. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67648/0.68934. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67572/0.68906. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67591/0.68925. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67355/0.68943. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67524/0.68901. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67465/0.68930. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67390/0.68911. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67333/0.68912. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67188/0.68960. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67221/0.68916. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67139/0.68906. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67122/0.68952. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.66971/0.68997. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67025/0.68919. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66927/0.68996. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66916/0.68922. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66791/0.69063. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66818/0.68992. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66709/0.69020. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66740/0.69194. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66608/0.69154. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66457/0.69229. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66406/0.69314. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66475/0.69305. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66358/0.69369. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66404/0.69464. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66255/0.69462. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66327/0.69568. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66118/0.69593. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66164/0.69563. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65896/0.69734. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65990/0.69726. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65650/0.69821. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65685/0.69942. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65565/0.69955. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65818/0.69872. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65687/0.69970. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65255/0.70166. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65268/0.70162. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65122/0.70385. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65151/0.70390. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65217/0.70554. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64927/0.70506. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65035/0.70719. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64905/0.70781. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64837/0.70823. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64683/0.70961. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64646/0.70957. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64543/0.71102. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64211/0.71037. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64465/0.71175. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64177/0.71404. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63848/0.71504. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63946/0.71764. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64007/0.71755. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63606/0.71913. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63696/0.71942. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63342/0.72299. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63366/0.72564. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63424/0.72486. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62934/0.72526. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62993/0.72680. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62484/0.72839. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63096/0.73072. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62819/0.73123. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62807/0.73229. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62402/0.73474. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62376/0.73446. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62044/0.73287. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62299/0.73622. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62004/0.73744. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61830/0.74054. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61372/0.74253. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61431/0.74208. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61031/0.74397. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61130/0.74604. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61419/0.74504. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61188/0.74757. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61041/0.74564. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.60824/0.74796. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60535/0.75115. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59987/0.75225. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60384/0.75404. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59818/0.75176. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59750/0.75343. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59494/0.75525. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59482/0.76030. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59556/0.76090. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58452/0.76331. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58919/0.76489. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69197. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69233/0.69067. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69024/0.68984. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68906/0.68933. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68831/0.68927. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68842/0.68938. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68801/0.68945. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68771/0.68958. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68812/0.68975. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68572/0.68987. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68717/0.69006. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68566/0.69032. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68502/0.69053. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68395/0.69106. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68411/0.69163. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68387/0.69216. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68317/0.69268. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68004/0.69362. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67972/0.69477. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67970/0.69593. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67828/0.69729. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67767/0.69886. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67704/0.70030. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67517/0.70188. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67459/0.70347. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67429/0.70532. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67204/0.70717. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67138/0.70904. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67023/0.71096. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67073/0.71261. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66923/0.71419. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67056/0.71542. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66944/0.71665. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66783/0.71831. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66608/0.71960. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66484/0.72075. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66358/0.72276. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66164/0.72431. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66279/0.72587. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66406/0.72668. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66466/0.72731. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65892/0.72864. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.65909/0.73102. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65719/0.73256. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65864/0.73448. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65602/0.73598. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65502/0.73747. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65500/0.73886. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65441/0.73970. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65383/0.74130. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65213/0.74241. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65158/0.74415. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65000/0.74550. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64702/0.74761. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64569/0.74994. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64620/0.75169. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64536/0.75314. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64549/0.75375. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64535/0.75457. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64224/0.75706. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63789/0.75876. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63823/0.76180. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63806/0.76292. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63636/0.76582. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63294/0.76802. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63327/0.76981. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62900/0.77250. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62872/0.77364. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63251/0.77395. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62861/0.77708. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62601/0.77901. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62352/0.78099. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62208/0.78294. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61996/0.78383. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61797/0.78622. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61921/0.79109. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61993/0.79106. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61391/0.79432. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60867/0.79712. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61173/0.80131. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60753/0.80488. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60700/0.80534. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.60240/0.80859. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60290/0.81041. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60463/0.81379. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59998/0.81477. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59703/0.81808. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59191/0.82077. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59514/0.82548. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59356/0.82622. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59428/0.82913. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58982/0.83191. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59072/0.82867. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58132/0.83233. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.58717/0.83666. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58635/0.83911. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58290/0.84080. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58086/0.84234. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57686/0.84643. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57168/0.84868. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.68998/0.69686. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68936/0.69773. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68924/0.69847. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68843/0.69913. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68795/0.69964. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68843/0.69994. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68759/0.69990. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.70001. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68689/0.69990. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68687/0.69967. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68635/0.69960. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68542/0.69919. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68513/0.69843. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68432/0.69823. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68393/0.69823. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68206/0.69818. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68092/0.69801. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68189/0.69860. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68134/0.69858. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67928/0.69952. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68006/0.69873. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67860/0.69956. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67846/0.70004. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67802/0.69963. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67695/0.70009. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67733/0.70110. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67650/0.70130. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67602/0.70140. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67599/0.70012. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67526/0.70144. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67419/0.70106. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67446/0.70200. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67535/0.70136. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67365/0.70196. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67354/0.70236. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67392/0.70173. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67294/0.70308. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67302/0.70281. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67340/0.70283. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67159/0.70288. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67194/0.70485. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67130/0.70335. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67100/0.70433. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67100/0.70391. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66994/0.70449. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66988/0.70488. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66955/0.70545. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66833/0.70678. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66998/0.70516. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66904/0.70601. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66856/0.70594. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66745/0.70701. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66646/0.70757. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66714/0.70679. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66559/0.70811. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66607/0.70925. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66533/0.70858. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66457/0.70911. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66418/0.71037. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66488/0.70970. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66466/0.71001. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66088/0.70928. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66321/0.71221. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66179/0.71141. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66130/0.71180. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66063/0.71295. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66108/0.71236. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65860/0.71101. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66064/0.71289. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65719/0.71221. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65553/0.71244. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65537/0.71384. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65687/0.71366. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65745/0.71621. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65461/0.71259. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65486/0.71406. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65469/0.71451. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65329/0.71363. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65067/0.71569. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65188/0.71750. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64938/0.71766. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64953/0.71647. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64830/0.71387. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64801/0.71809. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64733/0.71677. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64961/0.71598. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64556/0.71879. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64645/0.71850. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64414/0.71997. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64332/0.72124. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64296/0.72097. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63864/0.71915. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63801/0.72206. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63803/0.72202. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63412/0.72338. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63438/0.72252. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63528/0.72527. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63090/0.72417. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63189/0.72569. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62763/0.72974. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.68970/0.69561. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68720/0.69620. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68633/0.69690. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68652/0.69759. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68509/0.69852. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68464/0.69951. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68408/0.70080. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68330/0.70226. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68334/0.70354. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68282/0.70471. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.67983/0.70626. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67887/0.70808. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67810/0.71007. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.67732/0.71241. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67665/0.71453. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67613/0.71664. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67421/0.71827. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67498/0.71979. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67441/0.72053. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67284/0.72128. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67019/0.72292. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66950/0.72403. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67006/0.72465. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66793/0.72590. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66734/0.72659. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66678/0.72680. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66441/0.72838. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66363/0.72984. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66518/0.73000. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66127/0.72990. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66073/0.72957. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66184/0.73015. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66085/0.73038. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65863/0.73000. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65476/0.73085. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65147/0.73158. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65165/0.73317. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65124/0.73261. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64532/0.73236. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64521/0.73289. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64292/0.73598. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64233/0.73405. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64028/0.73476. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64047/0.73505. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63267/0.73667. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63257/0.73690. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63019/0.73501. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62791/0.73581. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62684/0.73807. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62126/0.73729. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62137/0.73962. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61994/0.73835. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.61302/0.74166. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61527/0.74834. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60743/0.74832. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60994/0.74615. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60332/0.74840. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60749/0.75159. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59773/0.75473. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59579/0.76078. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59634/0.76105. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59319/0.76212. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58804/0.76339. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59008/0.76860. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59133/0.76730. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58328/0.77294. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57873/0.77330. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57551/0.77459. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57317/0.78284. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57183/0.77842. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57627/0.78550. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56897/0.78339. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56140/0.79171. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.56176/0.79434. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55896/0.79739. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55885/0.80304. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55604/0.80456. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55061/0.80432. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54420/0.81863. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54751/0.81832. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55109/0.82021. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.55014/0.82537. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54435/0.82952. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54081/0.82942. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54145/0.82953. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53517/0.83667. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53066/0.84151. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52346/0.84854. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52915/0.85274. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51905/0.86112. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51947/0.85608. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52153/0.86724. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51323/0.86759. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51967/0.87517. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.50224/0.87505. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51485/0.87799. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50860/0.88285. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51052/0.89100. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49695/0.88982. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50146/0.89415. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69302/0.68795. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68704/0.68775. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68660/0.68686. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68527/0.68621. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68539/0.68561. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68475/0.68500. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68511/0.68473. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68283/0.68445. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68255/0.68428. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 0.68316/0.68383. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68186/0.68348. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68277/0.68316. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68160/0.68308. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68065/0.68306. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68053/0.68289. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68033/0.68265. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67995/0.68275. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67909/0.68252. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67912/0.68297. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67849/0.68326. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67688/0.68327. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67794/0.68330. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67679/0.68318. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67578/0.68391. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67587/0.68431. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67615/0.68458. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67558/0.68532. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67400/0.68590. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67217/0.68631. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67371/0.68695. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67240/0.68745. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67226/0.68801. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67205/0.68875. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67153/0.68960. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67135/0.68979. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66923/0.69043. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66991/0.69091. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66926/0.69169. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66844/0.69213. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66790/0.69370. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66600/0.69317. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66436/0.69419. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66570/0.69438. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66534/0.69531. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66294/0.69603. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66364/0.69570. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66085/0.69683. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66086/0.69752. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65884/0.69816. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65873/0.69898. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65687/0.69982. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65513/0.69950. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65354/0.70094. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65371/0.70201. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65074/0.70322. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65009/0.70531. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64976/0.70473. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65016/0.70424. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64580/0.70599. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64374/0.70650. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64387/0.70618. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64000/0.70613. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63981/0.70831. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63495/0.70863. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63358/0.71070. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63366/0.71415. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63204/0.71375. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63170/0.71454. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62226/0.71504. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62720/0.71524. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62317/0.71688. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62279/0.71727. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62138/0.71765. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62144/0.71932. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62078/0.72003. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61706/0.72035. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61881/0.72116. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61340/0.72168. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61467/0.72219. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60906/0.72598. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61035/0.72755. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61088/0.72724. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60864/0.72819. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60886/0.72642. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60371/0.72995. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60656/0.73111. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60175/0.73196. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.60003/0.73043. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60046/0.73022. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59149/0.73154. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59560/0.73468. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59244/0.73617. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58809/0.73949. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58736/0.73971. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59025/0.73843. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58794/0.73921. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58678/0.74359. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58257/0.74267. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58426/0.74453. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58231/0.74893. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.68905/0.67983. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68909/0.68042. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68776/0.68082. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68637/0.68099. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68662/0.68143. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68506/0.68211. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68481/0.68248. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68413/0.68302. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68271/0.68386. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68242/0.68424. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68217/0.68482. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68268/0.68566. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68099/0.68642. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68121/0.68719. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68014/0.68830. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68003/0.68910. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68008/0.68966. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67839/0.69050. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67917/0.69134. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67796/0.69213. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67810/0.69341. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67798/0.69383. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67665/0.69418. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67648/0.69476. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67643/0.69601. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67662/0.69657. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67418/0.69709. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67408/0.69817. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67449/0.69898. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67227/0.69942. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67272/0.70006. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67159/0.70029. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67260/0.70120. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67168/0.70212. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67024/0.70278. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67083/0.70341. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66922/0.70385. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66804/0.70436. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66716/0.70556. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66636/0.70630. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66637/0.70671. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66395/0.70731. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66430/0.70783. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66168/0.70823. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66233/0.70832. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66164/0.70925. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65985/0.70984. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65856/0.71075. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65925/0.71171. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65595/0.71142. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65450/0.71186. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65310/0.71305. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65079/0.71449. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64743/0.71408. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64754/0.71490. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64466/0.71585. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64182/0.71613. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64016/0.71536. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63733/0.71620. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63505/0.71816. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63466/0.71854. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63158/0.71849. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63157/0.71896. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63234/0.72045. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62394/0.72047. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62698/0.72179. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62473/0.72146. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61974/0.72378. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61819/0.72409. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61142/0.72646. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61650/0.72705. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61244/0.72921. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61073/0.72911. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60733/0.73015. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60941/0.73096. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60707/0.73150. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60194/0.73430. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60168/0.73266. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59993/0.73566. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59218/0.73862. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59525/0.73918. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58754/0.74374. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58582/0.74541. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58817/0.74434. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58410/0.74630. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58271/0.74994. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57393/0.75379. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57548/0.75185. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57515/0.75573. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56655/0.75651. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56728/0.76314. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56786/0.75933. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56049/0.76322. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56436/0.76694. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55721/0.76729. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55074/0.76864. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55249/0.77405. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55040/0.77576. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55147/0.77573. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54768/0.77962. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69038/0.68867. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68684/0.68631. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68541/0.68509. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68411/0.68476. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68455/0.68493. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68392/0.68530. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68401/0.68568. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68435/0.68610. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68382/0.68650. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68314/0.68686. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68334/0.68726. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68294/0.68792. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68260/0.68849. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68291/0.68887. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68231/0.68950. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68160/0.68975. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68176/0.69012. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68230/0.69064. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68103/0.69106. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68034/0.69159. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68033/0.69192. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68130/0.69235. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68039/0.69223. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67773/0.69319. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67914/0.69374. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67837/0.69402. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67738/0.69531. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67801/0.69518. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67758/0.69509. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67718/0.69565. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67580/0.69642. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67578/0.69575. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67616/0.69697. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67494/0.69887. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67512/0.69859. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67425/0.69755. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67217/0.69893. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67352/0.69915. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67261/0.69955. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67005/0.70010. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67089/0.70029. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67022/0.70120. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66812/0.70260. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66884/0.70328. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66741/0.70191. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66449/0.70352. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66553/0.70323. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66295/0.70308. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66189/0.70285. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66247/0.70416. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66346/0.70346. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66139/0.70489. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66051/0.70650. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66022/0.70592. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65859/0.70504. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65816/0.70653. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65589/0.70730. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65553/0.70830. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65535/0.70845. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65385/0.70685. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65323/0.70938. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64966/0.71067. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65152/0.71254. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64787/0.71359. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64867/0.71407. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64679/0.71540. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64511/0.71664. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64456/0.71705. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64069/0.71769. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64072/0.72113. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63902/0.72107. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63768/0.72310. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63696/0.72402. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63252/0.72477. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62934/0.72274. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63329/0.72248. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63182/0.72650. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63175/0.73003. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62399/0.73251. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62946/0.73497. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62415/0.73558. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62493/0.74012. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62021/0.73777. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61881/0.73954. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61670/0.74945. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61478/0.74617. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61499/0.74482. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61309/0.74709. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61113/0.74711. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61024/0.75210. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60422/0.75514. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.59819/0.75337. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60427/0.75560. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60100/0.75742. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59845/0.76003. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59410/0.76684. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59221/0.76801. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59081/0.76615. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58293/0.77521. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58345/0.77431. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70143/0.69527. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69710/0.69307. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69142/0.69145. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68639/0.69165. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68430/0.69298. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68163/0.69445. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68107/0.69562. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68191/0.69642. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68080/0.69691. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67997/0.69707. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67971/0.69728. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67952/0.69725. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.67838/0.69728. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67789/0.69704. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67749/0.69708. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67658/0.69713. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67686/0.69746. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67766/0.69701. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67676/0.69680. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67553/0.69658. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67491/0.69663. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67310/0.69649. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67438/0.69618. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67512/0.69576. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67151/0.69589. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67086/0.69537. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67118/0.69494. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66962/0.69473. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66835/0.69330. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66571/0.69409. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66643/0.69390. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66386/0.69350. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66365/0.69255. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66185/0.69237. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66209/0.69242. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65928/0.69108. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65682/0.69197. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65541/0.68974. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65553/0.69094. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65394/0.68837. Took 0.86 sec\n",
      "Epoch 40, Loss(train/val) 0.64968/0.68891. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64724/0.68797. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64665/0.68621. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64504/0.68944. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64423/0.68603. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.64464/0.68444. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63859/0.68424. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63732/0.68503. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63916/0.68730. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63517/0.68602. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63322/0.68427. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63305/0.68209. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62949/0.68115. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62443/0.68240. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62685/0.68667. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62761/0.68339. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62394/0.68502. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62264/0.68674. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62091/0.68505. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61809/0.68527. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61784/0.68749. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61638/0.68758. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61115/0.69103. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61374/0.69068. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.60887/0.68834. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60449/0.68982. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60464/0.69427. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.60383/0.69249. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59920/0.69584. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59711/0.69764. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59564/0.69636. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59901/0.69844. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59708/0.69783. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58859/0.70056. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58962/0.70156. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58748/0.70696. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58368/0.70407. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58605/0.70788. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58370/0.70687. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57679/0.70749. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57854/0.70887. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57479/0.70750. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57357/0.71208. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57039/0.71368. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.56897/0.71456. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57099/0.71546. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56733/0.71421. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56732/0.72177. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56364/0.72622. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55889/0.72934. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55958/0.72831. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56600/0.73095. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55510/0.73121. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55282/0.73043. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55491/0.73706. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55388/0.73651. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54473/0.73907. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54709/0.74276. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54774/0.74330. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54180/0.74211. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.68451/0.69789. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68134/0.70201. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68155/0.70358. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68039/0.70434. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.67966/0.70466. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.67923/0.70455. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.67914/0.70481. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.67770/0.70506. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.67773/0.70530. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67662/0.70547. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67620/0.70506. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67572/0.70458. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67636/0.70574. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67434/0.70604. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67312/0.70557. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67355/0.70626. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67321/0.70704. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67141/0.70683. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67111/0.70780. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67065/0.70759. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67125/0.70946. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67036/0.70890. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66912/0.70917. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66992/0.70999. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66870/0.71102. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66835/0.71149. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66563/0.71111. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66549/0.71263. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66494/0.71229. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66370/0.71398. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66330/0.71339. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66278/0.71477. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66239/0.71650. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66060/0.71615. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65962/0.71800. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65896/0.71669. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65667/0.71726. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65809/0.71874. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65536/0.71946. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.65213/0.71956. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65446/0.72029. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65329/0.72042. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65049/0.72051. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64924/0.72252. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64812/0.72327. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64649/0.72589. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64754/0.72539. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64663/0.72734. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64642/0.72778. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64089/0.72932. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63931/0.73162. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64068/0.72864. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63901/0.73435. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63739/0.73208. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63173/0.73235. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63469/0.73191. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62761/0.73234. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62447/0.73180. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62772/0.73616. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62773/0.73643. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62378/0.74234. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62173/0.74012. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61780/0.74003. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61994/0.74126. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61463/0.74263. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61568/0.74627. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60973/0.75003. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.61152/0.75024. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60592/0.75228. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60823/0.75062. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60027/0.75712. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60310/0.75579. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60014/0.76104. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60114/0.76257. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59490/0.76984. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59655/0.77110. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59431/0.77368. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59044/0.77705. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59094/0.78124. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59104/0.78292. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.58732/0.78311. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58280/0.78397. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58131/0.78607. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57650/0.79416. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57555/0.79259. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57496/0.79737. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57804/0.80778. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.56972/0.81031. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56814/0.80750. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56508/0.82157. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56483/0.81657. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56470/0.82252. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.56201/0.82510. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55910/0.83255. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55349/0.82810. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55967/0.83248. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54887/0.84223. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55119/0.84323. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.55744/0.84330. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55308/0.85456. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70108/0.69614. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.70030/0.69566. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69913/0.69460. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69713/0.69240. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69170/0.69017. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68658/0.68974. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68254/0.69042. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68109/0.69130. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.67972/0.69149. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67864/0.69129. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67846/0.69066. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67704/0.68969. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67749/0.68900. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.67565/0.68790. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67483/0.68710. Took 0.47 sec\n",
      "Epoch 15, Loss(train/val) 0.67301/0.68577. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67234/0.68458. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67180/0.68452. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67028/0.68316. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66934/0.68259. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66783/0.68160. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66671/0.68125. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66477/0.67974. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66260/0.67886. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.66256/0.67788. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66011/0.67631. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.65968/0.67625. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.65771/0.67575. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65659/0.67558. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65491/0.67338. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65207/0.67435. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65044/0.67272. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64970/0.67383. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64331/0.67241. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.64773/0.67143. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64191/0.67140. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.64308/0.67145. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.63930/0.66969. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63988/0.67141. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63542/0.67135. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63431/0.67137. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63232/0.66949. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.63410/0.67108. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63176/0.67371. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.63081/0.67276. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62649/0.67305. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62464/0.67579. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62427/0.67774. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62255/0.67873. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61777/0.68165. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.61709/0.68246. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61400/0.68193. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61599/0.68318. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61422/0.68620. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61411/0.68783. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60726/0.68576. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61119/0.68861. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60451/0.68876. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60523/0.68973. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60531/0.69475. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.60030/0.69192. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60311/0.69794. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59968/0.69711. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59615/0.69805. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59436/0.70204. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59510/0.70056. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59350/0.70764. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58871/0.70307. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58956/0.71489. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58492/0.71006. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58268/0.72118. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58377/0.72196. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57941/0.72591. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58181/0.72990. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57732/0.72508. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.57602/0.73047. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57205/0.73155. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56926/0.73946. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57102/0.74466. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57197/0.74661. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57022/0.74955. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56367/0.74937. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56007/0.75276. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55961/0.75281. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55369/0.76307. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55459/0.76442. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55390/0.76610. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55077/0.77040. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55482/0.76849. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54859/0.77860. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53868/0.78356. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54007/0.80035. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53793/0.79381. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.53994/0.80239. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53141/0.79770. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53734/0.80823. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53488/0.80380. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53586/0.81753. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53243/0.81937. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52142/0.81714. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.68794/0.68362. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68702/0.68189. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68564/0.68038. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68562/0.67901. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68422/0.67778. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68345/0.67694. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68342/0.67624. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68251/0.67572. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68295/0.67541. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68233/0.67503. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68157/0.67486. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68098/0.67477. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67994/0.67497. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68085/0.67482. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67890/0.67435. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.67884/0.67406. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.67744/0.67490. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67676/0.67460. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67665/0.67420. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67473/0.67423. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67382/0.67382. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67259/0.67463. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67302/0.67436. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67071/0.67410. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67136/0.67438. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66975/0.67408. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67052/0.67418. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66860/0.67402. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66562/0.67444. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66623/0.67446. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66579/0.67346. Took 0.47 sec\n",
      "Epoch 31, Loss(train/val) 0.66570/0.67307. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66415/0.67426. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66347/0.67307. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66289/0.67268. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66091/0.67241. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65910/0.67118. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65938/0.67077. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65667/0.67042. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65693/0.67023. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65643/0.66968. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65447/0.66913. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65273/0.66918. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65039/0.66821. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65131/0.66726. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65029/0.66760. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64877/0.66783. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64737/0.66650. Took 0.15 sec\n",
      "Epoch 48, Loss(train/val) 0.64605/0.66619. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.64254/0.66609. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63964/0.66513. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64010/0.66463. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64094/0.66496. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63848/0.66440. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63462/0.66435. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.63344/0.66458. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.63310/0.66378. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62971/0.66335. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62845/0.66306. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62567/0.66228. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.62599/0.66225. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.62375/0.66242. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62219/0.66330. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61864/0.66395. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61879/0.66420. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61307/0.66434. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61493/0.66531. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60925/0.66611. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60516/0.66496. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60598/0.66476. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.60443/0.66615. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59834/0.66798. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59534/0.66544. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59684/0.66654. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59164/0.66661. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59496/0.66763. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58872/0.66977. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58224/0.66796. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.58514/0.67143. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.57603/0.67060. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57319/0.67439. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57420/0.67122. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57079/0.67428. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56514/0.67543. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56829/0.67564. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56335/0.67360. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55892/0.67679. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56019/0.67828. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56091/0.67932. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55686/0.67920. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55476/0.67971. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54296/0.68275. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.54829/0.68213. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54695/0.68720. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.54073/0.68802. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53485/0.68619. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53345/0.68729. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53526/0.69103. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53308/0.68853. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52597/0.68732. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69030/0.69168. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68603/0.69251. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68559/0.69351. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68357/0.69429. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68282/0.69487. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68233/0.69545. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68129/0.69626. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68076/0.69672. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68076/0.69687. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68087/0.69718. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67938/0.69743. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67880/0.69801. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67859/0.69844. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67773/0.69887. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67905/0.69899. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67723/0.69923. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67575/0.69989. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67554/0.70067. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67466/0.70115. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67529/0.70141. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67500/0.70186. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67428/0.70191. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67348/0.70244. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67351/0.70271. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67341/0.70303. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67311/0.70341. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67038/0.70426. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67064/0.70508. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66931/0.70620. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66982/0.70643. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66777/0.70692. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66619/0.70724. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66599/0.70776. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66498/0.70885. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66250/0.70942. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66298/0.70980. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66143/0.71090. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66179/0.71070. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65928/0.71183. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65979/0.71409. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65691/0.71415. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65583/0.71390. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65516/0.71582. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65195/0.71647. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65456/0.71658. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65187/0.71629. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65066/0.71729. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64772/0.71870. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64715/0.71965. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64261/0.71900. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64431/0.72151. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64421/0.72231. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63890/0.72402. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63638/0.72289. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63759/0.72651. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63419/0.72292. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63237/0.72523. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63118/0.72511. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63290/0.72681. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62644/0.72765. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.62257/0.72767. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62252/0.72868. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62085/0.73062. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61977/0.72717. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61749/0.72998. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61593/0.73160. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61543/0.73657. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61140/0.73715. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60866/0.73811. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60948/0.74197. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60418/0.74004. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60484/0.73891. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.59825/0.74616. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60098/0.74423. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59714/0.74896. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.59518/0.74529. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59275/0.75279. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59325/0.74948. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59484/0.75348. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59222/0.75607. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58517/0.75996. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58220/0.76601. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58498/0.76113. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58184/0.76770. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57703/0.77158. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57795/0.77032. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.58116/0.77852. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57332/0.77357. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57431/0.77384. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56968/0.77982. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.56973/0.77926. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56449/0.78047. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56587/0.78593. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56231/0.80419. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55499/0.79819. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55659/0.81038. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55756/0.81001. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55476/0.81545. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55970/0.80560. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.54996/0.82082. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.68786/0.68415. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68515/0.68283. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68379/0.68203. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68302/0.68125. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68236/0.68046. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68148/0.67983. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68006/0.67949. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.67903/0.67907. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.67850/0.67876. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.67697/0.67865. Took 0.31 sec\n",
      "Epoch 10, Loss(train/val) 0.67695/0.67853. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.67621/0.67844. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67454/0.67863. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67432/0.67881. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67386/0.67897. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67280/0.67969. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67368/0.68052. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67193/0.68102. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.66984/0.68163. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66980/0.68233. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66731/0.68351. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66774/0.68444. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66674/0.68572. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66621/0.68670. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66241/0.68764. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66192/0.68992. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66223/0.69087. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65847/0.69246. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65768/0.69428. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65700/0.69601. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65585/0.69885. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65212/0.70155. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65266/0.70326. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65041/0.70558. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64640/0.70833. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64681/0.71022. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64737/0.71124. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64360/0.71342. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64233/0.71525. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64163/0.71698. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.63924/0.71929. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63984/0.72068. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63888/0.72210. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63302/0.72376. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63529/0.72625. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63288/0.72694. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63291/0.72749. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62913/0.72861. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62728/0.73092. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62781/0.73246. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62699/0.73456. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62131/0.73763. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62120/0.73948. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62108/0.74186. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61927/0.74408. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61720/0.74553. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61477/0.74648. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61825/0.74595. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61355/0.74929. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60865/0.75037. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61097/0.75221. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60936/0.75049. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60399/0.75269. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.60153/0.75700. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60688/0.75953. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59931/0.76023. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59930/0.76325. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59806/0.76471. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59672/0.76624. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.59673/0.77146. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59027/0.77112. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59155/0.77220. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58960/0.77450. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58310/0.77727. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58257/0.78229. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.57866/0.78709. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58235/0.78382. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57905/0.78630. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57609/0.78761. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57438/0.79158. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56459/0.79212. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56882/0.79882. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56719/0.80262. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57036/0.80559. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.56863/0.80713. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56422/0.81058. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56000/0.80796. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.56046/0.81582. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55931/0.81764. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54966/0.82100. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54833/0.81993. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54086/0.82697. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54506/0.83122. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54349/0.83441. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54989/0.82901. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53646/0.84154. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53560/0.84917. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53770/0.84008. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53138/0.85060. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53282/0.85729. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.68652/0.68831. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68415/0.68859. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68392/0.68879. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68280/0.68916. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68250/0.68948. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68213/0.68979. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68121/0.69025. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.67967/0.69073. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.67943/0.69119. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67827/0.69176. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.67877/0.69229. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67817/0.69277. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67764/0.69319. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67719/0.69335. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67615/0.69372. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67550/0.69420. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67569/0.69472. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67485/0.69517. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67480/0.69581. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67454/0.69624. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67359/0.69638. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67257/0.69654. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67130/0.69705. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67120/0.69746. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67144/0.69779. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67014/0.69836. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66942/0.69862. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66834/0.69870. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66818/0.69895. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66784/0.69885. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66712/0.69884. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66518/0.69948. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66508/0.69995. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66549/0.69967. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66403/0.69914. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66238/0.69970. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66182/0.70010. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65884/0.70003. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66058/0.69961. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65877/0.70023. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65513/0.69975. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65682/0.70001. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65205/0.70113. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65401/0.70126. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65275/0.70226. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65247/0.70159. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65273/0.70219. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65104/0.70247. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65051/0.70245. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64792/0.70303. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64870/0.70425. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64730/0.70495. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64714/0.70445. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64234/0.70422. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64452/0.70546. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64243/0.70517. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63996/0.70513. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63866/0.70567. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63703/0.70630. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.63591/0.70702. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63352/0.70829. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63666/0.70845. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63191/0.70874. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63218/0.70959. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62957/0.71105. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63000/0.71040. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62741/0.70948. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62584/0.71182. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62782/0.71035. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62864/0.71225. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62253/0.71292. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62353/0.71135. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61776/0.71299. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61832/0.71150. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62261/0.71250. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61499/0.71385. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61641/0.71653. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61433/0.71704. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61206/0.71657. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60979/0.71527. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61091/0.71535. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60669/0.71714. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60193/0.71769. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59822/0.71810. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59967/0.71780. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.59859/0.72129. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59381/0.72347. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59264/0.72350. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58852/0.72440. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59044/0.72387. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59074/0.72612. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59183/0.72628. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59106/0.72418. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57923/0.72589. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57897/0.72841. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57625/0.72956. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.57696/0.73078. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57363/0.73302. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57236/0.73008. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57393/0.73274. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69347/0.69305. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69208/0.69311. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.69305. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69210/0.69304. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69309. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69138/0.69320. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69210/0.69325. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69148/0.69332. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69124/0.69349. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69042/0.69352. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69009/0.69366. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69000/0.69403. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68937/0.69452. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68856/0.69516. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68900/0.69584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68847/0.69670. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68637/0.69746. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68834/0.69856. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68521/0.69955. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68706/0.70030. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68662/0.70085. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68590/0.70182. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68501/0.70274. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68516/0.70344. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68467/0.70437. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68360/0.70495. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68366/0.70494. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68325/0.70509. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68186/0.70593. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68210/0.70575. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68044/0.70591. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68115/0.70653. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68001/0.70699. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68051/0.70700. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67760/0.70815. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67761/0.70860. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67751/0.70859. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67725/0.70837. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67396/0.70868. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67626/0.70856. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67559/0.70952. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67397/0.70862. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67226/0.70855. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67293/0.70817. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67019/0.70885. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66800/0.70860. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66830/0.70805. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66845/0.70790. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66586/0.70769. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66518/0.70949. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66455/0.70906. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66280/0.70985. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66447/0.70885. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66142/0.70858. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65523/0.70887. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65775/0.71042. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65686/0.71148. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65371/0.71330. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65231/0.71424. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64582/0.71635. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65042/0.71556. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63908/0.71589. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64233/0.71712. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64054/0.72002. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63985/0.71990. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63691/0.72169. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63626/0.72217. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63296/0.72538. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62987/0.72730. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62815/0.72977. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62886/0.73092. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62651/0.73299. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62079/0.73501. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61954/0.73816. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62123/0.73534. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61618/0.74081. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60871/0.74410. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61347/0.74419. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61402/0.74588. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60531/0.74587. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60214/0.74782. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60155/0.75110. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60242/0.75373. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60111/0.75599. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59474/0.75735. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59794/0.76145. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59360/0.76159. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.58373/0.76725. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58265/0.76698. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58294/0.77392. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58403/0.77718. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57918/0.77931. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57607/0.78159. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57654/0.78247. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.56911/0.78986. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56916/0.78812. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57070/0.79014. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56731/0.79600. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56045/0.79032. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56399/0.79798. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69301. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.69422. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69253/0.69439. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69270/0.69424. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69135/0.69433. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69197/0.69422. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69233/0.69427. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69091/0.69391. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69095/0.69373. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69167/0.69403. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69014/0.69389. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69006/0.69403. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68899/0.69418. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68977/0.69356. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68853/0.69385. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68869/0.69393. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68811/0.69418. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68854/0.69455. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68712/0.69495. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68527/0.69521. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68525/0.69530. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68442/0.69525. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68465/0.69498. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68173/0.69668. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68252/0.69618. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68029/0.69559. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67982/0.69682. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67775/0.69804. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67712/0.69701. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67737/0.69698. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67392/0.69679. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67435/0.69805. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67358/0.69931. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67090/0.69931. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66874/0.69867. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66630/0.69853. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66264/0.70069. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66144/0.70033. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66091/0.70031. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65989/0.70156. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65562/0.70404. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65274/0.70860. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64992/0.71002. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64676/0.70802. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64581/0.70965. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64061/0.71424. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63946/0.71255. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63695/0.71478. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63108/0.71967. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63247/0.71964. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.62992/0.72111. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62631/0.72629. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62041/0.72680. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61931/0.72846. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61604/0.72772. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61456/0.73233. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61501/0.73427. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.61011/0.73244. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60524/0.73376. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60387/0.74518. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60024/0.74674. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59279/0.74996. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59265/0.75801. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59326/0.75390. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58899/0.75206. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57955/0.75703. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58278/0.77467. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58553/0.77223. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58070/0.77662. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57073/0.76314. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.57008/0.77830. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56781/0.78001. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56463/0.78338. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56562/0.78152. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56264/0.79039. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55076/0.79220. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54741/0.80038. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55256/0.79803. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.54430/0.81495. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54802/0.80428. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54734/0.80696. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53943/0.81846. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53774/0.82084. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53196/0.81965. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53771/0.81817. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.52232/0.83235. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.52166/0.83471. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52114/0.84586. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52368/0.85113. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51875/0.85824. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51536/0.86018. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51948/0.86482. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51819/0.86860. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50596/0.87454. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50838/0.86715. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50353/0.87040. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50093/0.87813. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.49833/0.89061. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49445/0.89496. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49737/0.89140. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69305/0.69299. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69314/0.69310. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69228/0.69327. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69283/0.69347. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69276/0.69366. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69248/0.69376. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69266/0.69392. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69200/0.69414. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69125/0.69437. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69082/0.69471. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69117/0.69509. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69089/0.69540. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69137/0.69575. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69023/0.69597. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69022/0.69639. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69016/0.69694. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68977/0.69721. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68854/0.69773. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68758/0.69834. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68825/0.69894. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69958. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68600/0.70015. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68500/0.70127. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68505/0.70267. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68510/0.70344. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68318/0.70461. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68270/0.70597. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68028/0.70725. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68104/0.70872. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67784/0.71132. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67647/0.71370. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67483/0.71467. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67486/0.71692. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67308/0.72032. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67093/0.72165. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67157/0.72494. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66755/0.72633. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66727/0.72946. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66293/0.73346. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66284/0.73414. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65986/0.73962. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65620/0.74307. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65507/0.74390. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65243/0.74754. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65040/0.75381. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64640/0.75746. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64295/0.76127. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.64084/0.76501. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63543/0.76824. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63466/0.76870. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62803/0.77602. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62553/0.78121. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62544/0.78174. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62203/0.78614. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61365/0.79347. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61714/0.79849. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61071/0.80343. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61077/0.80281. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60658/0.80988. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60243/0.81667. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60511/0.81668. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59713/0.82293. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.58842/0.82966. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58816/0.83300. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59019/0.84001. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58138/0.84021. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58260/0.84280. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57736/0.84448. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57310/0.85398. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56938/0.85415. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56314/0.85630. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.56025/0.86398. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56134/0.85499. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55655/0.87381. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54700/0.87150. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54796/0.87021. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54768/0.87937. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53955/0.88309. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54345/0.88225. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53475/0.88581. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.53337/0.90230. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52698/0.90514. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52646/0.90507. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51887/0.90994. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51192/0.90977. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51715/0.93266. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.51471/0.93258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.49733/0.92918. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50157/0.93600. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.49892/0.94426. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.48659/0.95983. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.49450/0.93847. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.47756/0.95853. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.46957/0.96031. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48045/0.98240. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.47223/0.96179. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.47419/0.97643. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.46027/0.98208. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.46127/0.99006. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.45724/0.98841. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69313/0.69240. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69281/0.69236. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69229/0.69234. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69207/0.69230. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69195/0.69224. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69165/0.69217. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69099/0.69219. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69067/0.69214. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69043/0.69212. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68988/0.69191. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69026/0.69189. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68988/0.69178. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68887/0.69172. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68756/0.69153. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68877/0.69145. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68751/0.69129. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68735/0.69123. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68651/0.69145. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68637/0.69179. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68586/0.69148. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68601/0.69163. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68443/0.69099. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68304/0.69106. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68398/0.69103. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68313/0.69100. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68300/0.69104. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68076/0.69165. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67953/0.69203. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68091/0.69220. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68181/0.69202. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68095/0.69226. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67818/0.69259. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67951/0.69351. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67722/0.69394. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67786/0.69475. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67692/0.69499. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67542/0.69431. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67598/0.69443. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67591/0.69506. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67381/0.69602. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67233/0.69624. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67254/0.69732. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67243/0.69737. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67139/0.69621. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67185/0.69716. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67166/0.69739. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66705/0.69822. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66736/0.69932. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66906/0.69908. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66664/0.70089. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66494/0.70219. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66463/0.70341. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66070/0.70486. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66382/0.70499. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65880/0.70652. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65920/0.70624. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65870/0.70797. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65803/0.70978. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65654/0.71145. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65887/0.71161. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65692/0.71266. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65337/0.71376. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65187/0.71398. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65570/0.71497. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65199/0.71716. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64966/0.71639. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65000/0.71706. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64883/0.71861. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64843/0.71946. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64540/0.72275. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64842/0.72310. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64538/0.72312. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64252/0.72743. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64348/0.73095. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63978/0.73204. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64022/0.73243. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63805/0.73152. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63724/0.73060. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63539/0.73520. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63279/0.73611. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63416/0.73757. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63518/0.73891. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63091/0.73864. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63351/0.73690. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.62515/0.73897. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62611/0.74300. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62238/0.74126. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62293/0.74501. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62163/0.74906. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61560/0.74775. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62317/0.74949. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61936/0.75212. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61760/0.75177. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61462/0.75598. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61127/0.75442. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60861/0.75579. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61114/0.75904. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60719/0.75940. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60270/0.76289. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60284/0.76320. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69237/0.70145. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.70136. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69267/0.70102. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69206/0.70107. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69206/0.70105. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69207/0.70130. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69192/0.70139. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69070/0.70121. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69065/0.70136. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69124/0.70213. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69031/0.70258. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68955/0.70333. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69036/0.70385. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68929/0.70464. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68882/0.70535. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68835/0.70666. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68847/0.70762. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68807/0.70693. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68796/0.70723. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68692/0.70852. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68650/0.70928. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68630/0.70993. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68470/0.70972. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.71168. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68327/0.71214. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68329/0.71143. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68344/0.71228. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68165/0.71213. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68225/0.71397. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67991/0.71512. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67931/0.71410. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67723/0.71577. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67904/0.71609. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67697/0.71707. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67631/0.71704. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67484/0.71988. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67376/0.71953. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67296/0.72266. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67350/0.72265. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.66863/0.72321. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66896/0.72588. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66701/0.72952. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66856/0.72753. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66638/0.72997. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66592/0.73208. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66346/0.73221. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66210/0.73662. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66123/0.73889. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66247/0.74182. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66115/0.74248. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65823/0.74196. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65883/0.74875. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65706/0.74850. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65493/0.74605. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65265/0.74916. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65002/0.75716. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65058/0.75608. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64891/0.75602. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64953/0.75761. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64785/0.76161. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64329/0.76624. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64436/0.77033. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64380/0.76974. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63786/0.77268. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64114/0.77618. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63418/0.77974. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63567/0.77997. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63318/0.78246. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63056/0.78694. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63069/0.78911. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63015/0.79186. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62584/0.79019. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62550/0.79284. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62638/0.79270. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61883/0.80455. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61506/0.80459. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61964/0.80578. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61365/0.80691. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61426/0.81610. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60884/0.82223. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61277/0.81794. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60786/0.82229. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60201/0.82596. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60453/0.82787. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60374/0.83167. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60049/0.83537. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59742/0.83420. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59527/0.83528. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59623/0.83804. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58547/0.84904. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.58864/0.84639. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58696/0.85618. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58518/0.85707. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58575/0.86353. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58092/0.87160. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57662/0.86273. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57144/0.88126. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57779/0.88198. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56671/0.88614. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56779/0.89301. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69393/0.68800. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69364/0.68797. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69318/0.68835. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69322/0.68879. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69296/0.68931. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69190/0.69017. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69180/0.69090. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69167/0.69184. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69116/0.69268. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69055/0.69349. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68973/0.69417. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68927/0.69426. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68784/0.69500. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68743/0.69641. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68704/0.69741. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68591/0.69753. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68421/0.69975. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68315/0.70178. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68310/0.70185. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68155/0.70394. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68044/0.70568. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67947/0.70549. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67800/0.70670. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67604/0.70945. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67481/0.71037. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67579/0.71044. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67248/0.71062. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67105/0.71335. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67075/0.71524. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66662/0.71606. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66553/0.71669. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66457/0.71741. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66437/0.71858. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66257/0.71982. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66066/0.72080. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65686/0.71868. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65668/0.72079. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65644/0.72163. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65357/0.72256. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64981/0.72355. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65044/0.72504. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65079/0.72612. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.64486/0.72494. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64352/0.72735. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64085/0.72998. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64240/0.73257. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63880/0.73047. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63919/0.73276. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63783/0.73203. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63459/0.73506. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63300/0.73638. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63175/0.73773. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63034/0.73939. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62911/0.74244. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62819/0.74415. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62340/0.74728. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61953/0.74812. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62067/0.75145. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62245/0.74932. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61595/0.75301. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61739/0.75395. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61175/0.75352. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61464/0.75611. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61228/0.75673. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61402/0.75737. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60964/0.76476. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60887/0.76364. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60610/0.76356. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60080/0.76302. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.60436/0.76923. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60010/0.77060. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59741/0.76976. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59525/0.77433. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59087/0.77392. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59455/0.77960. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59490/0.77796. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59264/0.78464. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.58527/0.77778. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58730/0.78277. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58741/0.78556. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58431/0.78641. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58107/0.79179. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57959/0.79241. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.58028/0.79336. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57904/0.79096. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58015/0.80030. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57082/0.80110. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57025/0.80461. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57466/0.79950. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57153/0.80306. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56855/0.80267. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56226/0.80672. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56425/0.81239. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56329/0.80934. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.55993/0.81204. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55462/0.82207. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55622/0.82078. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56038/0.82038. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55659/0.82091. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55163/0.82354. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69467/0.69553. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69235/0.69583. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69627. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69108/0.69668. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69061/0.69724. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68972/0.69769. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68988/0.69834. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68955/0.69905. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68869/0.69973. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68826/0.70040. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68753/0.70133. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68537/0.70246. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68641/0.70305. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68464/0.70379. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68417/0.70441. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68458/0.70484. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68394/0.70522. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68162/0.70581. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68134/0.70607. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68187/0.70630. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67907/0.70654. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67969/0.70671. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67884/0.70658. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67870/0.70643. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67537/0.70661. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67562/0.70677. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67383/0.70703. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67224/0.70663. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67214/0.70570. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66922/0.70507. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66749/0.70512. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66663/0.70526. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66360/0.70625. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66259/0.70561. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66217/0.70592. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65948/0.70604. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65951/0.70600. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65691/0.70657. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65477/0.70757. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65155/0.70886. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65120/0.70926. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65055/0.70977. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.64730/0.71120. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64477/0.71189. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64443/0.71251. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64248/0.71305. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64144/0.71317. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63703/0.71493. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63743/0.71803. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63437/0.71924. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63334/0.72082. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63010/0.72310. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63119/0.72492. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62698/0.72522. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62266/0.72535. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62631/0.72546. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62119/0.72933. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62088/0.73102. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61818/0.73296. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61465/0.73588. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61564/0.73412. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61265/0.73706. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60911/0.73843. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60838/0.74146. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61011/0.74112. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60291/0.74418. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60471/0.74451. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60078/0.74413. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60253/0.74762. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60179/0.74788. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59522/0.75062. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59104/0.74920. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59199/0.75433. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58806/0.75405. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58630/0.75592. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58669/0.75674. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58152/0.76166. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58064/0.76227. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58010/0.76467. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57950/0.76646. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57341/0.76394. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57432/0.76893. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57303/0.77223. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56536/0.77446. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56359/0.77553. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56659/0.78004. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55701/0.78122. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56079/0.78467. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55485/0.78612. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55581/0.79295. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54585/0.79327. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54694/0.79339. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54253/0.79813. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54422/0.80461. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.54984/0.80684. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53973/0.80569. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53806/0.81023. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53908/0.81434. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53105/0.81890. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52652/0.82143. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69352/0.69435. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.69415. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69291/0.69400. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69260/0.69390. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69236/0.69379. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69210/0.69368. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69181/0.69365. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69152/0.69368. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69381. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69130/0.69409. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69071/0.69440. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69100/0.69475. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69039/0.69520. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68943/0.69603. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68885/0.69713. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68849/0.69812. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68783/0.69890. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68739/0.69991. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68592/0.70112. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68628/0.70220. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68569/0.70302. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68432/0.70448. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68375/0.70574. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68322/0.70693. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68219/0.70836. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68095/0.70983. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68024/0.71101. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67983/0.71219. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67884/0.71367. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67807/0.71510. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67815/0.71611. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67691/0.71780. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67566/0.71887. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67547/0.71923. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67484/0.72014. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67424/0.72165. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67282/0.72177. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67268/0.72370. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67095/0.72451. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66972/0.72552. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66606/0.72703. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66649/0.72852. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66424/0.72956. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66251/0.73128. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66202/0.73212. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65927/0.73367. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65967/0.73478. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65846/0.73634. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65494/0.73653. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65555/0.73726. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65312/0.73934. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65514/0.74100. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65523/0.73946. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65252/0.74203. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64884/0.74208. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64731/0.74143. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64609/0.74188. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64668/0.74323. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64050/0.74503. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64287/0.74546. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64306/0.74496. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64053/0.74689. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63829/0.74749. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63662/0.74639. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63866/0.74685. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63616/0.74683. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63536/0.74703. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63068/0.74724. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63311/0.74798. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63311/0.74772. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62777/0.74761. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62907/0.74664. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62871/0.74632. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62381/0.75095. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62094/0.75098. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62166/0.75139. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61834/0.75485. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61725/0.75363. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61550/0.75415. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61643/0.75334. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61121/0.75558. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60917/0.75518. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61231/0.75436. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60562/0.75659. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60795/0.75664. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60161/0.75875. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59513/0.76064. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59939/0.76096. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59667/0.76211. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59527/0.76180. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59764/0.76601. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59276/0.76623. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58926/0.76762. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58901/0.76820. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58269/0.77186. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58109/0.77273. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57991/0.77462. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57831/0.77569. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57280/0.77622. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57061/0.78284. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69551/0.68961. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69177/0.68763. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69011/0.68592. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68860/0.68444. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68668/0.68282. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.68683/0.68041. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68641/0.67876. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68441/0.67694. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68453/0.67547. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68239/0.67461. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68296/0.67355. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68236/0.67263. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68213/0.67188. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67999/0.67105. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68016/0.67122. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68009/0.67029. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67899/0.67050. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67604/0.66947. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67657/0.67069. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67441/0.67110. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67578/0.67045. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67300/0.67122. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67084/0.67250. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66969/0.67206. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66912/0.67293. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66611/0.67385. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66610/0.67432. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.66364/0.67483. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66190/0.67656. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65960/0.67770. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66164/0.67661. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65561/0.67588. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65586/0.67617. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65159/0.67578. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65044/0.67712. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65061/0.67623. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64820/0.67662. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64768/0.67681. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64308/0.67551. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64274/0.67835. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64453/0.67820. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64173/0.67832. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63551/0.67923. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.63471/0.67901. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63693/0.67889. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63386/0.68023. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63272/0.68051. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62807/0.68021. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.62897/0.68240. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62700/0.68330. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62475/0.68002. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62692/0.68350. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62259/0.68359. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62065/0.68262. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62054/0.68424. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61665/0.68511. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61347/0.68628. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61244/0.68797. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61242/0.68932. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60949/0.68852. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60562/0.68888. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61188/0.68948. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60667/0.69018. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60283/0.69208. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60960/0.69340. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60523/0.69089. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60328/0.69357. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60051/0.69571. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59820/0.69474. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59680/0.69320. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59556/0.69590. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59042/0.69502. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.59184/0.69448. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58829/0.69933. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.58570/0.69959. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58694/0.70378. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58461/0.69956. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58390/0.69901. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58502/0.70191. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57804/0.71279. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.57665/0.70516. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57231/0.70819. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57139/0.70379. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.56878/0.70695. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57029/0.70757. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56459/0.70476. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56598/0.71269. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55914/0.71315. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56067/0.71519. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55646/0.71265. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55339/0.71205. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56094/0.71730. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55089/0.72094. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55279/0.72224. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55396/0.72444. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54920/0.72976. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54407/0.72695. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53885/0.73281. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54380/0.72628. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54131/0.73577. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69402/0.69046. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69315/0.69111. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69245/0.69166. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.69213. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69208/0.69239. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69149/0.69260. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69283. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69093/0.69281. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69034/0.69292. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69002/0.69299. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69034/0.69321. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68927/0.69341. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68908/0.69354. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68848/0.69400. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68873/0.69406. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68870/0.69407. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68866/0.69438. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68720/0.69474. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68729/0.69514. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68608/0.69533. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68631/0.69559. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68619/0.69612. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68542/0.69661. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68479/0.69710. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68384/0.69735. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68421/0.69787. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68381/0.69796. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68396/0.69847. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68192/0.69911. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68232/0.69941. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68182/0.70008. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68211/0.69966. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68060/0.69994. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68066/0.70054. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67875/0.70063. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67977/0.70024. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67928/0.70050. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67681/0.70067. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67632/0.70108. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67691/0.70101. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67500/0.70121. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67438/0.70119. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67484/0.70155. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67387/0.70081. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67396/0.70185. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67312/0.70109. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67162/0.70188. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67122/0.70063. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66925/0.70262. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66897/0.70200. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66900/0.70208. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66674/0.70255. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66736/0.70203. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66576/0.70163. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66342/0.70202. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66406/0.70256. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66186/0.70264. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65856/0.70434. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65827/0.70380. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65925/0.70488. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65580/0.70688. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65819/0.70514. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65501/0.70538. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65357/0.70601. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65211/0.70782. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65317/0.70739. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65093/0.70676. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65075/0.70681. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64662/0.70847. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64487/0.71120. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64549/0.71051. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.63893/0.71241. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63923/0.71230. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64007/0.71458. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63631/0.71730. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63082/0.71763. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63575/0.71651. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.63068/0.71650. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63332/0.71998. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62961/0.72633. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62478/0.72272. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62482/0.71938. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62059/0.72561. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.62472/0.72938. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62049/0.72960. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61662/0.72946. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61693/0.73045. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61713/0.73855. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61429/0.73595. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61146/0.73581. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60935/0.73811. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60902/0.73716. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.60444/0.73889. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60489/0.74145. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60658/0.73972. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60276/0.74031. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60158/0.74281. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59800/0.74783. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59536/0.74577. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59509/0.75045. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69588/0.70018. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69132/0.69987. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69069/0.70035. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68990/0.70120. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.70219. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68808/0.70326. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68674/0.70453. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68613/0.70572. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68435/0.70715. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68308/0.70874. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68302/0.71046. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68184/0.71172. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68015/0.71284. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.67859/0.71399. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67786/0.71506. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67737/0.71578. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67651/0.71621. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67327/0.71651. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67223/0.71736. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66957/0.71669. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66744/0.71692. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66602/0.71623. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66407/0.71410. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66457/0.71345. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66169/0.71292. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66059/0.71065. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65946/0.70941. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65397/0.70857. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65504/0.70805. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65055/0.70552. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65110/0.70253. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64774/0.69895. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65086/0.69996. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64462/0.69611. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64694/0.69759. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64247/0.69431. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.64136/0.69360. Took 0.12 sec\n",
      "Epoch 37, Loss(train/val) 0.64003/0.69056. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.63705/0.69005. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63268/0.68891. Took 0.15 sec\n",
      "Epoch 40, Loss(train/val) 0.63411/0.68700. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.63138/0.68493. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63209/0.68300. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.62786/0.68053. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.62729/0.68011. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.62583/0.67951. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.62233/0.68033. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62176/0.67671. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61430/0.67793. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61547/0.67831. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61608/0.67723. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61600/0.67748. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.61389/0.67643. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61099/0.67381. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60972/0.67158. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.60421/0.67299. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60113/0.67148. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60134/0.66686. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.59961/0.66647. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.59970/0.66646. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.59586/0.66640. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.59145/0.66710. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59287/0.66755. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58726/0.66640. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.59057/0.66586. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58177/0.66700. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58322/0.66455. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.57429/0.66380. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57686/0.66249. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.57365/0.66531. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56832/0.66559. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56879/0.66425. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56154/0.66746. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.56125/0.66899. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56826/0.66616. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56295/0.67066. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55421/0.66933. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54626/0.66876. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54818/0.67190. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55031/0.67017. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54264/0.67594. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53820/0.67799. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.54101/0.67741. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53335/0.67737. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53563/0.67594. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53630/0.67558. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52978/0.67828. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52716/0.67799. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51915/0.68264. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51704/0.69177. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52187/0.68834. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.51059/0.69079. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50970/0.68680. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51518/0.69059. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.50341/0.69352. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50189/0.69215. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.50349/0.69475. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50283/0.69234. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49211/0.69362. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.48954/0.69306. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69581/0.69222. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69427/0.69219. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69368/0.69217. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69374/0.69214. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69268/0.69218. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69232. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69182/0.69257. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68999/0.69274. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68993/0.69295. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68996/0.69285. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68930/0.69283. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68767/0.69305. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68735/0.69272. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68551/0.69315. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68530/0.69291. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68273/0.69337. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68293/0.69314. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67968/0.69320. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67824/0.69369. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67668/0.69330. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67495/0.69575. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67216/0.69570. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66911/0.69653. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66847/0.69685. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66607/0.69933. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66293/0.70083. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66095/0.70246. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66115/0.70340. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65667/0.70644. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65691/0.70791. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65346/0.70818. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65155/0.71116. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64828/0.71124. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64895/0.71376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64814/0.71680. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.64323/0.71926. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64315/0.72233. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64143/0.72412. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63847/0.72387. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63404/0.72923. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63826/0.72915. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63391/0.73232. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63199/0.73561. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.63181/0.73715. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62865/0.73611. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62681/0.73967. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62533/0.74071. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62420/0.74412. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61873/0.74323. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.62168/0.74656. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61727/0.75105. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.61618/0.74926. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61351/0.74986. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60966/0.75414. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60670/0.75485. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60746/0.75919. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59936/0.75959. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59877/0.76404. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.60203/0.76911. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60064/0.76739. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59408/0.76897. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.59679/0.77110. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59095/0.77414. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58962/0.77923. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58702/0.78145. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58212/0.78529. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58118/0.78543. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58324/0.79018. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57790/0.79369. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57246/0.79448. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.57391/0.79579. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57398/0.79254. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56350/0.80841. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56165/0.81173. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55996/0.81261. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55531/0.81359. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.55297/0.82543. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54993/0.82430. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55510/0.82235. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55009/0.82771. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54754/0.83138. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54614/0.83187. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54082/0.83603. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53684/0.84189. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53604/0.83477. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.52946/0.85020. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52992/0.84450. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52880/0.85617. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.52372/0.85333. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52187/0.85829. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51176/0.86823. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.50994/0.88346. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51281/0.88235. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50491/0.87793. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.50372/0.88797. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49780/0.90099. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49896/0.90248. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49041/0.89347. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.48761/0.90745. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.48523/0.91417. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69531/0.69985. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69316/0.69959. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69187/0.69871. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.69885. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69088/0.69855. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69108/0.69771. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69098/0.69760. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68984/0.69715. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68976/0.69636. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68943/0.69657. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68837/0.69648. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68890/0.69555. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68785/0.69604. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68856/0.69511. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68697/0.69596. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68620/0.69556. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68641/0.69512. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68585/0.69606. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68600/0.69497. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68503/0.69586. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68373/0.69658. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68416/0.69633. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68307/0.69653. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68313/0.69763. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68307/0.69789. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68349/0.69767. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68222/0.69876. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68098/0.69825. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67879/0.69893. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67940/0.70119. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67946/0.70036. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67638/0.70120. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67718/0.70054. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67491/0.70187. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67415/0.70316. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67464/0.70185. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67399/0.70432. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67281/0.70336. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67194/0.70437. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67046/0.70805. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66914/0.70502. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66930/0.70693. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66771/0.70786. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66571/0.70962. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66500/0.71058. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66116/0.71031. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65848/0.71010. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65934/0.71279. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65907/0.71495. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65722/0.71363. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65672/0.71594. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65508/0.71624. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65454/0.71649. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65128/0.71988. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64927/0.71930. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65157/0.71879. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64671/0.72121. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64709/0.72253. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64493/0.72455. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64177/0.72255. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64190/0.72366. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64224/0.72184. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63559/0.72903. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63620/0.72937. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63418/0.72925. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63238/0.73068. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63047/0.73118. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62716/0.73000. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63073/0.73207. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62380/0.73257. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62230/0.73357. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61945/0.73379. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61881/0.73707. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61303/0.73774. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61161/0.74122. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.61420/0.73869. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61161/0.74296. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60970/0.74459. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60316/0.74869. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59921/0.75136. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60510/0.74752. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60337/0.75216. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60104/0.74976. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59732/0.75272. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.59278/0.75711. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59232/0.76003. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59642/0.76150. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.58395/0.76188. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58390/0.76473. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58347/0.76485. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.57605/0.76891. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57808/0.77327. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57616/0.77374. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57727/0.77680. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56451/0.78371. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57072/0.78407. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56188/0.78384. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56371/0.79014. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55847/0.79004. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56237/0.79542. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69392/0.69465. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69261/0.69329. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69133/0.69199. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69039/0.69066. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69057/0.68967. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68995/0.68873. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68895/0.68795. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68918/0.68741. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68804/0.68684. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68876/0.68626. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68796/0.68578. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68814/0.68545. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68765/0.68490. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68727/0.68446. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68783/0.68404. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68711/0.68397. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68682/0.68369. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68629/0.68345. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68596/0.68316. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68582/0.68316. Took 0.37 sec\n",
      "Epoch 20, Loss(train/val) 0.68570/0.68282. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68529/0.68267. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68409/0.68233. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68397/0.68198. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68437/0.68175. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68328/0.68163. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68303/0.68177. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68324/0.68208. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68297/0.68149. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68154/0.68123. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68266/0.68102. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68201/0.68069. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68127/0.68076. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67997/0.68056. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68005/0.68064. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67959/0.68052. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67864/0.68082. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67800/0.68053. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67716/0.68046. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67855/0.68025. Took 0.12 sec\n",
      "Epoch 40, Loss(train/val) 0.67672/0.68047. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67600/0.68067. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67573/0.68066. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67425/0.68011. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67400/0.68086. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67188/0.68150. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67174/0.68190. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67232/0.68185. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66970/0.68269. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66970/0.68324. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66831/0.68334. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66796/0.68330. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66760/0.68334. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66678/0.68363. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66599/0.68472. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66474/0.68481. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66524/0.68475. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66174/0.68595. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66230/0.68672. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66297/0.68676. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66216/0.68725. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66028/0.68824. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66036/0.68838. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65722/0.68906. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65770/0.68926. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65622/0.68958. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65365/0.69064. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65571/0.69082. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65433/0.69190. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65194/0.69209. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65272/0.69382. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64819/0.69403. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65183/0.69507. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64672/0.69717. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64747/0.69848. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64622/0.69806. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64469/0.69838. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64229/0.69877. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64116/0.69930. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64213/0.70124. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64333/0.70201. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63864/0.70366. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64063/0.70264. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63695/0.70391. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63147/0.70504. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63697/0.70604. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63219/0.70605. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63307/0.70731. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63047/0.70976. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62754/0.70955. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62811/0.70970. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62742/0.71192. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62305/0.71266. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62254/0.71511. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61772/0.71663. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61991/0.71824. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61894/0.72129. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61658/0.72458. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61673/0.72358. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61506/0.72705. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69520/0.69181. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.69167. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69193/0.69188. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69218. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69106/0.69247. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68989/0.69278. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68981/0.69303. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68966/0.69332. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68788/0.69350. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68741/0.69409. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68687/0.69487. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68599/0.69555. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68388/0.69672. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68411/0.69750. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68275/0.69836. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68214/0.69899. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67924/0.70034. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67899/0.70145. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67884/0.70267. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67592/0.70349. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67662/0.70480. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67400/0.70610. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67153/0.70695. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67142/0.70871. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67032/0.71046. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66850/0.71133. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66639/0.71186. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66295/0.71313. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66083/0.71494. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65780/0.71551. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65853/0.71703. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65862/0.71914. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65621/0.72067. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65363/0.72126. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64995/0.72397. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64762/0.72454. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64796/0.72312. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64660/0.72492. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64323/0.72646. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64193/0.72698. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.64238/0.72999. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63758/0.73092. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63781/0.73092. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63414/0.73069. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62876/0.73246. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62758/0.73054. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62343/0.73430. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62578/0.73335. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62113/0.73459. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62127/0.73355. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61792/0.73481. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61916/0.73660. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.61140/0.73636. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61224/0.73607. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60792/0.73664. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60889/0.73726. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60313/0.73675. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60327/0.73799. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60076/0.73564. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59541/0.73212. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59904/0.73729. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.59742/0.73514. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59360/0.73342. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59098/0.73452. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58507/0.73138. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58443/0.74008. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58093/0.73508. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58154/0.73091. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58138/0.72948. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.57888/0.73503. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57758/0.73302. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56726/0.73927. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56850/0.73510. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56665/0.73436. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56751/0.73503. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55555/0.73686. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.56145/0.73585. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56235/0.73731. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55954/0.73639. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55770/0.73454. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55237/0.73028. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54561/0.73052. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54707/0.73379. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54672/0.72844. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54086/0.72945. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.54043/0.73182. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53879/0.73313. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53553/0.72903. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53487/0.72802. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52563/0.73045. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52606/0.73259. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.52234/0.73114. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51756/0.73300. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51860/0.73721. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50975/0.72761. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50610/0.73574. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51317/0.73494. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50516/0.73722. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49691/0.73724. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50219/0.73775. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69367/0.69281. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69292/0.69238. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69190. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69229/0.69167. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.69118. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69084/0.69092. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69098/0.69067. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.69071. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69002/0.69113. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69007/0.69165. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68919/0.69235. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68823/0.69282. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68688/0.69384. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68643/0.69493. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68577/0.69620. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68532/0.69736. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68361/0.69844. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68181/0.70024. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68033/0.70203. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68053/0.70235. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67945/0.70449. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67726/0.70543. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67576/0.70665. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67474/0.70670. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67468/0.70708. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67242/0.70720. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67032/0.70772. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66700/0.70823. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66599/0.70797. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66507/0.70790. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66440/0.70893. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66427/0.70932. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66064/0.71039. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65995/0.71140. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65812/0.71206. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65555/0.71218. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65523/0.71332. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65320/0.71483. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65136/0.71698. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65067/0.71655. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64766/0.71816. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64538/0.72078. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64127/0.72293. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64025/0.72265. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63772/0.72398. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63594/0.72449. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63627/0.72553. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63282/0.72491. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63492/0.72493. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63089/0.72522. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63127/0.72540. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62580/0.72782. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62904/0.72672. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62412/0.72784. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62039/0.72767. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62026/0.72769. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62134/0.72641. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61780/0.72913. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61289/0.73311. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61264/0.73482. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.61225/0.73290. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61008/0.73147. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60510/0.73121. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60942/0.73251. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60382/0.73419. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.60254/0.73557. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59519/0.73981. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59777/0.74010. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59282/0.74171. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59173/0.74160. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58842/0.74585. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58996/0.74748. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58078/0.74909. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58163/0.75080. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.57859/0.75126. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58174/0.75410. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57469/0.75505. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57433/0.75198. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57163/0.75753. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56899/0.75914. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.56825/0.76297. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56441/0.76700. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55801/0.76662. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55763/0.77356. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55311/0.77221. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55811/0.77717. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54863/0.77442. Took 0.17 sec\n",
      "Epoch 87, Loss(train/val) 0.54679/0.78085. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54271/0.79301. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54806/0.79085. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54076/0.79345. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.54855/0.79272. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53656/0.79855. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.53551/0.79325. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53751/0.79327. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53404/0.80120. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51719/0.80808. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52236/0.81240. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51610/0.81604. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52009/0.81370. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69339/0.70070. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69151/0.70025. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69021/0.70106. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68955/0.70201. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68871/0.70241. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68859/0.70331. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68793/0.70373. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68825/0.70445. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68672/0.70463. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68651/0.70524. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68612/0.70491. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68644/0.70497. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68618/0.70527. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68525/0.70498. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68451/0.70535. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68467/0.70573. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68411/0.70483. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68397/0.70537. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68332/0.70540. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68279/0.70539. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68192/0.70539. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68229/0.70584. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68038/0.70601. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68141/0.70607. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68022/0.70666. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68091/0.70709. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67979/0.70694. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68017/0.70757. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67944/0.70746. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67810/0.70836. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67871/0.70918. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67858/0.71004. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67721/0.70871. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67723/0.70875. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67617/0.70996. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67445/0.71031. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67532/0.71191. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67380/0.71210. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67226/0.71359. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67272/0.71363. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67350/0.71468. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66930/0.71576. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67034/0.71575. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67049/0.71639. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66935/0.71802. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66983/0.71838. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66888/0.71841. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66764/0.71922. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66679/0.71964. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66521/0.72075. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66329/0.72156. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66430/0.72293. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66355/0.72391. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66122/0.72553. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66024/0.72541. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65840/0.72648. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66047/0.72724. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65840/0.72844. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65546/0.72918. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65638/0.73099. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65503/0.73223. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65539/0.73324. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65375/0.73374. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65210/0.73506. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65356/0.73555. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65176/0.73577. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65120/0.73827. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65111/0.73876. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64613/0.74099. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64870/0.74231. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64759/0.74382. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64451/0.74454. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64397/0.74621. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64038/0.74856. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64494/0.75033. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64031/0.75002. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64045/0.75158. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63889/0.75507. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63435/0.75657. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63708/0.75870. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63696/0.75907. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63290/0.76055. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63129/0.76212. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63110/0.76487. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62947/0.76788. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62669/0.76955. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62660/0.77202. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62606/0.77473. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62558/0.77282. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62677/0.77368. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62066/0.78080. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62404/0.77997. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62235/0.78118. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62030/0.78410. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61403/0.78671. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61468/0.78847. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61657/0.78981. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61224/0.79024. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60961/0.79148. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60750/0.79697. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69429/0.70017. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.70133. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69211/0.70112. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69099/0.70114. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69084/0.70141. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69030/0.70165. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69035/0.70164. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69002/0.70148. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68918/0.70199. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.70240. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.70232. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68797/0.70249. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68719/0.70257. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68739/0.70297. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68711/0.70344. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68761/0.70315. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68649/0.70309. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68656/0.70355. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68634/0.70323. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68519/0.70325. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68513/0.70325. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68429/0.70400. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68509/0.70369. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68425/0.70349. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68406/0.70311. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68344/0.70341. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68244/0.70412. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68281/0.70351. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68100/0.70390. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68174/0.70447. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68167/0.70352. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68008/0.70419. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68034/0.70399. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67844/0.70437. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67898/0.70500. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67942/0.70446. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67788/0.70508. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67827/0.70483. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67702/0.70615. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67523/0.70556. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67572/0.70684. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67441/0.70773. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67472/0.70784. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67326/0.70788. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67259/0.70819. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67248/0.70989. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67081/0.71091. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67185/0.71057. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66952/0.71148. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66943/0.71221. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66827/0.71329. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66747/0.71324. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66589/0.71468. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66736/0.71501. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66715/0.71529. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66635/0.71529. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66551/0.71594. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66258/0.71621. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66367/0.71761. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66257/0.71788. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66460/0.71835. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66014/0.71939. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66242/0.72001. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65928/0.72103. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66132/0.72353. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66047/0.72183. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65702/0.72470. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65733/0.72506. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65627/0.72602. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65832/0.72772. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65322/0.72881. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65638/0.72999. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65372/0.72899. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65235/0.73104. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65538/0.73117. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64959/0.73135. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65010/0.73419. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64913/0.73375. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64860/0.73467. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64703/0.73685. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64672/0.74051. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64684/0.74008. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64483/0.73950. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64343/0.74188. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64578/0.74469. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64161/0.74637. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64020/0.74757. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64200/0.74880. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64083/0.74893. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63654/0.74825. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63822/0.75080. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63612/0.75151. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63326/0.75378. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63390/0.75603. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63431/0.75639. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63142/0.76138. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62804/0.76463. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62640/0.76283. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62706/0.76552. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62683/0.76598. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69302/0.69117. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69232/0.69034. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69222/0.69001. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69194/0.68948. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.68905. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69111/0.68870. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69058/0.68821. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69105/0.68771. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69062/0.68698. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.68619. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68959/0.68565. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.68505. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68695/0.68457. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68858/0.68424. Took 0.43 sec\n",
      "Epoch 14, Loss(train/val) 0.68774/0.68376. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68814/0.68335. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68593/0.68293. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68596/0.68267. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68575/0.68219. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68494/0.68177. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68549/0.68144. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68375/0.68117. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68464/0.68098. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68289/0.68115. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68229/0.68132. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68264/0.68109. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68212/0.68106. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68086/0.68130. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67992/0.68142. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68088/0.68088. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67958/0.68135. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67814/0.68153. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67821/0.68139. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68070/0.68134. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67845/0.68121. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67558/0.68150. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67756/0.68152. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67505/0.68164. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67359/0.68101. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67136/0.68100. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67189/0.68109. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67306/0.68200. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67057/0.68222. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66843/0.68199. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66882/0.68268. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66720/0.68388. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66444/0.68374. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66591/0.68422. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66738/0.68456. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66352/0.68457. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66088/0.68602. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66229/0.68737. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66060/0.68825. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65835/0.68839. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66054/0.68849. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65466/0.68882. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65684/0.68969. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65125/0.69138. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65241/0.69197. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65035/0.69229. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64902/0.69352. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65068/0.69312. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64560/0.69463. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64495/0.69613. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64057/0.70000. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64076/0.70240. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64124/0.70398. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63848/0.70386. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63857/0.70474. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63563/0.70540. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63126/0.70718. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63339/0.70680. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63010/0.70766. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63292/0.70821. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62998/0.70908. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62852/0.71169. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62463/0.71172. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62734/0.71140. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61748/0.71421. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62128/0.71562. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62189/0.71554. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61809/0.71767. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61374/0.71991. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61277/0.72205. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61250/0.72457. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61288/0.72534. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61357/0.72458. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60873/0.72726. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60333/0.73030. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60311/0.73372. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59866/0.73485. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60204/0.73664. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60314/0.73813. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59490/0.74033. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59244/0.73979. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59523/0.74018. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58747/0.74688. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58911/0.74563. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58858/0.74581. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58255/0.74717. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69579/0.69398. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69440/0.69351. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69432/0.69325. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69412/0.69316. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69341/0.69306. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69301/0.69311. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69212/0.69325. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69263/0.69343. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69216/0.69359. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69171/0.69382. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69154/0.69404. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69181/0.69439. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69087/0.69476. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69071/0.69523. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69166/0.69571. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69028/0.69618. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68940/0.69669. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68902/0.69740. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68784/0.69823. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68821/0.69887. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68654/0.69997. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68455/0.70146. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68553/0.70294. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68324/0.70408. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68302/0.70572. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68092/0.70723. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68073/0.70910. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68057/0.70982. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67668/0.71114. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67740/0.71274. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67568/0.71468. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67465/0.71599. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67011/0.71823. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66983/0.72087. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66593/0.72426. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66455/0.72673. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66550/0.72830. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66443/0.72813. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66248/0.73062. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65938/0.73124. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66103/0.73268. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65431/0.73349. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65041/0.73487. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65220/0.73820. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64910/0.73855. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64706/0.73995. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64537/0.74172. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64784/0.74233. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64114/0.74270. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.64228/0.74413. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63459/0.74534. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63874/0.74683. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63238/0.74973. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63789/0.74993. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62929/0.75255. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63211/0.75117. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62956/0.75185. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63097/0.75074. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62547/0.75087. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62354/0.75273. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62502/0.75493. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61872/0.75849. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62012/0.76149. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62130/0.76141. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61852/0.76094. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61679/0.76359. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61725/0.76594. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60991/0.76928. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61021/0.76781. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60990/0.77146. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61219/0.76479. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60296/0.76939. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60249/0.77199. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60169/0.77267. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60157/0.77463. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60117/0.77872. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60068/0.77854. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59764/0.78120. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58848/0.78346. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.59153/0.78206. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58795/0.78075. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58326/0.78855. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58896/0.78682. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58580/0.79089. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58167/0.79018. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58185/0.80236. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58132/0.79605. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56687/0.79744. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58070/0.79843. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57247/0.79893. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56980/0.80630. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56900/0.80720. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57327/0.80642. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55942/0.80930. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56397/0.81312. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56040/0.82323. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56186/0.81912. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55485/0.82002. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55994/0.82948. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54682/0.82915. Took 0.10 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69509/0.68570. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69476/0.68584. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69233/0.68456. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69154/0.68402. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69137/0.68304. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.68284. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.68251. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69117/0.68248. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68998/0.68192. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68979/0.68182. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69020/0.68160. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.68152. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68929/0.68127. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68976/0.68120. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68888/0.68108. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68877/0.68073. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68884/0.68095. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68825/0.68051. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68906/0.68061. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68815/0.68030. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68765/0.68014. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68757/0.68036. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68698/0.67991. Took 0.92 sec\n",
      "Epoch 23, Loss(train/val) 0.68771/0.67987. Took 0.12 sec\n",
      "Epoch 24, Loss(train/val) 0.68546/0.67942. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68603/0.67923. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68745/0.67911. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68514/0.67883. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68562/0.67878. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68507/0.67879. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68553/0.67857. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68368/0.67797. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68448/0.67813. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68356/0.67789. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68323/0.67693. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68253/0.67729. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68182/0.67636. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68246/0.67634. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67993/0.67582. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68217/0.67589. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67936/0.67545. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67900/0.67483. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67857/0.67432. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67694/0.67426. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67836/0.67477. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67731/0.67428. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67545/0.67423. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67603/0.67283. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67599/0.67372. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67275/0.67298. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67317/0.67205. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67288/0.67227. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67111/0.67325. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67015/0.67304. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67107/0.67099. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66902/0.66996. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66961/0.67176. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66685/0.67240. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66635/0.67092. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66686/0.67150. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66507/0.67141. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66238/0.67120. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66388/0.67123. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66132/0.67219. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66158/0.67224. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65999/0.67094. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65890/0.67251. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65895/0.67288. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65695/0.67368. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65640/0.67254. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65757/0.67375. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65296/0.67414. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65334/0.67327. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65171/0.67233. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.65031/0.67212. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64909/0.67314. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65048/0.67558. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64661/0.67879. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64705/0.67683. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64456/0.67595. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64195/0.67610. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64157/0.67892. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64314/0.67679. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63795/0.67656. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63627/0.68163. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63614/0.67969. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63173/0.68267. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63171/0.68138. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63037/0.68476. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62708/0.68023. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62341/0.68337. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62447/0.68440. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62448/0.68599. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62169/0.68746. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61864/0.68433. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61467/0.68736. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61668/0.69026. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61655/0.68970. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61095/0.69160. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61197/0.69524. Took 0.09 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69225/0.69141. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69121/0.69190. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68994/0.69223. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69009/0.69228. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68962/0.69238. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68982/0.69242. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68896/0.69241. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68900/0.69240. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68797/0.69249. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68747/0.69261. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68779/0.69277. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68713/0.69284. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68605/0.69319. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68607/0.69343. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68600/0.69360. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68536/0.69399. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68483/0.69436. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68539/0.69456. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68430/0.69502. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68365/0.69540. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68435/0.69571. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68117/0.69634. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68162/0.69674. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68208/0.69712. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68083/0.69749. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68161/0.69799. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68066/0.69838. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67913/0.69902. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67849/0.69998. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67779/0.70056. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67888/0.70112. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67710/0.70176. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67619/0.70210. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67488/0.70292. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67526/0.70350. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67268/0.70431. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67237/0.70530. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67084/0.70570. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67040/0.70664. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67111/0.70747. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67225/0.70758. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66817/0.70842. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66954/0.70851. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66710/0.70977. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66587/0.71001. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66307/0.71092. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66531/0.71173. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66316/0.71321. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66171/0.71309. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66079/0.71357. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65944/0.71408. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65798/0.71516. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65776/0.71619. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65198/0.71794. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65518/0.71746. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65420/0.71798. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64864/0.71893. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65219/0.72079. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64889/0.72196. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64732/0.72321. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64689/0.72436. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64487/0.72480. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64283/0.72661. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64567/0.72828. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64098/0.72800. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64213/0.72991. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63850/0.73098. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63940/0.73322. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63617/0.73484. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63534/0.73728. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63036/0.73962. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62990/0.73915. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63338/0.73937. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62704/0.74414. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62603/0.74450. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62506/0.74546. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62155/0.74695. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62312/0.74908. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62023/0.75222. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61810/0.75345. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61854/0.75367. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61298/0.75420. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61483/0.75851. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61015/0.76000. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61285/0.76125. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60885/0.76120. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60451/0.76458. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60145/0.76856. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59933/0.76932. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60018/0.76987. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59820/0.77350. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59739/0.77277. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59445/0.77650. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58846/0.77808. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58859/0.77856. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58630/0.78066. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58478/0.78184. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58501/0.78560. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58200/0.79036. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57377/0.78924. Took 0.08 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69226/0.68891. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69160/0.68869. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69154/0.68888. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69098/0.68894. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69155/0.68915. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69074/0.68915. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69096/0.68936. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69076/0.68944. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69041/0.68955. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68945/0.68990. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.69043. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68965/0.69127. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68862/0.69153. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68871/0.69193. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68777/0.69271. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68753/0.69303. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68688/0.69396. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68592/0.69480. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68641/0.69537. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68488/0.69598. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68412/0.69691. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68343/0.69742. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68290/0.69819. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68248/0.69839. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68203/0.69807. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68000/0.69833. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67771/0.69947. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67730/0.70009. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67706/0.69998. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67725/0.69957. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67474/0.69926. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67508/0.70020. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67184/0.70020. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67147/0.69933. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66921/0.69874. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67104/0.69875. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66683/0.69822. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66562/0.69788. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66270/0.69760. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66307/0.69553. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66359/0.69625. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65820/0.69620. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65818/0.69528. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65530/0.69515. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65030/0.69520. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65360/0.69429. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64935/0.69328. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64989/0.69541. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64110/0.69457. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64015/0.69520. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64166/0.69575. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63814/0.69294. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63429/0.69359. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63042/0.69414. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62917/0.69132. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62657/0.69411. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62306/0.69206. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61619/0.69204. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61657/0.69334. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61454/0.69520. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61197/0.69187. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61209/0.69249. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60819/0.69254. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60731/0.68991. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60342/0.68835. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59534/0.68984. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59658/0.69358. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58430/0.69090. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59067/0.69727. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58697/0.69777. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58072/0.69894. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57581/0.69456. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57170/0.69580. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56421/0.69488. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56634/0.69708. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55997/0.69599. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56013/0.69912. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55846/0.70386. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55450/0.69656. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55259/0.69653. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55349/0.69108. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54483/0.70363. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53378/0.70383. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.53657/0.70480. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53434/0.70219. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53395/0.70560. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53444/0.71861. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52671/0.70729. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52343/0.71332. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51488/0.71648. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51538/0.71152. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51511/0.70896. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.51408/0.72823. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50426/0.70982. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50627/0.71096. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.49388/0.72191. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49798/0.71952. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49009/0.72717. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49100/0.73408. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49644/0.73083. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69144/0.69431. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69060/0.69372. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68922/0.69344. Took 0.12 sec\n",
      "Epoch 3, Loss(train/val) 0.68827/0.69320. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68794/0.69301. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68740/0.69299. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68721/0.69273. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68633/0.69256. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68578/0.69213. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68473/0.69135. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68398/0.69069. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68283/0.69000. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68248/0.68923. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68140/0.68874. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68180/0.68834. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68048/0.68765. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67870/0.68795. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67887/0.68871. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67817/0.68849. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67488/0.68850. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67756/0.68933. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67509/0.68796. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67455/0.68832. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67454/0.68997. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67225/0.68980. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67115/0.69035. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66810/0.69205. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66908/0.69155. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66621/0.69185. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66647/0.69410. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66605/0.69226. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66435/0.69392. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66388/0.69398. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66156/0.69394. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65977/0.69358. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65897/0.69524. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65735/0.69517. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65577/0.69545. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65702/0.69724. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65377/0.70021. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65359/0.70242. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65146/0.70163. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64857/0.70356. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64783/0.70386. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64473/0.70247. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64652/0.70202. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64116/0.70605. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64232/0.70884. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64025/0.70719. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63561/0.71045. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63573/0.70992. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63241/0.70834. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62954/0.71008. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62673/0.71226. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62502/0.71560. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62212/0.71865. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61971/0.72304. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.61571/0.72588. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61526/0.72576. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61675/0.72422. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60975/0.72727. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61061/0.73082. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60521/0.73298. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59999/0.73386. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.59891/0.73785. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59634/0.74443. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59273/0.74662. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58860/0.74831. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58466/0.75583. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58132/0.75377. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58050/0.75409. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57810/0.75745. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57495/0.76095. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57535/0.76776. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56839/0.76846. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56179/0.77809. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56238/0.78040. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55676/0.77593. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55547/0.78286. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55215/0.78021. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54782/0.79243. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54509/0.80152. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54145/0.79501. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53821/0.81081. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53481/0.81469. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54110/0.81785. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.53460/0.80990. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52904/0.82894. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52149/0.82728. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52306/0.83077. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51948/0.83156. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51262/0.84124. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52186/0.84242. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51136/0.84479. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51397/0.84098. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49446/0.85744. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50386/0.86730. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.50386/0.86550. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50115/0.86891. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49461/0.87131. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69099/0.69576. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68710/0.69897. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68676/0.70008. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68606/0.69993. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68580/0.70031. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68519/0.70030. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68455/0.70028. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68427/0.70049. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68355/0.70103. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68294/0.70115. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68274/0.70145. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68325/0.70234. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68133/0.70330. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68076/0.70329. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68005/0.70444. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67976/0.70563. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67907/0.70685. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67862/0.70805. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67728/0.70901. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67652/0.71041. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67501/0.71102. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67635/0.71296. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67479/0.71388. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67481/0.71451. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67393/0.71648. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67228/0.71660. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67130/0.71849. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67121/0.71968. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67014/0.72045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66882/0.72155. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66778/0.72428. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66868/0.72544. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66591/0.72664. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66490/0.72729. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66372/0.72909. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66294/0.73089. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66216/0.73111. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66167/0.73244. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65992/0.73545. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65552/0.73731. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65648/0.74028. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65295/0.74160. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65407/0.74344. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65205/0.74541. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65095/0.74685. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65186/0.74836. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64865/0.75111. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64644/0.75046. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64626/0.75423. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64656/0.75613. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64377/0.75713. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64186/0.75781. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64118/0.76234. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63950/0.76251. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63741/0.76329. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63475/0.76620. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63691/0.76662. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63374/0.76855. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63183/0.77175. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63295/0.77138. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62776/0.77490. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62680/0.77236. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62756/0.77525. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62484/0.77641. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62589/0.77648. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61816/0.77917. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61837/0.78091. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61843/0.78273. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61568/0.77612. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61425/0.78077. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61113/0.78148. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61169/0.78649. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61224/0.78459. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60690/0.78516. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60906/0.78484. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.60477/0.78740. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60245/0.78629. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59983/0.78859. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60238/0.78737. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59606/0.78844. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59988/0.78665. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59042/0.78528. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59277/0.79389. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58900/0.79315. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.58445/0.78960. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58140/0.79527. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58041/0.79452. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58072/0.80230. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57999/0.79550. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57713/0.79914. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57569/0.79616. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57162/0.79476. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56338/0.79901. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.57491/0.80344. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57131/0.79386. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56519/0.79727. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56277/0.80187. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55905/0.81063. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55700/0.80244. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.55701/0.80660. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69166/0.68614. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68926/0.68640. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68915/0.68646. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.68647. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68844/0.68647. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68811/0.68640. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68825/0.68630. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68722/0.68622. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68801/0.68613. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68765/0.68602. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68712/0.68597. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68649/0.68611. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68544/0.68625. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68568/0.68630. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68576/0.68643. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68520/0.68655. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68401/0.68671. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68345/0.68691. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68418/0.68691. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68335/0.68669. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68223/0.68661. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68096/0.68721. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.68708. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68053/0.68724. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68039/0.68715. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67880/0.68674. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67821/0.68683. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67641/0.68698. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67678/0.68695. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67595/0.68658. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67552/0.68618. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67355/0.68591. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67334/0.68595. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67343/0.68467. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67279/0.68433. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67044/0.68493. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66888/0.68495. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66925/0.68432. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66870/0.68337. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66779/0.68311. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.66720/0.68357. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66473/0.68351. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66232/0.68287. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66084/0.68306. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66179/0.68379. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66014/0.68429. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65787/0.68386. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65819/0.68398. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65490/0.68548. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65608/0.68386. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65415/0.68586. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65185/0.68518. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64959/0.68656. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64813/0.68672. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64738/0.68810. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64345/0.68746. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64286/0.68856. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64111/0.68857. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64259/0.69010. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63632/0.68945. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63303/0.69153. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63271/0.69388. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62744/0.69315. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62984/0.69562. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62819/0.69771. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62748/0.69790. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62354/0.69620. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62443/0.69815. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62079/0.69816. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61916/0.69832. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61659/0.70174. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61683/0.69978. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61433/0.70241. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61174/0.70677. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60353/0.70667. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61150/0.70514. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60602/0.70759. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60478/0.70618. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60144/0.70969. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59606/0.71214. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59444/0.71487. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59197/0.71428. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58917/0.71659. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58397/0.71821. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58953/0.72132. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.58725/0.72223. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58408/0.72158. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57593/0.73268. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58955/0.72968. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57879/0.72997. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57294/0.72820. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57158/0.73638. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56433/0.74173. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56687/0.74399. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56386/0.74533. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56088/0.74845. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55712/0.75364. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55778/0.74877. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55375/0.76033. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54682/0.76846. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69204/0.69484. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69019/0.69440. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68929/0.69388. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68903/0.69357. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68893/0.69320. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68885/0.69284. Took 0.34 sec\n",
      "Epoch 6, Loss(train/val) 0.68856/0.69241. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68815/0.69207. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68760/0.69166. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68694/0.69134. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68651/0.69100. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68645/0.69061. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68621/0.69037. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68575/0.69030. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68560/0.69013. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68453/0.68981. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68404/0.68970. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68406/0.68970. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68386/0.68940. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68283/0.68941. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68268/0.68927. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68183/0.68922. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.68913. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68123/0.68946. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67986/0.68940. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67977/0.68911. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67857/0.68901. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67752/0.68901. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67774/0.68915. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67666/0.68896. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67611/0.68938. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67562/0.68971. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67493/0.69004. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67327/0.69042. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67421/0.69041. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67365/0.69089. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67128/0.69120. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67112/0.69137. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66921/0.69190. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66912/0.69204. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66820/0.69273. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66619/0.69306. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66628/0.69296. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66468/0.69324. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66531/0.69369. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66466/0.69422. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66268/0.69433. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66150/0.69482. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65995/0.69501. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65996/0.69538. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65698/0.69608. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65805/0.69658. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65528/0.69804. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65565/0.69808. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65313/0.69853. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65394/0.69812. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65015/0.69858. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64690/0.69984. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64925/0.70087. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64653/0.70270. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64318/0.70271. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64440/0.70270. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64138/0.70419. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64452/0.70351. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63951/0.70313. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64153/0.70421. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63649/0.70473. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63458/0.70618. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63430/0.70590. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63349/0.70719. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62736/0.70995. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63035/0.70941. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62494/0.70878. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62764/0.70767. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62756/0.71126. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.61924/0.71108. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62457/0.71300. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61743/0.71317. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61581/0.71558. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61876/0.71404. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61090/0.71690. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60837/0.71684. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60569/0.71915. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60275/0.72202. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60396/0.72115. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59898/0.72408. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60091/0.72524. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59145/0.72803. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59496/0.72947. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59487/0.72988. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59049/0.73253. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59141/0.73486. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58558/0.73630. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58098/0.74075. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58162/0.74385. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58179/0.74188. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57865/0.74181. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57452/0.74783. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57675/0.74932. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56837/0.75496. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69165/0.70114. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68911/0.70028. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68817/0.69995. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68732/0.69996. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68727/0.70002. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68616/0.70047. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68542/0.70097. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68482/0.70111. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68503/0.70174. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68469/0.70266. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68434/0.70294. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68249/0.70407. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68237/0.70441. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68192/0.70497. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68200/0.70552. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68208/0.70596. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68207/0.70628. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68197/0.70689. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68144/0.70671. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68019/0.70767. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68016/0.70856. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68039/0.70851. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67922/0.70854. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67974/0.70930. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67764/0.70963. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67774/0.71001. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67799/0.70986. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67716/0.71100. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67736/0.71061. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67732/0.71109. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67733/0.71174. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67607/0.71194. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67491/0.71265. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67452/0.71410. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67441/0.71377. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67266/0.71490. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67277/0.71579. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67419/0.71562. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67252/0.71716. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67020/0.71830. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67059/0.71931. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67004/0.71953. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67195/0.72035. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66734/0.72114. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66780/0.72027. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66648/0.72157. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66775/0.72318. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66520/0.72324. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66573/0.72542. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66209/0.72481. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66324/0.72528. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66365/0.72594. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66129/0.72674. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66053/0.72922. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65971/0.72887. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65771/0.73051. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65750/0.73158. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65371/0.73161. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65433/0.73162. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65332/0.73211. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65191/0.73601. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65457/0.73431. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64938/0.73691. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64747/0.73684. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64695/0.74056. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64537/0.74160. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64666/0.74219. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64027/0.74423. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64199/0.74628. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64007/0.74651. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64105/0.74948. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63660/0.75002. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63405/0.75435. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63809/0.75411. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63145/0.76000. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63282/0.76187. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63162/0.76287. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63050/0.76262. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62993/0.76687. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62532/0.76763. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62515/0.77298. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61760/0.77293. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61978/0.77813. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61572/0.78399. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61041/0.78166. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60829/0.78556. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60759/0.79041. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61419/0.79808. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60788/0.79806. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60795/0.80167. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60449/0.80775. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.60335/0.80495. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60116/0.81688. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59312/0.81350. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59813/0.82586. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58990/0.82081. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59053/0.83228. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58886/0.83280. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58634/0.84171. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58635/0.84676. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69369/0.68564. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69172/0.68490. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69101/0.68495. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.68543. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68950/0.68569. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68946/0.68580. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68789/0.68618. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68860/0.68669. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68749/0.68704. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68684/0.68732. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68603/0.68768. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68567/0.68831. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68429/0.68841. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68331/0.68894. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68213/0.68900. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68306/0.68964. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68066/0.69032. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68225/0.69080. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67872/0.69121. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67907/0.69139. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67904/0.69183. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67714/0.69215. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67809/0.69273. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67525/0.69332. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67723/0.69339. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67420/0.69379. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67420/0.69448. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67224/0.69509. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67176/0.69635. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66967/0.69642. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67086/0.69839. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66939/0.69932. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66788/0.70038. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66675/0.70064. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66482/0.70196. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66489/0.70243. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66244/0.70477. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66253/0.70554. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65845/0.70718. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65845/0.70790. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65649/0.70792. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65873/0.70902. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65644/0.71090. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65297/0.71217. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65134/0.71534. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64708/0.71767. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65068/0.71877. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64833/0.71975. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64583/0.72248. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64336/0.72455. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63826/0.72705. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63688/0.72707. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63653/0.73129. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63483/0.73167. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63303/0.73407. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62819/0.73641. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62897/0.74326. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62528/0.74364. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62662/0.74416. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62221/0.74466. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62017/0.74985. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61745/0.75134. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61091/0.75793. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61277/0.76148. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61333/0.76562. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60436/0.76488. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59750/0.77440. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60002/0.77401. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.59485/0.78021. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59678/0.78585. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59361/0.78822. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59163/0.79566. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58903/0.79461. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58298/0.80002. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58631/0.80975. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57574/0.81584. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57756/0.81513. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57146/0.81871. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57106/0.82807. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57203/0.83198. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56055/0.83992. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56172/0.83473. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56365/0.85070. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56313/0.85911. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55372/0.85271. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54899/0.86086. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54903/0.87354. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54264/0.87490. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54058/0.88990. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54157/0.88941. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53472/0.89946. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52715/0.89540. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52910/0.91538. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53503/0.91476. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52954/0.92599. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51788/0.92946. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52495/0.93588. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51351/0.94402. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.51155/0.95108. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51093/0.96122. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69125/0.69176. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69023/0.69116. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69044/0.69049. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68925/0.68988. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68948/0.68916. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68887/0.68838. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68898/0.68762. Took 0.45 sec\n",
      "Epoch 7, Loss(train/val) 0.68835/0.68698. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68788/0.68625. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68741/0.68551. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68675/0.68474. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68597/0.68377. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68611/0.68309. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68604/0.68222. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68527/0.68158. Took 0.41 sec\n",
      "Epoch 15, Loss(train/val) 0.68311/0.68093. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68389/0.68026. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68318/0.67988. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68237/0.67945. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68243/0.67908. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68184/0.67851. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68083/0.67818. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68101/0.67775. Took 0.43 sec\n",
      "Epoch 23, Loss(train/val) 0.68019/0.67723. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68003/0.67714. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67829/0.67681. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67925/0.67681. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67835/0.67659. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67712/0.67642. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67566/0.67641. Took 0.56 sec\n",
      "Epoch 30, Loss(train/val) 0.67511/0.67603. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67482/0.67595. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67476/0.67593. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67217/0.67576. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67132/0.67580. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66986/0.67546. Took 0.52 sec\n",
      "Epoch 36, Loss(train/val) 0.67036/0.67516. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66876/0.67511. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66870/0.67476. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.66835/0.67468. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66774/0.67471. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66319/0.67440. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66453/0.67424. Took 1.30 sec\n",
      "Epoch 43, Loss(train/val) 0.66290/0.67423. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66260/0.67420. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66107/0.67455. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65944/0.67494. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65796/0.67396. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65352/0.67275. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65604/0.67253. Took 0.52 sec\n",
      "Epoch 50, Loss(train/val) 0.65536/0.67271. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65279/0.67297. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65270/0.67211. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64987/0.67174. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.64931/0.67095. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64720/0.67091. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64290/0.67005. Took 1.03 sec\n",
      "Epoch 57, Loss(train/val) 0.64282/0.67095. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64078/0.67085. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63763/0.67149. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63640/0.67212. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63440/0.67180. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63321/0.67175. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63202/0.67331. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62887/0.67421. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62815/0.67339. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62668/0.67360. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62344/0.67573. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61883/0.67450. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62049/0.67680. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61743/0.67623. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61466/0.67637. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61442/0.67853. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61018/0.68160. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60893/0.68294. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60194/0.68271. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60351/0.68385. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59744/0.68418. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60121/0.68503. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59673/0.68472. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59384/0.68813. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59272/0.68981. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59149/0.68743. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59132/0.69077. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58245/0.69035. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58194/0.70026. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58015/0.69611. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57651/0.70009. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58126/0.69812. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56948/0.69704. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57550/0.70290. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56486/0.70411. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56549/0.70658. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55596/0.71026. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56198/0.71065. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55842/0.71222. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55353/0.71358. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55171/0.72049. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55333/0.71915. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54666/0.72357. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69668/0.69249. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69506/0.69497. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69295/0.69418. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69293/0.69410. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69185/0.69412. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69135/0.69435. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.69403. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69063/0.69431. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69010/0.69470. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68964/0.69440. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68919/0.69423. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68812/0.69420. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68831/0.69443. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68704/0.69457. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68753/0.69447. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68792/0.69425. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68670/0.69436. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68598/0.69366. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68679/0.69441. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68522/0.69379. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68589/0.69420. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68498/0.69397. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68464/0.69413. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68409/0.69365. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68291/0.69344. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68316/0.69381. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68324/0.69392. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68280/0.69365. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68201/0.69343. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67987/0.69329. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68000/0.69323. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67977/0.69379. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67913/0.69385. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67937/0.69320. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67896/0.69358. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67848/0.69361. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67709/0.69298. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67654/0.69345. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67565/0.69332. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67596/0.69340. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67563/0.69314. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67675/0.69480. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67392/0.69457. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67241/0.69366. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67293/0.69424. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67157/0.69563. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67070/0.69642. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66867/0.69604. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66992/0.69625. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66658/0.69583. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66867/0.69613. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66574/0.69783. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66646/0.69785. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66657/0.69936. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66250/0.69989. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66311/0.69991. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66331/0.70069. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66008/0.70066. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66070/0.70155. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66070/0.70247. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65786/0.70470. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65633/0.70358. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65768/0.70412. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65733/0.70512. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65562/0.70535. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65609/0.70493. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65182/0.70691. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65372/0.70707. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65307/0.70764. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64942/0.70855. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65087/0.71027. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64786/0.71018. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64769/0.71242. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64601/0.71196. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64769/0.71141. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64380/0.71250. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64584/0.71261. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64313/0.71257. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64093/0.71427. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64072/0.71502. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64046/0.71308. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64103/0.71210. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63595/0.71355. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63621/0.71644. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63844/0.71530. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63496/0.71570. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63324/0.71758. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62923/0.71951. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62874/0.71823. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62976/0.71811. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63093/0.71732. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62958/0.71662. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62761/0.71855. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62597/0.71772. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62534/0.71828. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62813/0.72046. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62095/0.72071. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62236/0.72073. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62328/0.72082. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62122/0.71993. Took 0.08 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69244/0.69683. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69700. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69736. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69156/0.69767. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69118/0.69796. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69109/0.69806. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69134/0.69827. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.69824. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68992/0.69798. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.69764. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68862/0.69741. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68836/0.69689. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68691/0.69686. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68653/0.69626. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68578/0.69586. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68365/0.69474. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68395/0.69442. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68301/0.69326. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68044/0.69220. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68051/0.69172. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67908/0.69026. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67824/0.68886. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67624/0.68766. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67305/0.68662. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67194/0.68421. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67058/0.68355. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66823/0.68109. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66494/0.68010. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66484/0.67882. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66317/0.67770. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65831/0.67563. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65679/0.67432. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.65581/0.67362. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65557/0.67276. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65197/0.67237. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65346/0.67379. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65102/0.67243. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64600/0.67141. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64566/0.67032. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64184/0.67022. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64085/0.67020. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63745/0.67087. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63830/0.67073. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63697/0.67199. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62925/0.67039. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63108/0.66980. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63284/0.66845. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62632/0.66707. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62374/0.66682. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62780/0.66577. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.62507/0.66551. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61727/0.66604. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61964/0.66499. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61385/0.66523. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61279/0.66384. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61015/0.66369. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.61049/0.66355. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61297/0.66251. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60420/0.66253. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60027/0.66227. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60701/0.66107. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60159/0.66101. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59895/0.66180. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59316/0.66191. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59288/0.66089. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59545/0.66097. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58755/0.66051. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58548/0.66186. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.58193/0.66111. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57976/0.66078. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57366/0.66036. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58303/0.65956. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57344/0.66337. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57469/0.66099. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56562/0.66244. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56988/0.66552. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56427/0.66312. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.55715/0.66268. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55330/0.66640. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55742/0.66799. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55395/0.67023. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54813/0.67128. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54161/0.66714. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55353/0.66941. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54560/0.66771. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54661/0.66743. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53934/0.67138. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53848/0.67122. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53665/0.67214. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53095/0.67117. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52986/0.67213. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52036/0.67319. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51904/0.67430. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51889/0.68283. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52619/0.67948. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51507/0.68300. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52237/0.68697. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51180/0.68296. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51207/0.68574. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50922/0.69191. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69390/0.69182. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69232/0.69078. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69242/0.69052. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.69038. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69190/0.69030. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69143/0.69030. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69059/0.69027. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69061/0.69026. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69040/0.69035. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68974/0.69055. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68958/0.69049. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68867/0.69072. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68811/0.69062. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68772/0.69099. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68673/0.69095. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68623/0.69120. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68466/0.69144. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68451/0.69189. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68384/0.69225. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68384/0.69251. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68223/0.69291. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68288/0.69334. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68077/0.69383. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67988/0.69413. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67749/0.69422. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67565/0.69456. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67499/0.69468. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67663/0.69520. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67116/0.69534. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67097/0.69571. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67013/0.69653. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66692/0.69655. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66757/0.69738. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66638/0.69852. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66391/0.69976. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66000/0.70154. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66082/0.70158. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65625/0.70296. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65356/0.70493. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65211/0.70735. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64917/0.70721. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64661/0.70798. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64509/0.71190. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64248/0.71144. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63985/0.71507. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63827/0.71513. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63695/0.71877. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63974/0.71897. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63198/0.72326. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63038/0.72430. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62890/0.72702. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62579/0.72942. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62273/0.73218. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61928/0.73294. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62421/0.73520. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61721/0.73922. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61870/0.73624. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61736/0.73834. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61564/0.74422. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61314/0.74228. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61450/0.74529. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.60883/0.74793. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60475/0.75429. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60480/0.75780. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60222/0.75419. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59882/0.75479. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59694/0.76362. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59791/0.76683. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59448/0.77018. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58994/0.76665. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58863/0.77568. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58956/0.77351. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58707/0.77992. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.57929/0.78258. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58277/0.78620. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57894/0.78848. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57191/0.79682. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57440/0.79457. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57313/0.79211. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57005/0.80403. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56734/0.80192. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56900/0.80642. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56832/0.80731. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56451/0.80856. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55701/0.81372. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.55690/0.81799. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56196/0.82252. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55309/0.82110. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55878/0.82146. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54988/0.82014. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54525/0.82705. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54887/0.83829. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54253/0.83476. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54565/0.83565. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54522/0.83318. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53536/0.82928. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52935/0.84933. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53416/0.84870. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52565/0.84788. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52949/0.84128. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69607/0.69479. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69468/0.69400. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69268/0.69344. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69174/0.69316. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.69290. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.69283. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69273. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69117/0.69278. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68998/0.69282. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.69313. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68869/0.69344. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68836/0.69391. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68814/0.69429. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68793/0.69480. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68750/0.69520. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68727/0.69571. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68682/0.69631. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68545/0.69711. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68420/0.69798. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68440/0.69882. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68473/0.69945. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68480/0.69982. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68231/0.70035. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68214/0.70093. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68341/0.70118. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68178/0.70159. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68137/0.70217. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67889/0.70257. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67965/0.70303. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67789/0.70362. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67898/0.70438. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67649/0.70569. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67715/0.70601. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67562/0.70697. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67266/0.70771. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67166/0.70832. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67151/0.70887. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66913/0.70964. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66988/0.71025. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66471/0.71190. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66645/0.71230. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66494/0.71355. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66387/0.71510. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66264/0.71582. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66117/0.71719. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66105/0.71877. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65888/0.71979. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65949/0.72222. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65668/0.72295. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65851/0.72547. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65453/0.72559. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65075/0.72931. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65197/0.72977. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64571/0.73193. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65021/0.73547. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64792/0.73371. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64450/0.73778. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64050/0.73967. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64404/0.73979. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64623/0.73940. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64232/0.74169. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64435/0.73875. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63934/0.74230. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63993/0.74059. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63804/0.74299. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63358/0.74654. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63648/0.74675. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63266/0.74923. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63239/0.75321. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63382/0.75396. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62814/0.75261. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63029/0.75048. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62766/0.75528. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62583/0.75220. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62813/0.75631. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62223/0.75564. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62807/0.75727. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62348/0.76128. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62023/0.76531. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61684/0.76335. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62083/0.76267. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61690/0.76003. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61445/0.76402. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61650/0.76526. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61596/0.76489. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61161/0.76659. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60477/0.77256. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61428/0.76948. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60456/0.77047. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61043/0.77554. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60661/0.77726. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60411/0.77350. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59956/0.76969. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59962/0.77744. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60413/0.77704. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59718/0.78343. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59339/0.78803. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59455/0.78800. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58920/0.78149. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59421/0.78946. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69294/0.69080. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69269/0.69055. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.69034. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69226/0.69031. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69171/0.69038. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69236/0.69051. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69048. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69174/0.69048. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69150/0.69041. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69047/0.69046. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69063/0.69072. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69076/0.69101. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68948/0.69100. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69049/0.69125. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68875/0.69178. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68889/0.69223. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68770/0.69253. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68746/0.69320. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68738/0.69375. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68700/0.69441. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68693/0.69486. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68670/0.69546. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68678/0.69597. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68585/0.69670. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68343/0.69733. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68271/0.69833. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68242/0.69938. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68163/0.70086. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.70169. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68106/0.70276. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67943/0.70342. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67800/0.70368. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67799/0.70425. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67572/0.70560. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67583/0.70718. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67486/0.70784. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67406/0.70799. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67057/0.70870. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67040/0.70924. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66829/0.71088. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66878/0.71166. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66800/0.71238. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66573/0.71244. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66478/0.71397. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66222/0.71357. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66198/0.71503. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66141/0.71631. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65877/0.71852. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65856/0.71985. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65451/0.71909. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65220/0.71916. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65311/0.72143. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65255/0.72192. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65031/0.72307. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64350/0.72189. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64171/0.72277. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64521/0.72346. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64263/0.72294. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64320/0.72513. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63681/0.72739. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63727/0.72898. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63616/0.72602. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63197/0.73095. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63054/0.73093. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63216/0.73238. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63106/0.73421. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62838/0.73201. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62279/0.73472. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62108/0.73780. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62202/0.73672. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61567/0.73604. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61725/0.74026. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61746/0.74063. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61393/0.74608. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61019/0.75182. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60541/0.75019. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60761/0.74707. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60359/0.74703. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60158/0.75280. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59600/0.75399. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59851/0.75127. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59526/0.75214. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59924/0.74820. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59529/0.75079. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58720/0.75964. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59150/0.75939. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57743/0.75628. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58163/0.76053. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57734/0.76969. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57680/0.77283. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57567/0.76754. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57737/0.76628. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57082/0.76727. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55890/0.77234. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56892/0.77266. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56453/0.77180. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55894/0.78075. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56331/0.78238. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55636/0.78467. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55615/0.78308. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69336/0.69118. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69161/0.69073. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69191/0.69069. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69128/0.69074. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69105/0.69073. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69084/0.69068. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.69082. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68950/0.69101. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69011/0.69113. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68978/0.69145. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.69166. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68914/0.69203. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68835/0.69230. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68662/0.69258. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68786/0.69302. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68688/0.69350. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68642/0.69385. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68607/0.69448. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68420/0.69490. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68483/0.69561. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68364/0.69558. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68181/0.69620. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68224/0.69653. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68102/0.69703. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67928/0.69737. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67941/0.69724. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67754/0.69743. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67705/0.69783. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67353/0.69793. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67285/0.69830. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66931/0.69818. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66886/0.69907. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66778/0.69810. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66587/0.69796. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66212/0.69829. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66033/0.69859. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65987/0.69782. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65421/0.69789. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65008/0.69702. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65129/0.69705. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65046/0.69703. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64527/0.69640. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64452/0.69708. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63889/0.69688. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.63885/0.69814. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63787/0.69881. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63425/0.69744. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63108/0.69784. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63043/0.69858. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62718/0.69990. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62649/0.70346. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62162/0.69902. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62260/0.69847. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61923/0.69804. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61590/0.69800. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61520/0.69953. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61370/0.70172. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60998/0.70413. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60828/0.70543. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60878/0.70738. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60172/0.70509. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59623/0.70640. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59610/0.70946. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59694/0.70940. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59052/0.71112. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59193/0.71130. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58787/0.71543. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58957/0.71254. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58296/0.71669. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57934/0.71262. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57853/0.71354. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57324/0.71300. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.57687/0.71230. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57283/0.71757. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56631/0.72052. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57201/0.72185. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56677/0.71970. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56263/0.72275. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56121/0.72480. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55592/0.72316. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55332/0.72094. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55140/0.72873. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55317/0.72729. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54454/0.73708. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55027/0.73828. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54233/0.73409. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54253/0.73673. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54142/0.74101. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53714/0.73601. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53544/0.74060. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52432/0.75445. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52915/0.74978. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52981/0.74674. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51727/0.75638. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52973/0.75590. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51922/0.75203. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52067/0.75941. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51698/0.75799. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50897/0.75965. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51364/0.76273. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69962/0.69211. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69443/0.69259. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69279/0.69393. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69272/0.69499. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69147/0.69595. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69063/0.69671. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69091/0.69728. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69078/0.69773. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69026/0.69818. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68925/0.69858. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68936/0.69899. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68857/0.69947. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68883/0.69947. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68815/0.69993. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68818/0.69996. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68794/0.70042. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68806/0.70038. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68658/0.70050. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68680/0.70047. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68709/0.70083. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68570/0.70101. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68470/0.70103. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68445/0.70112. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68406/0.70115. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68529/0.70123. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68399/0.70107. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68293/0.70138. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68256/0.70141. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68320/0.70163. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68156/0.70195. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67996/0.70184. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68114/0.70203. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68074/0.70189. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67936/0.70204. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67910/0.70233. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67941/0.70228. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67844/0.70251. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67622/0.70276. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67615/0.70337. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67480/0.70327. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67506/0.70357. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67312/0.70326. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67529/0.70403. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67201/0.70488. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67180/0.70493. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66947/0.70507. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66961/0.70480. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66799/0.70619. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66799/0.70594. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66830/0.70553. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66738/0.70569. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66503/0.70702. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66405/0.70731. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66256/0.70790. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66233/0.70899. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66161/0.70949. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66064/0.70900. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65937/0.70920. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65956/0.70949. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65860/0.70991. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65710/0.71018. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65217/0.71144. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65281/0.71215. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65366/0.71139. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65238/0.71164. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65171/0.71218. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64772/0.71198. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64970/0.71240. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64799/0.71301. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64578/0.71436. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64628/0.71290. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64537/0.71368. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64346/0.71321. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63664/0.71296. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64189/0.71342. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63635/0.71342. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63680/0.71516. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63721/0.71430. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63588/0.71291. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63306/0.71432. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63055/0.71314. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63293/0.71375. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62711/0.71169. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62515/0.71392. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62507/0.71279. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62457/0.71267. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61647/0.71415. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62174/0.71294. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62233/0.71326. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61946/0.71505. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61627/0.71294. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61401/0.71370. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61401/0.71295. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61111/0.71312. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61333/0.71396. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60829/0.71549. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60973/0.71310. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60394/0.71393. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60549/0.71570. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60686/0.71670. Took 0.08 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69429/0.68286. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.68502. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69160/0.68520. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69216/0.68629. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69107/0.68728. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69077/0.68788. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69067/0.68830. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68986/0.68819. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69053/0.68762. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68836/0.68884. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68862/0.68889. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68793/0.68869. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68729/0.68968. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68693/0.69091. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68634/0.68982. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68487/0.68961. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68507/0.68968. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68325/0.68840. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68362/0.68922. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68243/0.68863. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68088/0.68751. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68057/0.68720. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67770/0.68676. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67750/0.68465. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67406/0.68396. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67504/0.68068. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67218/0.68061. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67051/0.67906. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66724/0.67609. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66642/0.67463. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66158/0.67329. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66061/0.67397. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65919/0.67201. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65474/0.67049. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65692/0.66654. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65299/0.66473. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65224/0.67032. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64902/0.66685. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64587/0.66706. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64620/0.66574. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64213/0.66879. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64197/0.66631. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63624/0.67027. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63535/0.66639. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63439/0.66878. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63024/0.67264. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63049/0.66821. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62736/0.66719. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62500/0.67201. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62668/0.67243. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61983/0.67488. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61658/0.67236. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61486/0.67797. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61527/0.67525. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61673/0.67878. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.60836/0.67826. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61010/0.68542. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60575/0.68936. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60024/0.68517. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60230/0.68408. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60373/0.68374. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60152/0.68154. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59627/0.69050. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59593/0.69597. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.59334/0.69940. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58937/0.69564. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58716/0.69272. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58686/0.69293. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58652/0.70814. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58117/0.70758. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58318/0.70457. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58030/0.70985. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57571/0.70549. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57576/0.70753. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57462/0.71492. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57846/0.71047. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57039/0.72021. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56659/0.71797. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56800/0.72146. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56365/0.72034. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55866/0.72676. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55737/0.72302. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55437/0.72670. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55114/0.73215. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55235/0.73255. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54108/0.73561. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54932/0.73904. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54542/0.73439. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54475/0.73886. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53857/0.74411. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53670/0.74427. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53797/0.73676. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53143/0.74372. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53156/0.75303. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51958/0.75075. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52691/0.76734. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52119/0.76706. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52359/0.77682. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51253/0.77104. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51509/0.77601. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69452/0.69461. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69194/0.69540. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69144/0.69562. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69116/0.69542. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69043/0.69562. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68930/0.69558. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68922/0.69550. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68814/0.69492. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68807/0.69485. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68654/0.69527. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68656/0.69506. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68551/0.69498. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68513/0.69500. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68531/0.69463. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68409/0.69427. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68265/0.69395. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68208/0.69399. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68128/0.69320. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68006/0.69329. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67828/0.69320. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67702/0.69331. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67736/0.69235. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67593/0.69295. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67464/0.69271. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67405/0.69326. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66947/0.69417. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67197/0.69422. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67080/0.69498. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66719/0.69563. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66969/0.69577. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66641/0.69652. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66531/0.69637. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66371/0.69686. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66420/0.69573. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66305/0.69615. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66161/0.69661. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66155/0.69612. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66076/0.69453. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65869/0.69602. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65856/0.69556. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65773/0.69509. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65682/0.69542. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65683/0.69515. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65317/0.69500. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65310/0.69476. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65403/0.69445. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65147/0.69512. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65082/0.69334. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64843/0.69415. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64705/0.69487. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64886/0.69519. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64555/0.69513. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64521/0.69413. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64308/0.69426. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64143/0.69346. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63935/0.69482. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63848/0.69648. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63616/0.69740. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63991/0.69584. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63223/0.69423. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63171/0.69565. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62913/0.69723. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62833/0.69597. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62801/0.69963. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62513/0.70060. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62906/0.69899. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62295/0.70005. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62552/0.70280. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61806/0.70533. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62183/0.70055. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61608/0.70344. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61282/0.70353. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61469/0.70324. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61379/0.70649. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60573/0.70794. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60569/0.70789. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60306/0.70935. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59659/0.71384. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59864/0.71698. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60482/0.71959. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59416/0.72092. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58974/0.71748. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59491/0.72544. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58993/0.72312. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59222/0.72855. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58626/0.72655. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58130/0.73039. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58402/0.73138. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57290/0.72724. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58028/0.73161. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57495/0.73583. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57773/0.73919. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57590/0.73088. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56788/0.73207. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56571/0.74384. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56202/0.74437. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56070/0.74807. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56048/0.74895. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55645/0.74681. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.56283/0.75283. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69425/0.68535. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69191/0.68457. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69142/0.68521. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69044/0.68559. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68987/0.68622. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68849/0.68668. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68747/0.68760. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68679/0.68846. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68688/0.68963. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68578/0.69103. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68512/0.69233. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68444/0.69362. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68412/0.69472. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68312/0.69605. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68184/0.69748. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68274/0.69913. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68205/0.70035. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68144/0.70154. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68113/0.70282. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68085/0.70432. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68084/0.70532. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67946/0.70605. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67935/0.70727. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67788/0.70849. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67872/0.70975. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67841/0.71064. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67698/0.71136. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67561/0.71243. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67598/0.71309. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67597/0.71359. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67516/0.71411. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67316/0.71547. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67304/0.71663. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67082/0.71759. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67216/0.71860. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67144/0.71899. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67120/0.72010. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66983/0.72085. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66817/0.72187. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66774/0.72261. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66582/0.72354. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66492/0.72400. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66484/0.72426. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66439/0.72551. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66347/0.72580. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66160/0.72641. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65805/0.72848. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65968/0.72945. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65749/0.73088. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65523/0.73087. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65383/0.73226. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65104/0.73404. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65117/0.73498. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64909/0.73610. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64718/0.73667. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64622/0.73649. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64284/0.73706. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64189/0.73846. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63789/0.73996. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63917/0.74101. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63807/0.74006. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63566/0.74070. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63371/0.74211. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63187/0.74271. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63143/0.74259. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62546/0.74443. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62475/0.74324. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62458/0.74835. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62384/0.74853. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61663/0.74956. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61828/0.75211. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61709/0.75120. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61437/0.75047. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61443/0.74987. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60867/0.75046. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60796/0.75258. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60524/0.75255. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60460/0.75539. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60013/0.75573. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60444/0.75484. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60146/0.75687. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59767/0.75551. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59159/0.75770. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58646/0.75925. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59020/0.76197. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59070/0.76369. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58325/0.76328. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58428/0.76647. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58225/0.76954. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57943/0.76503. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57685/0.76738. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57277/0.77073. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57312/0.77168. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56753/0.77266. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56861/0.77538. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56595/0.77428. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56603/0.77529. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55718/0.77768. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56098/0.77751. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55332/0.78331. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69235/0.68238. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69030/0.68183. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69018/0.68134. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68993/0.68081. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68929/0.68023. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68794/0.67934. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68757/0.67843. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68621/0.67725. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68409/0.67577. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68313/0.67413. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68243/0.67303. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67954/0.67083. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67917/0.66931. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67830/0.66841. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67715/0.66693. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67567/0.66535. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67203/0.66392. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67164/0.66261. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67062/0.65905. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66927/0.65860. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.66659/0.65573. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66479/0.65447. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66265/0.65216. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65842/0.65061. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65938/0.64885. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65462/0.64664. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65114/0.64502. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65111/0.64444. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.64714/0.64212. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.64696/0.64159. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64285/0.64132. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64098/0.64095. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.63814/0.63942. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64043/0.63926. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63645/0.63933. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63294/0.64034. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.62984/0.63886. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63010/0.63776. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62764/0.63697. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62103/0.63687. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.61930/0.63724. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.61906/0.63631. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.61728/0.63752. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61254/0.63784. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.60784/0.63705. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.60107/0.63640. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.60185/0.63882. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60169/0.63971. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.59575/0.63952. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.59739/0.64414. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.59157/0.64427. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.58479/0.64846. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.58361/0.65006. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.57896/0.65024. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.57563/0.65568. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.56990/0.65745. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.56810/0.66094. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.56311/0.66351. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.56757/0.66535. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.55518/0.67139. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.55827/0.67245. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.54883/0.67427. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.55274/0.67541. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.54598/0.68661. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.53991/0.69161. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.53445/0.69082. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.53158/0.70270. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.53391/0.70350. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.53216/0.70361. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.52312/0.71113. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.52307/0.71976. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.51900/0.71825. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.51408/0.72915. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.51033/0.73147. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.51057/0.73464. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.50431/0.73494. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.50466/0.73900. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.50502/0.75060. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.48926/0.75324. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.49481/0.76180. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.48439/0.76475. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.48439/0.77339. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.48734/0.77366. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.47977/0.77761. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.47861/0.78557. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.47894/0.78870. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.47354/0.79353. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.46642/0.80703. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.46041/0.81018. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.45059/0.81466. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.45356/0.82440. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.45928/0.83308. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.45453/0.82911. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.45685/0.83213. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.45301/0.83876. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.43993/0.83898. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.44089/0.85284. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.43928/0.86229. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.43541/0.86462. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.41951/0.87297. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69417/0.69341. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69249/0.69300. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69289. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69179/0.69274. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69154/0.69275. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69185/0.69275. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69203/0.69284. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69120/0.69283. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69061/0.69283. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69060/0.69298. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68995/0.69289. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68993/0.69311. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68881/0.69339. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68913/0.69370. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68826/0.69412. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68764/0.69448. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68744/0.69483. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68674/0.69543. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68740/0.69618. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68566/0.69659. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68476/0.69711. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68524/0.69780. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68460/0.69824. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68362/0.69900. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68258/0.69971. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68119/0.69997. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68274/0.70069. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67990/0.70165. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67930/0.70207. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67777/0.70257. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67851/0.70337. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67691/0.70405. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67760/0.70419. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67523/0.70518. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67486/0.70576. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67475/0.70656. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67320/0.70751. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67215/0.70747. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67060/0.70816. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67006/0.70938. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66951/0.70972. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66911/0.70988. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66693/0.71002. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66623/0.71361. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66712/0.71329. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66441/0.71460. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66269/0.71600. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66256/0.71594. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66226/0.71651. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66020/0.71740. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65694/0.71935. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65805/0.72058. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65746/0.72195. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65523/0.72226. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65530/0.72242. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65338/0.72271. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65280/0.72384. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64890/0.72350. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65082/0.72709. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64626/0.72687. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64570/0.72766. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64339/0.72921. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64475/0.73227. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63897/0.73298. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63789/0.73741. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63944/0.73649. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63478/0.73845. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63005/0.73926. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63292/0.74177. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63034/0.74135. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62827/0.74407. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62434/0.74422. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62598/0.74576. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62610/0.74668. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62141/0.75037. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61900/0.75078. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62028/0.75044. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60723/0.75675. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61247/0.75692. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60491/0.75878. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60952/0.76221. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60168/0.76581. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60137/0.76679. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60069/0.76959. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60133/0.76820. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59400/0.77071. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59102/0.77202. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58470/0.77766. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59105/0.77852. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58545/0.77333. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58587/0.78217. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58088/0.78114. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57670/0.78218. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57901/0.78611. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57663/0.78661. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57555/0.78977. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56782/0.79166. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56878/0.79791. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56523/0.79934. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55486/0.79910. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69141/0.68943. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69112/0.68935. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69094/0.68925. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.68921. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69041/0.68905. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68995/0.68886. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68939/0.68880. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68985/0.68864. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68919/0.68843. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68881/0.68822. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68857/0.68797. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68685/0.68785. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68617/0.68770. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68634/0.68729. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68461/0.68683. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68332/0.68635. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68325/0.68575. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68148/0.68532. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68113/0.68441. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68037/0.68368. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67737/0.68265. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67709/0.68110. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67617/0.68135. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67175/0.67913. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67206/0.67879. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66774/0.67685. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66706/0.67607. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66505/0.67584. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66389/0.67310. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65878/0.67416. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65973/0.67076. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65878/0.67047. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65350/0.67115. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65219/0.66989. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65252/0.66884. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64837/0.67080. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64475/0.66813. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64538/0.66575. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64323/0.66673. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64179/0.66727. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63496/0.66677. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63195/0.66564. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63613/0.66469. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.63012/0.66325. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62683/0.66404. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62671/0.66231. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62101/0.66257. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62170/0.66257. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61724/0.66149. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61295/0.66073. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.61044/0.66182. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60950/0.65935. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60247/0.66030. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60891/0.65959. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60000/0.65938. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.60269/0.65944. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.59569/0.65967. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59580/0.65718. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.59228/0.65767. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58751/0.65898. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58222/0.65868. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57932/0.65988. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58028/0.65995. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57491/0.65838. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.56859/0.66361. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57103/0.66810. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56696/0.66403. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56552/0.66145. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56052/0.66089. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56169/0.66493. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55511/0.66273. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55077/0.66313. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55237/0.66460. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.54567/0.66923. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.54458/0.66797. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54553/0.67304. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.53374/0.67061. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.53565/0.67113. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.53359/0.66856. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.53313/0.67424. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.53679/0.67385. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.52629/0.67728. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52476/0.67594. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52478/0.67686. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51433/0.67867. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.51340/0.68449. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.50318/0.68030. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51243/0.67999. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50703/0.68290. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51101/0.68333. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50573/0.68256. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.48815/0.68448. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.49038/0.69044. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.49662/0.69298. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48787/0.68912. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48441/0.69097. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48525/0.69976. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.48622/0.70086. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.47803/0.69833. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47866/0.69730. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69556/0.66912. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69159/0.67046. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69020/0.66833. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68966/0.66773. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68984/0.66793. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68902/0.66713. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68925/0.66648. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68825/0.66663. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.66679. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68766/0.66659. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68748/0.66697. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68725/0.66687. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68643/0.66625. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68676/0.66622. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68631/0.66595. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68567/0.66641. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68513/0.66603. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68452/0.66564. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68443/0.66499. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68362/0.66526. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68335/0.66458. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68258/0.66467. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68256/0.66455. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68121/0.66398. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68123/0.66455. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67990/0.66400. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68019/0.66388. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67973/0.66396. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67937/0.66418. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67906/0.66398. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67703/0.66378. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67724/0.66472. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67576/0.66429. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67502/0.66450. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67418/0.66448. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67391/0.66414. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67308/0.66485. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67208/0.66497. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67047/0.66570. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67131/0.66571. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66987/0.66470. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67074/0.66674. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66983/0.66745. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66847/0.66754. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66914/0.66745. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66731/0.66903. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66403/0.66880. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66440/0.67104. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66314/0.67067. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66339/0.67052. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66296/0.67146. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66096/0.67522. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65940/0.67457. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65994/0.67722. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66051/0.67461. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65651/0.67775. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65683/0.67702. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65699/0.67879. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65690/0.68062. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65484/0.67881. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65517/0.68249. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65447/0.68359. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65291/0.68351. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65169/0.68353. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64992/0.68549. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65024/0.68459. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64867/0.68593. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64933/0.68831. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64787/0.68983. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64639/0.68793. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64470/0.69131. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64334/0.69106. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64284/0.69222. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64425/0.69119. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64271/0.69417. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63868/0.69171. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64315/0.69505. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63682/0.69372. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63551/0.69560. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63738/0.69623. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63544/0.69773. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63324/0.69776. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63442/0.70071. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63195/0.69726. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63267/0.70204. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63076/0.69968. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62699/0.70174. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62837/0.70130. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62820/0.70013. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62931/0.70260. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62368/0.70130. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62470/0.70333. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61943/0.70429. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61690/0.70540. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62152/0.70909. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61968/0.70854. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61660/0.71177. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61492/0.71122. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61163/0.71329. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61057/0.71447. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69022/0.70053. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68766/0.70117. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68654/0.70145. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68614/0.70219. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68482/0.70295. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68457/0.70399. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68446/0.70482. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68401/0.70535. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68348/0.70609. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68282/0.70679. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68261/0.70746. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68222/0.70798. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68191/0.70875. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68185/0.70924. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68180/0.70967. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68088/0.70962. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68100/0.71008. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68026/0.71057. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67992/0.71108. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68034/0.71092. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67957/0.71122. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68006/0.71147. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67889/0.71184. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67787/0.71247. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67796/0.71238. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67795/0.71248. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67736/0.71278. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67707/0.71317. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67656/0.71304. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67556/0.71316. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67550/0.71335. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67433/0.71382. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67436/0.71435. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67297/0.71456. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67223/0.71434. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67270/0.71492. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67157/0.71519. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67057/0.71525. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66864/0.71618. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67061/0.71614. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66881/0.71640. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66865/0.71661. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66763/0.71727. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66626/0.71769. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66640/0.71877. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66485/0.71849. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66449/0.71903. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66261/0.72039. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66368/0.72103. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66080/0.72123. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66003/0.72201. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66067/0.72275. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65883/0.72442. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65830/0.72427. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65719/0.72556. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65556/0.72721. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65529/0.72759. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65204/0.72908. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65554/0.72995. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65434/0.73140. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65267/0.73305. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65180/0.73266. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64991/0.73442. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64762/0.73638. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64537/0.73636. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64543/0.73926. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64295/0.74180. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64445/0.74209. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64369/0.74317. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63872/0.74754. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63795/0.74955. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.63852/0.75060. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63589/0.75202. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63791/0.75098. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63476/0.75273. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63326/0.75608. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63217/0.75770. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62896/0.76065. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63077/0.76250. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62752/0.76356. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62549/0.76629. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62240/0.76807. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62359/0.76879. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62238/0.77307. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61552/0.77706. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61653/0.77964. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61244/0.78239. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61078/0.78389. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61565/0.78824. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61224/0.78653. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61338/0.79209. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60865/0.79180. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60847/0.79093. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60881/0.79503. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60533/0.79370. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60125/0.80144. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60130/0.80218. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60268/0.80613. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59762/0.81178. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59583/0.81251. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69516/0.69491. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69241/0.69063. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68862/0.68798. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68761/0.68649. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68824/0.68582. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68640/0.68566. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68649/0.68589. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68506/0.68629. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68370/0.68686. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68253/0.68741. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68308/0.68828. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68321/0.68958. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68152/0.69050. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67894/0.69132. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67919/0.69147. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67773/0.69393. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67608/0.69443. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67334/0.69504. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67021/0.69656. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67043/0.69821. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66742/0.69813. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66555/0.69872. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66399/0.70166. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66110/0.70151. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.65974/0.70296. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65391/0.70416. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.65239/0.70331. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.64946/0.70475. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.64829/0.70483. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64667/0.70449. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64554/0.70272. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.64237/0.70587. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64284/0.70272. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.63804/0.70195. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.63329/0.70029. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63244/0.70144. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.62991/0.69981. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63206/0.69682. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62769/0.69870. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63009/0.69445. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62333/0.69433. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62374/0.69681. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.61689/0.69570. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61899/0.69374. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61606/0.69307. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61004/0.69374. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61294/0.69379. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61005/0.69300. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.60838/0.69214. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60615/0.68962. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60397/0.69221. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60374/0.69015. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60717/0.68813. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.59748/0.69073. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59461/0.69077. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59505/0.68714. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58748/0.68902. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59615/0.68656. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59067/0.69192. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58654/0.68971. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59307/0.68659. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58431/0.68938. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58161/0.69099. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57060/0.69124. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57054/0.69052. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56964/0.68954. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.56699/0.69336. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56650/0.69001. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56519/0.69211. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56604/0.69709. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56159/0.69099. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56468/0.69359. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.54736/0.69290. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55411/0.69625. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.55181/0.69497. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55482/0.69762. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54488/0.69897. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.54893/0.69908. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54278/0.68781. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54127/0.69566. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54222/0.69498. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53258/0.69845. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52715/0.69908. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.53696/0.69944. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53077/0.69975. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53301/0.70647. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52834/0.70251. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52370/0.70673. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51456/0.71399. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50894/0.72091. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51377/0.71273. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51025/0.71719. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50269/0.72063. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50706/0.72091. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50266/0.72565. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51371/0.72263. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50671/0.72412. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48687/0.73005. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49312/0.72912. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48174/0.73321. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69389/0.68296. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68873/0.68313. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68789/0.68319. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68726/0.68286. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68704/0.68242. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68659/0.68217. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68561/0.68187. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68611/0.68161. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68514/0.68127. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68502/0.68086. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68516/0.68080. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68500/0.68047. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68353/0.68023. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68488/0.67999. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68406/0.67963. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68292/0.67953. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68266/0.67946. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68262/0.67920. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68186/0.67887. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68110/0.67887. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68080/0.67887. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68158/0.67890. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68108/0.67877. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67985/0.67891. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67987/0.67910. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67932/0.67921. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67794/0.67913. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67855/0.67899. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67743/0.67887. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67716/0.67921. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67729/0.67938. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67686/0.67910. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67574/0.67939. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67580/0.67978. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67517/0.68062. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67328/0.68086. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67303/0.68076. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67434/0.68092. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67265/0.68133. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67149/0.68238. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67090/0.68281. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67160/0.68277. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67026/0.68353. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66908/0.68418. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66766/0.68444. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67052/0.68493. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66904/0.68489. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66896/0.68484. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66624/0.68489. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66519/0.68562. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66535/0.68673. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66278/0.68689. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66295/0.68724. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66192/0.68762. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66023/0.68816. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66063/0.68898. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65958/0.69019. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66094/0.69102. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65771/0.69122. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65611/0.69217. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65650/0.69241. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65578/0.69314. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65547/0.69394. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65478/0.69434. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65596/0.69497. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65403/0.69478. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65360/0.69560. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65321/0.69622. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64969/0.69604. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65033/0.69611. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65252/0.69726. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64696/0.69785. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64796/0.69795. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64586/0.69893. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64660/0.69994. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64320/0.70049. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64313/0.70066. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64518/0.70056. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64300/0.70016. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64112/0.70123. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64213/0.70255. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63850/0.70339. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63440/0.70443. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63927/0.70595. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63473/0.70490. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63497/0.70415. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63553/0.70584. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63625/0.70714. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63591/0.70720. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63169/0.70654. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63110/0.70889. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62868/0.70768. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63285/0.70812. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62878/0.70716. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62781/0.71064. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62339/0.71017. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62682/0.71126. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.62097/0.70995. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62208/0.71372. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61954/0.71267. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68941/0.68246. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68779/0.68301. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68668/0.68375. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68648/0.68445. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68578/0.68517. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68542/0.68578. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68467/0.68644. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68418/0.68718. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68380/0.68799. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68370/0.68873. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68297/0.68944. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68242/0.69015. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68209/0.69082. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68160/0.69140. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68181/0.69181. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68149/0.69211. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68112/0.69251. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68104/0.69308. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68054/0.69343. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68004/0.69368. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67964/0.69404. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67940/0.69436. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67902/0.69465. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67888/0.69495. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67914/0.69484. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67819/0.69470. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67798/0.69506. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67798/0.69511. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67750/0.69549. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67640/0.69555. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67634/0.69554. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67611/0.69553. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67544/0.69550. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67614/0.69565. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67547/0.69592. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67546/0.69623. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67542/0.69663. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67536/0.69633. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67303/0.69675. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67419/0.69729. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67387/0.69705. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67238/0.69789. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67262/0.69811. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67132/0.69882. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67249/0.69928. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67085/0.69951. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67119/0.69934. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67004/0.70014. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66957/0.70061. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66955/0.70075. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66901/0.70213. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67125/0.70133. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66821/0.70198. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66774/0.70253. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66812/0.70261. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66738/0.70317. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66547/0.70396. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66590/0.70398. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66524/0.70450. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66523/0.70500. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66472/0.70558. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66316/0.70615. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66358/0.70690. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66270/0.70717. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66099/0.70719. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66145/0.70856. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66029/0.70868. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66172/0.70960. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66282/0.71044. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65880/0.71159. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65731/0.71262. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65906/0.71315. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65466/0.71496. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65608/0.71640. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65620/0.71587. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65332/0.71902. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65203/0.71954. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65321/0.72068. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.65129/0.72177. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65158/0.72301. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65122/0.72260. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65075/0.72368. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65039/0.72508. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64936/0.72540. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64650/0.72819. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64535/0.72961. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64472/0.73109. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64313/0.73158. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64183/0.73431. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.64286/0.73466. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63976/0.73775. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.63765/0.73907. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63883/0.73992. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63584/0.74070. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63512/0.74414. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63443/0.74654. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63424/0.74800. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62940/0.75186. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62755/0.75347. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62818/0.75542. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.68830/0.70440. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68641/0.70745. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68607/0.71006. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68536/0.71166. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68458/0.71309. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68393/0.71415. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68377/0.71440. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68340/0.71514. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68234/0.71522. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68233/0.71491. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68085/0.71401. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68127/0.71322. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.67863/0.71245. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67864/0.71155. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67784/0.71044. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67512/0.70985. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67431/0.70790. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67135/0.70652. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.66963/0.70432. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66826/0.70142. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66586/0.70014. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66173/0.69478. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66144/0.69187. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.66035/0.68927. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65859/0.68574. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65476/0.68221. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65153/0.67784. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.64937/0.67589. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64659/0.67160. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.64488/0.67189. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64526/0.67000. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64008/0.66716. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.63925/0.66438. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.63876/0.66426. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63350/0.66106. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.62919/0.66182. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63521/0.65986. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.62802/0.66031. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62790/0.66082. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62680/0.66043. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62282/0.66058. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62282/0.66025. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.61984/0.65966. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61925/0.65992. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61707/0.65950. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61506/0.66022. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.61393/0.66032. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60895/0.66244. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61072/0.66321. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60662/0.66411. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60647/0.66500. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60468/0.66653. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.59850/0.66760. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60435/0.67014. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59332/0.67062. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59524/0.67251. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59447/0.67378. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59217/0.67341. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59357/0.67476. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.59096/0.67711. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58739/0.67835. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58230/0.67763. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57678/0.68142. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57569/0.68488. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57690/0.68580. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57831/0.68748. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56876/0.69170. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.56749/0.69289. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56448/0.69505. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56945/0.69557. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.56153/0.69752. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56345/0.69890. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55811/0.70062. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.55488/0.70474. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.55392/0.70560. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54881/0.70312. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55257/0.70834. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54868/0.71131. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.53839/0.71090. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54094/0.71566. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53203/0.71539. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52795/0.71900. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.52785/0.72223. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53307/0.72105. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53062/0.72422. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.51996/0.72736. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51992/0.72458. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52018/0.72818. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51483/0.73011. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50918/0.72944. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51295/0.73351. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51134/0.73976. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50269/0.74285. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49628/0.74797. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50941/0.74482. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49091/0.75679. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49522/0.75147. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49007/0.74897. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.48660/0.75749. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48910/0.75757. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69490/0.69724. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69081/0.69964. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68950/0.70131. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68934/0.70231. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68941/0.70275. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68889/0.70327. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68905/0.70396. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68825/0.70457. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68872/0.70469. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68805/0.70539. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68777/0.70585. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68772/0.70633. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.70698. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68662/0.70762. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68591/0.70829. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68631/0.70881. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68600/0.70956. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68450/0.70979. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68545/0.71076. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68596/0.71097. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68422/0.71165. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68588/0.71187. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68385/0.71243. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68419/0.71275. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68393/0.71370. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68218/0.71399. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68143/0.71497. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68143/0.71470. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68029/0.71551. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68025/0.71637. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67955/0.71643. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68057/0.71690. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67741/0.71604. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68128/0.71656. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67846/0.71756. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67554/0.71771. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67675/0.71879. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67485/0.71848. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67360/0.71836. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67397/0.71892. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67213/0.71864. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67116/0.71895. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66796/0.71949. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66543/0.71874. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66730/0.72011. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66341/0.72073. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66356/0.72302. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66134/0.72388. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66146/0.72371. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65832/0.72375. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65551/0.72629. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65688/0.72974. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65337/0.72797. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65051/0.73271. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64985/0.73305. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64609/0.73197. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64542/0.73416. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64353/0.73822. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64069/0.74081. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64263/0.74201. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63948/0.74354. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63708/0.74603. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63575/0.74607. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63116/0.74272. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63070/0.74804. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62294/0.74994. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63108/0.75338. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62707/0.75352. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62203/0.75680. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61936/0.75935. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61837/0.76381. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61757/0.76498. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61116/0.77280. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61268/0.76821. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60870/0.77132. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60883/0.77681. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60793/0.77806. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60285/0.77901. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60376/0.77848. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60102/0.78279. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59793/0.78093. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59905/0.78301. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59515/0.78767. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59076/0.79514. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58927/0.79256. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58529/0.80169. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58701/0.80417. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58859/0.80881. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58329/0.81012. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58380/0.80835. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58244/0.80806. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57095/0.81633. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57375/0.81758. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56632/0.82637. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56683/0.82922. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56678/0.83076. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55782/0.84244. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56149/0.84610. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55472/0.85378. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55751/0.84943. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69565/0.69185. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69341/0.69202. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69225. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69229/0.69245. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69168/0.69258. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69055/0.69280. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68950/0.69326. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68875/0.69385. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68871/0.69453. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68668/0.69521. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68559/0.69614. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68438/0.69700. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68442/0.69762. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68409/0.69814. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68334/0.69865. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68293/0.69857. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68131/0.69870. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68050/0.69915. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67998/0.69937. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67870/0.69997. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67843/0.70021. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67823/0.70020. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67640/0.70067. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67657/0.70098. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67449/0.70117. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67381/0.70152. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67294/0.70210. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67201/0.70225. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67184/0.70251. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67017/0.70310. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67007/0.70383. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67091/0.70339. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66748/0.70408. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.66557/0.70479. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66456/0.70523. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66491/0.70565. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66362/0.70512. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66120/0.70573. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66184/0.70514. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65931/0.70686. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65853/0.70693. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65438/0.70642. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65444/0.70775. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65370/0.70852. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65198/0.70985. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64945/0.70986. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64550/0.71015. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64656/0.71175. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64293/0.71150. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64254/0.71422. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64169/0.71735. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64049/0.71695. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63737/0.71998. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63368/0.72159. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63082/0.72286. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62943/0.72722. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63366/0.72931. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.62793/0.72894. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62387/0.73332. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62380/0.73476. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62445/0.73440. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61755/0.73566. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62059/0.73779. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61645/0.74176. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61544/0.74538. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61306/0.74418. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61492/0.74970. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61224/0.74763. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60676/0.75054. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60785/0.75295. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60721/0.76063. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60304/0.75729. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60369/0.75793. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60042/0.75663. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59450/0.76642. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59896/0.76566. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59625/0.76895. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59310/0.77015. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59137/0.77524. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58931/0.77666. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58504/0.77966. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.58655/0.77475. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58362/0.77737. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58107/0.78582. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57958/0.79030. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58544/0.78743. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57824/0.78734. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57510/0.79091. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57140/0.78931. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57442/0.79379. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57005/0.79721. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56141/0.79843. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56526/0.80423. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56376/0.80696. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56127/0.80955. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56100/0.80800. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55862/0.80635. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55873/0.81186. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55657/0.81421. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55141/0.82024. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69412/0.69256. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69311/0.69259. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69287/0.69265. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69262. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69220/0.69256. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69161/0.69255. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69202/0.69255. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69093/0.69264. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69178/0.69268. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69078/0.69272. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68946/0.69291. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68870/0.69331. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68791/0.69383. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68745/0.69438. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68584/0.69515. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68536/0.69592. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68391/0.69698. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68329/0.69808. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68037/0.69937. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68022/0.70055. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67941/0.70190. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67841/0.70325. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67498/0.70472. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67518/0.70691. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67319/0.70815. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67175/0.70954. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67037/0.71111. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66891/0.71275. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66916/0.71351. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66611/0.71616. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66371/0.71919. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66262/0.71976. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66067/0.72343. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65612/0.72624. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65611/0.72920. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65542/0.73228. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65509/0.73510. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64742/0.73697. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64521/0.74157. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64498/0.74663. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64697/0.74844. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64134/0.75166. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63844/0.75475. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63444/0.75878. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63551/0.76208. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63385/0.76408. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63047/0.76800. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62666/0.77173. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62012/0.77601. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62673/0.77743. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61973/0.77825. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61838/0.78313. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61644/0.78612. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61491/0.78937. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61541/0.78925. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.61038/0.79288. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60485/0.79634. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59864/0.79887. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60108/0.80197. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59721/0.80945. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59738/0.81038. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58814/0.81275. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59549/0.81366. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58625/0.81380. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58387/0.81839. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58065/0.82307. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57671/0.82541. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57626/0.83242. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57635/0.82809. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56849/0.83084. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56426/0.83138. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55952/0.83562. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55989/0.83909. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55961/0.84016. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.55638/0.85111. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55472/0.85244. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55096/0.85258. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54334/0.85344. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54546/0.85309. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53737/0.86097. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.53669/0.86808. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.53301/0.87330. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52883/0.86416. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53845/0.86025. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52863/0.87560. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.52384/0.87164. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.52018/0.88281. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.51698/0.88701. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51523/0.88213. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51185/0.88634. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51141/0.89332. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50417/0.89893. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50444/0.90267. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49658/0.89185. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.49900/0.91020. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48652/0.90560. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49048/0.90295. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48205/0.90671. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49927/0.90190. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48665/0.89857. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69441/0.69487. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69476/0.69455. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69379/0.69415. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69369/0.69382. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69292/0.69365. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69314/0.69347. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69317/0.69337. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69237/0.69330. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69252/0.69323. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69222/0.69323. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69177/0.69316. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69111/0.69318. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69128/0.69331. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69136/0.69334. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69044/0.69347. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69083/0.69359. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69005/0.69356. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68922/0.69367. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68900/0.69395. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68830/0.69414. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68835/0.69435. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68746/0.69456. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68720/0.69473. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68663/0.69482. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68548/0.69505. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68676/0.69492. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68491/0.69504. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68384/0.69524. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68531/0.69524. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68318/0.69565. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68406/0.69609. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68187/0.69626. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68220/0.69611. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68066/0.69610. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68021/0.69659. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67960/0.69677. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67736/0.69676. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67657/0.69794. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67663/0.69821. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67644/0.69895. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67334/0.69941. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67232/0.69919. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67172/0.70058. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67265/0.70101. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67318/0.70133. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67178/0.70225. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66720/0.70227. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66938/0.70254. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66590/0.70492. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66492/0.70507. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66327/0.70727. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66310/0.70620. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66110/0.70889. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66099/0.70912. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65779/0.70981. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65910/0.70997. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65663/0.71250. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65622/0.71421. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65088/0.71528. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65175/0.71613. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65106/0.71746. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64934/0.71886. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64976/0.72026. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64606/0.72158. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64702/0.72021. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64644/0.72352. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64355/0.72404. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64211/0.72367. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63978/0.72625. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64140/0.72582. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64376/0.72449. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63821/0.72611. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63636/0.72828. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63396/0.72911. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63023/0.72679. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62651/0.72947. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63142/0.73124. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62686/0.73118. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62565/0.73330. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62318/0.73621. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62062/0.73333. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61548/0.73281. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61562/0.73543. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60962/0.73777. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61974/0.73621. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61343/0.73641. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61674/0.74165. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61179/0.73599. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60596/0.74123. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60520/0.74279. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60593/0.74326. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60269/0.74425. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59535/0.74251. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60125/0.74584. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59359/0.74795. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59760/0.74880. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58990/0.74933. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60015/0.74460. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59446/0.75064. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59055/0.74806. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69441/0.68494. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.68644. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69148/0.68704. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69080/0.68774. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69020/0.68813. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69050/0.68865. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68884/0.68967. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68958/0.68971. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68865/0.68969. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68858/0.69023. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68818/0.69114. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68883/0.69062. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68770/0.69147. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68686/0.69164. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68693/0.69181. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68456/0.69224. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68551/0.69203. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68511/0.69210. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68366/0.69272. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68317/0.69287. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68506/0.69299. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68285/0.69191. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68252/0.69168. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68120/0.69245. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68027/0.69164. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68043/0.69235. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67969/0.69211. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68000/0.69230. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67735/0.69123. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67741/0.69183. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67769/0.69157. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67566/0.69129. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67619/0.69204. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67398/0.69085. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67455/0.69077. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67347/0.69194. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67314/0.69157. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67180/0.69399. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67385/0.69077. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67156/0.69147. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67115/0.69385. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66925/0.69249. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66689/0.69192. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66608/0.69403. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66573/0.69478. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66571/0.69409. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66607/0.69525. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66507/0.69671. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66294/0.69435. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66041/0.69835. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66388/0.69679. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65929/0.69789. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65818/0.69987. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66062/0.69826. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65643/0.69763. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65555/0.69982. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65577/0.70301. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65259/0.70070. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65319/0.70559. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65233/0.70313. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64960/0.70619. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65010/0.70318. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64996/0.70666. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64570/0.70734. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64523/0.71038. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64926/0.71022. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64139/0.71434. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64059/0.71350. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63940/0.71433. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63740/0.71512. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63823/0.71888. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63325/0.71942. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63067/0.72217. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62916/0.72231. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62515/0.72622. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62891/0.72881. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62539/0.73062. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62211/0.72928. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62300/0.72895. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62112/0.73051. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62011/0.73799. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61516/0.73568. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61282/0.74056. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61161/0.74141. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60815/0.74189. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60340/0.74531. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60734/0.75087. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60626/0.74914. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59990/0.75210. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59677/0.74933. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59934/0.75249. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59239/0.75569. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59006/0.75799. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59326/0.76424. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58647/0.76494. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58586/0.75987. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58733/0.76844. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58349/0.76681. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58033/0.77151. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57854/0.76812. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69184. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69093. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.69036. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69207/0.69020. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69178/0.68998. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69158/0.69005. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.69000. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.69014. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69011/0.69026. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68997/0.69056. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68943/0.69063. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68802/0.69131. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68781/0.69195. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68658/0.69228. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68680/0.69279. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68610/0.69354. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68481/0.69389. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68338/0.69488. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68491/0.69584. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68371/0.69620. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68244/0.69692. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68159/0.69833. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67952/0.69900. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68023/0.70033. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68016/0.70046. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67997/0.70108. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67821/0.70195. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67744/0.70264. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67767/0.70303. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67674/0.70375. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67644/0.70510. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67604/0.70604. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67502/0.70711. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67513/0.70725. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67463/0.70745. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67220/0.70769. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67297/0.70881. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67277/0.70906. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67273/0.70917. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67017/0.70935. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66984/0.70916. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67009/0.70891. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67006/0.70925. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66953/0.70989. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66723/0.71056. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66876/0.71245. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66552/0.71238. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66653/0.71238. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66583/0.71276. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66271/0.71324. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66569/0.71326. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66309/0.71207. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66327/0.71241. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66151/0.71355. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66010/0.71453. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66287/0.71482. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65680/0.71467. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65632/0.71466. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65736/0.71579. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65906/0.71598. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65506/0.71568. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65527/0.71610. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65408/0.71618. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65256/0.71631. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65293/0.71740. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65267/0.71798. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65137/0.71696. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65187/0.71842. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65177/0.71783. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64854/0.71715. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64544/0.71747. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64562/0.71863. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64828/0.71817. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64704/0.71784. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64332/0.71797. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64252/0.72033. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64256/0.72016. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64133/0.72005. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63833/0.72067. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64037/0.72102. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63576/0.71987. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63706/0.72071. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63559/0.71972. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62979/0.72284. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63090/0.72004. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62416/0.72354. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62760/0.72331. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62862/0.72554. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62389/0.72574. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62440/0.72823. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62302/0.72478. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62219/0.72926. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61903/0.72515. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61548/0.72742. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61656/0.72875. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61461/0.72887. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61575/0.73034. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61140/0.73160. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61212/0.73412. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60828/0.73508. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69200/0.68740. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.68850. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69223/0.68962. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.69071. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69062/0.69201. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68982/0.69362. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68848/0.69543. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68813/0.69710. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.69900. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68707/0.70106. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68652/0.70287. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68560/0.70472. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68386/0.70692. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68321/0.70900. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68318/0.71036. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68126/0.71204. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68054/0.71385. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67986/0.71520. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67970/0.71624. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67878/0.71720. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67796/0.71827. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67645/0.71914. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67768/0.71947. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67508/0.72017. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67520/0.72116. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67430/0.72175. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67314/0.72113. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67244/0.72115. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67262/0.72050. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67346/0.72080. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66994/0.72104. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66994/0.71998. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66863/0.71986. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66680/0.71945. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67064/0.71932. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66667/0.71896. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66601/0.71861. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66481/0.71817. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66506/0.71725. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66406/0.71639. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66303/0.71728. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65951/0.71663. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66018/0.71669. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65974/0.71590. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65638/0.71615. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65603/0.71625. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65706/0.71478. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65150/0.71531. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65178/0.71383. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65193/0.71187. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65058/0.71250. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64876/0.71006. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65197/0.70983. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64662/0.71200. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64540/0.71182. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64357/0.71219. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64142/0.71140. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63867/0.71154. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63790/0.70777. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63564/0.70996. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63689/0.70699. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63192/0.70899. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63346/0.70771. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62906/0.70834. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63077/0.70858. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62790/0.70501. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62474/0.70931. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62323/0.71020. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61992/0.70829. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62287/0.70806. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61663/0.71252. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61834/0.71721. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60839/0.71255. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61039/0.71080. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61018/0.71172. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60776/0.70772. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59790/0.71332. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60423/0.71096. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59894/0.70721. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60089/0.70887. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59602/0.70653. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59049/0.71333. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59082/0.71086. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58926/0.71313. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59417/0.71024. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58780/0.70784. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58055/0.70910. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57740/0.70218. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58103/0.70026. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57493/0.70813. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57848/0.71228. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56918/0.70240. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58113/0.70999. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56722/0.70583. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56677/0.70236. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56617/0.70222. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55637/0.70258. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55664/0.70538. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55984/0.70246. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55758/0.69293. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69471/0.69871. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69094/0.70016. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68892/0.70206. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68689/0.70444. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68592/0.70708. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68483/0.70927. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68380/0.71172. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68216/0.71416. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68177/0.71627. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68108/0.71811. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68078/0.71950. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68176/0.72063. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68077/0.72170. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68053/0.72251. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68129/0.72293. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67874/0.72430. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68042/0.72482. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67864/0.72555. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67844/0.72591. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67742/0.72698. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67744/0.72709. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67780/0.72734. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67715/0.72771. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67641/0.72780. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67672/0.72783. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67521/0.72829. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67506/0.72858. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67473/0.72886. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67503/0.72851. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67426/0.72917. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67448/0.72930. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67302/0.72969. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67334/0.73010. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67201/0.73066. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67329/0.72964. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67167/0.72936. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67058/0.72991. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67209/0.72934. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67172/0.72917. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66875/0.72909. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66854/0.72943. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66929/0.73030. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66675/0.73027. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66663/0.72934. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66523/0.72926. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66501/0.73001. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66473/0.73010. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66539/0.72956. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66592/0.72973. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66341/0.72958. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66096/0.72903. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66043/0.72937. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66172/0.72865. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65977/0.72787. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65901/0.72845. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65808/0.72874. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65571/0.73004. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65771/0.72831. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65565/0.72831. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65487/0.72918. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65344/0.72972. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65300/0.73101. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65107/0.73010. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65099/0.72954. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65016/0.73035. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64910/0.72999. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64789/0.73227. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64668/0.73063. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64756/0.72975. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64315/0.73150. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64415/0.73265. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64141/0.73251. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64213/0.73343. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64070/0.73462. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63922/0.73550. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63991/0.73645. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63590/0.73546. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63468/0.73711. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63306/0.74035. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63417/0.73905. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62977/0.73984. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62780/0.74006. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62832/0.74281. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62676/0.74129. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62281/0.74474. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62273/0.74407. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62104/0.74583. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61579/0.74685. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61766/0.74904. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61762/0.75088. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61502/0.74898. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61111/0.75320. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61735/0.75436. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61118/0.75435. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60286/0.75530. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60890/0.75872. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60467/0.76031. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59966/0.76148. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60008/0.76288. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60254/0.76184. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69715/0.69302. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69203/0.69372. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69063/0.69393. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69059/0.69417. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68926/0.69457. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68852/0.69502. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68678/0.69571. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68634/0.69637. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68662/0.69748. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68520/0.69831. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68452/0.69971. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68351/0.70108. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68356/0.70212. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68157/0.70333. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68251/0.70423. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68114/0.70518. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68157/0.70593. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68156/0.70653. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68080/0.70691. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68093/0.70756. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68111/0.70785. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67887/0.70838. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67924/0.70843. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67851/0.70912. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67978/0.70975. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67849/0.71025. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67760/0.71065. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67740/0.71117. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67791/0.71148. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67806/0.71168. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67670/0.71145. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67593/0.71166. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67638/0.71218. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67582/0.71199. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67496/0.71241. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67466/0.71277. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67386/0.71349. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67376/0.71410. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67442/0.71445. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67286/0.71435. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67408/0.71490. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67199/0.71531. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67148/0.71602. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67229/0.71637. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67088/0.71693. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66834/0.71728. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66930/0.71762. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67084/0.71758. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66856/0.71810. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66680/0.71904. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66807/0.71943. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66499/0.72005. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66678/0.72030. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66442/0.72151. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66375/0.72189. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66382/0.72262. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66196/0.72412. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66249/0.72561. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66211/0.72608. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65950/0.72781. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65679/0.72818. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65890/0.72895. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66039/0.72934. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65836/0.73032. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65823/0.73129. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65667/0.73304. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65395/0.73363. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65260/0.73493. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65076/0.73552. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65233/0.73598. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64928/0.73533. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64794/0.73884. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64824/0.73806. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64643/0.73946. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64673/0.73954. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64194/0.74187. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64133/0.74230. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64108/0.74530. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63362/0.74640. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63622/0.75147. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63728/0.75075. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63806/0.75331. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63329/0.75319. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63080/0.75512. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63019/0.75473. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62930/0.75870. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62896/0.76079. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62782/0.76067. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62113/0.75950. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62127/0.76658. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61896/0.76190. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61831/0.76663. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61942/0.76791. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61557/0.76716. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61666/0.76824. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61050/0.77093. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61008/0.77436. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60698/0.77340. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60569/0.78177. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60612/0.78294. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69516/0.69414. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69115/0.69738. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68997/0.69910. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68921/0.69925. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68911/0.69876. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68857/0.69829. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68777/0.69827. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68716/0.69805. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68632/0.69802. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68539/0.69761. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68422/0.69734. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68352/0.69702. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68334/0.69781. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68233/0.69755. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68204/0.69696. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68021/0.69847. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68013/0.69892. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67829/0.69884. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67944/0.70041. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67869/0.70009. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67723/0.70101. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67928/0.70133. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67585/0.70095. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67646/0.70278. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67601/0.70253. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67451/0.70371. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67589/0.70472. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67340/0.70522. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67388/0.70569. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67340/0.70680. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67261/0.70771. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67174/0.70903. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66993/0.70922. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67049/0.71103. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66881/0.71129. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66719/0.71162. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66821/0.71207. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66811/0.71234. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66631/0.71365. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66393/0.71439. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66257/0.71702. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66290/0.71616. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66416/0.71791. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66361/0.71959. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66080/0.71916. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66278/0.72036. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65997/0.72115. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65714/0.72165. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65809/0.72385. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65354/0.72339. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65632/0.72492. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65199/0.72638. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65128/0.72574. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64852/0.72952. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65312/0.73335. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64715/0.72941. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64743/0.73315. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64267/0.73482. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64336/0.73496. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64309/0.73919. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64282/0.73779. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64235/0.74379. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63685/0.74498. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63574/0.74601. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63535/0.74461. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63379/0.74975. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62943/0.74857. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63130/0.75110. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63143/0.75012. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62779/0.75518. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62570/0.75539. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62317/0.75658. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62251/0.75774. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62024/0.76160. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61663/0.75848. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61364/0.75836. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61562/0.76331. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61228/0.76815. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61479/0.76887. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61129/0.76813. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60740/0.77090. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60716/0.77028. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60865/0.76617. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60821/0.77306. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60011/0.76974. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59647/0.76912. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59695/0.77519. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60091/0.77371. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59119/0.77533. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58957/0.77476. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59286/0.77374. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59212/0.78059. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59163/0.77338. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58688/0.77766. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58471/0.78027. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58228/0.77783. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58152/0.77764. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57897/0.77154. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57076/0.77781. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57760/0.77752. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69247/0.68958. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.68962. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69228/0.68960. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.68959. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.68962. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69121/0.68966. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.68963. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69075/0.68964. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69061/0.68964. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69018/0.68953. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68914/0.68956. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.68958. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68877/0.68976. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68712/0.69002. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68645/0.69026. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68472/0.69031. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68357/0.69117. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68368/0.69186. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68246/0.69230. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68206/0.69289. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68026/0.69355. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67888/0.69458. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67730/0.69558. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67621/0.69617. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67652/0.69690. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67639/0.69721. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67449/0.69780. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67337/0.69844. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67225/0.69847. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67199/0.69917. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67139/0.69985. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67252/0.70006. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67080/0.70010. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66764/0.70032. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66868/0.69981. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66722/0.69917. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66630/0.69801. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66553/0.69811. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66448/0.69798. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66407/0.69809. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66083/0.69839. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65996/0.69921. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65760/0.69997. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66090/0.69988. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65636/0.70018. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65539/0.69990. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65702/0.69890. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65345/0.69963. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65167/0.70075. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64665/0.70076. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65086/0.70047. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65155/0.69992. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64693/0.70059. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64464/0.70121. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64596/0.70069. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64468/0.70126. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64127/0.70270. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63890/0.70373. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63879/0.70445. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63969/0.70397. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63622/0.70448. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63626/0.70454. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63187/0.70547. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63096/0.70749. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63177/0.70746. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62859/0.70701. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62642/0.70858. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62527/0.71115. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62522/0.71228. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62249/0.71231. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62033/0.71181. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61477/0.71502. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61747/0.71781. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61406/0.71668. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61309/0.71843. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61200/0.72148. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60726/0.72028. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60427/0.72202. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60268/0.72386. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60604/0.72292. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60313/0.72439. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59596/0.72850. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59729/0.73006. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59460/0.73118. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59195/0.73464. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58774/0.73317. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58578/0.73633. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58862/0.73698. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58652/0.73978. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58191/0.74423. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57718/0.74627. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57533/0.75049. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57671/0.75130. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57128/0.75250. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.57207/0.75381. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56934/0.75347. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56800/0.75785. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56553/0.76086. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56015/0.76329. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55591/0.76283. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69520/0.69026. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69334/0.69089. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69317/0.69176. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69371/0.69209. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69326/0.69275. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.69306. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69270/0.69333. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69196/0.69364. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69143/0.69402. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69057/0.69454. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69030/0.69493. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69098/0.69546. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69010/0.69596. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68933/0.69635. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68916/0.69669. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68820/0.69712. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68803/0.69764. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68816/0.69801. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68689/0.69891. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68660/0.69951. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68614/0.70028. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68596/0.70113. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68551/0.70181. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68512/0.70251. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68411/0.70311. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68212/0.70421. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68268/0.70498. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68209/0.70575. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68139/0.70622. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68221/0.70659. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67934/0.70723. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67892/0.70739. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67956/0.70828. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67699/0.70835. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67640/0.70872. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67725/0.70933. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67566/0.70976. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67359/0.71023. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67323/0.70999. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67236/0.71167. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67254/0.71113. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67000/0.71241. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67192/0.71286. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67045/0.71430. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66784/0.71383. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66794/0.71347. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66573/0.71312. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66419/0.71382. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66599/0.71472. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66199/0.71420. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66466/0.71271. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66275/0.71040. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66046/0.71112. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65526/0.71140. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65227/0.71236. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65551/0.71135. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65093/0.71111. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64932/0.71027. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64884/0.70940. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64707/0.70851. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64555/0.70740. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64159/0.70582. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64028/0.70657. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63385/0.70660. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63568/0.70560. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62950/0.70774. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63293/0.70512. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63050/0.70488. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62402/0.70346. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62421/0.70981. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61690/0.70787. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61747/0.70918. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61256/0.70869. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61332/0.71083. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60668/0.71042. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60667/0.71183. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60103/0.71394. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59892/0.71848. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59316/0.71744. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59012/0.71866. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58387/0.72387. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58345/0.72962. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57652/0.73291. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58156/0.73926. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57475/0.73396. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57084/0.73735. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57802/0.73583. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56317/0.74146. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57033/0.75057. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56444/0.75405. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56348/0.74896. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55367/0.75118. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55198/0.75841. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54772/0.76123. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55176/0.75764. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54930/0.77024. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54383/0.77181. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53649/0.77127. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53650/0.77059. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53239/0.77689. Took 0.10 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69463/0.69213. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.69110. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.69072. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69219/0.69013. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69211/0.69002. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69155/0.68935. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69170/0.68899. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69094/0.68868. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69114/0.68834. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68976/0.68805. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69007/0.68787. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68957/0.68733. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69012/0.68731. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68918/0.68682. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68928/0.68658. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68906/0.68669. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68757/0.68660. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.68547. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68698/0.68533. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68728/0.68546. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68637/0.68495. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68631/0.68511. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68515/0.68500. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68354/0.68501. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68428/0.68498. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68206/0.68446. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68329/0.68456. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68072/0.68448. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67949/0.68450. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67967/0.68432. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67841/0.68436. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67653/0.68400. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67487/0.68383. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67478/0.68359. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67396/0.68331. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67372/0.68292. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66943/0.68303. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66681/0.68341. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66597/0.68365. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66360/0.68455. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66292/0.68517. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66040/0.68564. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66227/0.68612. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65826/0.68595. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65332/0.68655. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65277/0.68695. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65005/0.68764. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65087/0.68723. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64695/0.68751. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64402/0.68856. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64181/0.69037. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63829/0.69315. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63854/0.69310. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63583/0.69520. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63442/0.69806. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63136/0.69971. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62540/0.70279. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63071/0.70492. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62596/0.70663. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62181/0.70816. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61369/0.71178. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61670/0.71648. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.61091/0.72025. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61018/0.72418. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60751/0.72632. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60419/0.72926. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60617/0.73379. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60163/0.73898. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59826/0.74224. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58933/0.74962. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59323/0.74920. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58967/0.75841. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58795/0.76006. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57998/0.76151. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.58140/0.76546. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57867/0.77104. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57454/0.77629. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57292/0.78257. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57403/0.78646. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56251/0.78874. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56853/0.79490. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56676/0.79383. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55585/0.80587. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55943/0.81117. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55244/0.81008. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55465/0.81473. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54985/0.82566. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54695/0.82248. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54026/0.82788. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53704/0.83759. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53686/0.84160. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53659/0.84943. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53347/0.85428. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52785/0.85977. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52456/0.86256. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51934/0.87300. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50984/0.87481. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51540/0.88593. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51647/0.89390. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50796/0.89812. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69509/0.69298. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69397/0.69268. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69415/0.69244. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69222/0.69235. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69307/0.69233. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69248/0.69223. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69177/0.69222. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69214. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.69086/0.69199. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69139/0.69186. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69015/0.69169. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.69161. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69017/0.69164. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68923/0.69153. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68828/0.69155. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68695/0.69144. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68578/0.69130. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68650/0.69150. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68374/0.69129. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68320/0.69079. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68099/0.69077. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68046/0.69035. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67570/0.69024. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67770/0.69068. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67425/0.69094. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67226/0.69161. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67347/0.69195. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66978/0.69113. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66794/0.69167. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66734/0.69256. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66544/0.69321. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66104/0.69453. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65732/0.69509. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65690/0.69622. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65024/0.69695. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65073/0.69815. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64576/0.69661. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64754/0.69834. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64446/0.70065. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64280/0.69976. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63953/0.69955. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63814/0.70198. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62847/0.70397. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62976/0.70692. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62913/0.70977. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62645/0.70994. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62186/0.70782. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61670/0.71049. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61081/0.71359. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.60762/0.71593. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60582/0.72127. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60413/0.71963. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.59952/0.72805. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60300/0.72280. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59225/0.72568. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59642/0.72875. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59259/0.72834. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.58493/0.72881. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.58608/0.72961. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58631/0.73264. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58122/0.73237. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.56978/0.73738. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.56750/0.73872. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.56788/0.74090. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.56292/0.73623. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.56182/0.73767. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.56142/0.74882. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.55434/0.74696. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.55585/0.74296. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.54497/0.74771. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.54759/0.74595. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54107/0.75684. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.53806/0.77096. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.53787/0.76360. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.52969/0.76288. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53377/0.76236. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.52524/0.76586. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.52587/0.77744. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.51971/0.78149. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.51419/0.78077. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.51239/0.78436. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.51344/0.78798. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.50314/0.79478. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51349/0.79322. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.50269/0.79423. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.50190/0.79372. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.50586/0.80922. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.49744/0.80202. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.49725/0.80748. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.48558/0.81033. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.49244/0.81155. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.48194/0.81940. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.48571/0.83618. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.47083/0.84265. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.47924/0.83344. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.47209/0.83757. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.46694/0.84965. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.46988/0.85322. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.46702/0.86484. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.45463/0.85948. Took 0.08 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69436/0.68779. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69197/0.69018. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69275. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69061/0.69573. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69084/0.69846. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68920/0.70097. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68927/0.70364. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68904/0.70574. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68783/0.70759. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68831/0.70958. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68737/0.71131. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68709/0.71299. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68713/0.71487. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.71599. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68649/0.71729. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68616/0.71816. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68588/0.71870. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68490/0.71948. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68434/0.71965. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68443/0.72032. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68366/0.72138. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68315/0.72143. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68185/0.72235. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68266/0.72294. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68204/0.72367. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68104/0.72424. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68005/0.72408. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67966/0.72370. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67883/0.72535. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67756/0.72505. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67719/0.72533. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67590/0.72513. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67587/0.72395. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67523/0.72477. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67387/0.72490. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67404/0.72542. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67092/0.72536. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67335/0.72506. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66862/0.72595. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66774/0.72703. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66958/0.72719. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66697/0.72657. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66708/0.72815. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66478/0.72824. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66395/0.72917. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66497/0.73072. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66135/0.73338. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66392/0.73327. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66073/0.73195. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65863/0.73276. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65928/0.73497. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65560/0.73679. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65481/0.73777. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65483/0.74041. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65302/0.73848. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65083/0.73690. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65058/0.74067. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64812/0.74191. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64752/0.74689. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64978/0.74655. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64381/0.74728. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64522/0.75038. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64204/0.75273. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63883/0.75608. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63605/0.75568. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63263/0.75579. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63650/0.76206. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63207/0.76208. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63125/0.76398. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62838/0.76298. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62455/0.76587. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62400/0.76545. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62638/0.76800. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62477/0.77014. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62096/0.77224. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62457/0.77197. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61807/0.77370. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61656/0.77106. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61314/0.77440. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61206/0.77469. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60992/0.77969. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60327/0.78381. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60345/0.78077. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60375/0.78483. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60164/0.78223. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60367/0.78330. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60032/0.78304. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59660/0.79044. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59902/0.78710. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59020/0.78634. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59179/0.78846. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58842/0.79102. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58891/0.79666. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58480/0.79726. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57764/0.79620. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58288/0.79463. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57098/0.80065. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57498/0.79572. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57022/0.79955. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57179/0.80748. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69533/0.69468. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69233/0.69520. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69228/0.69592. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69158/0.69667. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69095/0.69732. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69040/0.69802. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68982/0.69880. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.69969. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68873/0.70078. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68855/0.70148. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.70229. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68770/0.70305. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68646/0.70409. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68589/0.70496. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68438/0.70611. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68429/0.70743. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68432/0.70884. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68218/0.71029. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68069/0.71204. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68169/0.71314. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67846/0.71516. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67905/0.71734. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67879/0.71943. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67656/0.72110. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67299/0.72346. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67275/0.72612. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67005/0.72920. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66891/0.73130. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66458/0.73414. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66471/0.73572. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66609/0.73807. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66453/0.73945. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65635/0.74136. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65932/0.74376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65550/0.74474. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65630/0.74787. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65267/0.74908. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64887/0.75216. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64896/0.75405. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64873/0.75613. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64315/0.75748. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64036/0.75976. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63944/0.76222. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63915/0.76388. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64112/0.76355. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63399/0.76467. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62947/0.76773. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63023/0.76989. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63419/0.77043. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62849/0.77220. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62291/0.77508. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62379/0.77730. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61810/0.78083. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61453/0.78328. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61916/0.78312. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61148/0.78472. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61316/0.78648. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61025/0.78908. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60855/0.78782. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60657/0.78888. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60182/0.79210. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60004/0.79374. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60061/0.79654. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59477/0.79851. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58609/0.79986. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58646/0.79961. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58501/0.80053. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58609/0.80132. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57948/0.80463. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57872/0.80338. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57348/0.80799. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57951/0.80859. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56794/0.81203. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56814/0.81430. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.57153/0.81110. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55743/0.81120. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55536/0.81086. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55557/0.81149. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55460/0.81123. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54340/0.82034. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54904/0.81818. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54548/0.82199. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54327/0.82361. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54127/0.82572. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53902/0.82056. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.52770/0.83088. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53346/0.83310. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52885/0.83376. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52540/0.83023. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52312/0.83307. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51290/0.83486. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51273/0.83732. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51623/0.84143. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51563/0.84734. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51658/0.84403. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50741/0.84304. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50361/0.86098. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50202/0.86144. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49548/0.86035. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.49586/0.85452. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69281/0.68598. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69110/0.68383. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.68275. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68904/0.68217. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.68189. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68861/0.68135. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68874/0.68082. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68804/0.68017. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68815/0.68020. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68600/0.67951. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68580/0.67943. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68606/0.67919. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68429/0.67894. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68452/0.67886. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68419/0.67894. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68277/0.67843. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68174/0.67857. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68106/0.67825. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67847/0.67809. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67847/0.67825. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67749/0.67857. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67591/0.67857. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67377/0.67858. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67325/0.67937. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67117/0.67992. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67037/0.68035. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66796/0.68093. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66543/0.68162. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66394/0.68265. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66342/0.68408. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66032/0.68581. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65791/0.68684. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65796/0.68674. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65397/0.68911. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65030/0.68998. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64631/0.69229. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64376/0.69362. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64682/0.69562. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64308/0.69351. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64173/0.69534. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63483/0.69556. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63375/0.69735. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63244/0.70065. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63257/0.69885. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62740/0.70083. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62452/0.70156. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62366/0.70022. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62044/0.70239. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62494/0.70470. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61698/0.70354. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61392/0.70929. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61383/0.70844. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61536/0.70768. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60921/0.70962. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61463/0.70972. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60936/0.71292. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61024/0.71156. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60214/0.71102. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60411/0.71246. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60145/0.71768. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60400/0.71877. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59810/0.71909. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59239/0.72053. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59507/0.71869. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59416/0.72272. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59662/0.72331. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58454/0.72247. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58958/0.72330. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58594/0.72036. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58659/0.72113. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58426/0.72363. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58410/0.72233. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58052/0.72779. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57960/0.72538. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57719/0.72857. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57221/0.73381. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56956/0.72743. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.56888/0.73362. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56491/0.73741. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56593/0.73872. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.56030/0.73364. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55982/0.74273. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56018/0.74153. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55570/0.74630. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55346/0.74236. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55510/0.74326. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54690/0.75207. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55269/0.74864. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55102/0.74751. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54094/0.75722. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54495/0.76097. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53556/0.75784. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54685/0.75368. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53935/0.76224. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53551/0.75621. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53329/0.75965. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53260/0.76682. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52760/0.76023. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52584/0.76243. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52674/0.77261. Took 0.10 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69831/0.69394. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69089/0.69270. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68981/0.69293. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68961/0.69335. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68909/0.69380. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68685/0.69454. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68563/0.69541. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68455/0.69654. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68392/0.69775. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68362/0.69859. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68182/0.69978. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67977/0.70131. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.67997/0.70284. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67951/0.70450. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67903/0.70638. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67768/0.70794. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67593/0.70971. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67390/0.71181. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67371/0.71358. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67156/0.71588. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66908/0.71891. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66955/0.72081. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66579/0.72344. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66773/0.72521. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66393/0.72657. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66356/0.72903. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66098/0.73063. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65732/0.73224. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65706/0.73306. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65215/0.73584. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64929/0.73737. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64975/0.73947. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64587/0.74069. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64514/0.74319. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64285/0.74461. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63878/0.74560. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63534/0.74651. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63449/0.74972. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63279/0.75164. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62868/0.75358. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62424/0.75478. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.62330/0.75814. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62326/0.76008. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.61973/0.76230. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62141/0.76356. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61454/0.76464. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.61375/0.76669. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60981/0.77055. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60760/0.77182. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60572/0.77489. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60787/0.77552. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60208/0.77601. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60115/0.77578. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59543/0.77723. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59053/0.78534. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.58839/0.78815. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.58752/0.79143. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59090/0.79880. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.58668/0.79926. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58152/0.80137. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58019/0.80110. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.57597/0.80230. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57855/0.80641. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.56793/0.81144. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57315/0.82102. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56653/0.82520. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56250/0.82388. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.55828/0.82750. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.55740/0.83133. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55458/0.83274. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.55249/0.84203. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.54790/0.84931. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.54132/0.85369. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.54237/0.85581. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54217/0.86312. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.53957/0.87178. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.53439/0.87665. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53028/0.88576. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52963/0.88845. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.53316/0.89539. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.52807/0.89623. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52235/0.89547. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.51436/0.91161. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51952/0.92055. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51255/0.92438. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51638/0.92998. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.50111/0.93864. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.50694/0.93914. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.50815/0.94813. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.49576/0.95297. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.49729/0.96331. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.48669/0.97389. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.49138/0.98656. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49252/0.98923. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48199/0.99266. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48330/1.00025. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.47532/1.01189. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.47788/1.01390. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.47546/1.02145. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.47490/1.02737. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69621/0.69311. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69601/0.69322. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69604/0.69332. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69505/0.69341. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69452/0.69367. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69347/0.69404. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69488. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69076/0.69618. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68922/0.69752. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68951/0.69856. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68864/0.69962. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.70037. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68636/0.70153. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68498/0.70300. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68376/0.70461. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68326/0.70716. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68268/0.70926. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68019/0.71126. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68070/0.71295. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67985/0.71544. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67682/0.71803. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67803/0.72001. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67701/0.72199. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67476/0.72418. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67557/0.72610. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67481/0.72747. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67277/0.72897. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67353/0.73027. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67039/0.73168. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67018/0.73326. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66973/0.73424. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66716/0.73631. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66818/0.73814. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66367/0.74026. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66514/0.74163. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66239/0.74306. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66078/0.74440. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66155/0.74644. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65968/0.74794. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65765/0.74854. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65374/0.75003. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65059/0.75234. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65128/0.75507. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64983/0.75674. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64684/0.75824. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64527/0.76351. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64225/0.76826. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64246/0.77133. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64027/0.77439. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63500/0.78052. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63328/0.78198. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63313/0.78584. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62443/0.79158. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62444/0.79430. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62238/0.79546. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62074/0.79472. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61315/0.80166. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61123/0.80719. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61184/0.81146. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60475/0.81559. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60910/0.81109. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60615/0.81691. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60034/0.82334. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59557/0.82587. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59414/0.83038. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58653/0.83322. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58394/0.83208. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58490/0.83613. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58768/0.84229. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57476/0.84417. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58222/0.84661. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56639/0.85365. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56264/0.86578. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56594/0.86605. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56624/0.86946. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56088/0.87425. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55754/0.87561. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55207/0.86896. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55361/0.87298. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54460/0.89314. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54594/0.88255. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54721/0.88234. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.53890/0.89078. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52943/0.89583. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53120/0.89587. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53074/0.89954. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.52002/0.90518. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52366/0.91243. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52032/0.90787. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52035/0.91151. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51036/0.91389. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50912/0.92239. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50963/0.92921. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50827/0.92652. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50300/0.92426. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.50104/0.91501. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50238/0.91849. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49582/0.92502. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49473/0.94208. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48555/0.93937. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69442/0.68993. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69232/0.68977. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69114/0.68981. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69028/0.69015. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68901/0.69040. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68842/0.69063. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68774/0.69087. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68740/0.69101. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68718/0.69124. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68665/0.69138. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68605/0.69136. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68545/0.69196. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68454/0.69212. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68505/0.69236. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68379/0.69271. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68343/0.69302. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68401/0.69299. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68324/0.69320. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68280/0.69343. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68346/0.69366. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68245/0.69418. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68230/0.69468. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68160/0.69527. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68072/0.69545. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68150/0.69604. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68047/0.69662. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67902/0.69703. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67867/0.69757. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67880/0.69788. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67793/0.69868. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67735/0.69965. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67626/0.69981. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67699/0.70030. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67543/0.70083. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67590/0.70140. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67433/0.70214. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67514/0.70245. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67383/0.70319. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67278/0.70367. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67166/0.70447. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67350/0.70530. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67135/0.70493. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67278/0.70594. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67177/0.70702. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66979/0.70803. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66986/0.70791. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66653/0.70914. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66790/0.70862. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66763/0.70850. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66557/0.70889. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66614/0.71010. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66510/0.70922. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66439/0.71020. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66344/0.71053. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66420/0.71161. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66344/0.71217. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66269/0.71365. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66144/0.71349. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66177/0.71388. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65786/0.71478. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66005/0.71406. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65970/0.71512. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65706/0.71533. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65721/0.71620. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65446/0.71745. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65640/0.71736. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65288/0.71953. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65480/0.71895. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65192/0.71829. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65322/0.71958. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65392/0.71965. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65157/0.72040. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64773/0.72085. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64864/0.72080. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64850/0.72203. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64541/0.72185. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64651/0.72399. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64346/0.72509. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64132/0.72698. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64165/0.72626. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63980/0.72542. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64173/0.72908. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64235/0.72758. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63972/0.72795. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63741/0.72800. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63380/0.72847. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63340/0.73143. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63388/0.72988. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63105/0.73265. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62819/0.73375. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62680/0.73417. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62864/0.73491. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62812/0.73582. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62537/0.73548. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62289/0.73665. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62283/0.73647. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62135/0.73663. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61820/0.74159. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61686/0.74073. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61325/0.73943. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69366/0.69081. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69126/0.69169. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69086/0.69230. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69037/0.69284. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68991/0.69347. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68932/0.69412. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68869/0.69477. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68884/0.69550. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68806/0.69602. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68803/0.69674. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68687/0.69749. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68631/0.69820. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68583/0.69909. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68629/0.69999. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68446/0.70084. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68447/0.70190. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68359/0.70289. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68362/0.70364. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68313/0.70450. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68256/0.70592. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68204/0.70691. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68136/0.70806. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68009/0.70885. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67885/0.71056. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67788/0.71242. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67732/0.71393. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67738/0.71492. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67591/0.71615. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67554/0.71756. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67446/0.71824. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67249/0.71989. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67073/0.72186. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67063/0.72329. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66874/0.72497. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66920/0.72639. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66576/0.72838. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66611/0.72978. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66524/0.73092. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66440/0.73298. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66105/0.73482. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66090/0.73648. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65972/0.73814. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65736/0.74029. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65674/0.74228. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65511/0.74629. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65593/0.74717. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65037/0.74987. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65181/0.75166. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64987/0.75319. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64826/0.75669. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64541/0.75825. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64655/0.76204. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64490/0.76271. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64300/0.76600. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.64408/0.76635. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64073/0.76751. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63903/0.77057. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63696/0.77126. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63742/0.77471. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63453/0.77619. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63475/0.78056. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63332/0.78005. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62965/0.78127. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62815/0.78653. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62782/0.78739. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62606/0.79155. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62496/0.78879. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62773/0.78910. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62237/0.79254. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62115/0.79521. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61757/0.79731. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61735/0.79814. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61762/0.80271. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61514/0.80239. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61551/0.80022. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61318/0.80477. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60983/0.80573. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60981/0.80980. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61125/0.80920. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60967/0.80800. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60753/0.80960. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60247/0.80967. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60430/0.81022. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60153/0.81272. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60354/0.81471. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60068/0.81490. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59456/0.81871. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59364/0.81726. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59670/0.81843. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59037/0.82527. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58923/0.82333. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59079/0.82754. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58950/0.82632. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58629/0.82429. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.58278/0.82575. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58012/0.82914. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57952/0.82958. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57809/0.83496. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57163/0.83563. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57359/0.83705. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69324/0.69442. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.69423. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69267/0.69403. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69292/0.69384. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69223/0.69363. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69167/0.69340. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.69311. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69080/0.69269. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69057/0.69228. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69041/0.69181. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68877/0.69127. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68834/0.69068. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68770/0.69001. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68566/0.68914. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68565/0.68809. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68354/0.68700. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68236/0.68601. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68003/0.68512. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67699/0.68411. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67476/0.68253. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67124/0.68123. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67144/0.67846. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66694/0.67676. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66233/0.67572. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.66081/0.67500. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.65675/0.67590. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65562/0.67513. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65249/0.67539. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.64979/0.67567. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64994/0.67570. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64628/0.67582. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.64522/0.67635. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.64388/0.67571. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64087/0.67609. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63963/0.67626. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63833/0.67700. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63215/0.67721. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.62850/0.67531. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62576/0.67688. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63089/0.67850. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62460/0.67838. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.61785/0.67846. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.61720/0.67995. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62112/0.68142. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61385/0.68222. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.60992/0.68382. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.60858/0.68570. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.60292/0.68484. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60216/0.68577. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60250/0.68668. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.59870/0.68855. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.59213/0.68822. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59330/0.69056. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.58986/0.69345. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.58397/0.69565. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.58776/0.69943. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.58132/0.70214. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.57226/0.70162. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.57105/0.70249. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.56982/0.70859. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.56912/0.71355. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.56288/0.71528. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.55732/0.71817. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.56029/0.72115. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.55590/0.72389. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.55760/0.72673. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.54640/0.72649. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.54660/0.73423. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.54518/0.73718. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.53816/0.74433. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.53617/0.74769. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.53008/0.75157. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.53209/0.75304. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.52791/0.75837. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.52573/0.75904. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.52583/0.77024. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.51770/0.77263. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.51933/0.77269. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.51758/0.77618. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.50961/0.78010. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.50415/0.79202. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.51327/0.79136. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.49877/0.79236. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.50152/0.79960. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.48426/0.81176. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.49931/0.81992. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.48414/0.81799. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.49576/0.82855. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.47905/0.82478. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.49095/0.82074. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.47525/0.83229. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.47525/0.83714. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.47576/0.83384. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.47577/0.83471. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.46655/0.84027. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.45835/0.84894. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.44817/0.86279. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.45620/0.86725. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.45185/0.86093. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.44659/0.87826. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69384/0.69264. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69310/0.69318. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69373. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69435. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69216/0.69480. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69525. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.69553. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69107/0.69587. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69005/0.69626. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68893/0.69678. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68881/0.69722. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68747/0.69781. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68620/0.69842. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68445/0.69945. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68423/0.70063. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68240/0.70177. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67976/0.70313. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67726/0.70436. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67647/0.70556. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67387/0.70768. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67073/0.70985. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66865/0.71175. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66664/0.71400. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66309/0.71627. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66533/0.71804. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.65808/0.71865. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65673/0.72030. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65804/0.72013. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65500/0.72319. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65114/0.72674. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64690/0.72821. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64308/0.72863. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64168/0.73254. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64109/0.73319. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64185/0.73571. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63371/0.73652. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.63461/0.73626. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.62931/0.73864. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62837/0.73956. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62500/0.74064. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.62629/0.74074. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62245/0.74080. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.61868/0.74572. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61486/0.74876. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61328/0.74915. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.60902/0.75425. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.60577/0.75244. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.60630/0.75233. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.60291/0.75677. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60392/0.75953. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.59568/0.76435. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.59848/0.76472. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59264/0.76277. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.58954/0.76768. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.58607/0.77454. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.58612/0.77524. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.58415/0.78050. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.58169/0.77965. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58416/0.78381. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.57133/0.78685. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.57229/0.79323. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.57058/0.79485. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57231/0.79399. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.56402/0.79574. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.56666/0.80011. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.56287/0.80552. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.55845/0.81050. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55627/0.81718. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.55615/0.81188. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55240/0.82325. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.54764/0.82457. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54790/0.82874. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.54639/0.83397. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.53925/0.83420. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54673/0.83359. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.53340/0.84008. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.54303/0.84146. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.53588/0.85782. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52252/0.85834. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53001/0.85787. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.52649/0.86697. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.52163/0.87649. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52002/0.87427. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.51947/0.86823. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51449/0.88051. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51185/0.88826. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51497/0.89334. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.51445/0.88634. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51037/0.89863. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50142/0.91233. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50461/0.90034. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.50845/0.91122. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50519/0.90520. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49170/0.92071. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.49137/0.91385. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48799/0.92316. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48952/0.92576. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49124/0.93107. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.48572/0.93158. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48147/0.93976. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69320/0.69465. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69251/0.69478. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69232/0.69491. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69507. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69195/0.69511. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69179/0.69541. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69214/0.69559. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.69576. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69122/0.69601. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69146/0.69624. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69073/0.69632. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.69674. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68995/0.69685. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68894/0.69717. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68823/0.69749. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68722/0.69784. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68790/0.69830. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.69860. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68635/0.69926. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68617/0.69952. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68556/0.69970. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68502/0.69967. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68460/0.69978. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68230/0.70063. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68135/0.70114. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68170/0.70124. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68184/0.70183. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68000/0.70289. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67949/0.70295. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67869/0.70292. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67701/0.70255. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67601/0.70408. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67520/0.70452. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67626/0.70520. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67246/0.70450. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67096/0.70576. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67203/0.70507. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67204/0.70493. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66702/0.70486. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66615/0.70502. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66539/0.70541. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66602/0.70476. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66517/0.70474. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66229/0.70445. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66290/0.70418. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65948/0.70386. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65732/0.70428. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65589/0.70506. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65526/0.70373. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65073/0.70303. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65023/0.70237. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64700/0.70152. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64910/0.70098. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64660/0.70065. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64187/0.69980. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64063/0.69979. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64133/0.69894. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63954/0.69955. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63616/0.69556. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63422/0.69400. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62929/0.69434. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63063/0.69461. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62666/0.69659. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62165/0.69525. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62569/0.69483. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62127/0.69554. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61715/0.69341. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62053/0.69571. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61263/0.69456. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61093/0.69582. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61251/0.70001. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60701/0.69658. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60584/0.69915. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60424/0.70094. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60354/0.70165. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60473/0.70505. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59602/0.70209. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59125/0.70494. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58876/0.70719. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58795/0.70988. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59057/0.70899. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.58957/0.71108. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58592/0.71417. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58065/0.71249. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57513/0.71718. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57827/0.71985. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57412/0.71785. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57409/0.72010. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57162/0.71890. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.57296/0.72264. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56903/0.72308. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56157/0.72796. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55676/0.72743. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55684/0.73457. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55407/0.73455. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55685/0.73938. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55550/0.73734. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55817/0.74322. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55018/0.73936. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54817/0.74480. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69504/0.69578. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69498. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69174/0.69516. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69224/0.69494. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69093/0.69543. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.69573. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69060/0.69592. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69003/0.69619. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68940/0.69665. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68913/0.69696. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68896/0.69734. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68845/0.69788. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68722/0.69799. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68732/0.69866. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68647/0.69853. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68625/0.69901. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68529/0.69922. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68508/0.69944. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68317/0.69975. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68261/0.70011. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68309/0.70041. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68188/0.70033. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68087/0.70062. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67891/0.70090. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67896/0.70078. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67743/0.70018. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67665/0.70036. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67563/0.69948. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67228/0.70011. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67112/0.70132. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67073/0.70129. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66685/0.70210. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66721/0.70168. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66441/0.70220. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66447/0.70319. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66203/0.70368. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66122/0.70247. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65811/0.70225. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65572/0.70284. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65461/0.70409. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65072/0.70488. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65178/0.70383. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64875/0.70565. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64717/0.70591. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64438/0.70511. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64577/0.70581. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64329/0.70689. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63977/0.70620. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64257/0.70582. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63541/0.70546. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63617/0.71144. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63457/0.71311. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.63400/0.71020. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63191/0.71014. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62660/0.70992. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62388/0.71006. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62406/0.71267. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62243/0.71657. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62418/0.71641. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61614/0.71508. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61911/0.71617. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61407/0.71479. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61276/0.71745. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61439/0.71732. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60716/0.71974. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60901/0.72232. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60433/0.72436. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59850/0.72697. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60438/0.72215. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60085/0.72447. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59776/0.72950. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59388/0.72868. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59725/0.72767. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59085/0.72907. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58932/0.73435. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58772/0.73159. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58210/0.73242. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58389/0.73788. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57809/0.73336. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57788/0.73764. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57578/0.73645. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56914/0.74229. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57089/0.74266. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56351/0.74122. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55999/0.74381. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56061/0.74733. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56101/0.75372. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55485/0.74920. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55055/0.74617. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55361/0.75741. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55093/0.76101. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55251/0.76055. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54960/0.75265. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54498/0.75684. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53972/0.75358. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53661/0.75825. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53835/0.76727. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53200/0.77000. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52748/0.76388. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52943/0.77457. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69362/0.69495. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69391/0.69460. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69339/0.69418. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69250/0.69388. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69240/0.69356. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69264/0.69337. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69168/0.69317. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69145/0.69314. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69151/0.69306. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69185/0.69293. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69172/0.69309. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69143/0.69323. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69112/0.69350. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69061/0.69381. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69078/0.69401. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69049/0.69427. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69073/0.69451. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68999/0.69484. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68998/0.69511. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68941/0.69563. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68997/0.69593. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68811/0.69641. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68813/0.69678. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68839/0.69684. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68726/0.69727. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68708/0.69781. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68607/0.69880. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68580/0.70015. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68521/0.70103. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68465/0.70123. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68354/0.70193. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68403/0.70281. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68127/0.70319. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68253/0.70422. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67993/0.70507. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67922/0.70570. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67898/0.70714. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67910/0.70756. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67830/0.70900. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67752/0.70970. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67541/0.70958. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67573/0.71000. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67246/0.70975. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67373/0.71010. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66979/0.70918. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67142/0.71116. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66789/0.70967. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67003/0.71244. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66635/0.71351. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66545/0.71560. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66384/0.71215. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66092/0.71676. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65888/0.71851. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65951/0.71909. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65554/0.72155. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65326/0.72133. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65323/0.71896. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65085/0.71889. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64946/0.71919. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64660/0.71776. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64553/0.72113. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64173/0.71995. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63903/0.72385. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63584/0.72375. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63213/0.72399. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63073/0.72122. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63083/0.72130. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62749/0.72302. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62208/0.72147. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62173/0.72212. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62327/0.72243. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62053/0.72310. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61619/0.72351. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61064/0.72563. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61665/0.72475. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61116/0.72351. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60945/0.72917. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60629/0.72831. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60050/0.73072. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60036/0.73088. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59527/0.73473. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59408/0.72925. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59172/0.73198. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58801/0.73758. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58119/0.73970. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58472/0.74102. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57675/0.74367. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57405/0.74551. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57126/0.74478. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57268/0.75116. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56504/0.75148. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56261/0.75266. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56724/0.75984. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56634/0.75730. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56167/0.75438. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54848/0.76742. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55658/0.76672. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55039/0.76071. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54786/0.76299. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54761/0.76981. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69470/0.69600. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69331/0.69538. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69292/0.69454. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69284/0.69367. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69317. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69287/0.69295. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69187/0.69275. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69134/0.69256. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69152/0.69244. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69185/0.69243. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69218/0.69247. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69098/0.69253. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69103/0.69262. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69100/0.69263. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69034/0.69271. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69008/0.69277. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68977/0.69287. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69063/0.69286. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68984/0.69299. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68910/0.69316. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68844/0.69333. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68910/0.69352. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68780/0.69373. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68771/0.69387. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68643/0.69431. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68649/0.69459. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68632/0.69477. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68433/0.69504. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68409/0.69540. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68246/0.69616. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68205/0.69697. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68106/0.69739. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68069/0.69766. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68131/0.69791. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68118/0.69806. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67957/0.69917. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67962/0.70010. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67841/0.70148. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67841/0.70246. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67593/0.70287. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67625/0.70403. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67629/0.70498. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67329/0.70608. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67376/0.70705. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67210/0.70784. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66982/0.70932. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66873/0.71070. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66805/0.71139. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66715/0.71361. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66690/0.71501. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66562/0.71522. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66607/0.71547. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66474/0.71737. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66347/0.71789. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65969/0.71862. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66128/0.71997. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65647/0.72274. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65530/0.72343. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65488/0.72529. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65560/0.72601. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65064/0.72812. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64830/0.72943. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64638/0.73130. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64627/0.73294. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64522/0.73377. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64446/0.73681. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64115/0.73955. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64191/0.74003. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63794/0.74127. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63866/0.74194. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63118/0.74517. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63672/0.74691. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63496/0.74616. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62693/0.74916. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62751/0.75203. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62380/0.75239. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62361/0.75518. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62162/0.75696. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61919/0.75905. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61657/0.76250. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61247/0.76503. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61145/0.76352. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60557/0.76718. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60727/0.76861. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60161/0.77260. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59756/0.77018. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60552/0.77316. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60259/0.77977. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59840/0.77848. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59551/0.78043. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59263/0.78226. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59199/0.78794. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58812/0.78802. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58617/0.78779. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58498/0.78763. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58047/0.79556. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57914/0.79614. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58188/0.79625. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56898/0.79946. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56863/0.80107. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69449/0.69407. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69242/0.69438. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69393/0.69465. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69218/0.69495. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69100/0.69526. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69117/0.69571. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69098/0.69615. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69020/0.69631. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68841/0.69653. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68913/0.69654. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68909/0.69645. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68818/0.69616. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68689/0.69604. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68619/0.69610. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68530/0.69593. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68380/0.69544. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68413/0.69558. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68305/0.69555. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68184/0.69491. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68126/0.69554. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67791/0.69580. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67964/0.69539. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67677/0.69501. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67633/0.69461. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67313/0.69476. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67083/0.69469. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67034/0.69554. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66789/0.69392. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66756/0.69469. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66538/0.69560. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66166/0.69532. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66230/0.69633. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66121/0.69657. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65785/0.69786. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65456/0.69765. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65029/0.69960. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64904/0.70018. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65006/0.70121. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64690/0.70106. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64488/0.70289. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64215/0.70249. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64096/0.70337. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63485/0.70802. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63538/0.70672. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63067/0.70740. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62464/0.70898. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62717/0.70715. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62572/0.71175. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62236/0.71110. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61562/0.71395. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61764/0.71542. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61524/0.71116. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61000/0.71408. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61177/0.71547. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60634/0.71296. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60600/0.71324. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60802/0.71806. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60284/0.72094. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59190/0.72466. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59479/0.72404. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59462/0.72128. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58973/0.72383. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.58565/0.72245. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58281/0.72758. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58472/0.72590. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58467/0.72270. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57693/0.71903. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57627/0.72278. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57442/0.71963. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56522/0.72161. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57160/0.71980. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.55997/0.72616. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56011/0.72561. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.55648/0.73392. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55413/0.72713. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55361/0.73435. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55427/0.72904. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55254/0.72928. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55103/0.73252. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54988/0.73104. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.53861/0.73216. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54252/0.73507. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.53719/0.73547. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.52755/0.73664. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52466/0.73783. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53212/0.72866. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.52156/0.73473. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52183/0.74484. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.51709/0.74230. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50630/0.74018. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50760/0.74945. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51048/0.73969. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50884/0.74429. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50381/0.74209. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.49440/0.75197. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50202/0.74907. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49447/0.75184. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49989/0.75123. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49357/0.73399. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.48585/0.74204. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69789/0.69395. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69380/0.69287. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69224/0.69290. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69168/0.69322. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69140/0.69349. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69067/0.69383. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69413. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69044/0.69427. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68980/0.69463. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68905/0.69521. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68890/0.69565. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68879/0.69615. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68729/0.69683. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68778/0.69726. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68602/0.69797. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68500/0.69892. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68501/0.70012. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68294/0.70134. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68309/0.70242. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68241/0.70363. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68197/0.70484. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68131/0.70564. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68253/0.70671. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67804/0.70790. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67595/0.70916. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67638/0.71064. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67507/0.71196. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67394/0.71264. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67351/0.71398. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67257/0.71515. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67180/0.71646. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67050/0.71747. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67125/0.71806. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66855/0.71880. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66634/0.71955. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66571/0.72157. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66486/0.72313. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66264/0.72370. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66233/0.72427. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66023/0.72419. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65854/0.72423. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65888/0.72454. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65561/0.72481. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65639/0.72596. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65247/0.72632. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65163/0.72622. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64971/0.72749. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64823/0.72918. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64679/0.72934. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64321/0.72935. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64120/0.73059. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64135/0.73110. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63609/0.73292. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63935/0.73296. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63130/0.73394. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62947/0.73515. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62955/0.73510. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62573/0.73810. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62368/0.73634. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62120/0.73662. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62033/0.73669. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61758/0.73671. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61461/0.73914. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61534/0.74037. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61373/0.74220. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60639/0.74309. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60512/0.74530. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60579/0.74488. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60461/0.74432. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59897/0.74702. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59557/0.75084. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59485/0.75060. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59846/0.74932. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58827/0.75072. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58962/0.75009. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59083/0.75356. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58493/0.75360. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58215/0.75842. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57734/0.76335. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57239/0.76464. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57145/0.76351. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57180/0.76666. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56195/0.77041. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56667/0.77140. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55934/0.77473. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56113/0.77751. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56048/0.78297. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54983/0.78514. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.54649/0.78924. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54172/0.79079. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54167/0.79860. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53450/0.80986. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53303/0.80139. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53508/0.81066. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52662/0.81214. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53814/0.81930. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53124/0.82178. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52947/0.82604. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51987/0.83095. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51350/0.83539. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70114/0.69732. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69478/0.69407. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69342/0.69341. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69246/0.69291. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69113/0.69277. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.69249. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68991/0.69222. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68862/0.69208. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68880/0.69200. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68785/0.69223. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68678/0.69222. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68695/0.69216. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68710/0.69237. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68619/0.69224. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68562/0.69241. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68555/0.69234. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68462/0.69221. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68400/0.69199. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68390/0.69172. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68267/0.69158. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68219/0.69130. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68336/0.69130. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68255/0.69072. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68107/0.69075. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68131/0.68971. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68030/0.69039. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68097/0.68971. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67960/0.68969. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67900/0.68896. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67919/0.68865. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67705/0.68856. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67745/0.68808. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67681/0.68797. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67549/0.68777. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67437/0.68830. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67380/0.68729. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67241/0.68802. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67121/0.68807. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67201/0.68785. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67053/0.68862. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66784/0.68747. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66725/0.68682. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66869/0.68618. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66533/0.68579. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66332/0.68608. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66129/0.68493. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66373/0.68490. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66068/0.68388. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66111/0.68442. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65890/0.68433. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65548/0.68303. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65668/0.68158. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65494/0.68234. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65262/0.68293. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65281/0.68258. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64918/0.68257. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64733/0.68229. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64811/0.68300. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64731/0.68199. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64632/0.68352. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64261/0.68528. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64147/0.68359. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63893/0.68392. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63357/0.68408. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63384/0.68512. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63070/0.68556. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63372/0.68363. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63038/0.68461. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62728/0.68478. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62752/0.68465. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62659/0.68652. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62480/0.68445. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62297/0.68756. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62122/0.68969. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61571/0.69031. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61787/0.68925. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61367/0.68899. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61456/0.69193. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61008/0.69292. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60605/0.69636. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60384/0.69338. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59944/0.69480. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60167/0.69874. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60188/0.69690. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59510/0.69701. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59134/0.69936. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59016/0.70067. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58867/0.69963. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58903/0.69881. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58455/0.70124. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58292/0.70156. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59172/0.70252. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.57694/0.70678. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57705/0.70630. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58069/0.70476. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57219/0.71117. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56955/0.71624. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57302/0.70149. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55936/0.70621. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57104/0.71529. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69370/0.69682. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69221/0.69579. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69309/0.69594. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69121/0.69597. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69128/0.69620. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69043/0.69631. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68992/0.69685. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68899/0.69742. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68840/0.69810. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68663/0.69892. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68782/0.69981. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68634/0.70034. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68506/0.70127. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68502/0.70251. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68446/0.70276. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68438/0.70426. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68336/0.70412. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68207/0.70525. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68111/0.70609. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68095/0.70788. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67940/0.70870. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67920/0.70891. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.70958. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67951/0.71046. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67842/0.71182. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67750/0.71248. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67760/0.71349. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67526/0.71460. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67544/0.71487. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67468/0.71709. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67151/0.71774. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67322/0.71861. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67313/0.72064. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67178/0.72237. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67020/0.72267. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66965/0.72470. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66943/0.72637. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66969/0.72727. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66709/0.72828. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66444/0.73085. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66645/0.73289. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66510/0.73502. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66540/0.73564. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66184/0.73822. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66107/0.74015. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65701/0.74220. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66159/0.74317. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65923/0.74488. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65812/0.74702. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65829/0.74947. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65405/0.74937. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65382/0.75320. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65167/0.75422. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65071/0.75517. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64909/0.75840. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64767/0.76069. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64651/0.76330. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64761/0.76635. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64368/0.76795. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64432/0.77024. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64053/0.77291. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63992/0.77417. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63862/0.77510. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63742/0.77891. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63553/0.78192. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63636/0.78461. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63014/0.78512. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63445/0.78753. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63012/0.78967. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62458/0.79484. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62360/0.79755. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62341/0.80179. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62086/0.80390. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61583/0.80867. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61759/0.81388. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61907/0.81785. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61077/0.82198. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61441/0.82493. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61322/0.82774. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60837/0.83149. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60735/0.83518. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60183/0.83926. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60055/0.84841. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59797/0.85096. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59615/0.85324. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59205/0.85863. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59368/0.86149. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59483/0.86699. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58968/0.87381. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58451/0.87287. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58087/0.88061. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58429/0.88510. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57775/0.88880. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57816/0.89673. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57386/0.90035. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56765/0.90361. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56798/0.90802. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56623/0.91279. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56119/0.92053. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55681/0.92868. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69332/0.69134. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69272/0.69113. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.69111. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69182/0.69112. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69128/0.69102. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69046/0.69098. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68987/0.69111. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.69121. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68878/0.69150. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68735/0.69169. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68693/0.69227. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68528/0.69260. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68497/0.69328. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68277/0.69401. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68268/0.69477. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68034/0.69598. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67760/0.69821. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67732/0.69957. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67472/0.70202. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67318/0.70523. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67007/0.70759. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.66944/0.71125. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66719/0.71252. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66625/0.71505. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66261/0.71796. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66426/0.71936. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66053/0.72055. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65873/0.72414. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65672/0.72614. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.65656/0.72673. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64933/0.72955. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65008/0.73210. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65021/0.73196. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64977/0.73364. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64584/0.73638. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64049/0.73833. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64174/0.74113. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63916/0.74263. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63824/0.74635. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63705/0.74894. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63252/0.75255. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63459/0.75238. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62720/0.75288. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62804/0.75719. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62615/0.75870. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62694/0.76066. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62309/0.76393. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61830/0.76888. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61796/0.77163. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61558/0.77490. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61454/0.77618. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61174/0.78091. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60942/0.78635. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60355/0.78782. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60163/0.79389. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60298/0.79838. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59456/0.80139. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59268/0.80969. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59002/0.81170. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59150/0.81571. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59408/0.81229. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58939/0.81983. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58216/0.82310. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.58261/0.82753. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57369/0.83358. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.57261/0.83815. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57211/0.84266. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57182/0.84829. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56691/0.85737. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56515/0.85861. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55598/0.85419. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55986/0.85617. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55996/0.85976. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55161/0.86719. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.54595/0.86890. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.54755/0.87598. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.54026/0.88037. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.54356/0.88624. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.53610/0.89139. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53629/0.89600. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.52620/0.90075. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52365/0.89962. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52390/0.91188. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52568/0.91018. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51431/0.91781. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.51427/0.92334. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.51369/0.92368. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.50735/0.92715. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.51627/0.92449. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50643/0.92631. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.50776/0.92945. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.49782/0.93489. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.49051/0.95625. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.48910/0.96233. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.48408/0.96773. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48514/0.95044. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48834/0.96795. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48480/0.97136. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.47850/0.97288. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.47020/0.98904. Took 0.08 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69366/0.68333. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69266/0.68494. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69205/0.68533. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69159/0.68547. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69126/0.68559. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69084/0.68500. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69173/0.68518. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.68483. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68998/0.68479. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69018/0.68383. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68986/0.68323. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69040/0.68333. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68929/0.68296. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68877/0.68325. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68825/0.68323. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68781/0.68287. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68725/0.68303. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68682/0.68426. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68677/0.68317. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68613/0.68382. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68477/0.68456. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68479/0.68514. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68331/0.68592. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68448/0.68613. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68292/0.68811. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68224/0.68762. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68292/0.68843. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68173/0.68916. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68123/0.69036. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68053/0.69094. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68013/0.69133. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67888/0.69193. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67862/0.69133. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67994/0.69300. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67778/0.69287. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67458/0.69371. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67528/0.69509. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67635/0.69392. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67459/0.69487. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67234/0.69555. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67403/0.69529. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67089/0.69550. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67285/0.69480. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67058/0.69585. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66819/0.69611. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66606/0.69658. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66847/0.69623. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66615/0.69724. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66605/0.69725. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66288/0.69685. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66009/0.69718. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66255/0.69779. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66310/0.69816. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65873/0.69557. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65962/0.69628. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66055/0.69808. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65471/0.69857. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65615/0.69781. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65266/0.69762. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65354/0.69863. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65242/0.69881. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64969/0.70111. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64601/0.70059. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64630/0.70188. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64394/0.70171. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64323/0.70404. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64304/0.70444. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63698/0.70461. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63840/0.70222. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63635/0.70340. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63232/0.70506. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63291/0.70631. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63366/0.70974. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63049/0.71291. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62697/0.71655. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62595/0.71708. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62307/0.71758. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62246/0.71999. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62378/0.71986. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61951/0.72026. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61704/0.72525. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61280/0.72887. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61658/0.73019. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61027/0.73043. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61420/0.73590. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60625/0.73839. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60257/0.73939. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60546/0.74233. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59307/0.74622. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60412/0.74620. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59471/0.75372. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59846/0.75435. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59346/0.75890. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58754/0.75710. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58485/0.76113. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58292/0.76886. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58314/0.77268. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58071/0.77231. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57469/0.77401. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57456/0.77810. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69604/0.69335. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69356. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69174/0.69365. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69071/0.69314. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69068/0.69244. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69066/0.69173. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69020/0.69089. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68948/0.69005. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68908/0.68929. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68907/0.68851. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68816/0.68800. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68854/0.68694. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68682/0.68606. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68721/0.68529. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68640/0.68426. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68590/0.68381. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68552/0.68317. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68528/0.68236. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68561/0.68233. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68314/0.68112. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68400/0.68099. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68322/0.67990. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68367/0.68002. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68283/0.67971. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68284/0.67851. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68080/0.67816. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68128/0.67745. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67915/0.67730. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67938/0.67689. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67938/0.67654. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68034/0.67566. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67939/0.67588. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67716/0.67516. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67890/0.67527. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67695/0.67439. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67756/0.67405. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67680/0.67307. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67880/0.67395. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67723/0.67339. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67596/0.67329. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67328/0.67323. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67276/0.67196. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67492/0.67338. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67124/0.67182. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67296/0.67142. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67252/0.67036. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67262/0.67022. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66796/0.66943. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66805/0.66814. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66763/0.66922. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66887/0.66976. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66684/0.66771. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66588/0.66759. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66703/0.66771. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66389/0.66753. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66419/0.66691. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66555/0.66619. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66212/0.66621. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66438/0.66539. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66036/0.66388. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65862/0.66370. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65999/0.66508. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66053/0.66352. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65876/0.66361. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65875/0.66630. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65672/0.66254. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65510/0.66228. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65456/0.66308. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65277/0.66222. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65362/0.66015. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65036/0.66133. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64899/0.66160. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64775/0.66044. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64549/0.66174. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64653/0.66281. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64547/0.66028. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64481/0.66310. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64510/0.66243. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64325/0.66190. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63965/0.66268. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63794/0.66497. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63786/0.66278. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63236/0.66589. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63469/0.66427. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63600/0.66638. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63208/0.66075. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63300/0.66508. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63183/0.66638. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62646/0.66677. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62365/0.66631. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62036/0.66634. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62129/0.66885. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61765/0.67015. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61707/0.67344. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61586/0.66986. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61107/0.67132. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61174/0.67456. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61143/0.67402. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60560/0.67802. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60583/0.67955. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69563/0.69005. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69280/0.68677. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.68631. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.68654. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69014/0.68671. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68961/0.68692. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68909/0.68746. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68919/0.68812. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68849/0.68862. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68754/0.68990. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68702/0.69032. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68783/0.69169. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68610/0.69303. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68591/0.69353. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68647/0.69519. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68567/0.69618. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68532/0.69710. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68484/0.69799. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68430/0.69831. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68330/0.69963. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68372/0.69966. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68336/0.70009. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.70152. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68330/0.70166. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68251/0.70185. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68211/0.70255. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68212/0.70303. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68087/0.70349. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68122/0.70364. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67972/0.70433. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67909/0.70473. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67911/0.70505. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67902/0.70531. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67874/0.70510. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67869/0.70473. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67761/0.70453. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67766/0.70525. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67671/0.70448. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67571/0.70468. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67622/0.70536. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67444/0.70529. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67411/0.70558. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67220/0.70685. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67410/0.70531. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67249/0.70569. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67237/0.70647. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67197/0.70641. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67049/0.70724. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66911/0.70629. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67018/0.70647. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66648/0.70602. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66752/0.70560. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66546/0.70783. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66606/0.70677. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66539/0.70619. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66463/0.70737. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66202/0.70688. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66089/0.70574. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66031/0.70619. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65920/0.70525. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65805/0.70564. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65698/0.70881. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65506/0.70959. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65655/0.70947. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65489/0.70905. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65252/0.70927. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65225/0.70897. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64827/0.70963. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64859/0.70972. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65088/0.70930. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64363/0.70855. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64144/0.71032. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64031/0.71296. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64069/0.71081. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63738/0.71255. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63537/0.71072. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63544/0.71146. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63307/0.71091. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62943/0.71146. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62674/0.71532. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62806/0.71503. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62792/0.71485. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62553/0.71373. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62788/0.71326. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62560/0.71295. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61760/0.71507. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62008/0.71317. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61724/0.71488. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61192/0.71360. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61130/0.71318. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61122/0.71585. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61529/0.71544. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60829/0.71511. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60309/0.71719. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60528/0.71953. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59770/0.71886. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59778/0.71848. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59937/0.71667. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59171/0.71941. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59377/0.71845. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69182/0.69593. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68992/0.69637. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68939/0.69655. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68899/0.69684. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68760/0.69683. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68786/0.69717. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68716/0.69757. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68694/0.69784. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68591/0.69808. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68594/0.69829. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68593/0.69863. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68528/0.69925. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68441/0.69927. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68413/0.69949. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68392/0.69976. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68392/0.70025. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68418/0.70069. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68404/0.70052. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68427/0.70094. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68263/0.70109. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68303/0.70128. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68202/0.70091. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68195/0.70143. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68235/0.70128. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68063/0.70145. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68072/0.70210. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68096/0.70210. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67992/0.70207. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67901/0.70190. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67912/0.70214. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67862/0.70235. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67718/0.70186. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67704/0.70252. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67780/0.70253. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67586/0.70321. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67424/0.70298. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67443/0.70311. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67444/0.70326. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67348/0.70417. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67201/0.70364. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67236/0.70449. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67038/0.70486. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66964/0.70545. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67007/0.70562. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66756/0.70688. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66742/0.70718. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66582/0.70798. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66402/0.70759. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66601/0.70762. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66229/0.70820. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66328/0.70890. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66109/0.70948. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66447/0.70945. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66127/0.71005. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65910/0.71091. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65732/0.71184. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66007/0.71220. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65585/0.71379. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65124/0.71418. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65480/0.71497. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65464/0.71394. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65169/0.71475. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64996/0.71470. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64796/0.71486. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64732/0.71622. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64568/0.71593. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64495/0.71745. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64119/0.71652. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64174/0.71759. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63740/0.71900. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64043/0.71996. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63951/0.71932. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63410/0.71910. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63525/0.72271. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63660/0.72203. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62992/0.72434. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63418/0.72006. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62932/0.72149. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63103/0.72108. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62832/0.72260. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62848/0.72317. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62524/0.72321. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62429/0.72532. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62254/0.72415. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62137/0.72554. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62060/0.72379. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61396/0.72872. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61626/0.72519. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61358/0.73037. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61236/0.72786. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61066/0.72530. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61378/0.72276. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.61078/0.72941. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61020/0.72780. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60624/0.73137. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60755/0.73013. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59704/0.73161. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60351/0.73078. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60102/0.73171. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60432/0.72908. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69068/0.70294. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68992/0.70211. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68977/0.70168. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68872/0.70156. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68834/0.70128. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68812/0.70159. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68757/0.70144. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.70143. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68679/0.70178. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68653/0.70239. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68524/0.70294. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68444/0.70313. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68504/0.70332. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68415/0.70390. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68374/0.70393. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68328/0.70397. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68225/0.70517. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68134/0.70508. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68104/0.70570. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68045/0.70658. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67924/0.70635. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68059/0.70668. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67827/0.70634. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67713/0.70676. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67612/0.70771. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67465/0.70798. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67579/0.70834. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67465/0.70835. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67269/0.70941. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67388/0.70908. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67052/0.70860. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66727/0.70862. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66786/0.70924. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66569/0.70870. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66460/0.71005. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66295/0.71058. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66095/0.71092. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65825/0.71123. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65684/0.71234. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65505/0.71254. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65502/0.71180. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65111/0.71334. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64942/0.71493. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64630/0.71690. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64577/0.71708. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64230/0.71906. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64291/0.72029. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63962/0.72208. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63273/0.72449. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63289/0.72688. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63553/0.72956. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62793/0.73291. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62846/0.73283. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62870/0.73473. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62677/0.73773. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62565/0.74094. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61852/0.74389. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61843/0.74741. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.61513/0.74849. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61678/0.75202. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61500/0.75425. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61552/0.75613. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60925/0.76030. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61030/0.76292. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60583/0.76475. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60875/0.76720. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60179/0.76722. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60529/0.76687. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60214/0.77142. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59651/0.77463. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60155/0.77791. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59616/0.77870. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59254/0.78575. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59067/0.78740. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59220/0.78713. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58789/0.78854. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58982/0.79227. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58542/0.79199. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58361/0.79488. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58578/0.79616. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57752/0.80682. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57920/0.81011. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57118/0.80476. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56843/0.81196. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57090/0.81459. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57004/0.82028. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57352/0.82094. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57102/0.81865. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56375/0.82310. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56128/0.82342. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56297/0.82392. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56036/0.82793. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55761/0.83609. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54931/0.83724. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.55499/0.84270. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55163/0.84314. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54736/0.84201. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54544/0.84620. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54178/0.84789. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54520/0.85020. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69242/0.70430. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69081/0.70269. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68998/0.70167. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68969/0.70154. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68798/0.70139. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68750/0.70142. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68776/0.70123. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68624/0.70146. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68532/0.70160. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68504/0.70218. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68411/0.70250. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68401/0.70288. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68259/0.70290. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68285/0.70352. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68228/0.70400. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68142/0.70473. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67986/0.70549. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67902/0.70662. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67825/0.70773. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67852/0.70864. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67675/0.70956. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67459/0.71064. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67260/0.71172. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67152/0.71304. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67008/0.71407. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66892/0.71650. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66716/0.71780. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66489/0.71956. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66373/0.72278. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66378/0.72305. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66128/0.72431. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65821/0.72615. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65776/0.73018. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65630/0.73262. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65054/0.73382. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65243/0.73631. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64553/0.73772. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64306/0.74204. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64711/0.74067. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64451/0.74358. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64002/0.74572. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.63799/0.74751. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63880/0.74769. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63573/0.74770. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63094/0.74888. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63263/0.75216. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62943/0.75628. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62340/0.75665. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62561/0.75783. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62282/0.76016. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62198/0.76204. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61972/0.76085. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62011/0.76223. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61685/0.76588. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61999/0.76843. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61417/0.76727. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61397/0.76454. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61056/0.76538. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60931/0.76745. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60591/0.77023. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60312/0.77292. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59961/0.77875. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60158/0.77491. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59772/0.77674. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59715/0.78219. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.59119/0.78702. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59318/0.78444. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59460/0.78248. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58418/0.78694. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58606/0.79021. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58122/0.78933. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57722/0.79012. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58150/0.79516. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58385/0.79246. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57634/0.79371. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57049/0.79968. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56535/0.80213. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56165/0.80486. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56315/0.81028. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56585/0.80855. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56237/0.80992. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55505/0.81524. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55970/0.81234. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.55762/0.82363. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55109/0.82020. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55208/0.81422. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54255/0.81931. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53765/0.82496. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54981/0.82034. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54068/0.82440. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53080/0.81826. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53267/0.82187. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52524/0.82984. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52831/0.82467. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52301/0.83359. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52087/0.82943. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52348/0.82971. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50742/0.83686. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51013/0.84217. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51359/0.84506. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69479/0.69438. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69137/0.69523. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69106/0.69505. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.69473. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68994/0.69426. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69008/0.69386. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68997/0.69341. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68928/0.69312. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68891/0.69285. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68808/0.69252. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68783/0.69212. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68769/0.69188. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68748/0.69158. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68720/0.69131. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68700/0.69114. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68626/0.69095. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68516/0.69043. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68502/0.69015. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68506/0.69017. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68453/0.69019. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68324/0.69011. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68320/0.69010. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68208/0.69007. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68117/0.69015. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68011/0.69040. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67919/0.69059. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67929/0.69064. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67744/0.69102. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67728/0.69170. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67560/0.69198. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67468/0.69239. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67152/0.69288. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67152/0.69397. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66992/0.69519. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67016/0.69627. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66892/0.69728. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66495/0.69783. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66262/0.69917. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66282/0.70015. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66094/0.70156. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66044/0.70249. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65743/0.70295. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65587/0.70291. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65325/0.70498. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65166/0.70792. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64983/0.70850. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64670/0.71084. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64595/0.71160. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64656/0.71269. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64552/0.71397. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64450/0.71419. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64071/0.71396. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63983/0.71606. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63674/0.71821. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63575/0.72009. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.63202/0.72317. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62892/0.72539. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62850/0.72435. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62833/0.72556. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62476/0.72864. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62410/0.72867. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62397/0.72944. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61894/0.73399. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61668/0.73230. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61657/0.73699. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61332/0.74052. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61079/0.74420. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60746/0.74444. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61173/0.74384. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60982/0.74703. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60175/0.74724. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59561/0.75190. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59988/0.75861. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59538/0.75752. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59143/0.76325. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59120/0.76185. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59222/0.76467. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58502/0.76846. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58367/0.77036. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57693/0.77359. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57906/0.77779. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58080/0.77769. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57641/0.77637. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57098/0.78358. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57204/0.78670. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56542/0.79449. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56485/0.79039. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55980/0.78867. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56504/0.79891. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55381/0.80394. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55713/0.80910. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.55005/0.80831. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55367/0.81199. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55261/0.81082. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54565/0.81605. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54205/0.82014. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54156/0.82727. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53612/0.82889. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53387/0.83249. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52913/0.83336. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69161/0.69858. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69071/0.69828. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69790. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69032/0.69766. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69077/0.69715. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68952/0.69701. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68901/0.69712. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68843/0.69701. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68888/0.69599. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68774/0.69544. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68654/0.69542. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68622/0.69435. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68558/0.69435. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68574/0.69356. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68473/0.69306. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68402/0.69270. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68220/0.69073. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68147/0.68945. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68085/0.68770. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67956/0.68672. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67837/0.68503. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67808/0.68200. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67629/0.67973. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67391/0.67629. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67320/0.67281. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67049/0.67058. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66988/0.66500. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66919/0.66192. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66624/0.65790. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66444/0.65396. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66441/0.65129. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66140/0.64669. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66272/0.64211. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65932/0.63918. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65760/0.63559. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65670/0.63216. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65739/0.63019. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65235/0.62884. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65276/0.62658. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.64932/0.62320. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65084/0.62146. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64908/0.61998. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64644/0.61956. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64527/0.61692. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64107/0.61622. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64033/0.61503. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64061/0.61366. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63975/0.61185. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63756/0.60922. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63761/0.60840. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63653/0.60756. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63440/0.60683. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63491/0.60589. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63078/0.60392. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.62800/0.60269. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62616/0.60376. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62772/0.60276. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62689/0.60087. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.62071/0.60160. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62231/0.60116. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61673/0.60105. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61943/0.60079. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61665/0.60382. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61731/0.60114. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61460/0.60078. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61097/0.60177. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61198/0.59824. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.60718/0.59736. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60467/0.59959. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60509/0.60040. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60252/0.60108. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60174/0.60145. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59398/0.59975. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59475/0.60129. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59665/0.60127. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59505/0.60361. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58862/0.60465. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59092/0.60302. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58705/0.60171. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58595/0.60209. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58302/0.60160. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58079/0.60183. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57895/0.60411. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57422/0.60262. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57429/0.60487. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56820/0.60342. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56844/0.60495. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56738/0.60818. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56221/0.61048. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55902/0.60852. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55900/0.60724. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.55490/0.61181. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55581/0.61044. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54642/0.61386. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54734/0.61396. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54350/0.61385. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54266/0.61768. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53773/0.61716. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53433/0.61741. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53467/0.61784. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69531/0.69573. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69210/0.69534. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69108/0.69541. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.69556. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68982/0.69578. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68928/0.69606. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68931/0.69650. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68852/0.69698. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68812/0.69746. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68789/0.69781. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68621/0.69813. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68697/0.69844. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68579/0.69885. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68589/0.69913. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68581/0.69919. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68489/0.69945. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68567/0.70010. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68474/0.70042. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68399/0.70087. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68296/0.70113. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68241/0.70180. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68269/0.70209. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68148/0.70213. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68187/0.70214. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68195/0.70198. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.70171. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67960/0.70191. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68032/0.70211. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67960/0.70250. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67967/0.70315. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67845/0.70299. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67849/0.70306. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67792/0.70332. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67771/0.70355. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67612/0.70432. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67657/0.70489. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67484/0.70556. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67488/0.70516. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67327/0.70544. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67392/0.70654. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67309/0.70712. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67169/0.70764. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67024/0.70797. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66825/0.70932. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66834/0.71048. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66875/0.71074. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66649/0.71050. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66650/0.71241. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66490/0.71275. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66567/0.71393. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66149/0.71487. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66306/0.71542. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66092/0.71636. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65786/0.71723. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65857/0.71755. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65964/0.71930. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65738/0.72079. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65790/0.72232. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65249/0.72348. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65376/0.72393. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65332/0.72648. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65222/0.72661. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65108/0.72804. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65028/0.73090. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64674/0.73011. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64787/0.73207. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64284/0.73342. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64791/0.73486. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64157/0.73528. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64148/0.73828. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63956/0.74120. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63999/0.74417. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63896/0.74290. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63976/0.74619. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63248/0.74990. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63281/0.74941. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63653/0.75401. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62662/0.75590. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63259/0.75650. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62900/0.75885. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62492/0.76263. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62347/0.76381. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62051/0.76800. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61520/0.77084. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61692/0.77295. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61649/0.77696. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61561/0.77536. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60705/0.78047. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61737/0.78379. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61219/0.78234. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60787/0.78380. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60856/0.78660. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60611/0.78903. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60300/0.79233. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60399/0.79443. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60258/0.79593. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60126/0.79726. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60113/0.79893. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59465/0.80513. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58974/0.80446. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69636/0.69184. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69429/0.69221. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69338/0.69189. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69178/0.69166. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69133. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69154/0.69105. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69101/0.69100. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69008/0.69062. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69035. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68927/0.69005. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68941/0.68960. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68776/0.68891. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68813/0.68838. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68671/0.68858. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68568/0.68815. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68534/0.68789. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68363/0.68782. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68240/0.68807. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68268/0.68805. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68104/0.68806. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68056/0.68782. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67993/0.68738. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67873/0.68747. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67781/0.68806. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67546/0.68788. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67431/0.68852. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67569/0.68909. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67328/0.69066. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67026/0.69133. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67009/0.69122. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66648/0.69324. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66671/0.69479. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66503/0.69504. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66231/0.69551. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66121/0.69978. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65867/0.70056. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65600/0.70337. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65489/0.70681. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65599/0.70914. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65283/0.70878. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65073/0.70984. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64814/0.71163. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64803/0.71411. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64451/0.71841. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64398/0.71922. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64261/0.72511. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63991/0.72602. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63595/0.72894. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63437/0.72702. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63151/0.73134. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62922/0.73316. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63182/0.73933. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62962/0.73774. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62670/0.74397. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62345/0.74721. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61913/0.74740. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61713/0.75156. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61746/0.75599. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61703/0.75225. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61349/0.76004. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60975/0.76704. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60723/0.76869. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60128/0.77075. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60810/0.76462. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.60340/0.77503. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59524/0.77764. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59790/0.77642. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59751/0.77366. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59189/0.78057. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58978/0.78144. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58651/0.78689. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58755/0.79249. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58565/0.80109. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58057/0.80243. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57948/0.80320. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57314/0.80895. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57127/0.80724. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57784/0.79383. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56904/0.80529. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57140/0.81711. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56024/0.82012. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56209/0.82253. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56316/0.81948. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56196/0.81105. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55488/0.82773. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55586/0.82976. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55306/0.83082. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54815/0.83852. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55381/0.83106. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54673/0.83753. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54913/0.84979. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54209/0.85462. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53531/0.84959. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54117/0.84737. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53785/0.84642. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53439/0.85523. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53422/0.86062. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53269/0.85982. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52370/0.86867. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52310/0.86563. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69452/0.69592. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69258/0.69570. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69260/0.69567. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69161/0.69568. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69111/0.69583. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69117/0.69609. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68970/0.69635. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68939/0.69680. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68913/0.69724. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68786/0.69779. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68761/0.69857. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68752/0.69914. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68654/0.69990. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68601/0.70074. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68476/0.70145. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68426/0.70240. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68368/0.70325. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68238/0.70414. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68190/0.70499. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68125/0.70611. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68146/0.70689. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68038/0.70761. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67985/0.70810. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67873/0.70818. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67756/0.70921. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67731/0.71009. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67631/0.71088. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67562/0.71201. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67467/0.71246. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67344/0.71318. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67110/0.71455. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67011/0.71567. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66878/0.71649. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66789/0.71720. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66587/0.71814. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66166/0.72045. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65994/0.72108. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65742/0.72353. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65673/0.72500. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65424/0.72736. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65534/0.72735. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65113/0.73066. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64802/0.73304. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64598/0.73583. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64369/0.73696. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63898/0.73936. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63994/0.74149. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63921/0.74185. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63420/0.74541. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62949/0.74929. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63007/0.75342. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62981/0.75153. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62679/0.75364. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62260/0.75384. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61861/0.75623. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61517/0.76040. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.61815/0.76060. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61452/0.76300. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61123/0.76642. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60830/0.76772. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60747/0.76954. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59755/0.77509. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60173/0.77609. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59679/0.77868. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59211/0.78189. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59074/0.78331. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58654/0.78640. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58655/0.78981. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58116/0.79282. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58088/0.79735. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58166/0.80029. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57730/0.80134. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57401/0.80528. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57492/0.80486. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.57357/0.80597. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57076/0.81327. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56318/0.81472. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56015/0.81734. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55615/0.82384. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55445/0.82573. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55625/0.82670. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55432/0.83318. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54961/0.83813. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54430/0.84765. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.54810/0.84118. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54214/0.85381. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54271/0.85422. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53551/0.85630. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53232/0.86844. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54119/0.86563. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53490/0.86109. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52317/0.87014. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.52200/0.88069. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51682/0.87701. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51369/0.88521. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.50782/0.89473. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51807/0.91479. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51581/0.90349. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51018/0.90811. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51095/0.90633. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69345/0.69103. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69219/0.69123. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69154. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.69162. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69125/0.69166. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69005/0.69191. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68987/0.69210. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68823/0.69225. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68809/0.69218. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68752/0.69248. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68635/0.69262. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68564/0.69288. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68510/0.69293. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68483/0.69330. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68253/0.69394. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68288/0.69392. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68196/0.69431. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68056/0.69545. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68111/0.69534. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68056/0.69570. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68002/0.69594. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68051/0.69649. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67874/0.69619. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67759/0.69682. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67735/0.69751. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67764/0.69729. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67571/0.69772. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67439/0.69873. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67474/0.69841. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67235/0.69975. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67190/0.69989. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67119/0.69992. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66989/0.70094. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67128/0.70142. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66677/0.70209. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66818/0.70279. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66401/0.70250. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66491/0.70341. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66427/0.70332. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66013/0.70435. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65975/0.70665. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65803/0.70564. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65706/0.70669. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65561/0.70636. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65430/0.70758. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65244/0.70622. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64905/0.70767. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64972/0.70854. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65026/0.70917. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64496/0.71289. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64427/0.71476. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63962/0.71400. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63932/0.71456. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63737/0.71364. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63418/0.71436. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63408/0.71336. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63099/0.71522. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62915/0.71679. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62846/0.71399. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62660/0.71628. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62072/0.71861. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61513/0.72029. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61876/0.71960. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62073/0.71738. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61446/0.71960. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61204/0.71821. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60985/0.72160. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61027/0.71850. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60427/0.72232. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60347/0.72174. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59884/0.72307. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60299/0.72527. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59684/0.72418. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59348/0.72443. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58814/0.72793. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58909/0.72522. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59006/0.72843. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58322/0.73208. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58544/0.73250. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57903/0.73544. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58095/0.73325. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57810/0.73555. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57185/0.73790. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56891/0.74035. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56879/0.73658. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56114/0.73976. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56297/0.74324. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56235/0.74729. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55898/0.75256. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55267/0.75237. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55145/0.75459. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54412/0.75752. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54699/0.75941. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54414/0.76307. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54003/0.76856. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53843/0.76770. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53703/0.76886. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52916/0.77561. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52620/0.77988. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52622/0.78317. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69331/0.69455. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69358/0.69442. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69346/0.69431. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69282/0.69440. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69452. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69159/0.69462. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69211/0.69473. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69109/0.69492. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69087/0.69503. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68946/0.69520. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.69542. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68884/0.69578. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68776/0.69627. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68851/0.69658. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68563/0.69735. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68610/0.69808. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68499/0.69875. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68357/0.69978. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68503/0.70024. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68396/0.70051. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68265/0.70031. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68373/0.70037. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68280/0.70029. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68222/0.69985. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68168/0.69965. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68154/0.69975. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68077/0.69982. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68112/0.69989. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68088/0.69945. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68088/0.69900. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67937/0.69923. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67769/0.69877. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67843/0.69869. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67901/0.69886. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67919/0.69818. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67875/0.69805. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67617/0.69779. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67496/0.69800. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67569/0.69798. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67596/0.69850. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67508/0.69805. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67443/0.69768. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67492/0.69727. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67369/0.69732. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67289/0.69687. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67195/0.69712. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67158/0.69715. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67114/0.69756. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67040/0.69661. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66846/0.69661. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66858/0.69595. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66596/0.69703. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66638/0.69721. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66645/0.69775. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66632/0.69785. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66451/0.69829. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66286/0.69779. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66144/0.69714. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66202/0.69725. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65984/0.69753. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65906/0.69783. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65828/0.69949. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65610/0.69924. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65621/0.70023. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65710/0.70105. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65769/0.70068. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65520/0.69967. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65245/0.70026. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64662/0.70116. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65077/0.70257. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65049/0.70222. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64777/0.70361. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64697/0.70566. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64718/0.70575. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64455/0.70549. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64293/0.70531. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64001/0.70522. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63755/0.70487. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63938/0.70658. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63810/0.70789. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63561/0.71008. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63432/0.71241. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63432/0.71140. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63061/0.71376. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62944/0.71401. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62700/0.71776. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62349/0.71535. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62711/0.71813. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62169/0.71896. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62245/0.72253. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62114/0.72018. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61511/0.72329. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61607/0.72286. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61836/0.72236. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61241/0.72335. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61303/0.73255. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60891/0.72690. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60831/0.72632. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60753/0.72864. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60233/0.73429. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69463/0.69488. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69384/0.69486. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69502. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69280/0.69531. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69245/0.69557. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69176/0.69609. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69164/0.69647. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69206/0.69701. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69066/0.69736. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69052/0.69806. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69004/0.69870. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68960/0.69942. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68936/0.70026. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68909/0.70100. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68911/0.70155. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68857/0.70236. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68733/0.70316. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68711/0.70388. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68682/0.70475. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68703/0.70550. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68607/0.70611. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68550/0.70677. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68490/0.70721. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68371/0.70798. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68495/0.70822. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68361/0.70858. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68176/0.70953. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68138/0.71098. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68125/0.71155. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68158/0.71150. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68012/0.71160. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68038/0.71159. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67930/0.71249. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67764/0.71263. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67672/0.71308. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67699/0.71451. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67610/0.71433. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67308/0.71508. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67482/0.71488. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67300/0.71601. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67175/0.71667. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67207/0.71747. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67161/0.71852. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67112/0.71954. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66853/0.72045. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66866/0.72170. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66669/0.72172. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66505/0.72193. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66394/0.72363. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66442/0.72472. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66510/0.72582. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66260/0.72765. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65833/0.73054. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65913/0.72955. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65701/0.73007. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65657/0.73258. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65297/0.73348. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65418/0.73885. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65037/0.73941. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64923/0.73863. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65074/0.74100. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64799/0.74280. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64783/0.74242. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64540/0.74404. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64386/0.74544. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63888/0.74849. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63635/0.75292. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63686/0.75305. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63829/0.75264. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63684/0.75644. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63177/0.75767. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63139/0.75911. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62998/0.75587. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62727/0.76325. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62439/0.76693. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62492/0.76720. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61769/0.76986. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61802/0.77501. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61671/0.77416. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61718/0.78082. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61563/0.77704. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61387/0.78203. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60992/0.78105. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60889/0.78303. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59669/0.79280. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60617/0.79080. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60214/0.78711. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59785/0.79202. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59774/0.79224. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58633/0.80268. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59197/0.80439. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59263/0.80272. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58460/0.80970. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58573/0.80236. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58286/0.81585. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58348/0.81123. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57122/0.81080. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57694/0.81819. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57551/0.82666. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56855/0.82029. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69852/0.70210. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69427/0.69964. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69816. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.69679. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69100/0.69541. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69080/0.69429. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68929/0.69332. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68874/0.69240. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68809/0.69144. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68560/0.69010. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68595/0.68961. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68580/0.68970. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68471/0.68904. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68292/0.68885. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68080/0.68833. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68114/0.68832. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68026/0.68799. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.67909/0.68842. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67713/0.68773. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67542/0.68797. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67441/0.68790. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67317/0.68725. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67376/0.68772. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66929/0.68724. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66798/0.68865. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66575/0.68858. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66628/0.68753. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66168/0.68743. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66012/0.68704. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65890/0.68703. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65628/0.68791. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65495/0.68735. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65465/0.68592. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65199/0.68771. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64849/0.68806. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64672/0.68737. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64556/0.68970. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64202/0.69058. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64200/0.68964. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.64034/0.68945. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63730/0.68791. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63792/0.68668. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63825/0.68675. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63686/0.68801. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63340/0.68801. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62749/0.68734. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62894/0.68726. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.62698/0.68567. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62419/0.68380. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62402/0.68480. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62000/0.68350. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61771/0.68192. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.62100/0.68248. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61717/0.68010. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61029/0.67941. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60924/0.67922. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60533/0.67844. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60604/0.68044. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60450/0.67730. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.60072/0.67670. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59468/0.67329. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59781/0.67534. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59553/0.67633. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59269/0.67447. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59022/0.67327. Took 0.13 sec\n",
      "Epoch 65, Loss(train/val) 0.58755/0.67146. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58614/0.67253. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58282/0.67072. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58153/0.67138. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57885/0.66937. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.57529/0.66997. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.57441/0.67495. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57053/0.67462. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56692/0.67434. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56579/0.67322. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56107/0.67111. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55675/0.66940. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.55405/0.67518. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55127/0.67945. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55368/0.67221. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.54771/0.66704. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54718/0.67288. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54677/0.66888. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54579/0.67364. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54329/0.67782. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53487/0.67073. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53381/0.67209. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53686/0.68006. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52266/0.67432. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.52482/0.68114. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.52567/0.67874. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52151/0.68123. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52271/0.69414. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51119/0.68579. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51107/0.68533. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50807/0.68804. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50687/0.69518. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51095/0.69666. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50728/0.69587. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.48282/0.68982. Took 0.09 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69589/0.68538. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69332/0.68881. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69301/0.69055. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69280/0.69141. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69207/0.69269. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69185/0.69357. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69172/0.69476. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.69543. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69073/0.69598. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69055/0.69627. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69078/0.69702. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68993/0.69754. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69000/0.69801. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69028/0.69860. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68991/0.69910. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68948/0.69936. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68875/0.70011. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68892/0.70072. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68808/0.70098. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68853/0.70152. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68776/0.70244. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68684/0.70280. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68700/0.70389. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68595/0.70421. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68640/0.70537. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68578/0.70610. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68475/0.70706. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68410/0.70786. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68449/0.70949. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68364/0.71066. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68255/0.71184. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68260/0.71300. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68142/0.71480. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67961/0.71542. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67899/0.71782. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67920/0.72016. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67858/0.72142. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67653/0.72278. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67527/0.72456. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67497/0.72594. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67431/0.72781. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67279/0.72850. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67128/0.72882. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67109/0.72980. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66895/0.73194. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66755/0.73408. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66409/0.73432. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66286/0.73511. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66246/0.73645. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65995/0.73918. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65897/0.73921. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65954/0.73895. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65449/0.74268. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65487/0.74413. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65179/0.74577. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64846/0.74767. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64977/0.74830. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64628/0.75137. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64433/0.75209. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64258/0.75326. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64440/0.75727. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64072/0.75592. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64019/0.75766. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63770/0.75998. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63996/0.76657. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63946/0.76396. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63563/0.76312. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63305/0.76710. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62914/0.76849. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62942/0.76751. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63025/0.77093. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.62815/0.77296. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62562/0.77422. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62518/0.77476. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62011/0.77436. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62279/0.77773. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62234/0.78283. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61813/0.77983. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61457/0.78175. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61987/0.78563. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61627/0.78312. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61101/0.78828. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60783/0.79800. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61036/0.79303. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60512/0.79962. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60377/0.79993. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60028/0.80092. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60101/0.79692. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59855/0.79989. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59670/0.80568. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59537/0.80327. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59100/0.80668. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59166/0.80308. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59250/0.80532. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58305/0.81073. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58848/0.81760. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57691/0.80986. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58113/0.81647. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57950/0.82169. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57847/0.81040. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69282/0.69314. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69315/0.69290. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69214/0.69275. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69183/0.69256. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69158/0.69234. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69125/0.69204. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.69175. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68983/0.69151. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68853/0.69118. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68705/0.69064. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68610/0.69017. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68531/0.68969. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68294/0.68933. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68085/0.68889. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67873/0.68897. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67718/0.68912. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67212/0.68926. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67015/0.68967. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67056/0.69141. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.66635/0.69349. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.66386/0.69375. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66242/0.69469. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.65890/0.69600. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65470/0.69723. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.65675/0.69949. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65391/0.69954. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.65068/0.69887. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.64929/0.70088. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.64847/0.70080. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64752/0.70128. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64069/0.69861. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.64219/0.70388. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.63960/0.70327. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.63758/0.70478. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63702/0.70769. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.63556/0.70528. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63077/0.70506. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.62566/0.70341. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62500/0.70054. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62211/0.70547. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62303/0.70322. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.61612/0.70544. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.61549/0.70961. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61102/0.70202. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61106/0.70119. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61325/0.70141. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.60791/0.70479. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60438/0.70560. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.60511/0.70025. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60765/0.70539. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.59545/0.70483. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.58960/0.70203. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.58966/0.70429. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.58414/0.70410. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.58427/0.69665. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.58932/0.70101. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.57988/0.69164. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.57867/0.69948. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.57071/0.69801. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.56672/0.70077. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.56613/0.69997. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.55777/0.69596. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.55886/0.69188. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.54840/0.70244. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.54897/0.69380. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.54938/0.69038. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.54115/0.68751. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.53952/0.68601. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.54048/0.69029. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.54054/0.69156. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.52570/0.69362. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.52055/0.70110. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.52854/0.69684. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.51548/0.69642. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.53112/0.69397. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.51171/0.70359. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.50637/0.70524. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.50736/0.69762. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.50641/0.69916. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.50580/0.69258. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.49697/0.69749. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.49524/0.69064. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.49212/0.69505. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.49006/0.69305. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.48745/0.69868. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.48015/0.71024. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.47740/0.71010. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.47478/0.71327. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.46562/0.71760. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.46032/0.72037. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.46683/0.71593. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.46116/0.72289. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.45216/0.72603. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.45114/0.72444. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.45314/0.72915. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.44826/0.73608. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.44401/0.74227. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.44818/0.75112. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.43992/0.75123. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.42988/0.76745. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69409/0.69238. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69311/0.69185. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69177/0.69237. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.69254. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69161/0.69228. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69069/0.69280. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69052/0.69276. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69068/0.69299. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69025/0.69270. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68963/0.69329. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68934/0.69252. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68873/0.69277. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68813/0.69297. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68760/0.69331. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68681/0.69286. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68676/0.69336. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68574/0.69373. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68539/0.69413. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68398/0.69471. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68439/0.69555. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68337/0.69593. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68236/0.69637. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68294/0.69712. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68223/0.69770. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68073/0.69812. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68079/0.69868. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67937/0.69901. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67815/0.70125. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67775/0.70077. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67695/0.70216. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67611/0.70229. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67463/0.70348. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67360/0.70497. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67461/0.70632. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67279/0.70880. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67233/0.70802. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67053/0.71015. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66915/0.71258. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66854/0.71179. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66748/0.71599. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66661/0.71716. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66660/0.71997. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66589/0.72125. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66424/0.72073. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66321/0.72309. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66316/0.72388. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66075/0.72591. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65959/0.73014. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65890/0.72961. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65445/0.73354. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65589/0.73848. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65481/0.73622. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65379/0.74023. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65354/0.74227. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65377/0.74527. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65105/0.74540. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64912/0.74570. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64888/0.75101. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64703/0.75241. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64637/0.75549. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64684/0.75741. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64362/0.75914. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64438/0.75972. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64186/0.76222. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63735/0.76583. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63974/0.77019. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63833/0.77204. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63087/0.77594. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63204/0.77741. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62629/0.78217. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62853/0.78676. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62082/0.79021. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62525/0.79140. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62367/0.79322. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62317/0.79711. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61926/0.80346. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61590/0.80574. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62227/0.80842. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61383/0.80738. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61410/0.81111. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60951/0.81516. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60843/0.81803. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60897/0.81750. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60300/0.81910. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60515/0.82321. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60344/0.82919. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60381/0.83041. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59369/0.83024. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59297/0.83623. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59259/0.83751. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58870/0.84242. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58910/0.84616. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58382/0.85167. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58319/0.85353. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57898/0.85697. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58361/0.85459. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57634/0.86260. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56836/0.86668. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57394/0.86759. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57274/0.87575. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69472/0.69217. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69410/0.69144. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69375/0.69105. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69302/0.69090. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69222/0.69083. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.69087. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69076/0.69062. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69055/0.69053. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69117/0.69031. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68856/0.69003. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68865/0.68991. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68751/0.68984. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68723/0.68978. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68611/0.68938. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68617/0.68919. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68574/0.68847. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68503/0.68766. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68340/0.68701. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68268/0.68667. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68182/0.68584. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67950/0.68515. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67785/0.68363. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67749/0.68227. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67440/0.68066. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67311/0.67869. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67231/0.67745. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66951/0.67607. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66652/0.67394. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66714/0.67244. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66222/0.67008. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66104/0.66851. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65928/0.66630. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65408/0.66454. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65340/0.66238. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.65283/0.66076. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.64837/0.65882. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64931/0.65756. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64376/0.65685. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64091/0.65569. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63603/0.65541. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63389/0.65475. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63080/0.65390. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62999/0.65445. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62582/0.65546. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62319/0.65443. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61688/0.65508. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62005/0.65542. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62136/0.65495. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61477/0.65661. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61194/0.65670. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.60844/0.65733. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60897/0.65783. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60558/0.65825. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60114/0.65671. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59835/0.65990. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59982/0.66072. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59685/0.66169. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59070/0.66339. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58852/0.66565. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58207/0.66686. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58028/0.67058. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58073/0.67386. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.57885/0.67302. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58082/0.67510. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57301/0.67681. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.57366/0.68020. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56867/0.68093. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.56637/0.68466. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56506/0.68816. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55820/0.69245. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.55460/0.69252. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55937/0.69979. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55569/0.70421. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.54456/0.70421. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55317/0.70823. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54841/0.70787. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.54775/0.71285. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53824/0.71467. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.53750/0.71767. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53349/0.72035. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.53770/0.72578. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.53888/0.72626. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52427/0.73111. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.51961/0.73813. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52036/0.74217. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.51639/0.74940. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51341/0.74847. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.50802/0.75210. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51621/0.75426. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50660/0.75458. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50576/0.75607. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50243/0.76622. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.49564/0.76607. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50074/0.77150. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.49800/0.76774. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.49588/0.76527. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49693/0.76891. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.48936/0.77580. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.48794/0.77789. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48780/0.78494. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69161/0.68818. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69048/0.68805. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69012/0.68798. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.68801. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68941/0.68794. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68907/0.68782. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68911/0.68778. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68768/0.68772. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68778/0.68763. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68736/0.68769. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68671/0.68779. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68585/0.68775. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68432/0.68792. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68381/0.68793. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68212/0.68799. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68038/0.68835. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67817/0.68882. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67776/0.68891. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67502/0.68870. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67279/0.68817. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67055/0.68954. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66866/0.68918. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66558/0.69001. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66465/0.68982. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66067/0.68809. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65870/0.68642. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65624/0.68755. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65247/0.68630. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65346/0.68630. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64890/0.68650. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64889/0.68613. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64497/0.68504. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64059/0.68358. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64029/0.68375. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.63924/0.68300. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63812/0.68468. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63358/0.68410. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63016/0.68454. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62851/0.68542. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62846/0.68241. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.62685/0.68095. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62495/0.67956. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62234/0.67768. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62074/0.67708. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61655/0.67971. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61703/0.67578. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.61208/0.67812. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61314/0.67357. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.60620/0.67219. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60520/0.67368. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60057/0.67404. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60162/0.67191. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.59730/0.67260. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59345/0.67120. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59578/0.67292. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59415/0.66917. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58683/0.67172. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58696/0.67219. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58281/0.67030. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58216/0.67057. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.57500/0.67233. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57560/0.67271. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57070/0.67856. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.56886/0.67351. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57003/0.67384. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.56579/0.67608. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56130/0.67721. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55590/0.67482. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.55260/0.67909. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55325/0.67407. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.54864/0.67471. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55219/0.68031. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.54494/0.68539. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.53998/0.68690. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.53611/0.68533. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.53008/0.68403. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.53454/0.69103. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.53175/0.69131. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52137/0.68949. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.52367/0.69657. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.52001/0.69048. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.51880/0.69414. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.51412/0.70565. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51095/0.70561. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51118/0.70988. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.50830/0.71529. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51124/0.71526. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.49667/0.71823. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.49664/0.72133. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.50299/0.71090. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.48611/0.72608. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.48866/0.72588. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.48454/0.72775. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.48168/0.72810. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.48404/0.73438. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.47426/0.73009. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.47033/0.74252. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.47549/0.73880. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.46290/0.74161. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.45734/0.75001. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69144. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69188/0.69079. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69113/0.69071. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69125/0.69083. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69085/0.69098. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69032/0.69109. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69096/0.69121. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69019/0.69141. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68979/0.69171. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69018/0.69214. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68873/0.69243. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68913/0.69255. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68856/0.69276. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68825/0.69318. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68802/0.69362. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68724/0.69409. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68708/0.69461. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.69492. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68575/0.69513. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68536/0.69578. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68453/0.69611. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68353/0.69635. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68342/0.69674. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68301/0.69749. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68282/0.69762. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68046/0.69825. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68021/0.69866. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67865/0.69959. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67903/0.69995. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67685/0.70025. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67806/0.70057. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67564/0.70103. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67340/0.70203. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67315/0.70235. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67410/0.70251. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67402/0.70295. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67344/0.70377. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67258/0.70426. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66933/0.70452. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66848/0.70657. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66613/0.70697. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66352/0.70734. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66479/0.70797. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66485/0.70975. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66135/0.70982. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66099/0.71008. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66073/0.71066. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66165/0.71045. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65757/0.71214. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65659/0.71302. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65660/0.71372. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65197/0.71516. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65483/0.71630. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64791/0.71634. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64804/0.71682. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64374/0.71789. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64611/0.71971. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63970/0.72093. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63982/0.72134. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63741/0.72367. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63460/0.72767. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63588/0.72690. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63180/0.72751. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62899/0.72853. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62418/0.73104. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63126/0.73176. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62443/0.73141. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61894/0.73305. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61872/0.73508. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61840/0.73788. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61109/0.74029. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61179/0.74550. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61077/0.74575. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60760/0.74665. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60118/0.75134. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60509/0.75347. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59844/0.75750. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59794/0.76231. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59693/0.76166. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59722/0.76152. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59113/0.76361. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59027/0.76859. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58471/0.76993. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58127/0.77294. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58486/0.77623. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57691/0.78357. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57322/0.78224. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57414/0.78817. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56934/0.78929. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56691/0.79346. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56703/0.79727. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56218/0.80164. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55976/0.80228. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55211/0.80748. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55461/0.81242. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54990/0.81130. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55300/0.81483. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54186/0.81677. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54617/0.81811. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54693/0.81959. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69466/0.69200. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69087/0.69059. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69053/0.69062. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69030/0.69083. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69001/0.69104. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68914/0.69132. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68917/0.69160. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68970/0.69182. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68825/0.69208. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68899/0.69234. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68911/0.69269. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.69298. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68824/0.69338. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68788/0.69352. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68720/0.69380. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68664/0.69412. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68769/0.69429. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68727/0.69447. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68680/0.69445. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68563/0.69442. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68521/0.69482. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68471/0.69518. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68383/0.69562. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68343/0.69612. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68291/0.69623. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68389/0.69656. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68156/0.69693. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68157/0.69733. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68155/0.69778. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68182/0.69792. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68078/0.69850. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67968/0.69911. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67780/0.69953. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67833/0.70010. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67664/0.70041. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67712/0.70043. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67647/0.70035. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67500/0.70079. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67421/0.70124. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67418/0.70144. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67134/0.70222. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67236/0.70195. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67167/0.70207. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66977/0.70248. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66791/0.70330. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66851/0.70309. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66772/0.70361. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66793/0.70408. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66341/0.70423. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66387/0.70426. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66209/0.70446. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66229/0.70464. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65982/0.70471. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65800/0.70405. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65893/0.70467. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65734/0.70561. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65369/0.70604. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65422/0.70741. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65033/0.70794. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65016/0.70868. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64854/0.70968. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64912/0.71116. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64721/0.71222. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64361/0.71222. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64334/0.71348. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63959/0.71416. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63959/0.71549. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63686/0.71768. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63429/0.71885. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63579/0.71971. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63373/0.71741. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.62975/0.71932. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62957/0.72018. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63043/0.72034. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62500/0.72506. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62207/0.72960. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62773/0.72513. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61989/0.72417. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62299/0.72765. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61648/0.72862. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61618/0.72880. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61304/0.73056. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61136/0.72972. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61099/0.73175. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60869/0.73097. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60408/0.73560. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60828/0.73516. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60581/0.73940. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59345/0.74236. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59684/0.74534. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59292/0.74791. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58933/0.74717. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59122/0.74640. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58948/0.74584. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58517/0.75169. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58515/0.75129. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58951/0.75115. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58099/0.75452. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58084/0.75333. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57929/0.75646. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69187/0.68562. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69097/0.68499. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69044/0.68456. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69044/0.68421. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69007/0.68384. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68936/0.68345. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68918/0.68331. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68941/0.68320. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68926/0.68312. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68871/0.68313. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68887/0.68309. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68756/0.68280. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68754/0.68266. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68744/0.68254. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68702/0.68243. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68687/0.68228. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68661/0.68230. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68578/0.68228. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68494/0.68224. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68546/0.68256. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68495/0.68260. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68469/0.68253. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68327/0.68252. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68317/0.68283. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68236/0.68288. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68213/0.68284. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68196/0.68289. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68252/0.68318. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68004/0.68354. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67981/0.68326. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67978/0.68367. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67866/0.68321. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67703/0.68338. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67690/0.68433. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67644/0.68384. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67579/0.68348. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67596/0.68418. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67320/0.68449. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67196/0.68458. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67235/0.68422. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67112/0.68524. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67000/0.68507. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66961/0.68555. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67009/0.68612. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66675/0.68669. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66624/0.68743. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66647/0.68760. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66363/0.68823. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66320/0.68905. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66016/0.68987. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66068/0.69045. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65924/0.69091. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65771/0.69168. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65551/0.69152. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65791/0.69108. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65262/0.69152. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65139/0.69274. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64861/0.69364. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64845/0.69615. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64735/0.69462. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64517/0.69617. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64422/0.69839. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64336/0.69765. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64143/0.69677. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63777/0.69783. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63749/0.69992. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63479/0.69956. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63641/0.70043. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63134/0.70044. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63758/0.70302. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62901/0.70445. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63114/0.70544. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62600/0.70533. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61873/0.70509. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61935/0.70800. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62013/0.71211. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62342/0.71003. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61655/0.71087. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61665/0.71064. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61407/0.71064. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60916/0.71951. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60922/0.72073. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61139/0.71832. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60271/0.72105. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60409/0.72795. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60138/0.71747. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59330/0.72389. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59945/0.72649. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59462/0.72445. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60142/0.72791. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59490/0.72328. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59280/0.72373. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58949/0.73022. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58938/0.73372. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58435/0.74019. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58811/0.73988. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58313/0.73592. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57378/0.73393. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58229/0.74262. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57745/0.73949. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69424/0.69314. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69108/0.69119. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69060/0.68991. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69046/0.68885. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69033/0.68809. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68940/0.68739. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68902/0.68675. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68910/0.68647. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68826/0.68619. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68810/0.68594. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68749/0.68567. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68756/0.68532. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68683/0.68505. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68653/0.68490. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.68666/0.68474. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68606/0.68469. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68551/0.68487. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68622/0.68517. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68531/0.68524. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68453/0.68542. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68459/0.68582. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68403/0.68641. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68327/0.68712. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68307/0.68769. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68263/0.68841. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68241/0.68875. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68171/0.68944. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68120/0.68987. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67990/0.69039. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67947/0.69126. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67811/0.69218. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67852/0.69321. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67736/0.69382. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67654/0.69433. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67689/0.69524. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67572/0.69590. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67408/0.69775. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67513/0.69909. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67214/0.69971. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67092/0.70142. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67023/0.70255. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66817/0.70413. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66964/0.70570. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66737/0.70714. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66632/0.70810. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66730/0.70924. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66792/0.71008. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66492/0.71243. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66361/0.71206. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66259/0.71281. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66116/0.71502. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66173/0.71538. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65943/0.71704. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66023/0.71866. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65792/0.72021. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65681/0.72197. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65318/0.72253. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65710/0.72297. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65568/0.72409. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65351/0.72575. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65457/0.72588. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65024/0.72623. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64820/0.72897. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64996/0.73163. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64933/0.73143. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64741/0.73136. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64501/0.73351. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64490/0.73467. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64312/0.73782. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64201/0.73906. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64353/0.74048. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64016/0.74148. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64023/0.74187. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63833/0.74573. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63565/0.74620. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63303/0.74634. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63496/0.74863. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62937/0.75061. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63073/0.75377. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62912/0.75619. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62615/0.75725. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62405/0.76223. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62738/0.76341. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62456/0.76187. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62353/0.76278. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62343/0.76244. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.62264/0.76562. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61641/0.76739. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61815/0.77052. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61653/0.77043. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61310/0.77551. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61152/0.77781. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.60858/0.77982. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60594/0.78372. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60951/0.78198. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60731/0.78361. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60457/0.78785. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60185/0.79054. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60330/0.78937. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60397/0.79443. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69553/0.69002. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68963/0.68281. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68956/0.68074. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68952/0.67967. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68881/0.67943. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68819/0.67884. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68793/0.67828. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68811/0.67788. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68794/0.67760. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68768/0.67725. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68659/0.67672. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68589/0.67612. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68539/0.67554. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68628/0.67493. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68373/0.67455. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68415/0.67441. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68284/0.67404. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68260/0.67408. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68270/0.67385. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68172/0.67347. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67997/0.67436. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68049/0.67428. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68038/0.67537. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67664/0.67556. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67789/0.67581. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67829/0.67693. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67788/0.67811. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67464/0.67855. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67768/0.67949. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67443/0.67991. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67391/0.68047. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67137/0.68160. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67071/0.68421. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67065/0.68525. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66939/0.68744. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66730/0.68980. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66865/0.68850. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66403/0.68811. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66228/0.69251. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66576/0.69327. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66050/0.69586. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66029/0.69843. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65847/0.70023. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65457/0.70367. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65479/0.70625. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65519/0.70790. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65375/0.71165. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65314/0.71100. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64771/0.71718. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65038/0.72137. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64430/0.72490. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64538/0.72346. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64377/0.72599. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64424/0.73056. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64097/0.72835. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64033/0.73651. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63808/0.74073. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63375/0.74265. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63357/0.74585. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62830/0.74549. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62925/0.75418. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63106/0.75750. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62599/0.76171. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62483/0.75923. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62533/0.76337. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62521/0.76336. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61894/0.76529. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62423/0.77641. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61853/0.77295. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61803/0.77648. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61610/0.77979. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61414/0.78004. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61484/0.78076. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61288/0.78359. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60677/0.79495. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60787/0.78595. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60470/0.79051. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60192/0.79911. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59785/0.80019. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60115/0.80138. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59874/0.80749. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59343/0.81291. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59418/0.80888. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58232/0.82142. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59122/0.82457. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58643/0.83125. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59049/0.82657. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58258/0.82817. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57940/0.83225. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57826/0.83851. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57511/0.83991. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57998/0.84114. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57699/0.84863. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57291/0.84574. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56368/0.85794. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57579/0.85485. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56383/0.86205. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56611/0.86616. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55868/0.86883. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56136/0.86509. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.68900/0.69717. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68832/0.69810. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68814/0.69884. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68740/0.69950. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68728/0.69999. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68701/0.70018. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68702/0.70019. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68698/0.70006. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68673/0.70008. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68569/0.69997. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68596/0.70008. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68620/0.70020. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68512/0.70019. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68462/0.69997. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68422/0.70026. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68315/0.70040. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68431/0.69977. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68321/0.70041. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68286/0.70035. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68201/0.70060. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68055/0.70060. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68084/0.70058. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68045/0.70001. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67981/0.70082. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67960/0.70024. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67789/0.70022. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67737/0.70020. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67537/0.70029. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67595/0.69910. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67426/0.69967. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67516/0.69907. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67366/0.69791. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67325/0.69929. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67035/0.69734. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67066/0.69936. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66922/0.69752. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66744/0.69736. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66557/0.69653. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66253/0.69654. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66452/0.69597. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66056/0.69514. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66129/0.69537. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65922/0.69425. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66101/0.69423. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65986/0.69714. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65405/0.69392. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65542/0.69159. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65486/0.69431. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65285/0.69268. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65106/0.69090. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64935/0.69058. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64729/0.69295. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64525/0.69242. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64379/0.69287. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64155/0.69331. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64584/0.69435. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63913/0.68996. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63896/0.69131. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63900/0.68835. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.63775/0.69173. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63334/0.69271. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63566/0.68901. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62986/0.68962. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63371/0.69069. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63176/0.68889. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62835/0.69114. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62646/0.68794. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62299/0.69037. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62459/0.68946. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62507/0.69319. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61885/0.68997. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62550/0.68702. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61905/0.68693. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61662/0.68979. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61732/0.69062. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60944/0.69346. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61350/0.69491. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60774/0.69249. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61218/0.68809. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61467/0.69098. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60697/0.68985. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60218/0.68930. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60053/0.68741. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60370/0.68555. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59870/0.68621. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60169/0.68710. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59416/0.68986. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59630/0.69080. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59904/0.69166. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.59518/0.68958. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58836/0.69215. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59159/0.69213. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58588/0.68955. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58589/0.69068. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58499/0.69148. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58170/0.69633. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57963/0.70224. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58084/0.69574. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57587/0.69225. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58062/0.69263. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69439/0.69301. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69288/0.69279. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69063/0.69305. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68971/0.69368. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68784/0.69437. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68780/0.69497. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68736/0.69517. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68700/0.69549. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68688/0.69580. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68580/0.69600. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68612/0.69604. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68519/0.69618. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68501/0.69660. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68469/0.69659. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68454/0.69682. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68307/0.69728. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68332/0.69781. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68267/0.69832. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68200/0.69856. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68212/0.69951. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68119/0.70064. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67940/0.70085. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67882/0.70161. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67687/0.70284. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67705/0.70385. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67630/0.70537. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67525/0.70596. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67431/0.70733. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67411/0.70848. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67087/0.70894. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66948/0.71045. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67070/0.71078. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66782/0.71169. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66677/0.71260. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66475/0.71463. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66429/0.71550. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66110/0.71671. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65708/0.71775. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65561/0.71942. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65477/0.72152. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65302/0.72427. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65137/0.72488. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64979/0.72659. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64873/0.72765. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64371/0.73024. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64517/0.73243. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64183/0.73301. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63793/0.73428. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63503/0.73634. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63469/0.73459. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62947/0.73849. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62993/0.74213. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63078/0.74132. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62855/0.74233. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62535/0.74240. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62309/0.74631. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62230/0.74994. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61659/0.74677. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61652/0.75070. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61711/0.75140. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60964/0.75369. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.61312/0.75544. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60679/0.75670. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60469/0.75511. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60620/0.76054. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60066/0.75811. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60094/0.76176. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.59770/0.75887. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59672/0.76026. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59537/0.76573. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59741/0.77174. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58702/0.76902. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59160/0.77278. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58626/0.77408. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57866/0.77457. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58498/0.77400. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58103/0.78308. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58124/0.78416. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57530/0.78087. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57456/0.78521. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57310/0.78647. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57463/0.78392. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56933/0.78358. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56951/0.79253. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56937/0.79636. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56168/0.79997. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55676/0.79975. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54939/0.81159. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55603/0.81145. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55443/0.80831. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55161/0.80807. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54591/0.81750. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54604/0.81339. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54293/0.82009. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53676/0.83400. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52960/0.82720. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53747/0.83575. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.53764/0.83490. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52531/0.83573. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52205/0.83665. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69079/0.70231. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68903/0.70168. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68866/0.70197. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68712/0.70135. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68625/0.70156. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68621/0.70172. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68629/0.70205. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68539/0.70186. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68425/0.70183. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68407/0.70210. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68494/0.70202. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68461/0.70236. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68354/0.70288. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68338/0.70302. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68229/0.70314. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68240/0.70311. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68249/0.70413. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68143/0.70413. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68207/0.70409. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68150/0.70487. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68094/0.70514. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68037/0.70522. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68008/0.70603. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67977/0.70665. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67904/0.70708. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67936/0.70772. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67880/0.70789. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67741/0.70822. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67745/0.70897. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67649/0.70950. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67761/0.71009. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67675/0.71039. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67570/0.71025. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67614/0.71117. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67448/0.71161. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67481/0.71186. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67331/0.71199. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67247/0.71194. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67236/0.71322. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67159/0.71391. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67128/0.71385. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67071/0.71292. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67056/0.71447. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66847/0.71445. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66808/0.71376. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66699/0.71476. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66565/0.71509. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66566/0.71528. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66509/0.71653. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66312/0.71685. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66222/0.71643. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66186/0.71731. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65888/0.71520. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65777/0.71618. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65814/0.71582. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65725/0.71761. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65483/0.71642. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65342/0.71678. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65266/0.71602. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64979/0.71584. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65114/0.71677. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64614/0.71739. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64741/0.71886. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64293/0.71746. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63955/0.71681. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63986/0.71684. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63920/0.71715. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63654/0.72094. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63206/0.71897. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63320/0.71620. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63060/0.71842. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62605/0.71823. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62775/0.71855. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62571/0.71654. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62362/0.71702. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62304/0.71565. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61823/0.71952. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61980/0.71889. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61887/0.72268. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61217/0.71745. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61315/0.71633. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61341/0.72256. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60828/0.71249. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60685/0.72780. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60285/0.72175. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60018/0.72355. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.60060/0.72859. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59620/0.72654. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59845/0.73124. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59686/0.73019. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59698/0.72653. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59309/0.73408. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58924/0.73400. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58748/0.73958. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58169/0.74143. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58035/0.73972. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58124/0.74865. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58131/0.73905. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58226/0.74139. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58009/0.74370. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69241/0.69298. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69261/0.69282. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69246/0.69246. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69139/0.69230. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69099/0.69223. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69095/0.69216. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69009/0.69190. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69036/0.69164. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68843/0.69174. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68995/0.69174. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68844/0.69187. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68925/0.69200. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68874/0.69235. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68756/0.69269. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68588/0.69272. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68545/0.69321. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68502/0.69325. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68441/0.69297. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68386/0.69324. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68205/0.69368. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68240/0.69434. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68191/0.69420. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68157/0.69503. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68057/0.69476. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67900/0.69389. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67823/0.69442. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67688/0.69451. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67481/0.69409. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67531/0.69431. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67298/0.69448. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67326/0.69435. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67290/0.69428. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67192/0.69372. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67053/0.69294. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67066/0.69331. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66989/0.69276. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66874/0.69191. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66568/0.69180. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66385/0.69297. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66360/0.69102. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66383/0.68983. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66063/0.68923. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66044/0.68868. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65966/0.68909. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65740/0.68720. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.65426/0.68788. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65289/0.68532. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65133/0.68518. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65018/0.68627. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64783/0.68526. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64558/0.68492. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.64036/0.68450. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64394/0.68156. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63834/0.68358. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63861/0.68340. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63179/0.68370. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63204/0.68330. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62649/0.68223. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62848/0.68287. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62832/0.68335. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62373/0.68372. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62147/0.68159. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61428/0.67982. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.61086/0.68185. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62205/0.67968. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60733/0.67933. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61022/0.68007. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59781/0.68180. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60126/0.68283. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.59529/0.68063. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60039/0.68295. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59616/0.68393. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58683/0.68490. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58390/0.68638. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58640/0.68511. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58816/0.68516. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58645/0.68604. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58094/0.68750. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57700/0.68877. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56680/0.68946. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57224/0.69082. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57175/0.69433. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56880/0.69290. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56545/0.69137. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56345/0.69512. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55513/0.69270. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55677/0.69785. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55564/0.69662. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55204/0.70075. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54638/0.70463. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55150/0.70471. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54168/0.70864. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54934/0.71103. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54613/0.71202. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53039/0.71713. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53097/0.71747. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52285/0.72546. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52985/0.71627. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53189/0.72081. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.52334/0.71814. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69445/0.69277. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69195. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.69148. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69181/0.69108. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69134/0.69081. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69076/0.69041. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69044/0.68985. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68976/0.68945. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68851/0.68882. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68789/0.68835. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68730/0.68808. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68644/0.68755. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68500/0.68745. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68443/0.68725. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68383/0.68698. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68145/0.68685. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68039/0.68691. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67966/0.68693. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67728/0.68698. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67664/0.68700. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67416/0.68710. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67261/0.68665. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67162/0.68686. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66982/0.68709. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66850/0.68748. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66647/0.68753. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66516/0.68732. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.66074/0.68769. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66209/0.68853. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65843/0.68900. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65651/0.68979. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65391/0.69097. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65315/0.69105. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65154/0.69132. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65018/0.69330. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64697/0.69387. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64468/0.69688. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63964/0.69864. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64135/0.69875. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63952/0.70183. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63486/0.70599. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63853/0.70641. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63675/0.70603. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63457/0.70755. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63213/0.70794. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62849/0.71223. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62636/0.71673. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62282/0.71955. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62203/0.72257. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61858/0.72602. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61793/0.72751. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.61733/0.72980. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61196/0.73412. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61079/0.73663. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60997/0.74091. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60904/0.74101. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60735/0.74577. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.60212/0.75060. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60022/0.75768. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59970/0.74914. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59821/0.75902. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59721/0.75487. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59382/0.75740. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58772/0.76516. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58483/0.76944. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58713/0.76984. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58582/0.77708. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58279/0.78190. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57667/0.78852. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.57446/0.78925. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57554/0.79089. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57577/0.79430. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57506/0.79751. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56904/0.79814. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56965/0.79746. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.56222/0.80527. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56379/0.80015. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55915/0.80629. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55728/0.80721. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55518/0.82088. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55159/0.82244. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54806/0.82518. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54927/0.83306. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54905/0.82427. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.54675/0.83161. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54442/0.83617. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53865/0.84360. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53384/0.84163. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53297/0.84820. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52843/0.86102. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52647/0.85929. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52642/0.84758. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.52336/0.86198. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.52228/0.86269. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51836/0.86827. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52124/0.87219. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.51418/0.86750. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51551/0.87039. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.50670/0.87377. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50929/0.88057. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69745/0.69599. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69383/0.69481. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69283/0.69458. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69246/0.69463. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69480. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69163/0.69500. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69111/0.69524. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68993/0.69553. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68952/0.69585. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68981/0.69627. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68960/0.69654. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68874/0.69699. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68781/0.69762. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68779/0.69811. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68683/0.69866. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68652/0.69935. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68663/0.69995. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68641/0.70055. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68554/0.70084. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68416/0.70149. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68388/0.70176. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68308/0.70254. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68243/0.70297. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68156/0.70318. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68083/0.70317. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68125/0.70343. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68159/0.70388. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68104/0.70403. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67955/0.70436. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67891/0.70456. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67871/0.70478. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67729/0.70420. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67745/0.70372. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67701/0.70360. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67604/0.70395. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67634/0.70383. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67572/0.70336. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67339/0.70379. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67467/0.70391. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67218/0.70414. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67001/0.70444. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66735/0.70443. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66880/0.70455. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66635/0.70483. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66678/0.70457. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66578/0.70421. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66234/0.70449. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66073/0.70494. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66006/0.70546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65889/0.70544. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65792/0.70581. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65710/0.70520. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65346/0.70495. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65439/0.70568. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65222/0.70593. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64934/0.70826. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64761/0.70740. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64314/0.70839. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64353/0.71075. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64284/0.70825. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64029/0.70904. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64101/0.70919. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63531/0.70966. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63642/0.71143. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63408/0.71036. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62883/0.71119. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62846/0.71435. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62460/0.71344. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62206/0.71212. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61969/0.71570. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61801/0.71446. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62037/0.71741. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61136/0.71633. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61497/0.71679. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61197/0.71584. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60607/0.71806. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60585/0.72370. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60461/0.72442. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59984/0.72700. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59533/0.72805. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59922/0.72571. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59577/0.72996. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59476/0.72973. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58320/0.73620. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58206/0.73531. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58075/0.74043. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57637/0.73732. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57418/0.74308. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57090/0.74536. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56632/0.74578. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57387/0.74339. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56681/0.75136. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56661/0.75373. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55731/0.75229. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55507/0.75357. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55012/0.76089. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55132/0.76718. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55509/0.76229. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54599/0.76594. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54254/0.76973. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69408/0.69160. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.69301. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.69332. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69281/0.69340. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69159/0.69360. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69175/0.69317. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69144/0.69328. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69083/0.69344. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69081/0.69331. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69017/0.69312. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68929/0.69309. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68937/0.69308. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68895/0.69335. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68859/0.69247. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68751/0.69290. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68733/0.69404. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68692/0.69345. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68609/0.69378. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68525/0.69435. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68406/0.69430. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68293/0.69502. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68243/0.69663. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68067/0.69696. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68116/0.69641. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67919/0.69762. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67713/0.70001. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67709/0.70100. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67731/0.70259. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67489/0.70280. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67350/0.70311. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67132/0.70601. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67223/0.70902. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66994/0.70752. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66961/0.70960. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66763/0.71031. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66561/0.71359. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66555/0.71305. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66283/0.71514. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66264/0.71503. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66131/0.71760. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65574/0.71971. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65779/0.72035. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65645/0.72150. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65100/0.71908. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65226/0.72054. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65057/0.72072. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64790/0.72355. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64762/0.72546. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64195/0.72611. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64152/0.72770. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.63568/0.72556. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63875/0.72707. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63381/0.72757. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63103/0.72475. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63430/0.72614. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62787/0.72510. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62944/0.72443. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62505/0.72762. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62083/0.73059. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61855/0.72821. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61631/0.73385. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61513/0.72991. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61266/0.73330. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60927/0.73280. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60641/0.73234. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60222/0.73200. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60390/0.73854. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60073/0.74046. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.59392/0.74197. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59141/0.74395. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58979/0.74606. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58681/0.75210. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58237/0.76047. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58302/0.75365. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57347/0.76243. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.57406/0.76487. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57230/0.76855. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56592/0.76916. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57095/0.77166. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56789/0.77256. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56181/0.77437. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56233/0.78432. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55655/0.78534. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.55338/0.79016. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54544/0.80053. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55127/0.79873. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54508/0.80679. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54251/0.80280. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53607/0.81316. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.53873/0.81652. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53360/0.81731. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52457/0.83023. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53067/0.82906. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.52906/0.82718. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52695/0.83621. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51841/0.83695. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52770/0.84434. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51487/0.85454. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.50642/0.85363. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52030/0.86535. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69476/0.69320. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69244/0.69336. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69335/0.69354. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69304/0.69367. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69295/0.69377. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69250/0.69397. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69206/0.69421. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69172/0.69406. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69171/0.69404. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69121/0.69436. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69180/0.69443. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69154/0.69471. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69129/0.69472. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69034/0.69492. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69026/0.69510. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69034/0.69523. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69024/0.69572. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68899/0.69625. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68876/0.69701. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68819/0.69748. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68811/0.69855. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68771/0.69927. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68773/0.69980. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68657/0.70038. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68732/0.70092. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68597/0.70192. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68519/0.70229. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68402/0.70291. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68372/0.70367. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68314/0.70392. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68169/0.70435. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68069/0.70520. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67988/0.70576. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68085/0.70596. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67696/0.70702. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67642/0.70792. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67322/0.70809. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67531/0.70903. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67440/0.70848. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67249/0.70942. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67130/0.70941. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66702/0.71208. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66890/0.71351. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66488/0.71352. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66120/0.71241. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66277/0.71323. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65785/0.71287. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65481/0.71163. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65402/0.71205. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65107/0.71198. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64909/0.71278. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64608/0.71234. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64358/0.71149. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64169/0.71147. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63427/0.71049. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63725/0.71023. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63247/0.70993. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62341/0.70932. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62695/0.71138. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62435/0.70552. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62013/0.70059. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61654/0.70853. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61207/0.70688. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60831/0.70637. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61271/0.70544. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.59891/0.70915. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60623/0.70849. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60232/0.70677. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59587/0.70494. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59372/0.70692. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59579/0.71318. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58899/0.70757. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58896/0.70874. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58885/0.70818. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58691/0.71088. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58409/0.70951. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58487/0.70751. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57787/0.71216. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57785/0.71055. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57425/0.71169. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57350/0.71139. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56857/0.71275. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57050/0.71186. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56584/0.71251. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56360/0.71314. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55813/0.71849. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56162/0.71883. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56417/0.71816. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55637/0.71886. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55610/0.72363. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55026/0.72188. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54910/0.72265. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54021/0.72625. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54414/0.72921. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.54117/0.73147. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54322/0.73582. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53812/0.73305. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53567/0.73490. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53887/0.73773. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53048/0.74117. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69358/0.69648. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69347/0.69660. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69214/0.69638. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69254/0.69603. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69219/0.69594. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69196/0.69575. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69133/0.69568. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69074/0.69550. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69029/0.69515. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69054/0.69497. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68969/0.69511. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68966/0.69498. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68854/0.69516. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68973/0.69512. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68868/0.69512. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68797/0.69482. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68739/0.69508. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68646/0.69522. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68594/0.69512. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68565/0.69566. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68482/0.69569. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68488/0.69661. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68229/0.69664. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68331/0.69780. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68032/0.69834. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68090/0.69921. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67834/0.70023. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67694/0.70074. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67726/0.70225. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67517/0.70267. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67541/0.70323. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67499/0.70411. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67561/0.70536. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67563/0.70607. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67101/0.70679. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66949/0.70853. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66981/0.70963. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66695/0.71041. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66753/0.71238. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66657/0.71304. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66528/0.71330. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66070/0.71515. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66012/0.71675. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66232/0.71640. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65678/0.71813. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65877/0.71854. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65447/0.72104. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65439/0.72077. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65228/0.72175. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65258/0.72289. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65025/0.72517. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64946/0.72693. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64527/0.72972. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64543/0.73042. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64380/0.73211. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64311/0.73331. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64092/0.73419. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63602/0.73370. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63167/0.73731. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63478/0.73936. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63141/0.73954. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62790/0.74192. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62962/0.74456. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62509/0.74601. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62521/0.75005. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62067/0.75383. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61481/0.75501. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62135/0.75522. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61668/0.75610. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61007/0.75912. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61255/0.75648. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60639/0.76361. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60755/0.76624. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60372/0.77159. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60297/0.77438. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60155/0.77895. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59750/0.77468. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.59940/0.77661. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59305/0.77655. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59280/0.78043. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59223/0.78734. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58938/0.79143. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58087/0.80065. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58236/0.80263. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58283/0.80688. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58198/0.80703. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57305/0.81197. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57588/0.81125. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56983/0.81435. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56039/0.81877. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56498/0.82347. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56245/0.83098. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56086/0.83074. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55765/0.83550. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55507/0.84085. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55692/0.84392. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54959/0.84062. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54527/0.84848. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55534/0.85437. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54083/0.85823. Took 0.08 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69554/0.70310. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69501/0.70297. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69428/0.70291. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69424/0.70277. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69363/0.70217. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69311/0.70139. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69219/0.70099. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69141/0.70086. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69084/0.70050. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69076/0.70034. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.70071. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68844/0.70048. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68828/0.70071. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68718/0.70151. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.70210. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68592/0.70321. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68313/0.70321. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68240/0.70378. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68473/0.70322. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68166/0.70400. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68058/0.70399. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67918/0.70486. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67829/0.70670. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67629/0.70811. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67525/0.70780. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67111/0.70968. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67200/0.70976. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66806/0.71308. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66657/0.70730. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66190/0.71213. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66170/0.71236. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65905/0.70976. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65716/0.70904. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65189/0.70816. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64931/0.70628. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64666/0.70828. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64359/0.70185. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64051/0.70335. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64035/0.69659. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63483/0.69654. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.63753/0.69375. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62881/0.69214. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62880/0.68885. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.63112/0.68895. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62017/0.68845. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61799/0.68148. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.61155/0.68219. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61021/0.68030. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61500/0.67759. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60868/0.67575. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60528/0.67218. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.59443/0.67660. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59708/0.66972. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59482/0.67384. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59212/0.66970. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.58665/0.66829. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58793/0.66751. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58484/0.66003. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.57978/0.66433. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.57010/0.66123. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58124/0.66127. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.56825/0.66225. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.56816/0.66285. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57011/0.66600. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.56204/0.66650. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.55446/0.66760. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.55622/0.66746. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55634/0.67020. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.55013/0.66068. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.54812/0.66651. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.54070/0.66373. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54383/0.66816. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.53601/0.67076. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.53421/0.67640. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.53196/0.67669. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.53262/0.67981. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.53199/0.67456. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.52476/0.67756. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52125/0.68002. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.51995/0.67356. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.52448/0.68372. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.51807/0.68759. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.51138/0.69758. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.50008/0.69450. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.50415/0.70186. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.50416/0.69871. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.49465/0.70671. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.50132/0.71069. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.49897/0.71676. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.49332/0.71101. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.48534/0.71049. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.48518/0.72530. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.48600/0.71485. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.48338/0.71490. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.47870/0.73197. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.47930/0.72579. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.47454/0.71999. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.46920/0.72480. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.46846/0.72023. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.46806/0.73052. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69738/0.69672. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69377/0.69401. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69285/0.69309. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.69258. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69148/0.69216. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69018/0.69176. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68872/0.69129. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68813/0.69095. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68798/0.69076. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68750/0.69066. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68676/0.69033. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68628/0.69015. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68582/0.68995. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68435/0.68981. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68445/0.68975. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68265/0.68955. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68320/0.68931. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68264/0.68897. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68233/0.68938. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68058/0.68899. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68094/0.68866. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68087/0.68829. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67970/0.68822. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68080/0.68839. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67991/0.68765. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67901/0.68737. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67902/0.68697. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67839/0.68679. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67810/0.68672. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67742/0.68617. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67615/0.68613. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67541/0.68594. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67616/0.68564. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67627/0.68556. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67373/0.68558. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67364/0.68518. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67223/0.68526. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67377/0.68525. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67096/0.68520. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67155/0.68514. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66974/0.68490. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67063/0.68466. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67204/0.68459. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66923/0.68388. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66813/0.68433. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66783/0.68353. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66741/0.68392. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66529/0.68402. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66789/0.68533. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66587/0.68470. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66707/0.68411. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66464/0.68450. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66262/0.68421. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66107/0.68527. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66159/0.68569. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66056/0.68479. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66332/0.68615. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66068/0.68606. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66139/0.68510. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65958/0.68548. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65885/0.68690. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65934/0.68800. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65889/0.68786. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65660/0.68797. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65741/0.68771. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65619/0.68803. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65721/0.68885. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65546/0.68806. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65706/0.68880. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65419/0.69069. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65406/0.69099. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65248/0.69046. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65358/0.68923. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65158/0.68960. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65059/0.69087. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64997/0.69190. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65171/0.69198. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64974/0.69246. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64959/0.69214. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64675/0.69196. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64522/0.69303. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64573/0.69380. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64608/0.69510. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64534/0.69529. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64243/0.69425. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64401/0.69556. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64291/0.69597. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64455/0.69569. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64345/0.69688. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64131/0.69578. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64163/0.69511. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.63823/0.69728. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64037/0.69687. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63667/0.69591. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63244/0.69700. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63661/0.69835. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63755/0.69747. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63732/0.69805. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63352/0.69698. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.63405/0.69675. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69512/0.69369. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69496/0.69369. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69490/0.69365. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69482/0.69359. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69445/0.69349. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69455/0.69342. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69442/0.69341. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69390/0.69342. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69371/0.69338. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69371/0.69338. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69390/0.69347. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69338/0.69357. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69272/0.69362. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69295/0.69379. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69120/0.69394. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69126/0.69425. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69044/0.69487. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68971/0.69564. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68837/0.69629. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68726/0.69745. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68755/0.69875. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68643/0.70034. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68468/0.70176. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68432/0.70351. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68301/0.70492. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68150/0.70627. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68124/0.70706. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68036/0.70848. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67859/0.70895. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67835/0.71019. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67692/0.71160. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67755/0.71262. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67533/0.71251. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67447/0.71281. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67324/0.71454. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67332/0.71371. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67264/0.71509. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67297/0.71584. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67102/0.71594. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66893/0.71661. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66596/0.71805. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66620/0.71987. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66659/0.72058. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66331/0.72076. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66261/0.72295. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66341/0.72344. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66387/0.72266. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66066/0.72320. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65732/0.72406. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65792/0.72538. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65497/0.72621. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65687/0.72760. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65000/0.72925. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65328/0.72891. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64985/0.73183. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64974/0.73438. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64578/0.73346. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64687/0.73398. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64427/0.73493. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64390/0.73736. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63920/0.73768. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64004/0.74183. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63583/0.74043. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63397/0.74226. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63286/0.74274. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62991/0.74706. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63062/0.74932. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62705/0.75163. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62711/0.75039. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62506/0.75587. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62105/0.75572. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61546/0.75783. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61864/0.76205. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61565/0.76252. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61277/0.76453. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61274/0.76478. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60617/0.76535. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60578/0.76931. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60270/0.77372. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60061/0.77181. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59889/0.78067. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60171/0.77772. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59190/0.78458. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59559/0.79032. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59554/0.79626. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59198/0.79298. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59061/0.78787. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58454/0.79553. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58015/0.79533. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58160/0.79548. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57756/0.80051. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57793/0.80469. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57552/0.80149. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57361/0.80390. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.57030/0.81144. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56660/0.81476. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55897/0.81730. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55608/0.82404. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56078/0.82790. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55721/0.82619. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69192/0.69227. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69081/0.69211. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69007/0.69204. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68959/0.69223. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68853/0.69248. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68711/0.69227. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68635/0.69293. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68436/0.69331. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68288/0.69436. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68106/0.69640. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68032/0.69757. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67835/0.69930. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67927/0.70078. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67779/0.70251. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67711/0.70438. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67523/0.70597. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67536/0.70770. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67318/0.70923. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67222/0.71032. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67062/0.71214. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67059/0.71460. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66983/0.71530. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66965/0.71691. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66890/0.71840. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66593/0.71980. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66685/0.72143. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66646/0.72147. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66487/0.72420. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66732/0.72403. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66278/0.72576. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66201/0.72809. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66133/0.72749. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66091/0.73002. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.65815/0.73166. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65697/0.73329. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65561/0.73409. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65529/0.73549. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65324/0.73657. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65385/0.73792. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65381/0.73857. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64994/0.74068. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64903/0.74222. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64964/0.74371. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64560/0.74339. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64358/0.74545. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64317/0.74565. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64249/0.74926. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64001/0.75148. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63779/0.75288. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63784/0.75249. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63608/0.75367. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63551/0.75335. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63561/0.75600. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63148/0.75557. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62711/0.75888. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62938/0.76208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62612/0.76422. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62644/0.76639. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62453/0.76276. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.62226/0.76336. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62037/0.76418. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61663/0.76719. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61625/0.77116. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61859/0.77247. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61178/0.76624. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61084/0.76907. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60754/0.76816. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60534/0.77446. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60358/0.77012. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60165/0.77625. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59932/0.77857. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59223/0.77759. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59591/0.77826. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59457/0.77486. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.58821/0.77839. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59019/0.77838. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58101/0.78009. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58026/0.78806. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58358/0.78088. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57453/0.79186. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57534/0.78192. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57215/0.78874. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56841/0.79077. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56299/0.78743. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56542/0.79809. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56808/0.78794. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55852/0.79527. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55497/0.79527. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55106/0.79915. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55762/0.80410. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55007/0.79797. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54515/0.81019. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.54854/0.80183. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54470/0.80724. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54076/0.80958. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53298/0.80036. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53971/0.80090. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53080/0.82390. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53200/0.81494. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53359/0.82204. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69343/0.69247. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.69198. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69251/0.69149. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69123/0.69110. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69015/0.69069. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68901/0.69043. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68785/0.69039. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68821/0.69042. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68476/0.69043. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68397/0.69083. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68260/0.69108. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68136/0.69168. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68171/0.69219. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68079/0.69267. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67975/0.69309. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67945/0.69345. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67858/0.69379. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67700/0.69392. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67577/0.69432. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67452/0.69468. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67406/0.69558. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67452/0.69580. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67211/0.69648. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67290/0.69608. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67110/0.69627. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67072/0.69657. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67113/0.69679. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67014/0.69679. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66548/0.69760. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66965/0.69690. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66668/0.69659. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66592/0.69671. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66448/0.69719. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66644/0.69737. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66438/0.69747. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66410/0.69638. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66072/0.69720. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66070/0.69739. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65952/0.69796. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66005/0.69787. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65793/0.69860. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65643/0.69862. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65431/0.69818. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65067/0.69923. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65257/0.69831. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65102/0.69759. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65005/0.69809. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64888/0.69831. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64875/0.69836. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64766/0.69731. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64449/0.69736. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64487/0.69759. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64003/0.69801. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64169/0.69768. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63846/0.69715. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63648/0.69916. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63863/0.69882. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63584/0.69934. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63178/0.69831. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63178/0.69964. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62849/0.70014. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62822/0.69971. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62939/0.70005. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62242/0.70161. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62108/0.70109. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62090/0.70194. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61743/0.70209. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61788/0.70275. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61433/0.70338. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61117/0.70522. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60905/0.70630. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60935/0.70708. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60133/0.70751. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60496/0.70870. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60026/0.70912. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60025/0.71005. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59711/0.71589. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59397/0.71395. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59031/0.71956. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59492/0.71996. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58804/0.71999. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58509/0.71877. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.58807/0.72079. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57955/0.72674. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57503/0.72644. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58164/0.72792. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57516/0.72998. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57588/0.73197. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.57007/0.73384. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56886/0.73720. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56875/0.73453. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56071/0.73518. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56292/0.73956. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55462/0.73968. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55697/0.74186. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54615/0.74608. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55114/0.74775. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.55074/0.74883. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54264/0.74864. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53598/0.75448. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69710. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69297/0.69675. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69663. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69117/0.69662. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69056/0.69681. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69024/0.69714. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69004/0.69751. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68981/0.69798. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69008/0.69837. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68909/0.69874. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68956/0.69929. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.69976. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68824/0.70028. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68748/0.70077. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.70113. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68736/0.70147. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68680/0.70182. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68623/0.70259. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68619/0.70300. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68620/0.70324. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68491/0.70361. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68478/0.70389. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68470/0.70424. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68433/0.70467. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68344/0.70558. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68322/0.70630. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68325/0.70663. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68191/0.70725. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68099/0.70775. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68154/0.70850. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68032/0.70910. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67974/0.71008. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67908/0.71072. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67881/0.71180. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67639/0.71248. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67700/0.71345. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67552/0.71463. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67517/0.71534. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67420/0.71668. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67361/0.71844. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67270/0.71929. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67151/0.72109. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67173/0.72307. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67025/0.72540. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66936/0.72765. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66841/0.72896. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66820/0.73004. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66806/0.73291. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66605/0.73354. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66402/0.73590. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66358/0.73808. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66411/0.73943. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66093/0.74189. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65909/0.74236. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65914/0.74421. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65854/0.74541. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65796/0.74834. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65555/0.74989. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65585/0.75242. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65602/0.75416. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65316/0.75627. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65476/0.75807. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64955/0.75878. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64812/0.76186. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65053/0.76274. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64836/0.76429. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64669/0.76610. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64539/0.76915. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64545/0.77016. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64364/0.77087. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64208/0.77348. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63912/0.77555. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63910/0.77774. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63688/0.77983. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63570/0.78117. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63473/0.78284. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63832/0.78514. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63533/0.78699. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62802/0.78727. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63171/0.78903. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62475/0.79028. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62841/0.79506. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62429/0.79676. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62525/0.80057. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62452/0.80229. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62196/0.80289. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62002/0.80585. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62035/0.80622. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61731/0.80718. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61591/0.80858. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61512/0.81197. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61240/0.81416. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61575/0.81181. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61305/0.81545. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60364/0.81858. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60535/0.82001. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60888/0.82344. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60241/0.82530. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59940/0.82571. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59923/0.83063. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69428/0.69462. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69350/0.69406. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69261/0.69369. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69199/0.69349. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69211/0.69349. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69219/0.69356. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69172/0.69373. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69102/0.69394. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69120/0.69420. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69106/0.69453. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69098/0.69492. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69002/0.69526. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69016/0.69572. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68950/0.69627. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68951/0.69685. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68907/0.69748. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68844/0.69815. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68733/0.69909. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68700/0.70006. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68569/0.70090. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68547/0.70121. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68612/0.70203. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68626/0.70247. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68558/0.70287. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68302/0.70341. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68187/0.70415. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68125/0.70445. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68148/0.70510. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68018/0.70482. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67975/0.70616. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67895/0.70578. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67868/0.70637. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67746/0.70727. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67609/0.70772. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67378/0.70916. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67214/0.70877. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67354/0.70928. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67019/0.71054. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67176/0.71143. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66987/0.71113. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66563/0.71029. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66725/0.71172. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66834/0.71230. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66569/0.71195. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66452/0.71321. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66231/0.71427. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66498/0.71457. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66239/0.71467. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66114/0.71546. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65966/0.71642. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66037/0.71867. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65825/0.71675. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65658/0.71876. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65393/0.71929. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65261/0.71910. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65277/0.72076. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64837/0.72024. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65295/0.72251. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64850/0.72253. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64653/0.72326. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64310/0.72433. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64665/0.72430. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64493/0.72396. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64455/0.72473. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64367/0.72463. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63751/0.72469. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63852/0.72665. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63663/0.72612. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63545/0.72912. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63195/0.73034. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63383/0.73272. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62821/0.73192. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63020/0.73395. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62653/0.73529. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62571/0.73456. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62953/0.73611. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62215/0.73706. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62297/0.73744. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61611/0.74251. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61572/0.74394. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61501/0.74434. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61351/0.75170. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61100/0.74945. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61128/0.75576. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61145/0.75322. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60490/0.75446. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60903/0.75395. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60966/0.75671. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59920/0.76310. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59937/0.76463. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59344/0.76182. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59420/0.76697. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58978/0.77224. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59108/0.77255. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58819/0.77673. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58044/0.77943. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57304/0.78761. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57422/0.79038. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58065/0.79410. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57267/0.79368. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69398/0.69222. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69168/0.69116. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69245/0.69093. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.69066. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69081/0.69060. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.69070. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69052/0.69075. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68984/0.69089. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68983/0.69090. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68954/0.69118. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68892/0.69142. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68882/0.69181. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68890/0.69207. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68788/0.69227. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68833/0.69231. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68639/0.69279. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68800/0.69286. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68679/0.69320. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68696/0.69337. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68584/0.69410. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68597/0.69413. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68543/0.69405. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68342/0.69389. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68491/0.69374. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68344/0.69394. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68440/0.69368. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68324/0.69349. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68246/0.69334. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68200/0.69324. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68025/0.69314. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68014/0.69260. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67805/0.69221. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68168/0.69138. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67899/0.69108. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67789/0.69089. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67778/0.69078. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67673/0.69020. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67839/0.68946. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67423/0.69001. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67322/0.69030. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67167/0.69025. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67125/0.69005. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67056/0.68879. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66977/0.68872. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66936/0.68891. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66722/0.68875. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66867/0.68915. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66674/0.68834. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66663/0.68785. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66372/0.68766. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66417/0.68692. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66365/0.68630. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66182/0.68657. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66130/0.68720. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65887/0.68684. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65803/0.68642. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65503/0.68590. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65440/0.68455. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65508/0.68495. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64937/0.68567. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64961/0.68518. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64832/0.68455. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64392/0.68462. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64364/0.68444. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64203/0.68367. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64077/0.68279. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63966/0.68401. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63578/0.68382. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63370/0.68264. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63077/0.68323. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62914/0.68344. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62799/0.68487. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62541/0.68221. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62221/0.68210. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61855/0.68427. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62519/0.68399. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61874/0.68758. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61743/0.68445. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60911/0.68494. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60708/0.68435. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60491/0.68765. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59998/0.68773. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60028/0.68945. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59640/0.68840. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59469/0.69131. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59449/0.69253. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58861/0.69531. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58950/0.69490. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58321/0.69372. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58129/0.69683. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58210/0.70162. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57353/0.69456. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57488/0.69958. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56666/0.70400. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56858/0.71106. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55763/0.70816. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56053/0.70838. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55683/0.71173. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55291/0.71200. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54882/0.71569. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69174/0.69415. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69138/0.69413. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69029/0.69413. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69031/0.69402. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68996/0.69393. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68954/0.69378. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68869/0.69362. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68781/0.69308. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68677/0.69264. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68627/0.69209. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68560/0.69170. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68478/0.69113. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68388/0.69045. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68117/0.68951. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67935/0.68838. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67687/0.68747. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67676/0.68695. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67410/0.68582. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.66915/0.68471. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.66780/0.68420. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66493/0.68299. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66355/0.68325. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66003/0.68305. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65710/0.68331. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.65458/0.68254. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.65323/0.68290. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.65148/0.68257. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65254/0.68024. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.64864/0.68418. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.64780/0.68028. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64527/0.68262. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64126/0.68500. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64044/0.68103. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.63958/0.68189. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.63477/0.67899. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63498/0.68228. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63237/0.68052. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.63230/0.68090. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.62478/0.68019. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62716/0.67586. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.62453/0.68341. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.62386/0.68243. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62138/0.68335. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61413/0.68211. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.61769/0.68227. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61322/0.68002. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.60698/0.68661. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60888/0.68510. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.60650/0.68668. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60069/0.68753. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.60171/0.69005. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60015/0.69536. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.59454/0.69913. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.59231/0.69383. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59355/0.70228. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.59139/0.70571. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58923/0.70235. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.58681/0.70324. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58476/0.71152. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58165/0.70955. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58343/0.71289. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.57352/0.71740. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.57909/0.72001. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.56841/0.72105. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.56928/0.72253. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.56349/0.72879. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.55580/0.73478. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.55574/0.73313. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56202/0.73847. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.54749/0.74798. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55495/0.74671. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54343/0.75277. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.54755/0.75261. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55023/0.75223. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.54241/0.76013. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53535/0.77135. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.53430/0.77435. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.53253/0.77617. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52238/0.78137. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.52529/0.78688. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.52516/0.79722. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.52287/0.79745. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.51972/0.80100. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.52161/0.79947. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51462/0.80804. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.51133/0.80414. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51793/0.80741. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.51343/0.80173. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51133/0.80531. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.49519/0.81706. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50075/0.81034. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.50306/0.81441. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.49375/0.83468. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.48405/0.83337. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.47971/0.83391. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48253/0.84670. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.48654/0.84645. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.46873/0.85037. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.47387/0.85002. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.47177/0.86385. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69241/0.70467. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69128/0.70518. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69105/0.70497. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68961/0.70562. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68959/0.70568. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68940/0.70636. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68814/0.70662. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68790/0.70745. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68741/0.70734. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68739/0.70759. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68666/0.70791. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68557/0.70879. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68475/0.70963. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68423/0.70982. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68343/0.71025. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68190/0.71115. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68044/0.71198. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68004/0.71407. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67872/0.71521. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67963/0.71445. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67586/0.71527. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67542/0.71649. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67507/0.71570. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67548/0.71792. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67312/0.71722. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67191/0.71628. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67187/0.71699. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66960/0.71595. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66912/0.71699. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66724/0.71740. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66452/0.71700. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66510/0.71723. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66351/0.71418. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66431/0.71433. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65948/0.71513. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65867/0.71191. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65798/0.71554. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65754/0.71427. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65614/0.71726. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65623/0.71521. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65328/0.71645. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65483/0.71765. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64904/0.71793. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64994/0.71832. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64733/0.72160. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64753/0.72484. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64404/0.72714. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63998/0.72684. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64093/0.72989. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.63896/0.72909. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63499/0.73234. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63415/0.73387. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63298/0.73732. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63338/0.73833. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62965/0.74164. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62753/0.74195. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62691/0.74211. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62510/0.74409. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62428/0.74687. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62115/0.74318. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62027/0.74917. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61268/0.74951. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61184/0.75398. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60926/0.75783. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61251/0.76115. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60601/0.76284. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60859/0.76779. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60482/0.76650. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60401/0.77402. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59813/0.77138. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59609/0.77510. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59695/0.77950. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59269/0.78105. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59212/0.78063. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58482/0.78725. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58488/0.79281. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58420/0.79515. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57851/0.79698. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57997/0.80471. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57547/0.81106. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57001/0.81174. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56606/0.81910. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56949/0.81722. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56549/0.82006. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56328/0.82529. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56556/0.83085. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55408/0.83336. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55817/0.84154. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55462/0.84449. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55179/0.84904. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55349/0.85434. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54836/0.85494. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54241/0.86477. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53337/0.87184. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53404/0.87927. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53118/0.88323. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52843/0.88888. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52664/0.88911. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52711/0.89786. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52406/0.89751. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69222/0.69456. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69170/0.69404. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69166/0.69354. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68989/0.69308. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69033/0.69266. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69043/0.69218. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68899/0.69168. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68882/0.69114. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68901/0.69064. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68846/0.69024. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68745/0.68958. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68658/0.68880. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68644/0.68806. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68467/0.68735. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68306/0.68635. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68288/0.68521. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68302/0.68414. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.68091/0.68306. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67934/0.68218. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67930/0.68128. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67758/0.68029. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67614/0.67920. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67386/0.67823. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67340/0.67783. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67136/0.67714. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66771/0.67672. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66717/0.67592. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66602/0.67777. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66620/0.67864. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66665/0.67844. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66113/0.67990. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66071/0.68010. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66004/0.67822. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65744/0.67926. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65472/0.68099. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65378/0.68195. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65398/0.68416. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65416/0.68356. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65037/0.68295. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65100/0.68465. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64772/0.68578. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.64697/0.68990. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64342/0.69080. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64117/0.69313. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63836/0.69627. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63975/0.69671. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63765/0.69700. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63815/0.70024. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63548/0.70480. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63422/0.70590. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63494/0.70511. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62717/0.70824. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62731/0.71387. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62585/0.71711. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62042/0.71800. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62049/0.71822. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61867/0.72142. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61641/0.72333. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61231/0.72155. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61395/0.72462. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60605/0.73070. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60658/0.72912. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.60440/0.73690. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60151/0.74024. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60803/0.74543. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59808/0.75074. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59750/0.75176. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59523/0.75480. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58951/0.76189. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58676/0.76518. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58930/0.76520. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58097/0.77000. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57818/0.76853. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57875/0.77561. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57771/0.77554. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58513/0.78093. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.57169/0.78123. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57623/0.79008. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56768/0.78350. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56331/0.80345. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56815/0.79504. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55918/0.80158. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56103/0.80499. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55031/0.80721. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55801/0.81997. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54956/0.81643. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.54887/0.81814. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55080/0.81985. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54815/0.82595. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54546/0.82961. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53682/0.83816. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53527/0.83277. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.53221/0.84837. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53409/0.85240. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52433/0.86335. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52043/0.85251. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52450/0.85990. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52041/0.87667. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50426/0.88279. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50952/0.88315. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69195/0.69034. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69142/0.69036. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69118/0.69041. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69179/0.69044. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.69046. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69041/0.69047. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.69048. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.69052. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69015/0.69055. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68975/0.69059. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.69059. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68930/0.69065. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68902/0.69070. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68899/0.69072. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68862/0.69088. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68705/0.69102. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68752/0.69106. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68684/0.69128. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68678/0.69129. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68525/0.69162. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68371/0.69194. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68279/0.69226. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68402/0.69253. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68216/0.69323. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68043/0.69406. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68090/0.69520. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68051/0.69608. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67896/0.69712. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67796/0.69790. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67676/0.69850. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67557/0.69959. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67562/0.70097. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67229/0.70096. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67152/0.70141. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67134/0.70213. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66855/0.70454. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66728/0.70629. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66640/0.70711. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66451/0.70539. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66275/0.70582. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66140/0.70674. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66193/0.70796. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66488/0.71001. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66051/0.70783. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65696/0.71166. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66075/0.71054. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65406/0.71085. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65135/0.71191. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64905/0.71361. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65124/0.71342. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64673/0.71419. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64475/0.71753. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64491/0.71814. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64435/0.72081. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64206/0.71917. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64154/0.72258. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63670/0.72201. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63716/0.72586. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63543/0.72514. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63910/0.72438. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62980/0.72606. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62965/0.72854. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62840/0.73109. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62817/0.73331. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62518/0.73390. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62658/0.73751. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62199/0.73892. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61695/0.74059. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62071/0.74134. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61959/0.74122. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61517/0.74324. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61624/0.74654. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61248/0.74707. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61593/0.75193. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60922/0.75529. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60389/0.75740. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60350/0.76076. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60525/0.76572. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59920/0.76503. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59850/0.76505. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59805/0.77138. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60020/0.77167. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59600/0.77189. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59472/0.77262. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59571/0.77597. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59107/0.77524. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59127/0.77662. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59306/0.77882. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59116/0.78245. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57920/0.78922. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58103/0.79275. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57368/0.79565. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57765/0.79689. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57203/0.80192. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57596/0.80625. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57351/0.80987. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56743/0.81878. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56406/0.82097. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56472/0.82140. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56570/0.82112. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69556/0.69216. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69140. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69298/0.69097. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.69094. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69124/0.69092. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.69048/0.69082. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.69069. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.69066. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.69047. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68904/0.69055. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68774/0.69040. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68675/0.69056. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68618/0.69087. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68501/0.69077. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68487/0.69145. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68381/0.69097. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68202/0.69100. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68019/0.69181. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68042/0.69281. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67795/0.69266. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67481/0.69302. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67390/0.69352. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67241/0.69303. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66999/0.69371. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66931/0.69462. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66657/0.69255. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66215/0.69376. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66440/0.69276. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66051/0.69374. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65788/0.69486. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65800/0.69523. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65352/0.69688. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65189/0.69464. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65002/0.69824. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64650/0.70081. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64741/0.70309. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64509/0.70245. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64234/0.70517. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63886/0.70502. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63452/0.70564. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63345/0.70887. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63322/0.71070. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63066/0.71144. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62510/0.71611. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62395/0.71461. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62223/0.71802. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62097/0.71796. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.61693/0.72448. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61403/0.72289. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61604/0.72410. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61394/0.72143. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60492/0.72562. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60832/0.72860. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61070/0.72941. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60671/0.72848. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60152/0.72991. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60171/0.72968. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59426/0.73370. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59311/0.73453. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59157/0.73799. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59042/0.73556. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58819/0.74140. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.58597/0.74207. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58831/0.74703. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58513/0.74261. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58506/0.74511. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58065/0.74319. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57030/0.74789. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57235/0.74648. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.56630/0.75043. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57088/0.75061. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56504/0.75362. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56416/0.75114. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56039/0.75175. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55520/0.75541. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55245/0.75977. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.54873/0.76365. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.55260/0.76701. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54622/0.76340. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54110/0.76375. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54198/0.77042. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54307/0.76740. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.53500/0.76790. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53829/0.77120. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53291/0.77395. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.52993/0.76935. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52035/0.77902. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52564/0.77894. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52738/0.78041. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51684/0.78304. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51640/0.78776. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51927/0.78860. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.51408/0.79345. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50337/0.79699. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.50019/0.80425. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49750/0.80492. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50311/0.80691. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49885/0.80682. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.48706/0.81119. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49635/0.81540. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69300/0.69188. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69271/0.69160. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.69137. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69162/0.69116. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69129/0.69098. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69110/0.69086. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69031/0.69072. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.69075. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69011/0.69076. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.69060. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68880/0.69042. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68728/0.69021. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68614/0.68996. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68569/0.68963. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68427/0.68923. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68395/0.68902. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68125/0.68890. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67985/0.68890. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67780/0.68899. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67711/0.68964. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67506/0.69040. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67250/0.69033. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66972/0.69089. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66882/0.69230. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66507/0.69261. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66405/0.69316. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66112/0.69397. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66128/0.69538. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65794/0.69593. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65706/0.69718. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65216/0.69838. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65260/0.70037. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.64906/0.70208. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64750/0.70299. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64828/0.70428. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64316/0.70709. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64067/0.70861. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63802/0.70823. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63809/0.71057. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63539/0.71279. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63306/0.71583. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63254/0.71836. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62959/0.72043. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62636/0.72351. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62768/0.72646. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62062/0.72820. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62184/0.73167. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61969/0.73544. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61872/0.73635. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.61536/0.73947. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61244/0.74413. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60853/0.74662. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60894/0.74843. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60463/0.75380. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60693/0.75320. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60391/0.75822. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59954/0.76047. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60143/0.76431. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59438/0.76437. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59735/0.76745. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59384/0.76997. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58863/0.77675. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58982/0.78022. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59054/0.78031. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58643/0.78301. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58324/0.78475. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58239/0.78760. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57880/0.79226. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57796/0.79265. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57686/0.79800. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.56955/0.80183. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57412/0.80629. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57045/0.80662. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.57176/0.80998. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56520/0.81062. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56539/0.81789. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56134/0.81918. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56040/0.81792. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56062/0.82278. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.55372/0.82603. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55554/0.83023. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54791/0.83153. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.54709/0.83288. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55060/0.83790. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54557/0.84375. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54188/0.84318. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53769/0.84491. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53564/0.84946. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53999/0.85006. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.53593/0.84804. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.52314/0.85753. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.52449/0.86002. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.52607/0.86208. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51652/0.86392. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51621/0.87359. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51748/0.86769. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51010/0.87224. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51128/0.88224. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.50495/0.88400. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50061/0.88912. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69568/0.68924. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.68845. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.68834. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69103/0.68887. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.68962. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68974/0.69039. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68978/0.69155. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68788/0.69199. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.69331. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68534/0.69415. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68553/0.69560. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68400/0.69763. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68222/0.69825. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68149/0.69963. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67976/0.70157. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67792/0.70146. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67771/0.70245. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67681/0.70294. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67346/0.70448. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67373/0.70333. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67004/0.70425. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66955/0.70383. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66702/0.70185. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66421/0.70003. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66346/0.70144. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66183/0.70038. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66055/0.70122. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65672/0.70052. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65534/0.70012. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.65154/0.69875. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65128/0.70141. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.64814/0.70125. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64494/0.70051. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64279/0.70099. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.63992/0.70002. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63735/0.69778. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63387/0.70106. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63391/0.69703. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63268/0.69694. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63099/0.69724. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63094/0.70042. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62644/0.69897. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62225/0.69962. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62415/0.69923. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61955/0.70036. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.61717/0.69743. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.61497/0.70209. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.61484/0.70223. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61014/0.70296. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61104/0.70250. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.60978/0.70300. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.60516/0.70204. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60436/0.70659. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60415/0.69990. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60326/0.70453. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59923/0.70127. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59844/0.70397. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.59461/0.70341. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59235/0.70507. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59645/0.71025. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59319/0.70710. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58896/0.71624. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59107/0.71519. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58267/0.71429. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58358/0.71142. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.58422/0.71038. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57972/0.71540. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57735/0.71459. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57894/0.71470. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56947/0.71586. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57242/0.72141. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57075/0.72159. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57431/0.72235. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56619/0.72424. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56509/0.72173. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56149/0.71926. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56262/0.73569. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55948/0.72935. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55930/0.73412. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55757/0.72744. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55401/0.73564. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55137/0.73682. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55510/0.73265. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55084/0.73203. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54621/0.73921. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54702/0.73592. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54635/0.73195. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54570/0.74023. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54655/0.73644. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53688/0.74092. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53765/0.74682. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53098/0.75181. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52990/0.75838. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53787/0.75363. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52731/0.76037. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.53083/0.76020. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52407/0.75734. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52137/0.75847. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51938/0.76422. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52075/0.77036. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69033/0.69410. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69047/0.69350. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68922/0.69281. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68701/0.69320. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68683/0.69345. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68700/0.69360. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68632/0.69363. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68570/0.69355. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68632/0.69374. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68523/0.69412. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68422/0.69434. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68406/0.69424. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68475/0.69452. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68358/0.69463. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68443/0.69406. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68357/0.69477. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68299/0.69474. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68276/0.69465. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68225/0.69430. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68343/0.69433. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68152/0.69417. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68182/0.69403. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68097/0.69413. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68063/0.69393. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68080/0.69376. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68037/0.69365. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67859/0.69369. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67875/0.69340. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67888/0.69299. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67926/0.69308. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67683/0.69292. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67796/0.69257. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67721/0.69297. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67672/0.69129. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67510/0.69182. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67478/0.69169. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67488/0.69166. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67398/0.69145. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67218/0.69073. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67205/0.69081. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67385/0.69003. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67196/0.69054. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67088/0.69067. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67170/0.68897. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67082/0.68908. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66926/0.68876. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66959/0.68782. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67064/0.68745. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66858/0.68680. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66754/0.68750. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66815/0.68682. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66800/0.68668. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66471/0.68579. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66530/0.68640. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66465/0.68652. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66425/0.68794. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66459/0.68601. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66148/0.68691. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66384/0.68536. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66065/0.68556. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65995/0.68515. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66221/0.68547. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65906/0.68565. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65597/0.68525. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65924/0.68502. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65792/0.68510. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65549/0.68623. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65509/0.68459. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65511/0.68631. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65462/0.68660. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65318/0.68615. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65110/0.68695. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65017/0.68747. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64881/0.68754. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64909/0.68634. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64837/0.68854. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64825/0.69058. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64809/0.69055. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64654/0.68737. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64388/0.68970. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64304/0.69089. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64552/0.69055. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64079/0.69213. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64617/0.69093. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64128/0.69441. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64133/0.69451. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63757/0.69605. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63928/0.69397. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63513/0.69522. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63185/0.69852. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63312/0.69813. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63177/0.69987. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63339/0.69861. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63212/0.70040. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62745/0.70133. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62525/0.70543. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.62556/0.70448. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62347/0.70593. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62381/0.70529. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62420/0.70819. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69068/0.68691. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68953/0.68674. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69022/0.68654. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68923/0.68635. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68986/0.68620. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.68596. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68938/0.68573. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68890/0.68550. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68829/0.68539. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68849/0.68532. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.68518. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68764/0.68521. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68816/0.68501. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68710/0.68480. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68781/0.68465. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68677/0.68485. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68567/0.68502. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68572/0.68508. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68520/0.68517. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68443/0.68506. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68325/0.68498. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68334/0.68497. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68474/0.68514. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68221/0.68496. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68074/0.68448. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68104/0.68482. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67962/0.68497. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67918/0.68436. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67773/0.68439. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67552/0.68461. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67503/0.68449. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67339/0.68452. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67072/0.68472. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66946/0.68385. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66789/0.68391. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66526/0.68433. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66489/0.68481. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66323/0.68523. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66183/0.68633. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65721/0.68542. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65467/0.68686. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65299/0.68722. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65246/0.68866. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64875/0.68968. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64761/0.69018. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64399/0.69035. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63970/0.69123. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63927/0.69107. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63660/0.69173. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63037/0.69361. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63141/0.69367. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62709/0.69636. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62554/0.69687. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62069/0.69767. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61625/0.70257. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61339/0.70229. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61669/0.70228. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61002/0.70390. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60749/0.70771. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60290/0.70747. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60029/0.71038. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59650/0.71095. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59748/0.71466. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59223/0.71627. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58680/0.72105. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.59342/0.72452. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58522/0.72666. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58442/0.73083. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58316/0.73249. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57493/0.73510. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57667/0.73948. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56503/0.74592. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56155/0.75088. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56501/0.75218. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56083/0.75765. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55797/0.75810. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55936/0.75951. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.54988/0.76748. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54495/0.76641. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54019/0.77115. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.54701/0.77603. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54376/0.77550. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54480/0.78058. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53544/0.78816. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52860/0.79285. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53517/0.79506. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52789/0.79768. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52511/0.80200. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52602/0.80111. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.52083/0.80681. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51329/0.81040. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51486/0.81944. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51163/0.82207. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51098/0.82758. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50494/0.82597. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49594/0.83887. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50923/0.84210. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49705/0.83917. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49064/0.84209. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.49558/0.84863. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.68922/0.68713. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68759/0.68740. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68623/0.68797. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68619/0.68850. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68626/0.68908. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68636/0.68969. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68528/0.69016. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68478/0.69065. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68499/0.69113. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68476/0.69164. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68407/0.69203. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68352/0.69228. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68348/0.69225. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68326/0.69212. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68368/0.69230. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68257/0.69228. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68192/0.69234. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68127/0.69216. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68035/0.69255. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68031/0.69243. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67950/0.69250. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67953/0.69253. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67888/0.69249. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67667/0.69243. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67702/0.69267. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67644/0.69263. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67392/0.69246. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67439/0.69251. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67328/0.69227. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67155/0.69245. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66828/0.69220. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67015/0.69282. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66894/0.69223. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66641/0.69203. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66790/0.69270. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66522/0.69227. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66255/0.69204. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66224/0.69188. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65877/0.69178. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65418/0.69133. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65683/0.69201. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65597/0.69243. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65313/0.69135. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65444/0.69095. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64860/0.69319. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64758/0.69217. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64805/0.69128. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64622/0.69156. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64393/0.69106. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64586/0.69107. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64006/0.69215. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64158/0.69104. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63909/0.69151. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63610/0.69010. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63470/0.69199. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.63326/0.69182. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63223/0.69263. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63201/0.69161. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62877/0.69022. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62619/0.68975. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62420/0.69147. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62371/0.69231. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62118/0.69117. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61855/0.69066. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61841/0.69018. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61730/0.69184. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61645/0.69039. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61179/0.68886. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61176/0.68830. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60855/0.68881. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60903/0.68806. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60305/0.68720. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60752/0.68712. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60332/0.68766. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59943/0.68675. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59917/0.68640. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60196/0.68366. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59520/0.68463. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59656/0.68183. Took 0.13 sec\n",
      "Epoch 79, Loss(train/val) 0.58808/0.68407. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58883/0.68439. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59456/0.68443. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58727/0.68327. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58573/0.68155. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58325/0.68069. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58003/0.68101. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57791/0.68072. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57330/0.68126. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57617/0.68170. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57472/0.68084. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57203/0.67888. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.56670/0.68130. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56794/0.68375. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56627/0.68254. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56547/0.68278. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55765/0.68525. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55296/0.67931. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55757/0.68044. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55481/0.68030. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54628/0.68267. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69402/0.68918. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69038/0.68415. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68905/0.68149. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68727/0.67987. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68627/0.67901. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68595/0.67832. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68568/0.67797. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68587/0.67761. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68584/0.67735. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68512/0.67701. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68571/0.67703. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68455/0.67691. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68559/0.67685. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68546/0.67681. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68311/0.67653. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68445/0.67634. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68413/0.67620. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68356/0.67615. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68171/0.67622. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68346/0.67639. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68231/0.67620. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68195/0.67614. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68198/0.67608. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68136/0.67609. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67978/0.67597. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68027/0.67683. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67932/0.67747. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67913/0.67737. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67871/0.67755. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67704/0.67738. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67754/0.67861. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67536/0.67860. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67443/0.67851. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67339/0.67953. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67277/0.67985. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67333/0.68063. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67009/0.68062. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67104/0.68149. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66967/0.68307. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66882/0.68361. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66913/0.68323. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66737/0.68448. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66638/0.68521. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66740/0.68580. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66490/0.68684. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66186/0.68916. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65986/0.68873. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66250/0.69057. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66186/0.69110. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66038/0.69151. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65742/0.69411. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65582/0.69542. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65511/0.69698. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65722/0.69682. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65265/0.69604. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65209/0.69749. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65136/0.70003. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64817/0.70243. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64849/0.70336. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65034/0.70508. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64632/0.70639. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64655/0.70830. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64369/0.71108. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64284/0.71322. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64288/0.71477. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63784/0.71619. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63934/0.71750. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63755/0.72020. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63711/0.72117. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63524/0.72293. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63514/0.72431. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62855/0.72741. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62906/0.73000. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63018/0.73185. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62637/0.73330. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62856/0.73594. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62457/0.73628. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62395/0.73867. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62046/0.74220. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61907/0.74403. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62408/0.74297. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61805/0.74596. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62078/0.74716. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61768/0.75088. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61455/0.75557. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61376/0.75554. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61146/0.75588. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60921/0.75854. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60452/0.76000. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61301/0.76115. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61058/0.76211. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60484/0.76739. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.60449/0.76880. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60501/0.77358. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59686/0.77436. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59593/0.77984. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59525/0.78035. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59285/0.78223. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59350/0.78191. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58859/0.78493. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69227/0.68842. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68915/0.68521. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68619/0.68246. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68553/0.68082. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68449/0.67991. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68413/0.67946. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.68380/0.67916. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68339/0.67908. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68478/0.67883. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68364/0.67861. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68336/0.67857. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68345/0.67829. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68313/0.67805. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68230/0.67778. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68346/0.67760. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68273/0.67732. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68241/0.67701. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68129/0.67674. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68119/0.67646. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68051/0.67627. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68058/0.67591. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67869/0.67574. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67969/0.67543. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67965/0.67520. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67844/0.67499. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67890/0.67475. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67782/0.67452. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67757/0.67419. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67608/0.67326. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67580/0.67321. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67568/0.67327. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67504/0.67279. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67381/0.67257. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67254/0.67251. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.67309/0.67222. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67185/0.67174. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67180/0.67112. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66844/0.67140. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66804/0.67104. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66821/0.67126. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66781/0.67172. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66839/0.67176. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66516/0.67145. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66327/0.67193. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66366/0.67241. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66291/0.67253. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66114/0.67226. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66104/0.67218. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65953/0.67332. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65729/0.67384. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65721/0.67436. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65583/0.67504. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65464/0.67570. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65271/0.67803. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65047/0.67907. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64805/0.67943. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65002/0.67998. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64762/0.67945. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64417/0.67920. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64416/0.67988. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64122/0.68125. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64044/0.68317. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63920/0.68540. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64156/0.68733. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63835/0.68510. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63277/0.68846. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63300/0.69130. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63483/0.69460. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63149/0.69514. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62854/0.69904. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63244/0.69862. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62322/0.69803. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62622/0.69929. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62559/0.69877. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62145/0.69998. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61857/0.70555. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61789/0.70591. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61683/0.70220. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61847/0.70716. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61375/0.70568. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61430/0.71173. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61386/0.71091. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60720/0.71109. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60754/0.71091. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60795/0.71446. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60257/0.71094. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60589/0.71170. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60135/0.71606. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60252/0.71956. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59902/0.72385. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59861/0.71869. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59633/0.72087. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59601/0.72210. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59447/0.72376. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59323/0.72522. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59655/0.72686. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58674/0.72671. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58709/0.73002. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58364/0.73399. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58397/0.72834. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69471/0.69263. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68811/0.69000. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68392/0.68921. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68380/0.68904. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68272/0.68906. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68214/0.68913. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68263/0.68902. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68287/0.68903. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68181/0.68912. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68238/0.68916. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68143/0.68913. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68161/0.68908. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68100/0.68888. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67992/0.68890. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68032/0.68887. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68029/0.68892. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67892/0.68838. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68036/0.68848. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67942/0.68833. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67845/0.68862. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67930/0.68868. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67770/0.68817. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67794/0.68826. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67601/0.68856. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67680/0.68889. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67564/0.68885. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67656/0.68831. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67485/0.68742. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67552/0.68651. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67393/0.68670. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67309/0.68666. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67357/0.68600. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67180/0.68604. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67284/0.68606. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67022/0.68633. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67079/0.68667. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66951/0.68593. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66900/0.68538. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66793/0.68577. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66847/0.68554. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66687/0.68535. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66528/0.68571. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66337/0.68561. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66363/0.68551. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66595/0.68524. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66327/0.68500. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66287/0.68496. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66107/0.68462. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65973/0.68466. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65945/0.68517. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65821/0.68533. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65660/0.68620. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65684/0.68490. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65172/0.68627. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65353/0.68615. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65112/0.68588. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64856/0.68602. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64954/0.68522. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64736/0.68669. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64548/0.68761. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64841/0.68532. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64572/0.68581. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64301/0.68551. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64214/0.68744. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63678/0.68780. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63731/0.68608. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63835/0.68709. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63473/0.68891. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63050/0.69080. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62735/0.69179. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62665/0.69199. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62644/0.69280. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62695/0.69253. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62369/0.69435. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62075/0.69445. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62045/0.69490. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61802/0.69392. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61733/0.69674. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61783/0.69769. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61121/0.69628. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60738/0.69501. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60804/0.70025. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60634/0.70282. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60170/0.69973. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60011/0.70130. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59334/0.70675. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.59742/0.70700. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59471/0.70646. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59558/0.70798. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59098/0.71004. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58730/0.70764. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58809/0.70706. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57948/0.71506. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57891/0.71269. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57625/0.71303. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57875/0.71483. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58067/0.71429. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57287/0.71752. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56761/0.72009. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56582/0.72048. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69168/0.69089. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69062/0.68995. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68982/0.68830. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68803/0.68625. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68641/0.68448. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68594/0.68344. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68492/0.68299. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68438/0.68291. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68425/0.68302. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68381/0.68325. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68392/0.68342. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68387/0.68359. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68357/0.68400. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68321/0.68432. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68283/0.68461. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68256/0.68507. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68143/0.68543. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68104/0.68598. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68084/0.68698. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68138/0.68775. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68089/0.68853. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67988/0.68927. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67923/0.69040. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67830/0.69123. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67824/0.69247. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67675/0.69388. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67717/0.69483. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67637/0.69601. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67560/0.69714. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67424/0.69866. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67317/0.70022. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67380/0.70125. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67191/0.70214. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67218/0.70298. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67289/0.70339. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67058/0.70474. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67039/0.70583. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66897/0.70591. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66966/0.70779. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66852/0.70877. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66774/0.70934. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66664/0.70947. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66517/0.71119. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66533/0.71199. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66350/0.71304. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66388/0.71393. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66061/0.71518. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65981/0.71570. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66256/0.71514. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66085/0.71632. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65786/0.71683. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65823/0.71797. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65665/0.71907. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65205/0.71912. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65103/0.72037. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64942/0.72220. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64950/0.72217. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64649/0.72282. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64428/0.72334. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64744/0.72347. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64539/0.72362. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64201/0.72428. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64039/0.72479. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63792/0.72653. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63645/0.72876. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63699/0.73149. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62800/0.73288. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63305/0.73292. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62955/0.73295. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62927/0.73491. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62309/0.73712. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62257/0.73852. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61988/0.74121. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61684/0.74291. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61927/0.74414. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61724/0.74281. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61184/0.74446. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60919/0.74654. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61076/0.74851. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60447/0.75004. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60452/0.75033. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59901/0.74937. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60413/0.74797. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60141/0.74978. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59451/0.75514. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59502/0.75824. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59056/0.76119. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59366/0.76228. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58692/0.76115. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58985/0.76319. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58200/0.76698. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58197/0.76759. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58021/0.76709. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57605/0.76978. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57019/0.77303. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56766/0.77376. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56624/0.77756. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56186/0.78202. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56186/0.78540. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55712/0.78937. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68890/0.69120. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68583/0.69296. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68274/0.69539. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68222/0.69705. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68124/0.69786. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68025/0.69816. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68068/0.69786. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.67896/0.69773. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.67835/0.69750. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.67811/0.69675. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.67714/0.69563. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67687/0.69485. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.67566/0.69442. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67632/0.69445. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67423/0.69396. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67439/0.69392. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67425/0.69456. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67303/0.69427. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67114/0.69372. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67075/0.69307. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67056/0.69378. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66824/0.69315. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66757/0.69384. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66559/0.69370. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66415/0.69348. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66372/0.69432. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66205/0.69275. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66009/0.69385. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65901/0.69170. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65609/0.69253. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65544/0.69210. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65325/0.69149. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65267/0.69082. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64868/0.69126. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64778/0.69328. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64755/0.69250. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64152/0.69426. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64304/0.69462. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63861/0.69599. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63484/0.69691. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63506/0.69708. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63567/0.69821. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63220/0.69955. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63184/0.70195. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62636/0.70348. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62219/0.70592. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62242/0.70675. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62197/0.70631. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61892/0.70949. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61727/0.71039. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61438/0.71085. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61305/0.71309. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60799/0.71533. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60963/0.71463. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60795/0.71443. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60274/0.71926. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60676/0.71777. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59793/0.71787. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59765/0.72196. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59630/0.72286. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59415/0.72489. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59038/0.72512. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.58999/0.72681. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58844/0.72862. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58365/0.72873. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.57857/0.73261. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57538/0.73645. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57960/0.73471. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57554/0.73861. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57372/0.74050. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57542/0.74226. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56993/0.74233. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56944/0.74415. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56366/0.74558. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.55747/0.74833. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55769/0.75221. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56154/0.75417. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55422/0.75336. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55039/0.75578. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55052/0.75789. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54627/0.76384. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54477/0.76167. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54145/0.76713. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54029/0.76928. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54135/0.77139. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54051/0.77840. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53335/0.77802. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53713/0.78261. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.53106/0.78049. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52880/0.78703. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52182/0.79230. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52512/0.79880. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.52626/0.80248. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51204/0.79345. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51968/0.79807. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51008/0.79795. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51160/0.79995. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50420/0.80205. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50365/0.80738. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51225/0.81509. Took 0.08 sec\n",
      "ACC: 0.65625\n",
      "Epoch 0, Loss(train/val) 0.69063/0.70403. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68633/0.70526. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68570/0.70344. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68476/0.70202. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68444/0.70024. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68385/0.69896. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68354/0.69769. Took 0.12 sec\n",
      "Epoch 7, Loss(train/val) 0.68276/0.69613. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68306/0.69458. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68219/0.69375. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68196/0.69242. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68109/0.69053. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68033/0.68938. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68009/0.68838. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67923/0.68716. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67957/0.68652. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.67954/0.68609. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67851/0.68529. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67815/0.68420. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67743/0.68377. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67851/0.68342. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67689/0.68308. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67813/0.68257. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67685/0.68287. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67622/0.68259. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67638/0.68150. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67651/0.68154. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67612/0.68235. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67512/0.68102. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67602/0.68087. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67365/0.68175. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67418/0.68109. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67321/0.67949. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67202/0.67989. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67413/0.67891. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67297/0.67887. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66980/0.67909. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67118/0.67830. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67085/0.67895. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66909/0.67833. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66929/0.67747. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66799/0.67655. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66524/0.67627. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66578/0.67471. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66472/0.67427. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66297/0.67220. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66334/0.67263. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66115/0.67089. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66046/0.67155. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65728/0.66686. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65633/0.66619. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65625/0.66250. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65512/0.66190. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65354/0.66111. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65284/0.65944. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64845/0.65808. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64821/0.65622. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64622/0.65577. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64726/0.65329. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64575/0.65404. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63979/0.65153. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64138/0.65117. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64114/0.65013. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63525/0.65025. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63826/0.64999. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63312/0.64851. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63237/0.65015. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63111/0.65148. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62872/0.65165. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62801/0.65184. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62497/0.65202. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62242/0.65148. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61974/0.65416. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61828/0.65404. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61866/0.65642. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61831/0.65782. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61128/0.65797. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61380/0.65972. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60807/0.66176. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60734/0.66515. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60752/0.66699. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60419/0.66623. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60432/0.66877. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60181/0.67138. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59888/0.67413. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60068/0.67682. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59928/0.67781. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59272/0.68436. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58931/0.68332. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59102/0.68799. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59131/0.69008. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58570/0.69309. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58693/0.69636. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58449/0.70034. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.58391/0.70143. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58025/0.70552. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57849/0.71141. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58425/0.70684. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57753/0.71421. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57980/0.71841. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69995/0.69480. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69797/0.69409. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69728/0.69248. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69356/0.69008. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69127/0.68795. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68919/0.68694. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68753/0.68698. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68662/0.68737. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68582/0.68783. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68689/0.68813. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68502/0.68844. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68427/0.68883. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68511/0.68922. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68359/0.68975. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68310/0.69040. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68318/0.69102. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68204/0.69164. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68114/0.69245. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68157/0.69316. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68001/0.69378. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68000/0.69480. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67885/0.69561. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67831/0.69713. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67732/0.69815. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67498/0.69915. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67443/0.70075. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67400/0.70189. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67307/0.70285. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67281/0.70355. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67227/0.70432. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67080/0.70572. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66918/0.70745. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67027/0.70832. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66772/0.71024. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66562/0.71130. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66523/0.71288. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66314/0.71438. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66159/0.71518. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65904/0.71668. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65742/0.71826. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65525/0.71996. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65488/0.71993. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65442/0.72061. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65072/0.72383. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64883/0.72549. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64576/0.72711. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64420/0.72886. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64176/0.72984. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63831/0.73157. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63486/0.73210. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63487/0.73122. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63082/0.73183. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63093/0.73160. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62841/0.73322. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62376/0.73622. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62101/0.73837. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61814/0.74104. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62006/0.74290. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61160/0.74467. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61392/0.74615. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61524/0.74517. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60468/0.74852. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60741/0.75190. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60721/0.75453. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60507/0.75782. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59726/0.76240. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59950/0.76731. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59495/0.76712. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59159/0.76763. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59416/0.76852. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58897/0.77075. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58547/0.77442. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58765/0.77550. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58653/0.77645. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58716/0.77832. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58642/0.78181. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57977/0.78631. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57977/0.78902. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57466/0.79156. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57165/0.79396. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57785/0.79484. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56722/0.79871. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56515/0.79982. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56582/0.79831. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56342/0.79731. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56219/0.79681. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.55253/0.80290. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55423/0.80811. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55325/0.80957. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55221/0.81318. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54664/0.81435. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55044/0.81481. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54416/0.82124. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54372/0.82375. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53458/0.82893. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.53661/0.83451. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.53302/0.83465. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53158/0.83469. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53109/0.83943. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52606/0.84571. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69070/0.69332. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69056/0.69441. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69011/0.69503. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68935/0.69545. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68880/0.69601. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68820/0.69678. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68814/0.69721. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68780/0.69748. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68722/0.69859. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68565/0.69939. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68580/0.70066. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68531/0.70184. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68477/0.70272. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68374/0.70370. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68342/0.70507. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68342/0.70649. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68146/0.70679. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68146/0.70785. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68162/0.70883. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67942/0.70933. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68147/0.71015. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68002/0.71089. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67961/0.71106. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67961/0.71167. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67831/0.71200. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67805/0.71205. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67668/0.71334. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67810/0.71381. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67813/0.71409. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67656/0.71471. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67592/0.71521. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67604/0.71596. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67448/0.71611. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67330/0.71659. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67242/0.71703. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67488/0.71807. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67230/0.71844. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67204/0.71849. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67160/0.71879. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67074/0.71870. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67063/0.71888. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67072/0.71950. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66843/0.71983. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66802/0.71936. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66784/0.71946. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66686/0.71998. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66793/0.72011. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66421/0.72013. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66416/0.72135. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66219/0.72131. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66215/0.72091. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66131/0.72041. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66154/0.72089. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66157/0.72065. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66023/0.72177. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65841/0.72123. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65711/0.72171. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65476/0.72161. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65686/0.72202. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65460/0.72066. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65419/0.72118. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65228/0.72095. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64826/0.72080. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64963/0.72145. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64913/0.72023. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64535/0.72037. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64299/0.72116. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64500/0.72077. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64078/0.72009. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64075/0.71884. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63913/0.72085. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63930/0.71974. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63343/0.72070. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63180/0.72129. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63360/0.72111. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62643/0.72139. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62703/0.72054. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62646/0.71972. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62269/0.72081. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62103/0.72052. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62005/0.72048. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62186/0.71926. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61315/0.72195. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61369/0.72484. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61056/0.72534. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60815/0.72730. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60647/0.72913. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60488/0.73196. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59979/0.73164. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59510/0.73197. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59594/0.73139. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59447/0.73463. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59366/0.73506. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58917/0.73619. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58355/0.73608. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58392/0.74138. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57542/0.74093. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58011/0.74188. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57603/0.73973. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57257/0.73777. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69009/0.69332. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.68958/0.69342. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68948/0.69344. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68923/0.69366. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68806/0.69388. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68857/0.69394. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68801/0.69416. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68764/0.69427. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68714/0.69454. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68641/0.69478. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68487/0.69500. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68509/0.69538. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68532/0.69571. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68358/0.69607. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68367/0.69678. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68386/0.69718. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68219/0.69750. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68144/0.69800. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68068/0.69845. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68009/0.69906. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67728/0.69898. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67860/0.69927. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67701/0.69966. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67821/0.70017. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67744/0.70088. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67588/0.70122. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67674/0.70150. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67709/0.70188. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67427/0.70178. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67325/0.70094. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67396/0.70057. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67540/0.70052. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67181/0.70108. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67260/0.70127. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67246/0.70169. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67369/0.70223. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67074/0.70173. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67060/0.70168. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66839/0.70268. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66862/0.70199. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66856/0.70191. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66758/0.70205. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66627/0.70195. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66518/0.70129. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66433/0.70161. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66284/0.70132. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66245/0.70157. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66287/0.70160. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66133/0.70117. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66056/0.70168. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66000/0.70024. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65868/0.69998. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65665/0.69994. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65605/0.70003. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65390/0.69960. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65373/0.70046. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65138/0.69931. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64918/0.69938. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64747/0.69956. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64473/0.70035. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64643/0.69997. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64320/0.70215. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64184/0.70263. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64069/0.70117. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63983/0.70058. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63970/0.70092. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63668/0.70291. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63186/0.70150. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62597/0.70111. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62647/0.69987. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62636/0.70211. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62550/0.70348. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62296/0.70116. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62601/0.70260. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61865/0.69870. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61495/0.70013. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61285/0.70081. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61322/0.70130. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61144/0.70599. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60764/0.70460. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60796/0.70339. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60102/0.70441. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60290/0.70503. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59667/0.70560. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60064/0.70327. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.59769/0.70372. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59399/0.70713. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59327/0.70549. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58810/0.70110. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58874/0.70287. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58085/0.70466. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58362/0.70190. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58198/0.70299. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58022/0.70470. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57861/0.70816. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57644/0.70118. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57600/0.70910. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56746/0.70492. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56580/0.70266. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55932/0.70863. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69079/0.70664. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.68968/0.70575. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68898/0.70557. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68848/0.70570. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68841/0.70519. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68781/0.70553. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68719/0.70574. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68722/0.70556. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68590/0.70566. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68609/0.70601. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68646/0.70595. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68587/0.70624. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68501/0.70661. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68493/0.70664. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68529/0.70706. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68423/0.70717. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68425/0.70736. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68389/0.70829. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68355/0.70801. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68261/0.70859. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68266/0.70855. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68306/0.70863. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68146/0.70922. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68095/0.70968. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68150/0.70981. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68065/0.71033. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68042/0.71106. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67987/0.71079. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67998/0.71135. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67810/0.71129. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67890/0.71176. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67786/0.71087. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67679/0.71249. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67753/0.71235. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67529/0.71238. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67442/0.71364. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67370/0.71374. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67296/0.71302. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67120/0.71290. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67213/0.71293. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67006/0.71370. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66762/0.71406. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66859/0.71417. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66561/0.71330. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66857/0.71470. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66496/0.71462. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66287/0.71409. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66258/0.71255. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66111/0.71272. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65926/0.71430. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65874/0.71451. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65698/0.71378. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65441/0.71506. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65403/0.71250. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65216/0.71329. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64941/0.71275. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64821/0.71177. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64625/0.71443. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64536/0.71351. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64257/0.71354. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64079/0.71418. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64141/0.71513. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63716/0.71526. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63612/0.71586. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63407/0.71519. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63361/0.71714. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63641/0.71713. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62835/0.71562. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63220/0.72201. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62723/0.72064. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62874/0.72099. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62601/0.72188. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62176/0.72355. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61949/0.72550. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61884/0.72444. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61556/0.72521. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60981/0.72666. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61220/0.72627. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61202/0.72846. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61059/0.72758. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60296/0.72765. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60625/0.72978. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60298/0.73351. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.59637/0.73786. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59833/0.73602. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59817/0.73562. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59440/0.73923. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58987/0.73940. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59054/0.74095. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.59319/0.74077. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58971/0.74147. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58770/0.74545. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58222/0.74476. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57937/0.74960. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58056/0.75059. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57925/0.75268. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57748/0.75139. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57277/0.75161. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.57313/0.75885. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57155/0.75875. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69216/0.68960. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69169/0.69007. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68973/0.69036. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68996/0.69030. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68966/0.69050. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68923/0.69069. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.69103. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68843/0.69146. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68811/0.69174. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68775/0.69229. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68793/0.69253. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68736/0.69308. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68757/0.69319. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68665/0.69339. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68687/0.69379. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68604/0.69423. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68539/0.69457. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68572/0.69516. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68533/0.69562. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68497/0.69599. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68481/0.69603. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68428/0.69615. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68365/0.69673. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68375/0.69713. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68332/0.69750. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68230/0.69728. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68229/0.69838. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68042/0.69833. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68062/0.69956. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68076/0.69948. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67945/0.69980. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67989/0.70082. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67829/0.70154. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67662/0.70276. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67810/0.70329. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67741/0.70500. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67644/0.70486. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67489/0.70576. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67370/0.70634. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67344/0.70820. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67279/0.70830. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67272/0.71046. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67111/0.71043. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66976/0.71333. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67088/0.71347. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66808/0.71455. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66547/0.71484. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66613/0.71589. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66462/0.72012. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66302/0.72110. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66228/0.72093. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66078/0.72239. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66033/0.72614. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65925/0.72655. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65937/0.72853. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65697/0.72829. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65601/0.73122. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65547/0.73253. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65347/0.73356. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65401/0.73429. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65056/0.73432. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65044/0.73612. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65237/0.73656. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64935/0.73742. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64639/0.73851. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64674/0.74108. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64946/0.74107. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64567/0.74133. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64470/0.74071. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64215/0.74271. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.64073/0.74522. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63992/0.74647. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63945/0.74545. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63785/0.74457. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63378/0.74643. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63705/0.75043. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63530/0.74798. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63356/0.75007. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63085/0.74943. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63237/0.75553. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63362/0.75301. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62965/0.75214. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62524/0.75172. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62689/0.75293. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62485/0.75410. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62428/0.75630. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62123/0.75530. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62429/0.75625. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62058/0.75371. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62262/0.75542. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61695/0.75527. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61899/0.76038. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61639/0.75436. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61204/0.76027. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61156/0.75888. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61258/0.76116. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61521/0.76156. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61002/0.76446. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60848/0.76161. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61188/0.75948. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69831/0.69146. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69140/0.69197. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68986/0.69285. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68884/0.69369. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68762/0.69469. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68737/0.69557. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68712/0.69641. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68602/0.69726. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68637/0.69792. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68600/0.69842. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68531/0.69893. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68540/0.69933. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68523/0.69989. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68459/0.70019. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68415/0.70057. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68394/0.70100. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68330/0.70152. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68255/0.70209. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68320/0.70244. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68238/0.70299. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68240/0.70337. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68201/0.70379. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68154/0.70425. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68090/0.70495. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68109/0.70518. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68034/0.70567. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68003/0.70595. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67994/0.70675. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67851/0.70726. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67772/0.70801. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67647/0.70873. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67727/0.70901. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67613/0.70945. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67402/0.71018. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67449/0.71106. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67466/0.71171. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67343/0.71250. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67241/0.71281. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67216/0.71327. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67140/0.71438. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67046/0.71515. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66826/0.71617. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66715/0.71752. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66628/0.71835. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66646/0.71937. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66436/0.72030. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66327/0.72218. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66232/0.72341. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66239/0.72366. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66132/0.72500. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65825/0.72594. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66089/0.72611. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65737/0.72734. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65578/0.72674. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65547/0.73007. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65339/0.73057. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65043/0.73142. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65008/0.73154. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64825/0.73324. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64841/0.73504. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64583/0.73608. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64631/0.73762. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64505/0.73812. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64559/0.73678. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64328/0.73603. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64213/0.73844. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64117/0.73954. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64106/0.74126. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64341/0.74364. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63921/0.74149. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63366/0.74272. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63604/0.74347. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63421/0.74544. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63471/0.74707. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63305/0.74939. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63093/0.74981. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62767/0.74937. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62648/0.75196. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63030/0.75160. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62682/0.75433. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62928/0.75571. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62316/0.75526. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62518/0.75942. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62253/0.76059. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62107/0.76276. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62181/0.76250. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62033/0.76162. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61084/0.76160. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61032/0.76736. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61151/0.76925. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61006/0.77107. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61242/0.77562. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60974/0.77813. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61140/0.77507. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60879/0.77820. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60717/0.77609. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60058/0.78212. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60363/0.78169. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60204/0.78599. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60086/0.78314. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69534/0.70358. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69189/0.70098. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69013/0.69978. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68884/0.69876. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68781/0.69777. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68706/0.69701. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68668/0.69618. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68597/0.69541. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68555/0.69477. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68530/0.69381. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68446/0.69314. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68444/0.69255. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68377/0.69205. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68484/0.69183. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68260/0.69174. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68244/0.69168. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68248/0.69191. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68215/0.69192. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68161/0.69190. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68112/0.69204. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67969/0.69237. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68059/0.69265. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67954/0.69312. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67920/0.69363. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67912/0.69364. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67833/0.69409. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67903/0.69436. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67877/0.69490. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67804/0.69443. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67660/0.69493. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67510/0.69644. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67502/0.69621. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67541/0.69658. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67368/0.69730. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67371/0.69773. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67356/0.69850. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67167/0.69794. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67041/0.69907. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67009/0.69965. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66960/0.70015. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66828/0.70031. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66939/0.69989. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66789/0.69954. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66703/0.69985. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66379/0.70108. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66467/0.70116. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66276/0.70164. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66312/0.70222. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66177/0.70153. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66079/0.70112. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66113/0.70119. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65814/0.70232. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65534/0.70074. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65466/0.70184. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65477/0.70186. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65245/0.70176. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65010/0.70111. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65210/0.70385. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65017/0.70277. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64907/0.70322. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64556/0.70289. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64680/0.70275. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64358/0.70278. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64201/0.70300. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64215/0.70325. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64183/0.70559. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63983/0.70489. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63742/0.70431. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63124/0.70369. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63458/0.70404. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63271/0.70402. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63327/0.70576. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62995/0.70688. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62806/0.70331. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62927/0.70697. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62934/0.70421. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62802/0.70932. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62155/0.70765. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62238/0.70850. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61807/0.71016. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62173/0.71081. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61828/0.71020. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61529/0.71180. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61440/0.71251. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.61170/0.70973. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61270/0.71371. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61067/0.71181. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60613/0.71284. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60429/0.71563. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60320/0.71439. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60265/0.71858. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59878/0.71865. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59985/0.71436. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59586/0.71605. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59552/0.71817. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59205/0.71886. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58969/0.72144. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58834/0.72018. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58970/0.72336. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58624/0.72740. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69408/0.71812. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69097/0.71821. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68956/0.71544. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68833/0.71482. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68778/0.71404. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68669/0.71309. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68616/0.71196. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68518/0.71323. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68514/0.71256. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68438/0.71275. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68449/0.71176. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68362/0.71153. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68360/0.71162. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68265/0.71142. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68270/0.71147. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68184/0.71190. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68237/0.71204. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68162/0.71059. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68044/0.71215. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68112/0.71049. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68011/0.71357. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67979/0.71183. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67936/0.71100. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68015/0.71336. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67804/0.71162. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67857/0.71387. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67745/0.71459. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67718/0.71482. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67603/0.71436. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67626/0.71667. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67501/0.71666. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67517/0.71872. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67366/0.71805. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67364/0.72033. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67288/0.72038. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67253/0.72428. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67067/0.72372. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67158/0.72765. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66947/0.72480. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67003/0.72762. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66876/0.73046. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66661/0.73050. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66893/0.73258. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66739/0.73564. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66458/0.73680. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66424/0.73879. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66305/0.74049. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66272/0.74729. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66014/0.74563. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66226/0.74895. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66026/0.74793. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65693/0.75830. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65650/0.75915. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65291/0.75810. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65488/0.76156. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65303/0.76494. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65369/0.76892. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65075/0.76869. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64854/0.77625. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64937/0.77282. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64769/0.77566. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64807/0.78200. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64264/0.78879. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64378/0.79771. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64336/0.78846. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64187/0.79442. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63991/0.80142. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63744/0.80830. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63667/0.80301. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63597/0.80180. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63588/0.80088. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63362/0.80847. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63103/0.80972. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62915/0.81348. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63141/0.81379. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62805/0.82104. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62784/0.81709. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62493/0.82322. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61864/0.81011. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62189/0.82467. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62036/0.83153. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62167/0.83711. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61574/0.82135. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61380/0.83578. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61572/0.83344. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61419/0.84499. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61265/0.83320. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60890/0.82303. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60553/0.83364. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60869/0.85099. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60639/0.83990. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60503/0.84051. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60584/0.85099. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59976/0.83854. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59979/0.84330. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60090/0.84627. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59462/0.83575. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59314/0.84534. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59013/0.85033. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59193/0.84731. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69270/0.68729. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69208/0.68768. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.68810. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69071/0.68882. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69077/0.69004. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68919/0.69162. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68827/0.69395. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68686/0.69666. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68524/0.69930. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68453/0.70247. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68113/0.70490. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68028/0.70791. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67839/0.70969. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67656/0.71189. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67572/0.71231. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67449/0.71253. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67281/0.71222. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67081/0.71368. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.66839/0.71461. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.66798/0.71418. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66537/0.71547. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66531/0.71516. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66448/0.71516. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66140/0.71467. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.65827/0.71287. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.65824/0.71255. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.65696/0.71250. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65644/0.71155. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65524/0.71097. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.65314/0.70937. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.64921/0.70797. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65002/0.70632. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64924/0.70590. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64622/0.70492. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64633/0.70346. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.64349/0.70424. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64207/0.70439. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64146/0.70363. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63785/0.70345. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63811/0.70254. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.63962/0.70467. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63166/0.70399. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63483/0.70313. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63273/0.70400. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62894/0.70346. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62918/0.70246. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62517/0.70267. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62702/0.70555. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62470/0.70537. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62239/0.70482. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61799/0.70606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61796/0.70641. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61850/0.70688. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61606/0.70758. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61525/0.70715. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61050/0.70678. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61084/0.70946. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60897/0.71189. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60629/0.71203. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60452/0.71207. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60217/0.71357. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60298/0.71334. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60575/0.71585. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60093/0.71650. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59650/0.72156. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60008/0.72321. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59413/0.72399. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59455/0.72429. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59235/0.72361. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58655/0.72838. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59061/0.72610. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59046/0.73113. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58885/0.72833. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58791/0.73215. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58318/0.73466. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57903/0.73985. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57669/0.73305. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57555/0.73835. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57544/0.74118. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57897/0.74786. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56873/0.74808. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56654/0.74877. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56975/0.75355. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.56654/0.75481. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56163/0.75905. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56792/0.75805. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55922/0.76318. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55796/0.76559. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55148/0.76713. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55877/0.76630. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56106/0.76838. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55300/0.77266. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55152/0.77717. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54933/0.77813. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54512/0.77540. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54406/0.78506. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54530/0.78802. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.54212/0.79205. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54338/0.79119. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53888/0.79277. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69353/0.69775. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69252/0.69738. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69161/0.69712. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.69693. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68954/0.69614. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.68933/0.69501. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68745/0.69395. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68648/0.69260. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68511/0.69116. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68285/0.69091. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68047/0.68988. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.67909/0.68952. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67854/0.68896. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.67890/0.68744. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67691/0.68797. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67628/0.68904. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67661/0.68854. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67416/0.68776. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67238/0.68908. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67315/0.68823. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67388/0.68739. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67200/0.68633. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67144/0.68754. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.66870/0.68659. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66959/0.68691. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66909/0.68596. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66902/0.68483. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66753/0.68391. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66640/0.68271. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66762/0.68322. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66568/0.68132. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66464/0.68140. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66411/0.68153. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66328/0.67825. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66323/0.67876. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66177/0.67833. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65798/0.67748. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66096/0.67629. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66151/0.67645. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65825/0.67664. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65640/0.67501. Took 0.11 sec\n",
      "Epoch 41, Loss(train/val) 0.65898/0.67526. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65574/0.67527. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65105/0.67262. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65357/0.67178. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65240/0.67163. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65217/0.67179. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64925/0.66960. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64869/0.67015. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65080/0.67071. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64533/0.66579. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64427/0.66671. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64556/0.66727. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64405/0.66570. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64412/0.66604. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64216/0.66528. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64339/0.66798. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64003/0.66476. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64098/0.66598. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63291/0.66695. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63699/0.66421. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63562/0.66493. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63188/0.66559. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63553/0.66515. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63167/0.66541. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63075/0.66648. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62946/0.66431. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62954/0.66409. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62892/0.66230. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62830/0.66188. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62849/0.66502. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62470/0.66315. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62048/0.66471. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62124/0.66157. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62183/0.66628. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62124/0.66415. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61878/0.66365. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61510/0.66386. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61388/0.66748. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61144/0.66732. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61526/0.66455. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61561/0.66619. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61029/0.66760. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60588/0.66909. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60644/0.66638. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60730/0.66520. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60585/0.66893. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59973/0.66808. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59923/0.66405. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.59777/0.66653. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59934/0.66902. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59752/0.66887. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59095/0.66962. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59239/0.66725. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59221/0.66904. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59209/0.66878. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58601/0.66978. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58551/0.66972. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58535/0.66918. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58425/0.67084. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69443/0.68797. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69301/0.68859. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.68937. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69145/0.69001. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69049/0.69064. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68991/0.69123. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68947/0.69225. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68904/0.69328. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68916/0.69416. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68753/0.69540. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68761/0.69640. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68645/0.69765. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68689/0.69873. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68515/0.69996. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68532/0.70106. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68424/0.70228. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68495/0.70293. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68347/0.70384. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68337/0.70404. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68293/0.70448. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68211/0.70529. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68286/0.70608. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68112/0.70712. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68118/0.70817. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67994/0.70864. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67878/0.70937. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67892/0.70942. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67841/0.71008. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67846/0.71098. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67635/0.71116. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67534/0.71154. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67586/0.71282. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67473/0.71298. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67526/0.71301. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67298/0.71378. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67238/0.71489. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67324/0.71567. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67213/0.71689. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67152/0.71724. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66846/0.71693. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66706/0.71702. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66678/0.71758. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66406/0.71911. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66564/0.71992. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66324/0.72140. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66363/0.72203. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66201/0.72148. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66017/0.72306. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65752/0.72364. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65552/0.72514. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65355/0.72575. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65115/0.72725. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64978/0.72777. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64856/0.72786. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64617/0.72815. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64660/0.72988. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64535/0.72981. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64145/0.72924. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63864/0.73151. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64087/0.73087. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63527/0.73143. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63431/0.73106. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63000/0.73309. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62692/0.73384. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62586/0.73455. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62267/0.73515. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62342/0.73430. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62304/0.73635. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62336/0.73818. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62216/0.73334. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61615/0.73634. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61394/0.73501. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61593/0.73944. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61261/0.73658. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61220/0.73657. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60904/0.73927. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60411/0.73860. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60238/0.73943. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59993/0.74138. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59672/0.74321. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59331/0.74265. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59362/0.75085. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59365/0.74118. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58834/0.74409. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59074/0.75092. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59095/0.74671. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58427/0.74404. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58399/0.73925. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58268/0.74735. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57787/0.75106. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57461/0.75078. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57726/0.74668. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57244/0.75085. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56871/0.75092. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56314/0.74882. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56451/0.75407. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55905/0.74830. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55467/0.75322. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56109/0.75809. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55450/0.75531. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69371/0.69339. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69391/0.69345. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69288/0.69344. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69173/0.69350. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69158/0.69392. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69109/0.69418. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69017/0.69463. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68991/0.69538. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68845/0.69624. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68733/0.69739. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68612/0.69850. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68521/0.70006. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68400/0.70196. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68325/0.70371. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68361/0.70494. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68139/0.70678. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68078/0.70850. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67971/0.71016. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67965/0.71164. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67965/0.71280. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67815/0.71494. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67602/0.71627. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67597/0.71767. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67630/0.71939. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67550/0.72018. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67256/0.72190. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67313/0.72301. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67068/0.72497. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67252/0.72576. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66928/0.72758. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66930/0.72936. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66808/0.73020. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66526/0.73205. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66538/0.73416. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66496/0.73553. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66443/0.73687. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66076/0.73853. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65960/0.74072. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65807/0.74151. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65488/0.74476. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65199/0.74644. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65334/0.74918. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65127/0.75032. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64941/0.75169. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64912/0.75742. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64410/0.75764. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64268/0.75784. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63638/0.76143. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63681/0.76424. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63526/0.76661. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63053/0.76843. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63067/0.76802. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62737/0.77539. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62702/0.78038. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62336/0.78359. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62191/0.78412. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61829/0.78603. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61789/0.79313. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61964/0.79378. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61184/0.79352. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61597/0.79331. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.60874/0.79832. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.60464/0.80446. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59991/0.79953. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60112/0.80879. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60330/0.81241. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59994/0.81514. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59411/0.82348. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59575/0.82831. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59676/0.82391. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.58912/0.82662. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59004/0.82705. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58497/0.83214. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.57925/0.84418. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58373/0.84085. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58391/0.84036. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57254/0.83969. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57677/0.83729. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57006/0.84563. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.56594/0.85045. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56268/0.85548. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56371/0.85936. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56040/0.86417. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.55740/0.86658. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56002/0.86700. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54960/0.86341. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.55133/0.86791. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55487/0.86459. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54457/0.87449. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54271/0.86937. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53869/0.86736. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53834/0.87953. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53742/0.88863. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53193/0.88886. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52956/0.88078. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52337/0.88655. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52683/0.89548. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51791/0.89422. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52263/0.90402. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51869/0.90107. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69356/0.68474. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69318/0.68499. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69337/0.68502. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69342/0.68491. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69295/0.68493. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69248/0.68498. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.68516. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69172/0.68532. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69083/0.68538. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69029/0.68552. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.68613. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68919/0.68693. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68837/0.68707. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.68723. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68461/0.68748. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68580/0.68790. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68394/0.68775. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68210/0.68850. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68103/0.68934. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67980/0.69105. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67949/0.68954. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67759/0.69086. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67701/0.69075. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67608/0.69269. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67314/0.69183. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67237/0.69272. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67198/0.69354. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66905/0.69313. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66786/0.69387. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66607/0.69333. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66643/0.69412. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66285/0.69421. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66439/0.69622. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66269/0.69431. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66076/0.69365. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65712/0.69329. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65465/0.69433. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65738/0.69459. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65501/0.69469. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65456/0.69543. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64723/0.69451. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64879/0.69408. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64413/0.69463. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64363/0.69471. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64861/0.69683. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63728/0.69631. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64057/0.69662. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63851/0.69616. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63612/0.69590. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63424/0.69855. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63537/0.70205. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63474/0.70181. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63023/0.70067. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62262/0.70141. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62416/0.70221. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62401/0.70342. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.62037/0.70492. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61862/0.70591. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61702/0.71087. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61532/0.70837. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61624/0.71034. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61235/0.71428. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60686/0.71788. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60355/0.71966. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60254/0.71991. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60284/0.72104. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60122/0.72319. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59950/0.72606. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59596/0.72703. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58846/0.72980. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58771/0.73369. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58677/0.73353. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58849/0.73988. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.58032/0.73859. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58248/0.74183. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.57942/0.74806. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58108/0.75289. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57637/0.75799. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.56981/0.75882. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56736/0.76148. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56034/0.76426. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56008/0.76517. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55999/0.77365. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55912/0.77468. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55739/0.78165. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55355/0.78435. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55449/0.78929. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54600/0.79037. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54782/0.79709. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54000/0.79875. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54466/0.80573. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54030/0.80523. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53095/0.81613. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53228/0.81278. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52883/0.81345. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52823/0.81320. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53017/0.82598. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53338/0.81939. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.52721/0.81439. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51531/0.82830. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69641/0.69458. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69426/0.69262. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69235/0.69089. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69118/0.68973. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69020/0.68938. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68983/0.68972. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68755/0.69019. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68745/0.69072. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68683/0.69155. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68640/0.69250. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68585/0.69352. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68321/0.69453. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68284/0.69604. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68009/0.69679. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.67989/0.69821. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67810/0.69863. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67901/0.69956. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67527/0.70060. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67466/0.70074. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67273/0.70187. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67103/0.70241. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66817/0.70389. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.66719/0.70548. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66549/0.70639. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66262/0.70806. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66034/0.70861. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65771/0.70972. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65803/0.70985. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65547/0.70999. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.65483/0.70995. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65142/0.71147. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65014/0.71200. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64760/0.71312. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64503/0.71440. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64336/0.71677. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64279/0.71598. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64155/0.71748. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.63752/0.71899. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63804/0.71823. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63517/0.72077. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63150/0.72193. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63099/0.72188. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63111/0.72279. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62770/0.72735. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62494/0.72843. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62643/0.72787. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62181/0.72824. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62273/0.72958. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61850/0.73137. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.62091/0.73200. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.61649/0.73386. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61130/0.73565. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61392/0.73736. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60868/0.73615. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60718/0.74166. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61110/0.74140. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60585/0.74252. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60168/0.74555. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60242/0.74366. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59994/0.74860. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59315/0.75076. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59541/0.75306. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59165/0.75368. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58571/0.75288. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58793/0.75922. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58576/0.76181. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58078/0.76191. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.57758/0.75905. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57677/0.76432. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57438/0.76572. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57173/0.76849. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.56826/0.77148. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56676/0.77639. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56314/0.77457. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56224/0.77241. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56207/0.77997. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56034/0.77848. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55637/0.77973. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55455/0.79431. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.55162/0.79030. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.54339/0.79359. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54934/0.79629. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54443/0.79664. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54154/0.80268. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53327/0.80566. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53587/0.81158. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53206/0.80291. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.53300/0.80659. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52471/0.80591. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52208/0.80907. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51705/0.81745. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52430/0.81979. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.50583/0.82303. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51602/0.83040. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50078/0.83102. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51151/0.82754. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49667/0.83452. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49991/0.83613. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.48921/0.84608. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.49102/0.83726. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69684/0.69503. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69534/0.69466. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69545/0.69428. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69585/0.69394. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69525/0.69361. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69550/0.69327. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69456/0.69282. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69453/0.69249. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69390/0.69200. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69315/0.69132. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69212/0.69057. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69059/0.68978. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68993/0.68903. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68897/0.68850. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68894/0.68798. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68806/0.68733. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68770/0.68650. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68655/0.68562. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68651/0.68474. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68358/0.68395. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 0.68327/0.68261. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68292/0.68160. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68146/0.68131. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68213/0.68092. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67977/0.67983. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68057/0.67967. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67988/0.67842. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67764/0.67858. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67668/0.67858. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67459/0.67886. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67241/0.67787. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67303/0.67860. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67200/0.67857. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67404/0.67811. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66923/0.67950. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66781/0.67937. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66636/0.67994. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66377/0.68106. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66291/0.68019. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66147/0.68142. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66039/0.68183. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66124/0.68207. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65713/0.68172. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65541/0.68367. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65247/0.68371. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65029/0.68335. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64864/0.68502. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64965/0.68605. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64487/0.68739. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64584/0.68887. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64301/0.68732. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64055/0.68892. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63694/0.69000. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63413/0.69172. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63382/0.69315. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63061/0.69223. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63033/0.69529. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63072/0.69233. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62618/0.69525. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62745/0.69896. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61854/0.69898. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61597/0.69757. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61271/0.69720. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61489/0.69636. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60936/0.69986. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60838/0.70293. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60388/0.70525. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60433/0.70457. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60283/0.70700. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59623/0.70720. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59394/0.70705. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59140/0.70665. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59491/0.70845. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58501/0.70940. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58561/0.70821. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58410/0.71116. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58387/0.71078. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57914/0.71525. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57663/0.71788. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56815/0.71728. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.57038/0.72140. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56752/0.72253. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56347/0.72508. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56158/0.72803. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55417/0.73117. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55039/0.72890. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54928/0.72726. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54685/0.73333. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54714/0.73711. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.54670/0.73795. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54011/0.73922. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53409/0.74358. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.53436/0.73912. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.53393/0.74186. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53609/0.74266. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52743/0.74850. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52012/0.75320. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51156/0.75393. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.51601/0.76162. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.51762/0.75793. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.68985/0.69321. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68970/0.69383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69009/0.69416. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68869/0.69437. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68913/0.69450. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68835/0.69475. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68781/0.69465. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68749/0.69475. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68688/0.69478. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68815/0.69465. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68690/0.69451. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68624/0.69446. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68566/0.69477. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68567/0.69495. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68452/0.69484. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68465/0.69469. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68398/0.69447. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68270/0.69449. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68326/0.69444. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68235/0.69497. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68202/0.69465. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68093/0.69403. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67915/0.69353. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68007/0.69346. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67859/0.69278. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67748/0.69209. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67850/0.69180. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67639/0.69247. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67501/0.69249. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67579/0.69299. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67165/0.69111. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67418/0.69256. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67125/0.69250. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67111/0.69341. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66934/0.69353. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66728/0.69254. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66875/0.69413. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66541/0.69306. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66442/0.69247. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66250/0.69308. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66210/0.69333. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65934/0.69547. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65652/0.69488. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65787/0.69476. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65497/0.69616. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65481/0.69599. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65193/0.69492. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64791/0.69649. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64937/0.70010. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64832/0.70038. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64508/0.70010. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64135/0.70039. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64124/0.70168. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63762/0.70360. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63831/0.70372. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63403/0.70213. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63291/0.70820. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63140/0.71045. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62935/0.71132. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62426/0.71344. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62398/0.71686. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.62097/0.71623. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61720/0.72233. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61536/0.72150. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61563/0.72680. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61389/0.72392. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60774/0.72773. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60396/0.72963. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60033/0.73461. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60072/0.74156. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59757/0.73843. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59593/0.74119. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58854/0.74195. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59482/0.74304. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58332/0.75130. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58640/0.75394. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58363/0.75960. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57968/0.76531. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57831/0.75884. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57391/0.76907. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57766/0.77124. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56644/0.77282. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56484/0.77775. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56338/0.78058. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55854/0.78376. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55832/0.78130. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55752/0.79111. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55579/0.79003. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54902/0.79549. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.54541/0.79781. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54180/0.80730. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.53798/0.80427. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54181/0.81154. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53569/0.80841. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52768/0.81479. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.52974/0.81197. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52700/0.81690. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52071/0.81656. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52119/0.81988. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51955/0.82369. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69306/0.70324. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68712/0.70273. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68678/0.70166. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68656/0.70222. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68605/0.70211. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68455/0.70250. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68448/0.70297. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68517/0.70306. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68379/0.70324. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68331/0.70321. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68217/0.70389. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68277/0.70429. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68213/0.70370. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68238/0.70397. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68188/0.70440. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67943/0.70440. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68016/0.70471. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67976/0.70442. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67993/0.70382. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67935/0.70291. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67833/0.70280. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67709/0.70232. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67715/0.70158. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67638/0.70087. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67516/0.70102. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67492/0.70149. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67460/0.70079. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67311/0.70049. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67242/0.69999. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66976/0.69801. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67038/0.69741. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66908/0.69749. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66853/0.69829. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66680/0.69645. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66406/0.69659. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66659/0.69567. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66301/0.69359. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66283/0.69365. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66041/0.69410. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66318/0.69357. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65844/0.69466. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65852/0.69344. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65707/0.69183. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65531/0.69167. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65501/0.69220. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65398/0.69173. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65252/0.69011. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65048/0.69060. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65015/0.69099. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64899/0.69352. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64806/0.69378. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64196/0.69548. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64541/0.69532. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64039/0.69562. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64265/0.69536. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63673/0.69599. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64057/0.69677. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63814/0.69695. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63719/0.69712. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63485/0.69836. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63326/0.70023. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63054/0.70311. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63259/0.70261. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62687/0.70258. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62985/0.70627. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62859/0.70783. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62879/0.70698. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63031/0.70786. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62080/0.71018. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62723/0.71023. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62627/0.71253. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61981/0.71424. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61407/0.71487. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61846/0.71767. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61666/0.71822. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61382/0.72024. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61171/0.72020. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61231/0.72210. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61290/0.72467. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61023/0.72617. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.61078/0.72622. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60115/0.72923. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60050/0.73044. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60239/0.73283. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60505/0.73112. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59610/0.73281. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59746/0.73173. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59286/0.73511. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59181/0.74025. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59533/0.73883. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58765/0.73677. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58682/0.74160. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58710/0.74375. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58638/0.74305. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58287/0.74369. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58256/0.74614. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58122/0.74781. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57781/0.74873. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.57574/0.75067. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57947/0.75227. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.68945/0.67882. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68802/0.67955. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68780/0.68034. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68735/0.68099. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68628/0.68150. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68596/0.68221. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68521/0.68323. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68572/0.68410. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68512/0.68476. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68462/0.68541. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68465/0.68643. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68416/0.68744. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68467/0.68819. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68338/0.68922. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68305/0.68985. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68133/0.69099. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68075/0.69156. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68056/0.69255. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67961/0.69326. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67994/0.69394. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67853/0.69437. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67794/0.69532. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67723/0.69519. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67721/0.69649. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67614/0.69671. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67512/0.69776. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67451/0.69816. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67481/0.69911. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67151/0.69972. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67398/0.69931. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67032/0.69847. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67171/0.69956. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67099/0.69985. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66971/0.70057. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66722/0.70053. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66615/0.70014. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66519/0.70020. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66316/0.70232. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66256/0.70384. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66153/0.70549. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65923/0.70544. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65823/0.70454. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65847/0.70393. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65683/0.70559. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65406/0.70533. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65336/0.70605. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65318/0.70623. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65136/0.70650. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65007/0.70469. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65018/0.70537. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64878/0.70642. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64499/0.70770. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64523/0.70624. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64201/0.70536. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64152/0.70512. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64241/0.70652. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63961/0.70627. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63855/0.70524. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63932/0.70472. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63519/0.70418. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63587/0.70552. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63435/0.70400. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63083/0.70418. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62904/0.70477. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62796/0.70544. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62665/0.70416. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62670/0.70582. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62591/0.70729. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61949/0.70742. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62093/0.70392. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61924/0.70614. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62067/0.70293. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61973/0.70284. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61482/0.70388. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61555/0.70752. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61272/0.70424. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61311/0.70264. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61029/0.70243. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60423/0.70058. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60791/0.70224. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60181/0.70747. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60182/0.70223. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59974/0.70291. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59933/0.70174. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59325/0.70518. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60055/0.70333. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59025/0.70175. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59031/0.70177. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59501/0.70108. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58771/0.70389. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58272/0.70724. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.58592/0.70784. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58093/0.70490. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58009/0.70322. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57425/0.70077. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.57295/0.70487. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57401/0.70678. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57323/0.70596. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57001/0.70465. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56286/0.70451. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69288/0.69103. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68999/0.69048. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68811/0.69062. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68728/0.69118. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68663/0.69157. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68656/0.69181. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68600/0.69195. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68545/0.69199. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68504/0.69197. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68503/0.69176. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68461/0.69123. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68440/0.69082. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68396/0.69052. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68428/0.69018. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68281/0.68976. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68183/0.68874. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68214/0.68817. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68138/0.68751. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68109/0.68714. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68031/0.68676. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68004/0.68616. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67925/0.68616. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67843/0.68575. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67808/0.68483. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67775/0.68486. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67758/0.68463. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67679/0.68388. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67557/0.68404. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67530/0.68355. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67593/0.68322. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67514/0.68277. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67374/0.68253. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67407/0.68292. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67295/0.68279. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67304/0.68237. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67249/0.68230. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67244/0.68159. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67052/0.68221. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67157/0.68157. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67010/0.68165. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66935/0.68105. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66972/0.68049. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66841/0.68037. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66783/0.67974. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66720/0.67943. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66707/0.67876. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66421/0.67951. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66486/0.68028. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66470/0.67934. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66400/0.67900. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66406/0.67854. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66188/0.67814. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66162/0.67959. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66113/0.67899. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65941/0.67996. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65901/0.67851. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65835/0.67833. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65868/0.67807. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65779/0.67829. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65642/0.67992. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65485/0.68038. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65263/0.68096. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65269/0.67999. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65388/0.67842. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65283/0.67976. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65106/0.67779. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64871/0.67814. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64892/0.67947. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64760/0.67909. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64762/0.67997. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64472/0.67941. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64305/0.67707. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64170/0.68120. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64390/0.67861. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63820/0.67973. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63573/0.68096. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63586/0.67880. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63783/0.67892. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63232/0.68176. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63227/0.68196. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62794/0.68127. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63052/0.68059. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62685/0.68365. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62911/0.68282. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62590/0.68408. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62491/0.68689. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.62143/0.68471. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62155/0.68387. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61912/0.68529. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61702/0.68501. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61365/0.68459. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61454/0.69005. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.61380/0.68789. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60854/0.69225. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61009/0.69041. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61055/0.68999. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60687/0.68857. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60297/0.69463. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59755/0.69570. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59704/0.69402. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69543/0.69349. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69370/0.69454. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69018/0.69636. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68880/0.69853. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68785/0.70043. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68796/0.70187. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68791/0.70277. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68738/0.70341. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68628/0.70386. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68647/0.70410. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68485/0.70442. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68563/0.70480. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68451/0.70514. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68395/0.70560. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68337/0.70572. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68256/0.70626. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68249/0.70689. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68178/0.70716. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68160/0.70735. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68041/0.70808. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67982/0.70856. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68042/0.70925. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67926/0.70916. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67867/0.70982. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67762/0.71022. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67785/0.71115. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67721/0.71111. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67590/0.71099. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67604/0.71198. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67529/0.71143. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67411/0.71233. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67473/0.71315. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67356/0.71277. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67144/0.71248. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67280/0.71410. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67153/0.71455. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66987/0.71627. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66726/0.71641. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66850/0.71812. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66707/0.71897. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66697/0.72000. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66677/0.72136. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66639/0.72201. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66583/0.72261. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66492/0.72350. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66418/0.72358. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66338/0.72515. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66461/0.72568. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66151/0.72581. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66062/0.72738. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66029/0.72906. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65814/0.72845. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65745/0.72943. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65755/0.73135. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65837/0.73053. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65614/0.73316. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65473/0.73422. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65566/0.73324. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65313/0.73325. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65463/0.73483. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65447/0.73530. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65420/0.73459. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65023/0.73529. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65037/0.73513. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64904/0.73507. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64892/0.73721. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64642/0.73666. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64657/0.73871. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64720/0.73933. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64476/0.73945. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64529/0.73855. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64191/0.74026. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64471/0.74106. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63970/0.74080. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63999/0.74217. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64181/0.74422. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63666/0.74255. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63452/0.74335. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63874/0.74338. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63530/0.74684. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63320/0.74518. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63373/0.74544. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62946/0.74443. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63120/0.74672. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62620/0.74804. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63222/0.74275. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62793/0.74717. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62744/0.74612. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.62265/0.74985. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62449/0.75297. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62347/0.75531. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62181/0.75332. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62483/0.75069. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61856/0.75030. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61669/0.75354. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61288/0.75434. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61509/0.75502. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.61692/0.75505. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61426/0.75555. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61224/0.75632. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69091/0.68834. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68839/0.68473. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68721/0.68244. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68571/0.68093. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68562/0.67974. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68460/0.67875. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68367/0.67817. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68388/0.67790. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68277/0.67763. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68262/0.67755. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68179/0.67738. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68211/0.67730. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68140/0.67752. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68104/0.67773. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68031/0.67782. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67969/0.67765. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68086/0.67767. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67878/0.67761. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67761/0.67747. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67712/0.67738. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67681/0.67727. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67569/0.67711. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.67594/0.67745. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67599/0.67729. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67448/0.67721. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67314/0.67714. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67304/0.67733. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67281/0.67736. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67175/0.67703. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67054/0.67801. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66934/0.67712. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66866/0.67780. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66754/0.67728. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66707/0.67837. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66534/0.67846. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66815/0.67931. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66473/0.67985. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66401/0.68026. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66109/0.68037. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66140/0.68099. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66154/0.68199. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66146/0.68237. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66057/0.68263. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66010/0.68376. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65738/0.68397. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65698/0.68453. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65472/0.68518. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65435/0.68548. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65477/0.68688. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65327/0.68950. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65252/0.68862. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65254/0.69064. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65022/0.69112. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65230/0.69444. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65143/0.69501. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64907/0.69649. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64683/0.69656. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64501/0.69727. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64303/0.69841. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64462/0.69815. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64057/0.69962. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64052/0.70181. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64188/0.70345. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63716/0.70449. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63560/0.70580. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63428/0.70901. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63681/0.70854. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63417/0.70998. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63401/0.71314. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63118/0.71408. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62867/0.71366. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62939/0.71760. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62767/0.71926. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62647/0.71801. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62620/0.72285. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62264/0.72060. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62247/0.72271. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62055/0.72859. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62033/0.72814. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61930/0.72922. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61593/0.72877. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61333/0.72878. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61050/0.73440. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60806/0.74298. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60614/0.73456. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60795/0.73832. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60668/0.74745. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60506/0.74579. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.60376/0.75062. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59816/0.75296. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60164/0.75728. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59802/0.75777. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59631/0.75670. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59593/0.76093. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59060/0.76485. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.59018/0.76750. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59572/0.77122. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58676/0.77171. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58126/0.77641. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58358/0.77903. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69376/0.68972. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69251/0.69031. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69180/0.69073. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69120/0.69098. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69049/0.69115. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68937/0.69136. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68929/0.69180. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68833/0.69228. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.69285. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68706/0.69322. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68714/0.69360. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68607/0.69409. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68502/0.69444. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68517/0.69483. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68410/0.69537. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68438/0.69598. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68446/0.69647. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68355/0.69695. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68252/0.69747. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68176/0.69832. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68179/0.69912. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68176/0.69953. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68148/0.70022. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68125/0.70068. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67998/0.70058. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67961/0.70143. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67932/0.70171. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67794/0.70205. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67820/0.70210. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67854/0.70236. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67681/0.70294. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67643/0.70355. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67541/0.70406. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67432/0.70497. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67409/0.70476. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67299/0.70479. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67287/0.70491. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67064/0.70526. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66976/0.70649. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66868/0.70731. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66848/0.70875. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66843/0.70980. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66620/0.71048. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66479/0.71124. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66471/0.71289. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66388/0.71305. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66153/0.71369. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65942/0.71470. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65847/0.71611. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65927/0.71658. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65676/0.71836. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65454/0.72054. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65246/0.72117. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65174/0.72342. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65174/0.72423. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64863/0.72738. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64462/0.72848. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64910/0.72919. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64542/0.73182. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64268/0.73476. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64290/0.73285. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64015/0.73560. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63948/0.73707. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63409/0.73821. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63641/0.74275. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63468/0.74205. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63191/0.74446. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63311/0.74903. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62962/0.74978. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62835/0.74926. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62732/0.75440. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62187/0.75548. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62077/0.75130. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62095/0.75482. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61942/0.75738. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61447/0.76035. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61253/0.76158. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61401/0.75703. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60971/0.76120. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61032/0.76314. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60912/0.76542. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60640/0.76644. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60631/0.76429. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60495/0.76691. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59478/0.77560. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59543/0.76765. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59775/0.76762. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59967/0.77110. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58743/0.78013. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59032/0.77941. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58516/0.77182. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58778/0.78278. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58457/0.77438. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58207/0.78123. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58065/0.78093. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58283/0.78746. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57244/0.78908. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.57577/0.78555. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57242/0.78630. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56728/0.79350. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69388/0.69137. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69410/0.69111. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69347/0.69093. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69082. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69239/0.69089. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69200/0.69116. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.69166. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69004/0.69208. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68923/0.69209. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68875/0.69207. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68770/0.69138. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68798/0.69086. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68639/0.69053. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68506/0.68932. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68334/0.68892. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68271/0.68851. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68144/0.68815. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68024/0.68687. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67883/0.68785. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67665/0.68762. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67564/0.68628. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67357/0.68715. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67388/0.68805. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66994/0.68540. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66768/0.68665. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66899/0.68533. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66329/0.68477. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66254/0.68545. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66352/0.68307. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65909/0.68561. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65640/0.68489. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65628/0.68258. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.65296/0.68338. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64734/0.68381. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64914/0.68269. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64515/0.68311. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64266/0.68330. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64209/0.68490. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63723/0.68420. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63596/0.68207. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63404/0.68365. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63325/0.68443. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.62609/0.68622. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62548/0.68516. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62318/0.68439. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62388/0.68416. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.61753/0.68488. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61401/0.68469. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61591/0.68510. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61228/0.68755. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.60650/0.68728. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60624/0.68491. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.60446/0.69535. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.59827/0.69332. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.59872/0.69006. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59373/0.69546. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.58907/0.69600. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.58544/0.69733. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.58277/0.69871. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58421/0.70532. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58527/0.70206. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58011/0.70943. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.57567/0.70591. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.57231/0.70691. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.56554/0.70583. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56249/0.71725. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.56573/0.71466. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.55707/0.71886. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.55256/0.72577. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55326/0.72649. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.54403/0.72451. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.54337/0.72564. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.53655/0.73370. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.53298/0.73096. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.53476/0.73708. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.53498/0.74258. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.53220/0.73816. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53534/0.73802. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52170/0.74450. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.52345/0.74734. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.51640/0.74882. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.51998/0.75256. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.50943/0.75524. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51250/0.75433. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51096/0.75384. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.50654/0.75956. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.49663/0.76410. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.49812/0.76267. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.49853/0.76430. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.49290/0.76723. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.49816/0.77163. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.48766/0.77448. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.47981/0.78000. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.48410/0.78081. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.47807/0.78772. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48509/0.79122. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.47542/0.79428. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.46403/0.80263. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.46817/0.80427. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.46845/0.80618. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69356/0.69247. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69341/0.69234. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69208/0.69230. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69218/0.69219. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69120/0.69195. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69089/0.69180. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69065/0.69186. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69125/0.69151. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69010/0.69152. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68998/0.69139. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68953/0.69114. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68974/0.69095. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68879/0.69091. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.69089. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68782/0.69090. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68644/0.69086. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68748/0.69079. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68601/0.69063. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68649/0.69077. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68414/0.69061. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68476/0.69112. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68583/0.69135. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68316/0.69172. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68300/0.69192. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68343/0.69184. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68288/0.69188. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68142/0.69226. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68155/0.69289. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67851/0.69359. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68005/0.69412. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68032/0.69416. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67857/0.69455. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67769/0.69505. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67782/0.69608. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67648/0.69628. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67604/0.69701. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67574/0.69698. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67669/0.69696. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67533/0.69753. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67391/0.69775. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67175/0.69885. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67195/0.69984. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66939/0.70059. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67223/0.70002. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67188/0.69997. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67007/0.70009. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66827/0.70069. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66560/0.70210. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66637/0.70272. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66306/0.70366. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66606/0.70312. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66113/0.70407. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66065/0.70380. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66114/0.70542. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65875/0.70506. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65723/0.70541. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65629/0.70485. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65681/0.70427. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65051/0.70360. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65429/0.70233. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65071/0.70280. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64898/0.70374. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64918/0.70237. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64585/0.70372. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64143/0.70597. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64243/0.70510. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63838/0.70688. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63862/0.70552. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63723/0.70845. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63777/0.70811. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63355/0.70811. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62899/0.70908. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62746/0.70832. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62731/0.70861. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62368/0.70616. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62173/0.70834. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61833/0.70610. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61865/0.70870. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61593/0.71198. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61363/0.71122. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61418/0.70961. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60727/0.71208. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60836/0.71051. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60817/0.71397. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60100/0.71007. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60435/0.71358. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59638/0.71681. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59542/0.71850. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59668/0.71708. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59209/0.72011. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58497/0.71901. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58635/0.72244. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58539/0.72468. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58217/0.72456. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.57684/0.72696. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58285/0.72613. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57531/0.72779. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57666/0.73049. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57112/0.73055. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56407/0.73090. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69226/0.69296. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69225/0.69266. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69242. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69153/0.69171. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.69134. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69190/0.69077. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.69046. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69059/0.68987. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69099/0.68947. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69000/0.68861. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68950/0.68756. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68944/0.68697. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68931/0.68612. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68790/0.68528. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.68434. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68525/0.68359. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68688/0.68221. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68516/0.68188. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68316/0.68192. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68317/0.68020. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68242/0.68081. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68197/0.68026. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67984/0.68017. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67995/0.67848. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67815/0.67904. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67815/0.67793. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67721/0.67800. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67598/0.67884. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67518/0.67915. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67401/0.67838. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67347/0.67817. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67124/0.67932. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67002/0.67836. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66684/0.67857. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66871/0.67962. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66568/0.67782. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66310/0.67855. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66134/0.67850. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66220/0.67729. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66047/0.67858. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65894/0.67819. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65556/0.67962. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65492/0.67856. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65370/0.67939. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65149/0.67894. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64904/0.67959. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64742/0.67923. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64697/0.67962. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64537/0.67947. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64324/0.68098. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63908/0.68066. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63798/0.68187. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63249/0.68326. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63525/0.68275. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63110/0.68206. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62987/0.68212. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63394/0.68309. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62915/0.68455. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62601/0.68313. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62581/0.68312. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62263/0.68281. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62254/0.68360. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.61918/0.68447. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62081/0.68393. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61630/0.68529. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61412/0.68421. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61461/0.68490. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60913/0.68866. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61105/0.68670. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60467/0.68939. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60097/0.68807. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.60441/0.68810. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59873/0.68928. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59452/0.68953. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59837/0.68815. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59561/0.68780. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58968/0.68839. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59508/0.69113. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58557/0.69075. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58769/0.69115. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.58255/0.69443. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58448/0.69310. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58023/0.69512. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58237/0.69503. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57730/0.69753. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57018/0.69893. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56345/0.69847. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56821/0.69851. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56137/0.70133. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55810/0.69576. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55782/0.70303. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54786/0.69913. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55750/0.70542. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55415/0.70316. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54261/0.70518. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55222/0.70068. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54664/0.70824. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54348/0.70634. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54567/0.70677. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53700/0.70888. Took 0.08 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69448/0.69548. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69289/0.69530. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69302/0.69507. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69227/0.69498. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69488. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69162/0.69492. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69149/0.69506. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69103/0.69511. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69077/0.69521. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69046/0.69554. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68901/0.69569. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68971/0.69595. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68855/0.69631. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68839/0.69670. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68776/0.69703. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68693/0.69754. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68507/0.69813. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68503/0.69848. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68456/0.69851. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68363/0.69842. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68118/0.69856. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68067/0.69939. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68051/0.69974. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68051/0.69934. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68061/0.69979. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67744/0.69991. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67920/0.70042. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67796/0.70028. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67670/0.70023. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67541/0.70132. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67338/0.70200. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67417/0.70261. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67285/0.70272. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67203/0.70259. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66821/0.70419. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66944/0.70529. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66780/0.70627. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66735/0.70867. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66553/0.70904. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66497/0.70953. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66405/0.71051. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66478/0.71212. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66468/0.71116. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66122/0.71280. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66145/0.71447. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66130/0.71282. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65580/0.71341. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65774/0.71478. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65555/0.71580. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65220/0.71841. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65232/0.71761. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64827/0.71849. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65052/0.72051. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64955/0.71956. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64930/0.72168. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64693/0.72175. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64692/0.72123. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64506/0.72397. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64101/0.72335. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63979/0.72629. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63795/0.72981. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63978/0.72824. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63837/0.72995. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63849/0.72959. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63534/0.72968. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63553/0.73019. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63296/0.73169. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63099/0.73267. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63181/0.73423. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62621/0.73458. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62811/0.73597. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62791/0.73616. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62505/0.73819. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62775/0.73886. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62085/0.73984. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62129/0.74119. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62025/0.74337. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61952/0.74113. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61029/0.74474. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61351/0.74232. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61146/0.74423. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60838/0.74542. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60782/0.74787. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60795/0.74967. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60226/0.75070. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60947/0.75220. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59917/0.75203. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59865/0.75391. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59586/0.75385. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59798/0.75391. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59218/0.75580. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59601/0.75627. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59172/0.75815. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58660/0.75706. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58727/0.75879. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58819/0.76146. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58162/0.76168. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.57957/0.76195. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57287/0.76243. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58104/0.76581. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69275/0.69334. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69241/0.69303. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69228/0.69309. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69334. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69152/0.69342. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69106/0.69366. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69410. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69121/0.69427. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69046/0.69424. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68938/0.69503. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69063/0.69583. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.69614. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68920/0.69624. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68865/0.69665. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68755/0.69789. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68721/0.69845. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68565/0.69937. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68487/0.69977. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68377/0.70081. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68268/0.70072. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68012/0.70053. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67880/0.70178. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67790/0.70168. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67493/0.70221. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67551/0.70241. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67424/0.70098. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67124/0.70210. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66902/0.70285. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66854/0.70532. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66463/0.70464. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66533/0.70344. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66327/0.70374. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65791/0.70748. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65687/0.70688. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65845/0.70685. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65582/0.70785. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65178/0.71008. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65202/0.71148. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64944/0.71082. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64563/0.71040. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64676/0.71183. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64553/0.71328. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64204/0.71579. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64313/0.71536. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64113/0.71357. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63750/0.71490. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63982/0.71507. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63380/0.71540. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63080/0.71671. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63294/0.71755. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63083/0.71814. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62763/0.71961. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62765/0.72031. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.62289/0.71935. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62027/0.72000. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62390/0.71605. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62154/0.71322. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61731/0.71346. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61534/0.71670. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61044/0.71545. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61122/0.71560. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60458/0.72103. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60330/0.71957. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60548/0.72220. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60446/0.72312. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60245/0.72513. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59918/0.72697. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59832/0.73085. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59485/0.73161. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59377/0.73100. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58991/0.72755. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58455/0.73083. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57932/0.73986. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57845/0.74412. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57603/0.74007. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57204/0.74105. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56940/0.74047. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.57145/0.74656. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56172/0.75770. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56464/0.75300. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.56561/0.75834. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55517/0.75886. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56179/0.76284. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54831/0.76469. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54884/0.77155. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54624/0.77314. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55060/0.78650. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54122/0.79332. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53627/0.79108. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53295/0.80325. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53480/0.80311. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52816/0.79591. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52479/0.80773. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52571/0.82344. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51836/0.82594. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.51443/0.83019. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51515/0.83031. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51322/0.83242. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.50269/0.84407. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50227/0.84951. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69303/0.70268. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69271/0.70186. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69225/0.70134. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.70075. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69139/0.70012. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.69959. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.69935. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68996/0.69892. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69071/0.69832. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68926/0.69757. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68920/0.69674. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.69619. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68823/0.69571. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68823/0.69480. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68786/0.69377. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68729/0.69302. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.69259. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68448/0.69174. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.69225. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68374/0.69145. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68419/0.69109. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68226/0.69031. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68164/0.68943. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68007/0.69051. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67848/0.68910. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67808/0.68866. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67921/0.68814. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67722/0.68832. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67593/0.68619. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67597/0.68733. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67768/0.68671. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67630/0.68708. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67482/0.68666. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67209/0.68757. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67339/0.68812. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67290/0.68708. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67164/0.68732. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67210/0.68834. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67132/0.68755. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67025/0.68973. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66866/0.68944. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66402/0.68936. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66704/0.68926. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66954/0.69033. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66354/0.68944. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66653/0.69137. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66559/0.69141. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66287/0.69244. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66178/0.69124. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66024/0.69070. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66195/0.69322. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66161/0.69280. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65806/0.69499. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65641/0.69481. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65245/0.69650. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65366/0.69791. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65763/0.69716. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65340/0.69944. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64903/0.69795. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65378/0.69810. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64831/0.69862. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64521/0.70047. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64667/0.70043. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64620/0.70182. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64273/0.70353. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64169/0.70430. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63869/0.70370. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63732/0.70205. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63744/0.70760. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63691/0.70721. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63643/0.70722. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63545/0.70583. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62685/0.70651. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63031/0.71018. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62872/0.71196. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62886/0.71198. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63000/0.71274. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62352/0.71343. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61700/0.71612. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61580/0.72004. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62543/0.71750. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61048/0.71929. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60929/0.71860. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61019/0.72169. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60817/0.72084. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60641/0.72199. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60299/0.72300. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60240/0.72607. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60179/0.72634. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60211/0.72764. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59553/0.72835. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.59534/0.73085. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59267/0.72850. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59235/0.73151. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58734/0.73635. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58664/0.74263. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58556/0.73844. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.57854/0.73818. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57925/0.74020. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57741/0.73786. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69492/0.69357. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69205/0.69503. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69195/0.69455. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69075/0.69462. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69025/0.69501. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.69500. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68930/0.69545. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.69578. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68808/0.69627. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68744/0.69724. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68659/0.69811. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68571/0.69852. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68584/0.69867. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68554/0.69947. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68500/0.70007. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68303/0.70021. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68272/0.70103. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68118/0.70209. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67993/0.70210. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67925/0.70330. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67791/0.70354. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67702/0.70414. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67466/0.70468. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67352/0.70544. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67204/0.70598. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66803/0.70774. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66590/0.70882. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66499/0.70943. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66301/0.70948. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66006/0.71042. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65757/0.71125. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65477/0.71205. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65595/0.71221. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65020/0.71299. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64792/0.71372. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64736/0.71582. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64384/0.71676. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64446/0.71679. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.64300/0.71801. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64199/0.71910. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64050/0.71934. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63816/0.72088. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.63600/0.72272. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63511/0.72519. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63182/0.72502. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63108/0.72917. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62783/0.72900. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.62753/0.73203. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62340/0.73301. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62296/0.73422. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62263/0.73786. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61829/0.74063. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61866/0.74049. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61303/0.74360. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61428/0.74703. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61125/0.74925. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.61103/0.75138. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.60666/0.75562. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60657/0.75942. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60133/0.75846. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59864/0.76395. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59697/0.76990. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59613/0.77027. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59472/0.77191. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59160/0.77581. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.59035/0.77998. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58892/0.78109. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58855/0.78956. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58475/0.78990. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58145/0.78980. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57942/0.79764. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57403/0.79529. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57388/0.80811. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57104/0.80714. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56782/0.80982. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.56050/0.81720. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56445/0.82140. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56192/0.82498. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56134/0.83295. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55350/0.83591. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55288/0.84119. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54986/0.84213. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54697/0.85065. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54916/0.85638. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53889/0.85818. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53823/0.86567. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53288/0.86452. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54085/0.86160. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53349/0.87828. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.52738/0.88305. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.52513/0.88654. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52277/0.88976. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52668/0.89896. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53331/0.89619. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51693/0.89528. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52275/0.90681. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50742/0.91615. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50712/0.91364. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49796/0.91809. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50628/0.92948. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69653/0.69244. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69529/0.69224. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69480/0.69215. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69289/0.69225. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69294/0.69232. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69173/0.69245. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69126/0.69259. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69038/0.69256. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69113/0.69270. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68988/0.69271. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68945/0.69234. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68804/0.69218. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68717/0.69222. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68551/0.69199. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68363/0.69135. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68612/0.69122. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68233/0.69009. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68329/0.69022. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68144/0.69076. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68134/0.68978. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67988/0.68925. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67835/0.68828. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67769/0.68871. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67621/0.68862. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67238/0.68825. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67270/0.68774. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67112/0.68887. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67126/0.68709. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66986/0.68679. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66804/0.68634. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66829/0.68652. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66861/0.68648. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66652/0.68562. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66450/0.68293. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66178/0.68504. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66242/0.68344. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66102/0.68216. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66023/0.68296. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65688/0.68292. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65139/0.68135. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65038/0.68303. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64749/0.68202. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64995/0.68175. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64541/0.68295. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64283/0.68512. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64098/0.68504. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.63713/0.68516. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63541/0.68262. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63256/0.68415. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63037/0.68531. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62528/0.68902. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62302/0.69159. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62378/0.69571. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61891/0.69315. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61797/0.69204. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.61331/0.69411. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61142/0.70039. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60655/0.69923. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.60962/0.70287. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60399/0.70291. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60450/0.70947. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59979/0.70652. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59181/0.71597. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58551/0.71446. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58858/0.72901. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58766/0.72689. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57853/0.73876. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58022/0.73506. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.57717/0.73945. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58117/0.74455. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57805/0.74636. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.56406/0.73803. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56432/0.73992. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.56398/0.75727. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56216/0.76035. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55648/0.76196. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55085/0.76197. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55516/0.77133. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55528/0.77202. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54639/0.77651. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.54805/0.78933. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54381/0.78518. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.53839/0.80495. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.54155/0.80525. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54054/0.80144. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53458/0.79819. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53414/0.79274. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52605/0.80530. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.52386/0.82995. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52207/0.82216. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51522/0.82216. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51160/0.83302. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.51817/0.84995. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51584/0.85086. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50715/0.85909. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.50383/0.85191. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50659/0.85507. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50680/0.85770. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.49677/0.86451. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.49865/0.86817. Took 0.09 sec\n",
      "ACC: 0.3854166666666667\n",
      "Epoch 0, Loss(train/val) 0.69277/0.69622. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.69571. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69270/0.69520. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.69461. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.69391. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69014/0.69338. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.69259. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68808/0.69184. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68742/0.69057. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68670/0.69011. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68508/0.68981. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68333/0.69073. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68229/0.68937. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68027/0.68829. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67993/0.68888. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67723/0.68791. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67768/0.68671. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67723/0.68785. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67514/0.68778. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67304/0.68857. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67190/0.68888. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67332/0.68734. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67055/0.68891. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66993/0.68783. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66738/0.68637. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66587/0.68820. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66604/0.68853. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66538/0.68636. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66094/0.68545. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66192/0.68491. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65859/0.68232. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65447/0.68258. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65631/0.68080. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65449/0.68048. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.64975/0.68013. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65160/0.67909. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64725/0.68034. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64494/0.67869. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64624/0.67301. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63991/0.67434. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63826/0.67419. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63457/0.67461. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63676/0.66986. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63399/0.67246. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.62909/0.66869. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62665/0.66813. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62373/0.66533. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62373/0.66745. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61967/0.66780. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61495/0.66756. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61690/0.66889. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61261/0.66705. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.60382/0.67041. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60284/0.67052. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60214/0.67051. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.59673/0.66999. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.59735/0.67126. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59235/0.67028. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.58965/0.67178. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.58740/0.67185. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58485/0.67549. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58266/0.67571. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57815/0.68045. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.57739/0.68040. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.57298/0.68621. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56745/0.68507. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.57164/0.68894. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56400/0.68887. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.56150/0.68544. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55416/0.69504. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.55739/0.69871. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55341/0.70031. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.55384/0.70471. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.54577/0.69933. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.53372/0.71791. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54249/0.71276. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.53586/0.71819. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.53562/0.71606. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.52731/0.72340. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.53015/0.71933. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.52324/0.72222. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.51577/0.72756. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.52290/0.71543. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.51596/0.73459. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.51971/0.72776. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.50875/0.73417. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51115/0.74336. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.50571/0.73677. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.50585/0.74436. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.49115/0.75557. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.48923/0.75079. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.48963/0.75631. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.48826/0.75963. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.49060/0.75896. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.48215/0.75743. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48290/0.75429. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.48307/0.76068. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.46744/0.77526. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.48388/0.77020. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.47065/0.77551. Took 0.10 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69321/0.69481. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69278/0.69494. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69225/0.69486. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69133/0.69472. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69045/0.69439. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69061/0.69413. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68965/0.69379. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68899/0.69302. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.69296. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.68701/0.69205. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68545/0.69157. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68393/0.69067. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68218/0.69035. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68228/0.68985. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67998/0.68857. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.67898/0.68916. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67731/0.68849. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67613/0.68739. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67575/0.68556. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67356/0.68630. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67141/0.68638. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66939/0.68351. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66982/0.68262. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66802/0.68267. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.66390/0.67994. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66252/0.67852. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66069/0.67867. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65778/0.67662. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.65487/0.67394. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65643/0.67364. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.65396/0.67453. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.65288/0.67200. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64787/0.66939. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.64742/0.66877. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64753/0.66819. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64504/0.66970. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64233/0.66637. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.64079/0.66682. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64406/0.66599. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63798/0.66780. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63651/0.66426. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63454/0.66369. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.63406/0.66524. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.63145/0.66255. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62836/0.66108. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.62611/0.66326. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62954/0.66052. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.62084/0.66069. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.62135/0.66079. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62312/0.66283. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62002/0.66070. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61557/0.65807. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61428/0.65908. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61211/0.65937. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.61576/0.65736. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.61221/0.65623. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60553/0.65755. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.60852/0.65956. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60596/0.65876. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60398/0.65647. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.60131/0.65627. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59679/0.65581. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59810/0.65605. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59612/0.65202. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59557/0.65408. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58821/0.65888. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58383/0.65520. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58561/0.65281. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58495/0.65368. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.58326/0.65023. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.57953/0.65079. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57971/0.65200. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57501/0.65270. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.57564/0.65312. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.57035/0.65471. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57084/0.65481. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56518/0.65105. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.56587/0.64805. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.56466/0.64994. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55952/0.65339. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55708/0.65379. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55801/0.65353. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55243/0.64971. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55600/0.65172. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.54768/0.65228. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54780/0.65529. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.54858/0.65558. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54940/0.65540. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54068/0.65540. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53393/0.65629. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53811/0.65313. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53271/0.65832. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.52760/0.65715. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52529/0.66358. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52586/0.66519. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51948/0.66117. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52132/0.66225. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.51779/0.65926. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51929/0.66160. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51566/0.66563. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69099. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69346/0.69085. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69388/0.69077. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69064. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69283/0.69054. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69213/0.69038. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69144/0.69016. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69172/0.69000. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69104/0.68987. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69093/0.68973. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69013/0.68957. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69003/0.68941. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68857/0.68928. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68930/0.68904. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68707/0.68899. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68619/0.68890. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68690/0.68896. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68547/0.68914. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68393/0.68945. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68166/0.69004. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68109/0.69103. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67796/0.69151. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67812/0.69303. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67668/0.69439. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67578/0.69507. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67538/0.69589. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67175/0.69708. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66947/0.69835. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66631/0.69930. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66607/0.70033. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66554/0.70062. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66041/0.70073. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.65865/0.70293. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65266/0.70410. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64967/0.70470. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.65059/0.70538. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64670/0.70616. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64512/0.70730. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.64031/0.70803. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63998/0.70846. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63312/0.70873. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63700/0.70758. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63464/0.70776. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62342/0.70960. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62847/0.70903. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62214/0.70988. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62273/0.70813. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62170/0.70727. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61759/0.70770. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.61321/0.70848. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61273/0.70856. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61094/0.71133. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61118/0.70996. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60619/0.71094. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60451/0.70923. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60307/0.71144. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.59652/0.71355. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59699/0.71421. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59654/0.71599. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.58832/0.71573. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.58719/0.71671. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.58634/0.71901. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.57970/0.72323. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58301/0.72401. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.57881/0.72319. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57552/0.72608. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57301/0.72682. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.56825/0.73020. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.56700/0.73375. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.55957/0.73595. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.56616/0.73421. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.55908/0.73824. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56326/0.74416. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.55074/0.74690. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.55290/0.75077. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54311/0.75593. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55331/0.76047. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.55285/0.75840. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54938/0.75982. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.53929/0.76384. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.54679/0.76318. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.53010/0.77221. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.53032/0.77218. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.53204/0.78330. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53154/0.79013. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.51928/0.78622. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52419/0.79046. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52643/0.79122. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.52112/0.79162. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51967/0.78999. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51977/0.79711. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51478/0.80616. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.51177/0.81387. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50534/0.81528. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.49836/0.81097. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.49942/0.82884. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49882/0.82785. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.49402/0.83978. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.49122/0.83744. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.49243/0.84695. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69292/0.69391. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69275/0.69397. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69221/0.69388. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69182/0.69380. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69248/0.69404. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69412. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69112/0.69444. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69154/0.69502. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69092/0.69536. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69025/0.69597. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69015/0.69654. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68998/0.69718. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68877/0.69812. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68760/0.69878. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68762/0.69931. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68861/0.70041. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68761/0.70135. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68653/0.70147. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68651/0.70176. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68596/0.70224. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68429/0.70272. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68440/0.70346. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68449/0.70455. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68367/0.70415. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68557/0.70437. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68197/0.70447. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68181/0.70570. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68173/0.70602. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68195/0.70637. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67934/0.70745. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67997/0.70697. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68017/0.70708. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67728/0.70892. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67701/0.70808. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67686/0.70850. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67593/0.70881. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67390/0.71041. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67133/0.70873. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67042/0.71016. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66874/0.71057. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66665/0.71193. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66247/0.71378. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66468/0.71338. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65950/0.71564. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66240/0.71427. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65659/0.71459. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65353/0.71550. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65077/0.71869. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65111/0.72412. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65040/0.72376. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64716/0.72425. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63972/0.72695. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63895/0.72985. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63575/0.72784. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63179/0.73034. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62870/0.73254. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62966/0.73575. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62316/0.73483. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62437/0.73734. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61697/0.74073. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62175/0.74225. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61644/0.74700. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61675/0.75402. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60900/0.74946. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60383/0.75562. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60634/0.75842. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60418/0.76064. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60101/0.75847. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59645/0.75678. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59773/0.75737. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59169/0.75891. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59150/0.76375. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58765/0.76681. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58761/0.77041. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58920/0.77085. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58449/0.77084. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58436/0.77623. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58383/0.76828. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57800/0.76822. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56842/0.78203. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57945/0.78076. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57171/0.78413. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57340/0.77961. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.56346/0.78441. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56679/0.78107. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56435/0.78974. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56591/0.78258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55902/0.78695. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55638/0.79218. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55403/0.79406. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55892/0.79782. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55272/0.79692. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54620/0.78463. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54444/0.80144. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54478/0.79745. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54392/0.79779. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54317/0.80206. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54901/0.79516. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54326/0.80102. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53939/0.80190. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69395/0.69246. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69349/0.69213. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69189. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69188/0.69173. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69153. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69123/0.69135. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69117/0.69104. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69107/0.69070. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69078/0.69038. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68985/0.69010. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68924/0.68962. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68872/0.68917. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68917/0.68868. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68822/0.68818. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68882/0.68772. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68712/0.68733. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68709/0.68682. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68396/0.68639. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68552/0.68597. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68382/0.68556. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68323/0.68516. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68199/0.68447. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68144/0.68416. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67974/0.68334. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67929/0.68397. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67863/0.68347. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67632/0.68318. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67606/0.68252. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67547/0.68208. Took 0.11 sec\n",
      "Epoch 29, Loss(train/val) 0.67384/0.68141. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67318/0.68137. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67026/0.68125. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66854/0.68031. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66899/0.68045. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66650/0.68023. Took 0.11 sec\n",
      "Epoch 35, Loss(train/val) 0.66581/0.68035. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66531/0.68042. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66384/0.68060. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66279/0.67936. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66083/0.67945. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65927/0.68040. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65768/0.68091. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65545/0.68159. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65322/0.68294. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65389/0.68300. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65312/0.68377. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65126/0.68496. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65123/0.68654. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64627/0.68545. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64613/0.68856. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64249/0.68911. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64893/0.68962. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64297/0.69120. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64269/0.69119. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64265/0.69287. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63742/0.69477. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63338/0.69625. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63431/0.69690. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63440/0.69838. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63133/0.70096. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62870/0.70110. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62648/0.70387. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62471/0.70634. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62688/0.70751. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62277/0.70798. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62114/0.71304. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62064/0.71279. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61563/0.71554. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61665/0.71723. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61349/0.71874. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61049/0.72010. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60795/0.72017. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60276/0.72287. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60537/0.72505. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60025/0.72807. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60066/0.73141. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59952/0.73419. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59348/0.73432. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59340/0.73432. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58925/0.73959. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59243/0.74160. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58554/0.74407. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58507/0.74997. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.58347/0.75237. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58646/0.75509. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57866/0.75617. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57492/0.75658. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57612/0.76079. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57236/0.76545. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.56270/0.77058. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57115/0.76920. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56029/0.77557. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55677/0.77983. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55479/0.78546. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54902/0.78965. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54835/0.79234. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55286/0.80030. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55359/0.79807. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53858/0.80314. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54592/0.80844. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69400/0.67407. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69184/0.67563. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69013/0.67590. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68971/0.67676. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68824/0.67659. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68755/0.67648. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68628/0.67759. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68598/0.67911. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68515/0.67869. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68438/0.68067. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68408/0.68118. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68346/0.68280. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68256/0.68343. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68307/0.68448. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68100/0.68399. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68028/0.68580. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68009/0.68614. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67947/0.68716. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67915/0.68793. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67927/0.68822. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67799/0.68912. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67704/0.68859. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67726/0.69037. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67695/0.69130. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67682/0.69060. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67531/0.68852. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67524/0.69336. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67449/0.69106. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67320/0.69015. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67204/0.69057. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67060/0.69304. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67240/0.69209. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67103/0.68947. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67122/0.68993. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66874/0.69217. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66728/0.69356. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66752/0.68909. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66673/0.68916. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66735/0.68881. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66714/0.68818. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66280/0.69032. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66229/0.68913. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66179/0.68560. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66131/0.68457. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66048/0.68275. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65959/0.68296. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66085/0.68007. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65687/0.68038. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65581/0.67774. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65521/0.67503. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65489/0.67391. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65386/0.67581. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65235/0.67242. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65138/0.67065. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64910/0.67052. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64881/0.66799. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64840/0.66394. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64546/0.66451. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64380/0.66373. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64509/0.65993. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64035/0.66330. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64166/0.66013. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64168/0.65797. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63873/0.65425. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63858/0.65604. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63793/0.65757. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63257/0.65317. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63282/0.65610. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63395/0.65405. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63177/0.65498. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63049/0.64983. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63016/0.64894. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62961/0.65124. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62564/0.65212. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62401/0.65429. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62402/0.65032. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62061/0.65398. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62286/0.65229. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62168/0.65194. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61846/0.65064. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61617/0.65317. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61701/0.65396. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61443/0.65596. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61847/0.65169. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61409/0.65235. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61310/0.65576. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60986/0.64967. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61010/0.65462. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60510/0.65650. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60881/0.64692. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60321/0.65398. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.60319/0.65759. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60075/0.65137. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59983/0.65571. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60021/0.65844. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60470/0.65534. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59996/0.65298. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59469/0.65080. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59358/0.65712. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59107/0.65658. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69224/0.70464. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69051/0.69997. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68942/0.69843. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68840/0.69680. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68762/0.69594. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68664/0.69455. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68608/0.69400. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68561/0.69350. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68440/0.69287. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68442/0.69249. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68397/0.69209. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68385/0.69155. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68229/0.69117. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68129/0.69107. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68086/0.69072. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68059/0.68977. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68040/0.68934. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68039/0.68850. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67974/0.68804. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67782/0.68751. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67817/0.68753. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67822/0.68701. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67685/0.68730. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67660/0.68614. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67597/0.68689. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67478/0.68676. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67383/0.68603. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67513/0.68577. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67307/0.68507. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67274/0.68551. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67210/0.68479. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67079/0.68483. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66947/0.68421. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66995/0.68386. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66975/0.68354. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66769/0.68379. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66565/0.68376. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66424/0.68407. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66508/0.68338. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66383/0.68360. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66276/0.68304. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66178/0.68177. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65992/0.68144. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65900/0.68160. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65676/0.68166. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65556/0.68138. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65368/0.68186. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65259/0.68231. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65086/0.68120. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64899/0.68035. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64972/0.68055. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64743/0.68062. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64530/0.67956. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64507/0.68113. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64252/0.68056. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64002/0.67795. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63964/0.67838. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63953/0.67987. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63656/0.67928. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63637/0.67781. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63336/0.68124. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63460/0.67996. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63076/0.67852. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63278/0.67906. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62733/0.67984. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63017/0.68031. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62755/0.67939. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62663/0.67837. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62497/0.67785. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62644/0.67737. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62224/0.67886. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62042/0.67865. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61991/0.67826. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61775/0.67780. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62191/0.67656. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61444/0.67497. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61287/0.67430. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61275/0.67396. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61121/0.67576. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60957/0.67399. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60866/0.67408. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60492/0.67573. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60438/0.67687. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60494/0.67628. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60105/0.67427. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60040/0.67641. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60054/0.67742. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60070/0.67850. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59721/0.67717. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59863/0.67979. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59264/0.67774. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58913/0.68112. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58716/0.67843. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59543/0.68014. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58529/0.67600. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58534/0.68080. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58492/0.68109. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58209/0.67959. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57886/0.68086. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57723/0.68007. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69561/0.69050. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69434/0.69179. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69189/0.69399. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69051/0.69658. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68792/0.69948. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68773/0.70204. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68660/0.70348. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68622/0.70434. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68622/0.70438. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68549/0.70427. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68516/0.70425. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68464/0.70317. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68290/0.70241. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68270/0.70285. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68180/0.70190. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68043/0.70126. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67855/0.70057. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67760/0.70022. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67233/0.70076. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67224/0.70036. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66877/0.69956. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.66684/0.70044. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.66581/0.70100. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.66511/0.69895. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65963/0.70160. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66012/0.70284. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65790/0.70421. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65628/0.70329. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65319/0.70326. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65226/0.70648. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65153/0.70704. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65072/0.70813. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65094/0.70646. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64771/0.70753. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64485/0.70726. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64721/0.70796. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64243/0.71078. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64315/0.70904. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63855/0.71280. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.63828/0.71133. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.64019/0.71072. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.63863/0.71133. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63498/0.71359. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63551/0.71039. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.63411/0.71296. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62942/0.71549. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63109/0.71306. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62903/0.71604. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62671/0.71253. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62185/0.71825. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.62548/0.71557. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62363/0.71634. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62042/0.71634. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62190/0.71716. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.62099/0.71679. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61229/0.71977. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62091/0.72133. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61512/0.72225. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61495/0.72097. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61596/0.72072. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61241/0.71992. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60471/0.72090. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60640/0.71932. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60421/0.72193. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60447/0.72438. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59913/0.72658. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59887/0.72411. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59449/0.72195. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59787/0.72400. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59571/0.72463. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59487/0.72352. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59234/0.72167. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58693/0.71723. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58645/0.72022. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58818/0.72166. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58088/0.72620. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58522/0.72224. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57835/0.72990. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57469/0.72465. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57834/0.72657. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57538/0.73160. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57713/0.73016. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57419/0.73634. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56729/0.73435. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57141/0.73198. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56917/0.73618. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.56380/0.73643. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56094/0.73429. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55807/0.73640. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55896/0.73821. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55377/0.73394. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55937/0.73690. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54900/0.73529. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54630/0.73702. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55064/0.73993. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55043/0.74757. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54379/0.74594. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53750/0.75006. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53793/0.75907. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53589/0.74936. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68967/0.71174. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.68963/0.70645. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68785/0.70736. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68730/0.70775. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68639/0.70784. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68632/0.70836. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68568/0.70939. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68468/0.70951. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68479/0.70966. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68425/0.71016. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68372/0.71078. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68277/0.71080. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68245/0.71065. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68273/0.71157. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68208/0.71134. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68102/0.71170. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68096/0.71132. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68097/0.71183. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68022/0.71278. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68023/0.71147. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68028/0.71175. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67845/0.71194. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67754/0.71236. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67841/0.71238. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67748/0.71227. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67703/0.71201. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67611/0.71309. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67635/0.71318. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67521/0.71424. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67493/0.71403. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67492/0.71414. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67386/0.71426. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67336/0.71400. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67415/0.71500. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67334/0.71464. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67219/0.71599. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67179/0.71530. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67076/0.71541. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67139/0.71721. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66794/0.71706. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66994/0.71717. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66783/0.71762. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66889/0.71786. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66840/0.71820. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66692/0.71732. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66738/0.71726. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66651/0.71636. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66742/0.71809. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66509/0.71778. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66377/0.71771. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66537/0.71707. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66396/0.71950. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66237/0.71812. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66135/0.71784. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66166/0.71826. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66008/0.71778. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65938/0.71779. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65820/0.71854. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65668/0.72170. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65794/0.71926. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65627/0.71916. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65585/0.71896. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65684/0.72064. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65434/0.71959. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65396/0.71718. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65277/0.71760. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65308/0.71842. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65012/0.72045. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65166/0.71823. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64826/0.71945. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65013/0.71987. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64757/0.71955. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64498/0.71657. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64533/0.71715. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64442/0.71886. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64314/0.71637. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64285/0.71757. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64078/0.71802. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63993/0.71680. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63869/0.71637. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63663/0.71640. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63874/0.71574. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63551/0.71365. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63634/0.71336. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63349/0.71383. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63267/0.71369. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63268/0.71221. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62925/0.71517. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63070/0.71244. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62641/0.71222. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62567/0.70804. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62572/0.70937. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62366/0.70832. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61906/0.70696. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61874/0.70552. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62222/0.70489. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61840/0.70750. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61914/0.70243. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.61867/0.69905. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61894/0.70326. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69343/0.69459. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69133/0.69602. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68965/0.69730. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68961/0.69836. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68954/0.69889. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68843/0.69935. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68864/0.69960. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68785/0.70000. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68796/0.70035. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68673/0.70083. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68683/0.70132. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68712/0.70168. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68550/0.70233. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68438/0.70309. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68420/0.70359. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68403/0.70390. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68350/0.70488. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68271/0.70520. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68192/0.70624. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68141/0.70646. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68149/0.70692. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67964/0.70769. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67896/0.70813. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67864/0.70836. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67767/0.70943. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67724/0.70965. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67513/0.71068. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67454/0.71067. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67293/0.71089. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67329/0.71260. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67182/0.71371. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67120/0.71342. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66993/0.71308. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67199/0.71368. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66894/0.71528. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66975/0.71552. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66657/0.71655. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66663/0.71687. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66515/0.71898. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66405/0.71605. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66070/0.71952. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66344/0.71974. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66343/0.71875. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66051/0.72096. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65903/0.72246. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65694/0.72255. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65403/0.72356. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65459/0.72414. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65426/0.72671. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65318/0.72707. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65151/0.72653. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64943/0.72782. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64932/0.72838. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64660/0.72806. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64592/0.73075. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64490/0.73040. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64168/0.72918. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64006/0.73064. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63852/0.73259. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63510/0.73180. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63287/0.73227. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63429/0.73149. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62975/0.73633. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62747/0.73418. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62843/0.73719. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62744/0.73704. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62397/0.73600. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62177/0.73539. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62063/0.73640. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61825/0.73353. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61657/0.73575. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61422/0.73739. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61333/0.74093. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61306/0.74241. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60793/0.74122. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.60729/0.73921. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60547/0.73832. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60572/0.74384. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60266/0.74484. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59454/0.74508. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59454/0.74659. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59933/0.74501. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59787/0.74583. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59472/0.74689. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58589/0.74644. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58221/0.75141. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58309/0.75143. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57944/0.75301. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57540/0.75542. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57604/0.75382. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57069/0.75671. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57107/0.75608. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56744/0.75910. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56477/0.75640. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56654/0.76319. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55899/0.76442. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55262/0.76641. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55338/0.76249. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54880/0.76634. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55056/0.76329. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69317/0.69413. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69109/0.69403. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.69387. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69023/0.69377. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68973/0.69377. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.69372. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68991/0.69357. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68907/0.69364. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68917/0.69363. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68822/0.69369. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68858/0.69373. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68781/0.69374. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68677/0.69398. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68676/0.69423. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68665/0.69449. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68622/0.69467. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68512/0.69506. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68522/0.69496. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68476/0.69552. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68461/0.69561. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68340/0.69595. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68268/0.69663. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68332/0.69753. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68124/0.69754. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68061/0.69862. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68121/0.69840. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68120/0.69872. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67869/0.69903. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67908/0.69899. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67979/0.69939. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67984/0.69974. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67861/0.70018. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67720/0.69992. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67700/0.70028. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67588/0.69924. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67526/0.69985. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67433/0.69959. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67327/0.70004. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67298/0.69980. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67208/0.69935. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67108/0.69877. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67103/0.69873. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66920/0.69845. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66770/0.69914. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66525/0.69812. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66689/0.69695. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66459/0.69622. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66232/0.69538. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66518/0.69475. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65975/0.69337. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65803/0.69297. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65594/0.69341. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.65666/0.69281. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65592/0.69099. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65207/0.69056. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65356/0.68897. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65203/0.68811. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64977/0.68838. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64708/0.68925. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64612/0.68658. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64526/0.68463. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64271/0.68529. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64050/0.68630. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63828/0.68497. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63685/0.68442. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63460/0.68284. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63446/0.68234. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63365/0.68255. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63300/0.68247. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63084/0.67976. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.62737/0.68011. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62942/0.67976. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62495/0.67931. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62363/0.68162. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62369/0.68067. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61493/0.67913. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61509/0.67970. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61376/0.67968. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61473/0.68128. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61039/0.68111. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60497/0.68108. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60871/0.68201. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60740/0.68133. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60449/0.68257. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60425/0.68386. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59854/0.68234. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59826/0.68248. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59382/0.68551. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59258/0.68159. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59247/0.68513. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58757/0.68605. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58995/0.68214. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58596/0.68791. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57958/0.68810. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58375/0.68751. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58084/0.68551. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57569/0.68942. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.57025/0.68930. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57803/0.69041. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56607/0.68990. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69323/0.69260. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69265. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69158/0.69275. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69198/0.69285. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69184/0.69299. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.69305. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.69303. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69122/0.69285. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69086/0.69304. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69071/0.69320. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68858/0.69335. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68830/0.69342. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68895/0.69346. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68897/0.69366. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68878/0.69387. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68716/0.69402. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68753/0.69434. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68619/0.69492. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68654/0.69467. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68572/0.69453. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68544/0.69471. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68343/0.69466. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68377/0.69529. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68202/0.69521. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68125/0.69456. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67934/0.69487. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68060/0.69566. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68074/0.69528. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67614/0.69601. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67793/0.69644. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67489/0.69673. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67593/0.69732. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67283/0.69706. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67327/0.69767. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66947/0.69892. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67069/0.69915. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66599/0.69905. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66550/0.69925. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66568/0.70066. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66311/0.70040. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66065/0.70089. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66118/0.70212. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65977/0.70121. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65757/0.70008. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65498/0.70311. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65386/0.70387. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65127/0.70402. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65230/0.70651. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64883/0.70797. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64299/0.70793. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64581/0.70967. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.64273/0.70980. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64004/0.70909. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63705/0.71134. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63649/0.71213. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63533/0.71689. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62990/0.71718. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63154/0.71779. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62857/0.71931. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62784/0.71968. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62480/0.72095. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62441/0.72232. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62046/0.72399. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61652/0.72408. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61753/0.72646. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61579/0.73056. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61424/0.73091. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60815/0.73064. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60548/0.73319. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60317/0.73406. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60389/0.73430. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59726/0.73692. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60186/0.73851. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59303/0.73670. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58969/0.73985. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.58838/0.74207. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58772/0.73991. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58422/0.74485. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58125/0.74250. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57389/0.74414. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57485/0.74705. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57637/0.74852. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57799/0.75048. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56975/0.75156. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57462/0.75319. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56448/0.75320. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56472/0.75718. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56203/0.75295. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55000/0.75765. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54804/0.76153. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55188/0.76069. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55037/0.76288. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54682/0.76728. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55136/0.76608. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54432/0.76907. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53532/0.76834. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53100/0.77086. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.52994/0.77409. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52310/0.77237. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.52641/0.76805. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69635/0.69444. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69451/0.69350. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69338/0.69245. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69157. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69260/0.69093. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69144/0.69021. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69144/0.68973. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69120/0.68917. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69020/0.68897. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69065/0.68861. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69038/0.68841. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69046/0.68828. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68981/0.68832. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69019/0.68851. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68962/0.68849. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68897/0.68857. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68943/0.68883. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68878/0.68901. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68888/0.68916. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68856/0.68931. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68840/0.68957. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68792/0.68982. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68771/0.69003. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68776/0.69057. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68775/0.69094. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68728/0.69080. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68659/0.69150. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68597/0.69174. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68582/0.69221. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68584/0.69270. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68491/0.69331. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68555/0.69380. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68450/0.69433. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68455/0.69451. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68369/0.69496. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68373/0.69521. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68364/0.69590. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68327/0.69614. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68210/0.69657. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68111/0.69747. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68176/0.69784. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68146/0.69886. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67982/0.69897. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67975/0.69944. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67958/0.70015. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67956/0.70036. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67856/0.70091. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67654/0.70092. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67741/0.70093. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67649/0.70140. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67559/0.70172. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67629/0.70226. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67499/0.70214. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67534/0.70232. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67340/0.70267. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67321/0.70301. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67239/0.70381. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67180/0.70384. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67196/0.70442. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66880/0.70475. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66905/0.70478. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66803/0.70457. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66622/0.70534. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66879/0.70609. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66642/0.70641. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66452/0.70661. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66518/0.70751. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66295/0.70805. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66559/0.70780. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66191/0.70824. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66228/0.70895. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66060/0.70883. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66027/0.70914. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66108/0.71099. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66026/0.71044. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65687/0.71049. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65744/0.71133. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65792/0.71158. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65717/0.71212. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65588/0.71341. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65867/0.71361. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65579/0.71311. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.65110/0.71257. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65324/0.71297. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.65186/0.71356. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65060/0.71344. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64885/0.71478. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64792/0.71550. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.64726/0.71685. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64731/0.71781. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64203/0.71671. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.64507/0.71814. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64483/0.71777. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63988/0.71838. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63918/0.72034. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63752/0.72004. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63620/0.72173. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63705/0.72272. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63540/0.72382. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.63858/0.72323. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69567/0.69226. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69234/0.69155. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69199/0.69151. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69140. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.69126. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.69115. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69031/0.69129. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68906/0.69127. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68936/0.69130. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68922/0.69133. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68809/0.69158. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68763/0.69191. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68857/0.69241. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68611/0.69291. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68682/0.69309. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68596/0.69346. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68752/0.69391. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68544/0.69417. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68519/0.69470. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68374/0.69505. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68358/0.69564. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68287/0.69623. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68248/0.69685. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68275/0.69728. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68274/0.69802. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68243/0.69823. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68062/0.69862. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68110/0.69919. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68070/0.69933. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67932/0.69943. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67856/0.70002. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67768/0.70058. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67675/0.70102. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67789/0.70132. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67682/0.70193. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67488/0.70300. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67742/0.70278. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67323/0.70319. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67294/0.70420. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67200/0.70460. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67138/0.70495. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66892/0.70637. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66719/0.70765. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66880/0.70789. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67032/0.70842. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66887/0.70936. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66756/0.70922. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66702/0.71051. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66560/0.71199. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66478/0.71275. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66366/0.71352. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66160/0.71401. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66132/0.71605. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65975/0.71805. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65656/0.71977. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65875/0.72049. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65417/0.72243. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65149/0.72395. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65178/0.72605. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65250/0.72649. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64888/0.72892. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64589/0.73406. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64594/0.73418. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64079/0.73715. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64275/0.74051. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63870/0.74057. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63867/0.74227. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63466/0.74702. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63820/0.74974. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63503/0.75126. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63620/0.75203. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63041/0.75432. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62731/0.75649. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62792/0.75996. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62255/0.76270. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62174/0.76600. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61781/0.76784. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61415/0.77315. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61277/0.77664. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61398/0.77777. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61375/0.78373. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61188/0.78658. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.60730/0.78853. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60466/0.79409. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59554/0.80206. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59863/0.79983. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59895/0.80253. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59732/0.80907. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59084/0.81091. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58861/0.81645. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.59371/0.81839. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58813/0.82463. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58341/0.82823. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58156/0.83451. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57040/0.84143. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57997/0.84417. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.58017/0.84735. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56970/0.84974. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55952/0.85619. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56678/0.86227. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69426. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69379/0.69400. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69312/0.69379. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69323/0.69364. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69311/0.69354. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69273/0.69342. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69316/0.69329. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69266/0.69303. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69246/0.69265. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69279/0.69215. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69210/0.69134. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69073/0.69044. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69083/0.68961. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69000/0.68869. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69012/0.68790. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68993/0.68750. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68929/0.68713. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68943/0.68711. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68840/0.68708. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68893/0.68706. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68804/0.68720. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68738/0.68690. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68669/0.68699. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68621/0.68659. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68663/0.68733. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68626/0.68728. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68482/0.68727. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68416/0.68763. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68394/0.68735. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68344/0.68792. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68234/0.68757. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68212/0.68754. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68188/0.68924. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68124/0.69022. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67842/0.68975. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67989/0.69125. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67718/0.69105. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67716/0.69143. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67588/0.69060. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67708/0.69063. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67480/0.69027. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67245/0.68974. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67238/0.69150. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67383/0.69223. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66940/0.69284. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67187/0.69304. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66706/0.69202. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66692/0.69221. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66534/0.69225. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66371/0.69544. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66308/0.69627. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66126/0.69546. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65788/0.69405. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65842/0.69397. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65340/0.69785. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65258/0.69937. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65106/0.70038. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65097/0.70091. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64824/0.70038. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64361/0.70091. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64110/0.70452. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64024/0.70082. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64165/0.70300. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63333/0.70814. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63248/0.70758. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63427/0.71025. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62688/0.70955. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62362/0.71137. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62322/0.71296. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62194/0.71460. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61865/0.71260. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61036/0.71474. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61670/0.71828. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60837/0.71866. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60712/0.72300. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60308/0.72372. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59970/0.72332. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59451/0.72209. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59501/0.72896. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58862/0.73467. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58997/0.73228. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58662/0.73104. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58541/0.72987. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57130/0.73738. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58070/0.73947. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57924/0.74165. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57065/0.73727. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56715/0.73643. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55809/0.74676. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56115/0.74636. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55993/0.74705. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54969/0.74621. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55323/0.74574. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55645/0.74531. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53966/0.75147. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54032/0.74746. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54815/0.75401. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53291/0.75378. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53365/0.75116. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52982/0.76163. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69491/0.68818. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69192/0.68595. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69083/0.68517. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68980/0.68484. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68983/0.68491. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68969/0.68504. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68936/0.68542. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68842/0.68547. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68918/0.68600. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68953/0.68657. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.68702. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68729/0.68757. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68779/0.68811. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68801/0.68886. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68710/0.68944. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68583/0.69041. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68718/0.69123. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68639/0.69232. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68441/0.69370. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68370/0.69499. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68344/0.69569. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68331/0.69635. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68308/0.69693. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68130/0.69821. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68030/0.69938. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68104/0.70049. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67797/0.70119. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67842/0.70207. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68049/0.70284. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67627/0.70377. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67609/0.70422. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67411/0.70622. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67211/0.70664. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67154/0.70694. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67219/0.70741. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66743/0.70795. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67017/0.70796. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66549/0.70819. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66495/0.71040. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66318/0.71121. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66051/0.71117. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65855/0.71013. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65632/0.71035. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65642/0.71120. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65581/0.71315. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65313/0.71404. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64991/0.71538. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65003/0.71348. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64528/0.71333. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64533/0.71359. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64283/0.71614. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63767/0.72063. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63624/0.72250. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63570/0.72250. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63053/0.72367. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62800/0.72790. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62359/0.72711. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62250/0.72974. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62153/0.73306. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61602/0.73547. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61401/0.73549. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61347/0.73488. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61236/0.73600. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60602/0.73603. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60116/0.74381. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.59710/0.74243. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60179/0.74680. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59147/0.74966. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.58892/0.75650. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.58776/0.75584. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58627/0.75420. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57915/0.76362. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57578/0.76788. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57390/0.76796. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56666/0.76432. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56446/0.77268. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.55995/0.78221. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55384/0.79084. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55278/0.78682. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54880/0.79044. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54834/0.79624. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.53539/0.80193. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.53912/0.81287. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.53413/0.80828. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.53218/0.81100. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.52591/0.81696. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.51980/0.82124. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52430/0.82606. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51593/0.82942. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51211/0.83449. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.51210/0.82545. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51246/0.83981. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50046/0.84668. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.50393/0.85780. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50647/0.85201. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.49991/0.85525. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49558/0.86220. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48655/0.87889. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.48856/0.87658. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.47793/0.87731. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69189. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69128/0.69083. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68993/0.69031. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68885/0.69014. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68812/0.69029. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68737/0.69053. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68677/0.69081. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68665/0.69125. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68625/0.69183. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68532/0.69248. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68531/0.69331. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68459/0.69419. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68443/0.69503. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68436/0.69590. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68366/0.69670. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68338/0.69763. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68313/0.69845. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68305/0.69909. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68288/0.69950. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68261/0.70011. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68212/0.70065. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68212/0.70118. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68142/0.70181. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68145/0.70242. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68113/0.70301. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68149/0.70328. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68034/0.70388. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68040/0.70434. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68036/0.70473. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68009/0.70524. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67880/0.70618. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67937/0.70640. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67825/0.70713. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67761/0.70780. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67748/0.70850. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67680/0.70917. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67706/0.70983. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67726/0.71075. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67724/0.71138. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67602/0.71172. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67690/0.71233. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67390/0.71326. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67553/0.71414. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67320/0.71491. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67458/0.71541. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67333/0.71628. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67269/0.71649. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67320/0.71673. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67189/0.71786. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67188/0.71858. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67026/0.71842. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67067/0.71890. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66857/0.71983. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67060/0.72071. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66992/0.72175. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66947/0.72185. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66877/0.72210. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66672/0.72266. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66697/0.72352. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66637/0.72405. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66651/0.72472. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66494/0.72580. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66584/0.72650. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66540/0.72626. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66299/0.72688. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66439/0.72719. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66311/0.72767. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66192/0.72868. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66205/0.72942. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66022/0.72972. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66003/0.73076. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65778/0.73092. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65854/0.73079. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65723/0.73258. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65598/0.73324. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65533/0.73460. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65662/0.73522. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65323/0.73469. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65156/0.73380. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65334/0.73633. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65082/0.73814. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65216/0.73842. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.65043/0.74038. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64966/0.73997. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64811/0.74018. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64531/0.74158. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64584/0.74197. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64589/0.74521. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.64472/0.74570. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64462/0.74506. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64221/0.74674. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.64029/0.74875. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63848/0.74916. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63891/0.74929. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.63813/0.74953. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63702/0.75124. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63591/0.75354. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63479/0.75488. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63117/0.75360. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.63217/0.75738. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69322. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69084/0.68944. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69046/0.68794. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68941/0.68724. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68926/0.68651. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68899/0.68608. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68920/0.68556. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68787/0.68522. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68852/0.68500. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68778/0.68466. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68742/0.68401. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68669/0.68340. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68612/0.68282. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68550/0.68214. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68561/0.68190. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68556/0.68130. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68419/0.68066. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68475/0.68090. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68458/0.68028. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68280/0.67994. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68391/0.67964. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68294/0.67926. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.68136/0.67934. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68103/0.67909. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67927/0.67891. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67901/0.67894. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67884/0.67875. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67758/0.67861. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.67753/0.67889. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67774/0.67935. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67565/0.68010. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67683/0.68007. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67389/0.68021. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67328/0.68123. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67454/0.68093. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67245/0.68091. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67075/0.68136. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66976/0.68248. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67190/0.68268. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66756/0.68414. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66791/0.68465. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66659/0.68431. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66640/0.68588. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66417/0.68694. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66186/0.68651. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65999/0.68688. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66134/0.68666. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66097/0.68761. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65687/0.68913. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65667/0.68888. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65194/0.69003. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65076/0.68959. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65028/0.69063. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65168/0.69166. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64873/0.69070. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64587/0.69217. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64652/0.69347. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64092/0.69428. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64164/0.69458. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63550/0.69719. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63668/0.69808. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63434/0.69804. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63294/0.70144. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62673/0.70516. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62832/0.70349. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63051/0.70526. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62629/0.70559. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62478/0.70626. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61445/0.71022. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61449/0.71282. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61009/0.71435. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61062/0.71949. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60803/0.72249. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61201/0.72732. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60356/0.71981. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60471/0.72735. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60172/0.72407. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59504/0.73407. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58840/0.74378. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59536/0.73976. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58845/0.73847. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58334/0.74065. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58250/0.74074. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58027/0.74992. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58595/0.75530. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57474/0.75247. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57281/0.76538. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57112/0.75423. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56404/0.76083. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55951/0.77826. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57113/0.77194. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55805/0.77529. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56214/0.78532. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55804/0.79237. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55355/0.78606. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54653/0.79639. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55521/0.80138. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54135/0.79912. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54079/0.81318. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53411/0.81117. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69184/0.69554. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68921/0.69450. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68805/0.69436. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68769/0.69444. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68708/0.69470. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68697/0.69496. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68641/0.69521. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68555/0.69574. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68468/0.69625. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68561/0.69672. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68509/0.69760. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68438/0.69828. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68286/0.69878. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68334/0.69945. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68162/0.70047. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68186/0.70113. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68084/0.70157. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67980/0.70285. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68112/0.70410. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67796/0.70491. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67670/0.70607. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67772/0.70753. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67635/0.70809. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67711/0.70891. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67478/0.71030. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67312/0.71165. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67426/0.71318. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67130/0.71486. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67367/0.71686. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67017/0.71864. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67158/0.71908. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66961/0.72125. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66618/0.72277. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66703/0.72501. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66717/0.72677. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66644/0.72831. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66622/0.72863. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66269/0.73088. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66102/0.73182. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65960/0.73410. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66213/0.73581. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65781/0.73563. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65904/0.73656. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65831/0.73785. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65559/0.74058. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65425/0.74201. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65317/0.74303. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65435/0.74369. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65129/0.74443. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64867/0.74794. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64937/0.74823. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64779/0.74884. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64518/0.75219. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64140/0.75549. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64503/0.75483. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63822/0.75493. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64161/0.75496. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63798/0.75953. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63489/0.76200. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63729/0.76217. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63498/0.76481. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63300/0.76449. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63205/0.76885. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62913/0.76985. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62817/0.77022. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62566/0.77400. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62268/0.77938. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62627/0.77614. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62039/0.77679. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62007/0.77899. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61449/0.78522. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61636/0.78690. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61497/0.78598. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60851/0.79051. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60967/0.79208. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60835/0.79467. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60903/0.79549. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60202/0.79742. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60474/0.80106. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59945/0.80358. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59489/0.80350. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59386/0.80735. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59242/0.81259. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59750/0.81135. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58844/0.81679. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58949/0.81978. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58375/0.82048. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58439/0.82220. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58430/0.82326. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57465/0.82636. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57258/0.83269. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56998/0.83791. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57532/0.83638. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57020/0.84574. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57073/0.84029. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56904/0.84746. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56430/0.84753. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55463/0.84301. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56238/0.84736. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55703/0.85913. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.68914/0.69274. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68796/0.69341. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68748/0.69294. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68675/0.69177. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68566/0.69104. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68489/0.68977. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68461/0.68822. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68405/0.68671. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68339/0.68523. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68260/0.68498. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68266/0.68371. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68163/0.68335. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68124/0.68241. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68137/0.68213. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67972/0.68124. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67937/0.68086. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67945/0.67999. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67870/0.67945. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.67842/0.67796. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.67714/0.67792. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67755/0.67822. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67738/0.67770. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67591/0.67650. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67647/0.67571. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67524/0.67490. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67457/0.67377. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67463/0.67321. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67300/0.67228. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67236/0.67187. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67229/0.67123. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67008/0.67004. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67006/0.67043. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66899/0.66877. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66955/0.66867. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66727/0.66889. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66673/0.66744. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66511/0.66909. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66460/0.66777. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66266/0.66832. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66297/0.66668. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66088/0.66624. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66014/0.66606. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65606/0.66768. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65670/0.66705. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65610/0.66664. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65363/0.66619. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65281/0.66998. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65113/0.67090. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64942/0.67216. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64739/0.67107. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64680/0.67268. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64346/0.67227. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64162/0.67400. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64091/0.67639. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63703/0.67461. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63672/0.67989. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63533/0.67583. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63310/0.68171. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63125/0.67978. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63014/0.68225. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62942/0.68226. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62672/0.68287. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.62380/0.68371. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62220/0.68294. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61670/0.68375. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61396/0.68713. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61318/0.68906. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60949/0.68967. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60674/0.69029. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60489/0.69007. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.60294/0.69438. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60025/0.69307. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59860/0.69836. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59731/0.69795. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59319/0.69855. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59306/0.69371. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58848/0.69377. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58487/0.70264. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58225/0.70292. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58250/0.70480. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57655/0.70383. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57540/0.71152. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57294/0.70828. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57143/0.70746. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56681/0.71122. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56655/0.71523. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56480/0.71034. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56408/0.71114. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55187/0.71533. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55226/0.71237. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.54482/0.71868. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54201/0.71564. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54481/0.72107. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.53896/0.71983. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.53497/0.72565. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.53735/0.72495. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53752/0.72512. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53134/0.72979. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52737/0.73482. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52924/0.72311. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69599/0.69474. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68956/0.69595. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68840/0.69637. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68885/0.69701. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68936/0.69765. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68713/0.69846. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68727/0.69907. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68690/0.69990. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68768/0.70013. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68701/0.70080. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68580/0.70132. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68625/0.70199. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68555/0.70230. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68517/0.70290. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68434/0.70328. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68538/0.70345. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68415/0.70363. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68392/0.70406. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68385/0.70421. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68361/0.70444. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68274/0.70453. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68226/0.70513. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68279/0.70552. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68172/0.70523. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68280/0.70559. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68107/0.70565. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67980/0.70588. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67954/0.70596. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67985/0.70570. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67858/0.70576. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67869/0.70550. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67622/0.70556. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67611/0.70565. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67620/0.70596. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67557/0.70584. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67304/0.70602. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67273/0.70626. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67235/0.70608. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67130/0.70624. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67180/0.70620. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66963/0.70644. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66663/0.70662. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66793/0.70655. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66435/0.70619. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66680/0.70615. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66251/0.70667. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66252/0.70645. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66134/0.70704. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65842/0.70605. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65960/0.70493. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65348/0.70439. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65078/0.70438. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65116/0.70369. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65287/0.70396. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65042/0.70342. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64769/0.70259. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64567/0.70420. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64439/0.70282. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64354/0.70093. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64120/0.70204. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63896/0.70059. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63379/0.69930. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63557/0.69834. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63447/0.69864. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63290/0.69691. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62590/0.69784. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63032/0.69806. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62861/0.69731. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62603/0.69479. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62234/0.69399. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62007/0.69257. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.61741/0.69127. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61662/0.69260. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61966/0.69184. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61061/0.68912. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61131/0.69140. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61008/0.69260. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60699/0.69034. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60704/0.69075. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59964/0.68759. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60538/0.69320. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.60129/0.69187. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59660/0.69081. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59798/0.69560. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58858/0.69533. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58897/0.69371. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59185/0.69135. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58770/0.69563. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58522/0.69168. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58144/0.69837. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57673/0.69893. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58511/0.70344. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57316/0.70197. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.57477/0.70090. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57306/0.70626. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.56928/0.70508. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56409/0.70038. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56292/0.70551. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56190/0.70742. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55513/0.71327. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69456/0.69611. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.69566. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69260/0.69536. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69245/0.69520. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69508. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69159/0.69495. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69176/0.69492. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.69488. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68983/0.69475. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68971/0.69475. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68882/0.69474. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68762/0.69474. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68812/0.69479. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68675/0.69493. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68592/0.69505. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68521/0.69524. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68378/0.69550. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68387/0.69617. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68407/0.69672. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.69732. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68276/0.69781. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68079/0.69888. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.69993. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68109/0.70050. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68031/0.70126. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67930/0.70126. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67832/0.70200. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67807/0.70252. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67675/0.70330. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67700/0.70381. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67692/0.70425. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67601/0.70448. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67422/0.70509. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67377/0.70602. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67339/0.70698. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67215/0.70758. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67445/0.70815. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67190/0.70977. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67199/0.71095. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67185/0.71136. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66894/0.71207. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66838/0.71349. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66802/0.71449. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66687/0.71536. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66386/0.71664. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66386/0.71814. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66085/0.71993. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65882/0.72004. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66066/0.72148. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65955/0.72189. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65958/0.72278. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65539/0.72438. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65573/0.72519. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65438/0.72722. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65192/0.72840. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64738/0.72993. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65029/0.73210. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64989/0.73377. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64940/0.73371. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64377/0.73557. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64263/0.73647. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64121/0.73799. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63648/0.74285. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63876/0.74321. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63478/0.74632. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63290/0.74612. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62979/0.74770. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62644/0.74892. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62503/0.75653. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62367/0.75760. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61851/0.76020. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61815/0.76163. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61472/0.76431. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61084/0.76808. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61089/0.77138. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60817/0.77289. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60620/0.77863. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60008/0.77629. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59969/0.78102. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59890/0.78407. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59334/0.78649. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59128/0.78617. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59335/0.80085. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58506/0.79815. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58856/0.79850. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58395/0.80208. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58108/0.80793. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58123/0.80829. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57342/0.80923. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57589/0.81444. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56712/0.81379. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56481/0.81166. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56325/0.81900. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56931/0.81822. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55612/0.82509. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55190/0.82851. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55402/0.82909. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.54426/0.83522. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54361/0.84141. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54115/0.84660. Took 0.08 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69360/0.69538. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69350/0.69475. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69325/0.69437. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69326/0.69394. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69236/0.69387. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69193/0.69348. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69210/0.69312. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69081/0.69300. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69023/0.69272. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68982/0.69242. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68958/0.69221. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68982/0.69225. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68921/0.69173. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68839/0.69171. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.68728/0.69193. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68606/0.69150. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68510/0.69148. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68269/0.69209. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68376/0.69276. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68331/0.69296. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67980/0.69369. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67877/0.69516. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67643/0.69602. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67608/0.69713. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67456/0.69800. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67229/0.69939. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67076/0.70051. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66884/0.70187. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66542/0.70333. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66644/0.70442. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66458/0.70580. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66565/0.70740. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66196/0.70793. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66043/0.71031. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65928/0.71164. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65632/0.71374. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65663/0.71558. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65652/0.71729. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65214/0.71893. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64827/0.72213. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64838/0.72264. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64650/0.72663. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64767/0.72792. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64758/0.73007. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64449/0.73000. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64495/0.73237. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64164/0.73550. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63886/0.73963. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63965/0.74068. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63886/0.74138. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63405/0.74343. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63530/0.74711. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63295/0.74831. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63262/0.75104. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63104/0.75173. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63011/0.75265. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62562/0.75792. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62373/0.75974. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62393/0.76128. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62305/0.76354. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62405/0.76575. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61991/0.76590. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.62234/0.76771. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61632/0.77169. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61213/0.77444. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61465/0.77614. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61451/0.78001. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61058/0.77950. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60651/0.78382. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60707/0.78740. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60444/0.79160. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60106/0.78967. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60146/0.79315. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59800/0.79514. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60159/0.79810. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59948/0.80270. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60002/0.80070. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59234/0.80275. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59574/0.80713. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58946/0.81184. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58738/0.81100. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58414/0.81455. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59030/0.81364. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58348/0.81824. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58074/0.81965. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58428/0.82471. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57940/0.82525. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57088/0.82867. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57294/0.83203. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56702/0.83463. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56476/0.83857. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56708/0.84013. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56812/0.84232. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56293/0.84215. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55807/0.84848. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56034/0.84870. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55427/0.85419. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55334/0.85296. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54580/0.86322. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54797/0.86554. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69260/0.69588. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69247/0.69582. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69320/0.69580. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69309/0.69564. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69256/0.69568. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69301/0.69575. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69246/0.69582. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69091/0.69582. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69143/0.69583. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69176/0.69603. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69035/0.69615. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69129/0.69648. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69081/0.69687. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68946/0.69740. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68942/0.69783. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69001/0.69812. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68901/0.69843. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69904. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68766/0.69926. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68610/0.69981. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68523/0.70071. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68655/0.70121. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68597/0.70139. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68395/0.70172. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68243/0.70116. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68262/0.70150. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68145/0.70199. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68102/0.70296. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68029/0.70345. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68044/0.70356. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67756/0.70419. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67887/0.70478. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67499/0.70536. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67444/0.70565. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67467/0.70560. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67364/0.70557. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67268/0.70652. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67086/0.70665. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67198/0.70721. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67121/0.70735. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67157/0.70769. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66676/0.70812. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66845/0.70843. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66512/0.70783. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66558/0.70749. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66318/0.70925. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66284/0.71032. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66292/0.70958. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66397/0.71125. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65972/0.71147. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65627/0.71279. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65467/0.71233. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65696/0.71211. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65232/0.71288. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65191/0.71303. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64944/0.71415. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64713/0.71545. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64399/0.71727. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64985/0.71488. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64388/0.71547. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64075/0.71551. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64453/0.71419. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64073/0.71351. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63975/0.71415. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63831/0.71838. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63596/0.71692. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63101/0.71637. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63064/0.71550. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62645/0.71688. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62203/0.71687. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62263/0.71871. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62625/0.72177. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62248/0.72173. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61811/0.72520. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61911/0.72647. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61553/0.73018. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61517/0.72480. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60743/0.73167. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60212/0.73213. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60998/0.73222. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61220/0.73027. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59948/0.73962. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60401/0.73668. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60404/0.74407. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59700/0.73727. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59823/0.73545. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59328/0.74220. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59106/0.74391. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59743/0.74577. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59062/0.75070. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58964/0.74991. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58429/0.74796. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.58614/0.75249. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58756/0.74757. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58417/0.75260. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57889/0.75671. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57701/0.75787. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56712/0.76314. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57185/0.77065. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.57209/0.76824. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69292/0.69442. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69257/0.69464. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.69486. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69239/0.69512. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69128/0.69522. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69142/0.69592. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69113/0.69645. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68999/0.69749. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68957/0.69841. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68878/0.69899. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68926/0.70026. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68866/0.70115. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68803/0.70172. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68729/0.70196. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68659/0.70293. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68677/0.70351. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68504/0.70460. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68491/0.70518. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68365/0.70580. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68270/0.70659. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68309/0.70756. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68092/0.70766. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68248/0.70806. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68076/0.70790. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67948/0.70703. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67846/0.70728. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67773/0.70741. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67656/0.70702. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67621/0.70637. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67569/0.70625. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67204/0.70459. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67320/0.70539. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66985/0.70433. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66970/0.70383. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66590/0.70258. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66570/0.70191. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66528/0.70120. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66212/0.70057. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66059/0.70022. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65840/0.69912. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65538/0.69824. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65326/0.69573. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65399/0.69731. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64890/0.69485. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64678/0.69370. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64292/0.69241. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.63799/0.69151. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.63991/0.69060. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63439/0.68879. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63934/0.68860. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63641/0.68954. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63527/0.68755. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63162/0.68778. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62438/0.68781. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62061/0.68610. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62520/0.68181. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62447/0.68151. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61988/0.68093. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61572/0.68072. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61693/0.68101. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61051/0.68251. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61427/0.68244. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60937/0.68198. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60516/0.68231. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59856/0.68284. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60863/0.68500. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60209/0.68666. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59630/0.68980. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.59488/0.68887. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59462/0.68996. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60329/0.68477. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59690/0.68905. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59294/0.69231. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58681/0.69096. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58605/0.69004. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58827/0.69804. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58439/0.69231. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.58094/0.69292. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.57611/0.69187. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57829/0.69800. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57037/0.69794. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57077/0.69754. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57068/0.70317. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56778/0.70063. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56786/0.69606. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56308/0.69949. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56026/0.71087. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56605/0.70741. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55013/0.70806. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55578/0.71082. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55759/0.71770. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55490/0.71626. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54481/0.71516. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54843/0.71815. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54424/0.71961. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54261/0.72018. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54789/0.71839. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54217/0.72170. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54819/0.71568. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53171/0.72187. Took 0.08 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69637/0.69553. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69559/0.69535. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69439/0.69517. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69337/0.69500. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69205/0.69496. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69122/0.69496. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69152/0.69507. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.69511. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69011/0.69518. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68963/0.69524. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69033/0.69509. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68973/0.69506. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69021/0.69505. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68950/0.69496. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68871/0.69500. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68921/0.69510. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68783/0.69484. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68788/0.69465. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68659/0.69459. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68707/0.69443. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68607/0.69433. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68753/0.69389. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68481/0.69395. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68527/0.69409. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68482/0.69446. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68519/0.69435. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68534/0.69429. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68366/0.69489. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68499/0.69484. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68226/0.69456. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68168/0.69423. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67919/0.69393. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68066/0.69470. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68079/0.69420. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67891/0.69500. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67949/0.69444. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67686/0.69483. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67654/0.69529. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67572/0.69413. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67503/0.69477. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67312/0.69465. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67405/0.69427. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67090/0.69499. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67146/0.69572. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67218/0.69558. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67025/0.69580. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66852/0.69551. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66558/0.69535. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66363/0.69574. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66150/0.69729. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66034/0.69722. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65895/0.69794. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65845/0.69803. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65830/0.69852. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65919/0.70016. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65177/0.70220. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64982/0.70198. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65170/0.70592. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64901/0.70444. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64681/0.70484. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64342/0.70742. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64344/0.70946. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63955/0.71020. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63720/0.71289. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63478/0.71333. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63396/0.71216. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63297/0.71494. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62732/0.71721. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62596/0.72131. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62587/0.72342. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62646/0.72293. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61793/0.72343. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61855/0.72607. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61533/0.73079. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62034/0.73447. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60893/0.73819. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61194/0.74147. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60968/0.74347. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60976/0.74618. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60138/0.75088. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60015/0.75374. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59638/0.75475. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59713/0.75951. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59977/0.75986. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59497/0.75991. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58640/0.76558. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58946/0.76733. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58535/0.76867. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.58576/0.76828. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58068/0.77634. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58272/0.77203. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57555/0.77309. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57608/0.78189. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57347/0.77335. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.57057/0.78183. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56905/0.78495. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56639/0.78600. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.56426/0.79443. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.56309/0.79052. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56521/0.79262. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69321/0.69477. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69184/0.69446. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69198/0.69427. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69152/0.69395. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69112/0.69426. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69094/0.69417. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.69412. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69019/0.69408. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69037/0.69441. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68940/0.69478. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.69489. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68974/0.69509. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.69495. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68933/0.69537. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68900/0.69555. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68861/0.69588. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68810/0.69628. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68854/0.69645. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68766/0.69649. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68737/0.69631. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68761/0.69630. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68659/0.69664. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68736/0.69663. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68566/0.69654. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68573/0.69645. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68578/0.69649. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68533/0.69634. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68519/0.69574. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68491/0.69618. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68409/0.69593. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68311/0.69619. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68269/0.69623. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68225/0.69533. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68137/0.69579. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68148/0.69462. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68078/0.69383. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67896/0.69397. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67816/0.69334. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.67833/0.69250. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67644/0.69250. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67427/0.69092. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67507/0.69051. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67167/0.68979. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67169/0.68906. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.66941/0.68731. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66740/0.68690. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66683/0.68672. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66635/0.68594. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66368/0.68444. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66383/0.68448. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66131/0.68114. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66049/0.67976. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65722/0.68056. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65789/0.68108. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65536/0.68066. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65241/0.68111. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65212/0.67782. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65178/0.68057. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64817/0.68137. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64701/0.68270. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64646/0.67940. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64659/0.67603. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64127/0.67903. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64223/0.67787. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63928/0.67541. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63998/0.68079. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64235/0.67784. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63782/0.68056. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.63919/0.67936. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63505/0.67496. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63403/0.67530. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63003/0.67634. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62775/0.67659. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62414/0.67684. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62933/0.67208. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62104/0.67521. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62121/0.67351. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61746/0.67468. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61494/0.67496. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61827/0.67437. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62016/0.67135. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61214/0.67576. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61197/0.67417. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61118/0.67163. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60525/0.67228. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61038/0.67398. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.60663/0.67044. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60968/0.66857. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60478/0.67209. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60424/0.66776. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60016/0.66704. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59870/0.67187. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.59502/0.66831. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59192/0.66819. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59342/0.66723. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58966/0.66784. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58984/0.66531. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59127/0.66714. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58892/0.66365. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59261/0.66159. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69563/0.69325. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69369/0.69172. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.69102. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69146/0.69079. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69200/0.69073. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69149/0.69082. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69062/0.69127. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69033/0.69175. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68996/0.69237. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69040/0.69307. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68974/0.69365. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68926/0.69443. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68836/0.69508. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68754/0.69585. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68717/0.69664. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68660/0.69800. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68581/0.69925. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68654/0.70008. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68496/0.70116. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68555/0.70191. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68417/0.70265. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68210/0.70391. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68219/0.70535. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68158/0.70558. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68342/0.70566. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68042/0.70581. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67939/0.70696. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68001/0.70686. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67785/0.70746. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67792/0.70819. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67483/0.70830. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67687/0.70836. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67346/0.70823. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67269/0.70874. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67113/0.70853. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67102/0.70770. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66909/0.70784. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66821/0.70775. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66853/0.70849. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66896/0.70931. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66473/0.71161. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66328/0.71212. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66290/0.71074. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66187/0.71067. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65669/0.71129. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65605/0.71119. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65698/0.71295. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65375/0.71351. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64902/0.71496. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65005/0.71424. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64720/0.71500. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64382/0.71909. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64464/0.71895. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64072/0.72099. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63954/0.72026. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63838/0.72208. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63583/0.72549. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63332/0.72747. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63202/0.72624. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63017/0.73140. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63030/0.73198. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62976/0.73057. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62451/0.73088. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62278/0.73418. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62097/0.73492. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61671/0.73381. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61990/0.73878. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61866/0.74013. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60988/0.73836. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61204/0.74136. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61181/0.74382. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60846/0.74777. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60695/0.74943. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60648/0.74713. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59862/0.75312. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59505/0.75208. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59865/0.74979. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58817/0.75177. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58503/0.74984. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58943/0.75808. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58409/0.75199. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58484/0.75750. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57830/0.75408. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57642/0.75989. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58415/0.76663. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57817/0.75863. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57184/0.76770. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56730/0.76576. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.56379/0.76995. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56192/0.77452. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56124/0.77216. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56360/0.76399. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55765/0.77127. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55768/0.77875. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55209/0.77467. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55274/0.77846. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54105/0.78065. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53939/0.77293. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53844/0.77884. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54238/0.78608. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69457/0.69340. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69185/0.69687. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69126/0.69935. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68961/0.70114. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.70250. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68889/0.70336. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68840/0.70376. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68734/0.70423. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68748/0.70501. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68560/0.70608. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68504/0.70776. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68312/0.70949. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68231/0.71051. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68256/0.71218. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68075/0.71449. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68094/0.71450. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68019/0.71576. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67830/0.71726. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67937/0.71753. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67709/0.71958. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67620/0.72194. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67616/0.72269. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67686/0.72054. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67499/0.72341. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67362/0.72410. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67411/0.72443. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67360/0.72647. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67136/0.72600. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67049/0.72786. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67021/0.72899. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66837/0.72999. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66727/0.73198. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66807/0.73464. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66903/0.73512. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66741/0.73429. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66645/0.73669. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66665/0.73555. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66215/0.73757. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66487/0.73965. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66323/0.74032. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66154/0.73968. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66146/0.74248. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66287/0.74324. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65966/0.74426. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65876/0.74609. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65851/0.74762. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65582/0.74717. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65598/0.75188. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65541/0.75350. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65300/0.75436. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65237/0.75834. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65168/0.75984. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65273/0.76068. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65237/0.75915. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65214/0.76214. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64952/0.76253. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64919/0.76464. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65097/0.76627. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64764/0.76550. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64759/0.76954. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64707/0.76711. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64483/0.77165. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64398/0.77247. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64331/0.77356. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64164/0.77298. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64093/0.77456. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63696/0.77864. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64218/0.78008. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63669/0.77962. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63428/0.78293. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63653/0.78344. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63437/0.78779. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63493/0.78808. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63163/0.78441. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62957/0.78931. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63104/0.78886. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63125/0.78970. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62797/0.79245. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62496/0.79401. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62866/0.79333. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62066/0.79500. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61865/0.80094. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62425/0.79617. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61858/0.79662. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61742/0.80120. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61726/0.80293. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61635/0.80431. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61266/0.80654. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61503/0.80507. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61257/0.80436. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61203/0.80750. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60589/0.81091. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61033/0.81071. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60709/0.80975. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60345/0.81483. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59772/0.81701. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.60003/0.81623. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59835/0.81820. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59762/0.81531. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59484/0.81719. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69266/0.69203. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69101/0.69178. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.69160. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69161/0.69137. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69135/0.69121. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69080/0.69101. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.69074. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69022/0.69046. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68957/0.68998. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68942/0.68942. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.68875. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68854/0.68812. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68771/0.68745. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68705/0.68679. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68684/0.68613. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68578/0.68569. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68516/0.68521. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68546/0.68495. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68402/0.68427. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68519/0.68403. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68271/0.68408. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68273/0.68438. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68365/0.68441. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68213/0.68450. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68224/0.68427. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68200/0.68430. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68143/0.68481. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68097/0.68462. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68107/0.68387. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67932/0.68464. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67902/0.68404. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67915/0.68466. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67989/0.68447. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67884/0.68532. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67723/0.68548. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67898/0.68495. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67672/0.68568. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67670/0.68591. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67526/0.68582. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67462/0.68621. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67522/0.68711. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67592/0.68645. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67457/0.68560. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67383/0.68699. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67229/0.68663. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67153/0.68856. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67329/0.68949. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66992/0.68966. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66929/0.69031. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67002/0.68980. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67049/0.69185. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66869/0.69010. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66639/0.69014. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66536/0.69109. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66412/0.69220. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66410/0.69320. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66261/0.69355. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66308/0.69436. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66170/0.69377. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65894/0.69553. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65946/0.69874. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65861/0.69801. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65578/0.69735. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65447/0.69919. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65595/0.70179. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65469/0.70050. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65417/0.70167. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65240/0.70346. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65170/0.70347. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65042/0.70329. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65080/0.70463. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64869/0.70803. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64721/0.70819. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64553/0.70727. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64158/0.71081. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64318/0.71041. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64403/0.71186. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64043/0.71407. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64043/0.71489. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63560/0.71473. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63740/0.71569. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63779/0.71724. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63687/0.71943. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63688/0.72127. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63448/0.72066. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63113/0.72357. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63078/0.72536. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62563/0.72580. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63290/0.72685. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62558/0.72694. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62142/0.72716. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61951/0.72816. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61941/0.73347. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61767/0.73239. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62070/0.73416. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61899/0.73706. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61521/0.73968. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61269/0.73778. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60750/0.74140. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61405/0.74482. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69350/0.69272. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.69134. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.69074. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69026/0.69050. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68968/0.69017. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68947/0.68993. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68842/0.68964. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68776/0.68944. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68732/0.68924. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68716/0.68893. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68558/0.68881. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68521/0.68876. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68506/0.68876. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68379/0.68867. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68480/0.68881. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68250/0.68901. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68113/0.68945. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68118/0.68967. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67899/0.68941. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67885/0.68988. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67795/0.69023. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67678/0.69140. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67641/0.69147. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67645/0.69188. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67427/0.69291. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67261/0.69385. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67113/0.69506. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67086/0.69601. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67084/0.69627. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66984/0.69676. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66597/0.69639. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66527/0.69648. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66615/0.69732. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66180/0.69871. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66061/0.69800. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66123/0.69740. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65813/0.69701. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65543/0.69712. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65348/0.69677. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65109/0.69675. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65048/0.69602. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64645/0.69727. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64313/0.69675. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64274/0.69756. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.63926/0.69721. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.63689/0.69895. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63358/0.70049. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63352/0.69989. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63228/0.69761. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63086/0.70007. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62600/0.69977. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62130/0.69999. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62156/0.70172. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.61440/0.70309. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61694/0.70244. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.61618/0.70223. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60989/0.70330. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60664/0.70398. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.59899/0.70549. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.60611/0.70450. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59827/0.70880. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59819/0.70609. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.59301/0.70773. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59288/0.71220. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.59490/0.71507. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.58501/0.71560. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.58246/0.71720. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.58386/0.71905. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.58058/0.72229. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57737/0.72784. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57507/0.72874. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.57095/0.72845. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56555/0.72979. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56946/0.73990. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56775/0.74114. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56341/0.73789. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.56279/0.74588. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55966/0.74298. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.55628/0.74931. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.55074/0.75525. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55124/0.74800. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55362/0.75968. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54290/0.76682. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54264/0.76906. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54180/0.76291. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.53600/0.76772. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.53409/0.77271. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52860/0.77379. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52085/0.78261. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.52139/0.79877. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.52453/0.79377. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52689/0.79385. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.51288/0.80042. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52064/0.80721. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51616/0.81311. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51927/0.81573. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.50863/0.81442. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51198/0.82873. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.50104/0.83390. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.50154/0.83877. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70828/0.69053. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69114/0.68972. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69012/0.68843. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69019/0.68756. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68874/0.68675. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68820/0.68589. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68769/0.68513. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68575/0.68449. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68599/0.68396. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.68649/0.68341. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68520/0.68308. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68425/0.68267. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68530/0.68249. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68405/0.68225. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68401/0.68219. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68445/0.68197. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68396/0.68167. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68345/0.68149. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68201/0.68136. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68264/0.68122. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68176/0.68129. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68200/0.68130. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68066/0.68136. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68022/0.68137. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68057/0.68109. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68095/0.68102. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67877/0.68135. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67836/0.68148. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67762/0.68143. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67779/0.68150. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67765/0.68173. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67654/0.68190. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67573/0.68175. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67676/0.68174. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67557/0.68161. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67317/0.68170. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67489/0.68156. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67406/0.68156. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67073/0.68192. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67092/0.68259. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67180/0.68287. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66792/0.68289. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66731/0.68358. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66495/0.68367. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66647/0.68414. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66509/0.68480. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66651/0.68452. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66549/0.68562. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66372/0.68528. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66303/0.68537. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65968/0.68577. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66041/0.68569. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65920/0.68699. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65815/0.68767. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65634/0.68817. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65566/0.68839. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65381/0.68946. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65191/0.69029. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65190/0.69074. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64827/0.69189. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65102/0.69163. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64963/0.69298. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65145/0.69179. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64714/0.69296. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64698/0.69163. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64804/0.69460. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64458/0.69309. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63886/0.69679. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64285/0.69681. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64231/0.69847. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63748/0.69864. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63498/0.69925. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63349/0.70094. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63491/0.70359. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63479/0.70339. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63150/0.70504. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63086/0.70575. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62880/0.70631. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62762/0.70690. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62750/0.70768. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62775/0.71121. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62426/0.71320. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62028/0.71087. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62633/0.71312. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62176/0.71605. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62502/0.71671. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62391/0.71358. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61914/0.71639. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61493/0.71781. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61502/0.71524. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61364/0.72091. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61774/0.71946. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61272/0.71968. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61160/0.72398. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60968/0.72261. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60631/0.72588. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61031/0.72748. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60443/0.72916. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59978/0.73182. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60276/0.73332. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69129/0.68854. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68989/0.68785. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68888/0.68757. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68827/0.68742. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68683/0.68748. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68649/0.68754. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68546/0.68773. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68469/0.68801. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68556/0.68835. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68464/0.68879. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68375/0.68908. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68314/0.68948. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68288/0.68993. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68247/0.69002. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68215/0.69021. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68165/0.69046. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68110/0.69096. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68113/0.69145. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68113/0.69199. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68041/0.69221. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67924/0.69282. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67880/0.69341. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67767/0.69403. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67833/0.69460. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67681/0.69532. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67618/0.69618. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67647/0.69667. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67518/0.69762. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67531/0.69775. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67504/0.69858. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67454/0.69944. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67423/0.70023. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67357/0.70095. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67303/0.70215. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67098/0.70280. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67126/0.70376. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67088/0.70467. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66886/0.70586. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66810/0.70713. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66943/0.70808. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66775/0.70885. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66730/0.70982. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66455/0.71074. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66468/0.71219. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66289/0.71350. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66332/0.71456. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66261/0.71487. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66306/0.71562. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65974/0.71733. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65845/0.71896. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65830/0.72102. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65788/0.72161. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65589/0.72216. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65482/0.72315. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65430/0.72475. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65314/0.72567. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65236/0.72773. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64967/0.72851. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65183/0.72928. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64941/0.72992. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64801/0.73040. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64648/0.73146. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64588/0.73282. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64167/0.73362. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64305/0.73531. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64433/0.73603. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64024/0.73771. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64058/0.73786. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64100/0.73849. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63351/0.73984. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63696/0.74035. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63502/0.74171. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63257/0.74260. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63578/0.74212. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63099/0.74458. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.62815/0.74611. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62862/0.74770. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62445/0.74865. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62249/0.75061. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62588/0.75067. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62571/0.75168. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62133/0.75277. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61688/0.75333. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61808/0.75495. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62084/0.75541. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61767/0.75508. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61462/0.75609. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61144/0.75584. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61475/0.75800. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61045/0.75916. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.60955/0.76015. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60863/0.76230. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60793/0.76273. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60225/0.76319. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59998/0.76613. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60312/0.76653. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59653/0.76839. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59981/0.76905. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59558/0.76870. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.59558/0.77097. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69439/0.68929. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69436/0.68940. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69436/0.68980. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69366/0.69026. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69390/0.69095. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69259/0.69206. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69344. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69039/0.69464. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69112/0.69599. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68969/0.69683. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68872/0.69756. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68845/0.69761. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68882/0.69756. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68790/0.69790. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68646/0.69827. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68591/0.69774. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68644/0.69755. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68267/0.69749. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68398/0.69757. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68235/0.69781. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68118/0.69841. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67924/0.69865. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67970/0.69999. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67767/0.69987. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67782/0.70067. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67487/0.69956. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67285/0.69866. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67226/0.70069. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67016/0.70124. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66944/0.69983. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66755/0.70313. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66436/0.70296. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.66084/0.70187. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66059/0.70390. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65734/0.70350. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65341/0.70248. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65111/0.70325. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65210/0.70564. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.64693/0.71082. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.64660/0.70987. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.64514/0.71095. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64060/0.71268. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64312/0.71163. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.63934/0.71555. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.63383/0.71986. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63410/0.71416. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.62747/0.71766. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62381/0.72265. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.62708/0.72308. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.62574/0.72447. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62124/0.72500. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.62287/0.72548. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61567/0.73042. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61239/0.73480. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.61284/0.73350. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60847/0.73393. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60467/0.73888. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60088/0.74311. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60136/0.74464. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.59941/0.74983. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59517/0.74800. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.59183/0.75228. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.58536/0.75467. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.59499/0.75580. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58531/0.75736. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58497/0.75901. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57520/0.76045. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57467/0.76552. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.57187/0.77028. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57275/0.77124. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57265/0.77566. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56498/0.77971. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.56446/0.78211. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.55277/0.78053. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55905/0.78363. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.55961/0.78838. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.54816/0.79037. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55104/0.79106. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54568/0.80027. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54232/0.79662. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.54266/0.80317. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.53249/0.81051. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.53672/0.81223. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.53644/0.80982. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53199/0.80937. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53263/0.80704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51963/0.81463. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.52563/0.81128. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.51227/0.81959. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51391/0.82916. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.51139/0.82570. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.51038/0.82901. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50482/0.83205. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50521/0.83289. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.50082/0.83566. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.48612/0.84252. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.49202/0.84227. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.49375/0.85073. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.48974/0.84926. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.48212/0.85481. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69469/0.69778. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69216/0.70086. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69159/0.70157. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69183/0.70247. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.70347. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69056/0.70415. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69086/0.70508. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68967/0.70576. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69027/0.70634. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68852/0.70786. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68910/0.70849. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68910/0.70958. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68761/0.71045. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68734/0.71178. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68694/0.71238. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68661/0.71308. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68627/0.71493. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68514/0.71441. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68535/0.71557. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68532/0.71574. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68514/0.71664. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68508/0.71731. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68233/0.71705. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68121/0.71938. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68080/0.71798. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67944/0.71925. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67958/0.72207. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68011/0.71992. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67996/0.72136. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67876/0.72291. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67489/0.72221. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67763/0.72226. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67513/0.72252. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67372/0.72359. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67165/0.72410. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66956/0.72447. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67001/0.72415. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66848/0.72522. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66727/0.72808. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66648/0.72691. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66308/0.72759. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66323/0.72795. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65937/0.72993. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65917/0.72758. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65878/0.72760. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65565/0.73019. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65197/0.72771. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64850/0.73202. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64881/0.73393. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64440/0.73257. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64543/0.73184. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64137/0.73831. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64197/0.73738. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63982/0.74049. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63714/0.73348. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63521/0.73216. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63208/0.73573. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62710/0.73352. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62873/0.73157. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62814/0.73498. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62484/0.73310. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61989/0.73002. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62039/0.73324. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61826/0.73448. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.61127/0.73949. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61291/0.73168. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60650/0.73809. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60868/0.73241. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60437/0.72810. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60079/0.72727. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60284/0.73497. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59902/0.72958. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59703/0.72575. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59306/0.73717. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59302/0.73308. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58612/0.73013. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58552/0.72601. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58370/0.72881. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57256/0.73639. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57015/0.73480. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57654/0.73556. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57624/0.73814. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.56861/0.73189. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56317/0.73328. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56207/0.73617. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55959/0.74388. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55522/0.73507. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56179/0.73677. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.55317/0.74068. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55139/0.73567. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55263/0.74498. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54806/0.74336. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54847/0.74969. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54661/0.74348. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.53630/0.75434. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53869/0.74662. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53154/0.76814. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53962/0.75569. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.53289/0.75710. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52670/0.76497. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69342/0.69746. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69290/0.69738. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69727. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69274/0.69717. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69262/0.69715. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69269/0.69707. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69259/0.69704. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69190/0.69695. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69212/0.69680. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.69640. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69204/0.69614. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.69132/0.69597. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69096/0.69567. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69120/0.69538. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68978/0.69505. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69031/0.69546. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68931/0.69578. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68837/0.69568. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68850/0.69583. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68770/0.69624. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68760/0.69632. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68741/0.69670. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68523/0.69678. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68352/0.69726. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68395/0.69709. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.69738. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68216/0.69779. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68160/0.69854. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68017/0.69830. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68058/0.69855. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67680/0.70001. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67700/0.70092. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67386/0.70134. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67463/0.70168. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67409/0.70175. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67098/0.70398. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66977/0.70412. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66788/0.70481. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66656/0.70504. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66561/0.70677. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66626/0.70792. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66112/0.70924. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65909/0.71007. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66108/0.71189. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65464/0.71309. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65394/0.71388. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65485/0.71712. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64921/0.71936. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65071/0.71925. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64651/0.72359. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.64262/0.72532. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63962/0.72898. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63892/0.73195. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63427/0.73490. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63653/0.73798. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63238/0.74198. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63490/0.74440. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63260/0.74640. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63155/0.74987. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62836/0.75229. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62214/0.75669. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62133/0.75977. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61859/0.76411. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61633/0.76698. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61564/0.77368. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61304/0.77768. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61153/0.78437. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60641/0.78850. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60541/0.79332. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60270/0.79528. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60183/0.80064. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59842/0.80044. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59693/0.80490. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59358/0.81143. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59735/0.81401. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59018/0.81520. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59023/0.82314. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59036/0.82587. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58924/0.83416. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57724/0.83836. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58418/0.83212. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.57865/0.84041. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58028/0.84465. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57464/0.85236. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57572/0.85269. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56849/0.85928. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57086/0.86316. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57382/0.86498. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56421/0.87129. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55745/0.87689. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56195/0.88501. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56075/0.88770. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.55273/0.89199. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55362/0.88443. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55309/0.89296. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.54919/0.90670. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54197/0.90395. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54010/0.90636. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54271/0.91069. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54550/0.91728. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69749/0.69092. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69246/0.69346. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69130/0.69584. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69055/0.69785. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68988/0.69983. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68901/0.70163. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68888/0.70299. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68814/0.70407. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68771/0.70513. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68720/0.70571. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68729/0.70654. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.70710. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68670/0.70787. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68555/0.70823. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68537/0.70874. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68493/0.70947. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68483/0.70999. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68357/0.71047. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68400/0.71060. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68288/0.71113. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68350/0.71208. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68208/0.71291. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68171/0.71324. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68228/0.71380. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68015/0.71434. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68003/0.71512. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67965/0.71581. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67864/0.71636. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67925/0.71723. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67700/0.71816. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67589/0.71955. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67552/0.72105. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67356/0.72291. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67420/0.72380. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67170/0.72598. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67264/0.72669. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67256/0.72797. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67163/0.72969. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66979/0.73109. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66944/0.73234. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66835/0.73364. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66731/0.73497. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66752/0.73489. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66691/0.73567. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66480/0.73802. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66351/0.73990. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66405/0.74141. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66009/0.74348. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66367/0.74481. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65879/0.74498. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66017/0.74736. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65931/0.75007. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65526/0.75136. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65513/0.75328. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65343/0.75568. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65436/0.75888. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65018/0.76126. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65290/0.76247. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64961/0.76293. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65266/0.76488. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65094/0.76587. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64670/0.76929. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64391/0.77045. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64247/0.77351. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64606/0.77429. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64419/0.77525. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63785/0.77589. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63427/0.78174. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63992/0.78282. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.63924/0.78323. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63610/0.78424. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63658/0.78751. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63325/0.79023. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63387/0.79238. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63143/0.79330. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63146/0.79522. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62957/0.79996. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62599/0.80043. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.62563/0.79854. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62543/0.80541. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.62214/0.80458. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.62301/0.80691. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61699/0.81176. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61620/0.81587. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61729/0.81671. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61269/0.81739. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.61676/0.82102. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60782/0.82271. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61114/0.82669. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.60601/0.83159. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60850/0.83052. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60265/0.83878. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60116/0.84146. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60090/0.84302. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60624/0.84262. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59585/0.84232. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59803/0.84222. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58876/0.84565. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58978/0.85197. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58908/0.85060. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69440/0.69012. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69289/0.68958. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69179/0.68957. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69075/0.68948. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69073/0.68952. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69027/0.68971. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68954/0.68997. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68916/0.69043. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68948/0.69082. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.69118. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68778/0.69179. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68790/0.69213. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.69275. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68699/0.69339. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68614/0.69428. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68458/0.69524. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68488/0.69615. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68477/0.69717. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68485/0.69848. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68313/0.69937. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68248/0.70053. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68310/0.70147. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68160/0.70268. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68005/0.70386. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68080/0.70533. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67968/0.70629. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67935/0.70736. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67843/0.70877. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67819/0.70988. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67558/0.71149. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67535/0.71263. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67403/0.71414. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67455/0.71533. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67348/0.71638. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67258/0.71738. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67059/0.71857. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67139/0.71913. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67012/0.71992. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66820/0.72111. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66840/0.72171. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66732/0.72323. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66550/0.72445. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66320/0.72633. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66393/0.72713. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66250/0.72776. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66138/0.73013. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66049/0.73054. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66084/0.73118. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65862/0.73198. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65402/0.73247. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65613/0.73373. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65350/0.73512. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65115/0.73636. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65207/0.73770. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65049/0.73781. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65017/0.73808. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64816/0.73874. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64455/0.73910. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64416/0.73732. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64008/0.73751. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64184/0.73798. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63818/0.73914. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63330/0.74107. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63632/0.74118. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62893/0.74260. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62995/0.74199. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62753/0.74366. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62404/0.74213. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62488/0.74221. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62439/0.74099. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62151/0.74122. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61945/0.74272. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61371/0.74351. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61082/0.74414. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60996/0.74482. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60828/0.74666. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60640/0.74549. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60407/0.74456. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60186/0.74384. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60075/0.74576. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.59528/0.74530. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59183/0.74576. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59189/0.74874. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58720/0.75356. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58727/0.75330. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57873/0.75252. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58146/0.75284. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57674/0.76157. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57762/0.76319. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57223/0.76605. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57097/0.76959. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56969/0.76451. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56485/0.76884. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55896/0.77509. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56094/0.77514. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56387/0.77787. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55488/0.78416. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55926/0.77913. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54458/0.78812. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54680/0.79147. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69315/0.68598. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69276/0.68579. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69237/0.68543. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69208/0.68514. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69164/0.68480. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.68446. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69165/0.68404. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69083/0.68352. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69003/0.68293. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69064/0.68238. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68915/0.68179. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68842/0.68122. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68839/0.68045. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68625/0.67943. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68658/0.67847. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68461/0.67722. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68306/0.67608. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68317/0.67481. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68096/0.67406. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67890/0.67274. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67780/0.67126. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67442/0.66969. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67291/0.66829. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66941/0.66621. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66723/0.66554. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.66456/0.66305. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66070/0.66161. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65978/0.66102. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65355/0.65674. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65649/0.65589. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65211/0.65571. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.64866/0.65333. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.64585/0.65218. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64567/0.65129. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.64120/0.64727. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63999/0.64839. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63381/0.64608. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63173/0.64693. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.62672/0.64590. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.62969/0.63823. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.62245/0.64322. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.61814/0.64197. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62067/0.63977. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.61521/0.64067. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61030/0.63778. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.61049/0.64039. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.60790/0.64204. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.60531/0.63840. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.59968/0.64478. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.59955/0.64863. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.59472/0.64792. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.59306/0.64907. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.58815/0.64118. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.58772/0.64930. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.58336/0.65038. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.57957/0.65799. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.57580/0.65957. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.57535/0.66573. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.57679/0.66384. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.57447/0.66517. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.56407/0.66730. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.55593/0.67317. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.56058/0.67638. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.54910/0.67334. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.55120/0.68203. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.55718/0.68385. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.54613/0.69575. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.54330/0.69163. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.54176/0.69820. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.54030/0.70845. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.53234/0.71861. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.52774/0.71875. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.53140/0.71829. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.52559/0.72971. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.52652/0.72320. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.52249/0.72792. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.50997/0.73351. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.51449/0.74087. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.51054/0.74363. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.51116/0.76139. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.50450/0.76118. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.49351/0.75583. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.49927/0.76069. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.48804/0.77045. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.49487/0.76792. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.48954/0.76474. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.49176/0.77140. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.48318/0.77744. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.48355/0.78888. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.47766/0.79715. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.47643/0.78707. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.47491/0.79610. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.46722/0.79937. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.46259/0.80837. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.45452/0.80720. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.46298/0.81218. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.47084/0.81026. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.45196/0.81680. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.44530/0.82408. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.44029/0.83106. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69301/0.69180. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69278/0.69151. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.69120. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69165/0.69080. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69111/0.69041. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69114/0.69001. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.68952. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69032/0.68912. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68908/0.68843. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68828/0.68778. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68724/0.68710. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68651/0.68639. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68624/0.68584. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68487/0.68550. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68455/0.68513. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68451/0.68463. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68261/0.68438. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68172/0.68430. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68071/0.68471. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68013/0.68489. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67775/0.68499. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67886/0.68512. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67823/0.68547. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67679/0.68620. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67626/0.68697. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67472/0.68766. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67412/0.68848. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67163/0.68936. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67249/0.69046. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67196/0.69136. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66944/0.69266. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66804/0.69448. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66525/0.69603. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66457/0.69668. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66429/0.69866. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66122/0.70090. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65853/0.70234. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65884/0.70405. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65772/0.70598. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65740/0.70770. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65621/0.70891. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65418/0.71064. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65035/0.71275. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65088/0.71608. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64685/0.71759. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64795/0.71842. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64699/0.71979. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.64453/0.72119. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64285/0.72190. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63872/0.72460. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64025/0.72647. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63756/0.72867. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63856/0.72980. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63361/0.73286. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63346/0.73453. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63067/0.73711. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.63252/0.73754. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62988/0.73666. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63013/0.73738. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62679/0.73945. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62230/0.74342. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62287/0.74501. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61759/0.74691. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61607/0.74602. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62158/0.74920. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61154/0.74928. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61441/0.74904. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61093/0.75203. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60800/0.75234. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61092/0.75200. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60734/0.75399. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.60870/0.75470. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60447/0.75356. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60085/0.75769. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59909/0.76003. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60031/0.76243. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59733/0.76400. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59678/0.76243. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58688/0.76175. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59522/0.76575. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59028/0.76448. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58447/0.76723. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59024/0.76875. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58815/0.76921. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.57939/0.76920. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58243/0.77044. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57938/0.77160. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57589/0.77046. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57774/0.77388. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58360/0.77144. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57056/0.76880. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56730/0.77157. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56756/0.77231. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56447/0.77455. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56638/0.77313. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55326/0.77626. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55839/0.77948. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55830/0.77913. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55227/0.78087. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55103/0.78298. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69483/0.69381. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69401/0.69278. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69289/0.69176. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69176/0.69103. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69123/0.69050. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69135/0.69036. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69006/0.69026. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68987/0.69014. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68995/0.69015. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68918/0.69009. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68899/0.68988. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68807/0.68978. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68825/0.68979. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68737/0.68990. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68700/0.68966. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68599/0.68941. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68500/0.68944. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68425/0.68940. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68369/0.68956. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68330/0.68933. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68280/0.68946. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68095/0.68946. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68186/0.68970. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68119/0.68989. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67967/0.69022. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67902/0.69055. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67895/0.69000. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67954/0.68996. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67698/0.69007. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67591/0.69015. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67532/0.69001. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67616/0.68973. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67409/0.68968. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67270/0.69008. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67261/0.69075. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67126/0.69096. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67144/0.69128. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66753/0.69211. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66802/0.69234. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66968/0.69285. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66551/0.69353. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66471/0.69369. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66486/0.69330. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66230/0.69494. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66036/0.69587. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66077/0.69624. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65998/0.69778. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65947/0.69816. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65724/0.70008. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65435/0.70006. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65612/0.70032. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65197/0.70016. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64905/0.70242. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64801/0.70378. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64588/0.70450. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64699/0.70720. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64623/0.70618. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64413/0.70552. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64262/0.70904. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64238/0.70919. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63550/0.70951. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63753/0.70960. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.63575/0.70868. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63179/0.71142. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63105/0.71386. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62887/0.71636. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63155/0.71509. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62393/0.71761. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62542/0.71771. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62464/0.71797. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61813/0.71872. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61851/0.71933. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62137/0.72293. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61888/0.72214. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61045/0.72474. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61465/0.72526. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61769/0.72646. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60873/0.72850. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61415/0.72961. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61015/0.72957. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.60132/0.73433. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.60441/0.73281. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.60208/0.73489. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59482/0.73673. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59483/0.73378. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59224/0.73815. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.59853/0.73848. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59607/0.73775. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59054/0.74335. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58869/0.74404. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57841/0.74594. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.58653/0.74912. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57354/0.75094. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58449/0.75098. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57770/0.75562. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58246/0.75323. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.57431/0.75747. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.57040/0.75670. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.57538/0.76018. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56935/0.75682. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69222/0.69318. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69141/0.69319. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69077/0.69319. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69315. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68948/0.69324. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68943/0.69355. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68803/0.69367. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68800/0.69410. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68661/0.69486. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68603/0.69579. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68513/0.69697. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68578/0.69796. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68353/0.69955. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68296/0.70072. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68037/0.70189. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67954/0.70314. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68115/0.70438. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68081/0.70512. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67736/0.70646. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67733/0.70771. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67744/0.70830. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67527/0.70957. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67361/0.71066. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67280/0.71145. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67294/0.71260. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67100/0.71337. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66885/0.71482. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67007/0.71636. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66851/0.71743. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66610/0.71865. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66416/0.71946. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66342/0.71976. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66189/0.72136. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66088/0.72217. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65887/0.72259. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65757/0.72445. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65775/0.72574. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65647/0.72595. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65349/0.72706. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65162/0.72755. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65068/0.72869. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64804/0.72923. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64263/0.72999. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64645/0.73154. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64481/0.73276. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64145/0.73292. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64267/0.73476. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.63732/0.73587. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63777/0.73538. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63634/0.73541. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63347/0.73647. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63020/0.73666. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62556/0.73771. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62840/0.73774. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62743/0.73768. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62405/0.74098. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62271/0.74250. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62271/0.74218. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.61915/0.74157. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61725/0.74370. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61392/0.74355. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61479/0.74338. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61286/0.74776. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61034/0.75211. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61091/0.74771. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60593/0.75150. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60571/0.75132. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60528/0.75134. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60113/0.75143. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59744/0.75636. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59371/0.75928. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59094/0.76225. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59509/0.76538. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59303/0.76245. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58832/0.76156. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58704/0.75825. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58596/0.76269. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57789/0.76966. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57728/0.76225. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57026/0.76644. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.57670/0.77701. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57549/0.76710. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57012/0.76921. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.56736/0.77478. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56768/0.78297. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56158/0.77595. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56206/0.77560. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55360/0.77655. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55766/0.77752. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55721/0.77431. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55467/0.78291. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54636/0.78878. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.54448/0.79126. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54043/0.79041. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54176/0.79078. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54954/0.79866. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53076/0.79730. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53303/0.80288. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53014/0.80356. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52504/0.81132. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69511/0.68436. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.68453. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.68486. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69007/0.68496. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68847/0.68578. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68778/0.68604. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68637/0.68658. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68630/0.68724. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68449/0.68828. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68474/0.68826. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68366/0.68819. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68337/0.68848. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68207/0.68861. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68234/0.68924. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68020/0.68834. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68027/0.68891. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67945/0.68832. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67775/0.68860. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67792/0.68850. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67721/0.68912. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67705/0.68878. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67503/0.68851. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67511/0.68923. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67273/0.68852. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67336/0.68835. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67114/0.68843. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67186/0.68805. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66899/0.68864. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66885/0.68787. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66710/0.68742. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66762/0.68800. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66365/0.68792. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66125/0.68811. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66070/0.68847. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66091/0.68750. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65776/0.68712. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65800/0.68740. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65498/0.68867. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65547/0.68897. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65258/0.68699. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65074/0.68700. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64712/0.68780. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64597/0.68800. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64563/0.68821. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64429/0.68817. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64185/0.68857. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.63741/0.68908. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63736/0.68879. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63552/0.69030. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63284/0.69194. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.62941/0.69330. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62527/0.69397. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.62749/0.69590. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.62612/0.69703. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.62494/0.69623. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62378/0.69818. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.61886/0.70002. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61722/0.70391. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61327/0.70653. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61397/0.70583. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61384/0.70877. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61253/0.71142. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.60916/0.71120. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60536/0.71300. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60184/0.71748. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60445/0.72270. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.59682/0.72670. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59882/0.72599. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59697/0.72600. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59505/0.72890. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59257/0.73083. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.58973/0.73564. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.58972/0.73730. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58520/0.73981. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58180/0.74611. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58230/0.74449. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58385/0.74304. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.57720/0.75067. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57254/0.75412. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57209/0.75895. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57361/0.75728. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57266/0.76689. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57034/0.76115. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56665/0.76717. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.55990/0.77325. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56157/0.77251. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.56036/0.78185. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55781/0.77684. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55529/0.78243. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55116/0.78916. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55494/0.79108. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.54458/0.79584. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54092/0.80759. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54259/0.79982. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53940/0.81081. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53415/0.81081. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53239/0.81546. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.53321/0.81894. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52660/0.82069. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52378/0.82353. Took 0.08 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69463/0.68689. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69144/0.68392. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69083/0.68389. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69077/0.68499. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68967/0.68520. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68961/0.68603. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68842/0.68683. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68829/0.68671. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68755/0.68711. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68781/0.68754. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68675/0.68802. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68655/0.68824. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68593/0.68850. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68610/0.68864. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68505/0.68953. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68498/0.69003. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68344/0.69019. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68412/0.68993. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68336/0.69062. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68195/0.69022. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68112/0.69035. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68194/0.69058. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67924/0.69048. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68021/0.69104. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67879/0.69195. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67840/0.69188. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67769/0.69228. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67730/0.69148. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67473/0.69129. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67516/0.69247. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67437/0.69244. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67423/0.69216. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67201/0.69315. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67162/0.69054. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67101/0.69123. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67100/0.69123. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66803/0.68996. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66803/0.69050. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66768/0.69113. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66408/0.69078. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66356/0.69013. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66284/0.69186. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66365/0.69102. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66199/0.69051. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66052/0.69336. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65690/0.69315. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65534/0.69077. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65483/0.69008. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65344/0.69037. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65017/0.69155. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64881/0.69320. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64671/0.69247. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64515/0.69290. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64494/0.69283. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64210/0.69793. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64152/0.69327. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63842/0.69258. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63434/0.69393. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63367/0.69394. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63348/0.69307. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63271/0.69654. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62859/0.69672. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62281/0.69951. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62451/0.70078. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62495/0.70452. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61676/0.70639. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61794/0.70802. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61475/0.70751. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61298/0.71195. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60998/0.71686. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60645/0.71579. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60718/0.72051. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60333/0.71896. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59952/0.71722. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60392/0.73156. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59796/0.72574. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59287/0.71941. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59415/0.72382. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58993/0.73298. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58263/0.73234. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57903/0.73015. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58338/0.73581. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57877/0.73744. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57530/0.73886. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57149/0.74938. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56972/0.74432. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57464/0.74360. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56507/0.75159. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.57228/0.75557. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56730/0.75038. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55922/0.75310. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55514/0.75457. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56005/0.75022. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55333/0.76814. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55122/0.76321. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.55000/0.77231. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54210/0.76865. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54403/0.77331. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54112/0.78102. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54386/0.77235. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69122/0.68445. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68812/0.68408. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68815/0.68409. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68654/0.68430. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68559/0.68467. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68613/0.68520. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68551/0.68559. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68339/0.68626. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68350/0.68663. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68385/0.68699. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68343/0.68728. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68253/0.68733. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68189/0.68775. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68172/0.68772. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68124/0.68753. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67912/0.68783. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67835/0.68768. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67934/0.68702. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67726/0.68682. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67667/0.68664. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67707/0.68671. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67574/0.68705. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67431/0.68700. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67385/0.68639. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67087/0.68603. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67050/0.68499. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67017/0.68475. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66918/0.68432. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66664/0.68429. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66624/0.68412. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66541/0.68378. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66277/0.68445. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66177/0.68503. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66054/0.68457. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66005/0.68353. Took 0.11 sec\n",
      "Epoch 35, Loss(train/val) 0.65743/0.68394. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65738/0.68525. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65516/0.68397. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65336/0.68506. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65324/0.68502. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65121/0.68568. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64859/0.68501. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64823/0.68329. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64774/0.68480. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64231/0.68405. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64414/0.68396. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64074/0.68463. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63584/0.68252. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.63325/0.68402. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63798/0.68429. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63663/0.68374. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63594/0.68409. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63017/0.68193. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.62936/0.68407. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62858/0.68404. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62730/0.68206. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62505/0.68052. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.62269/0.68118. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62193/0.68354. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62423/0.68162. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62066/0.68075. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61760/0.68077. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61752/0.68165. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61313/0.68178. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61443/0.67997. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.61314/0.68057. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60824/0.67985. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60581/0.68377. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60773/0.68100. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60550/0.67922. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60430/0.68190. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60398/0.68066. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60378/0.67841. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59765/0.68052. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59612/0.68021. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59504/0.68052. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59909/0.67589. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.59285/0.67549. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58659/0.67694. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58640/0.67988. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58588/0.67842. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58335/0.67887. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58701/0.67974. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58199/0.67239. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.57829/0.67320. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57898/0.67518. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57530/0.67676. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57392/0.67957. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57355/0.67545. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57527/0.67598. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56736/0.67487. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57072/0.67364. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56214/0.67352. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56220/0.67198. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55857/0.67586. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55516/0.67864. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55259/0.67295. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.55202/0.67811. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54593/0.68198. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54738/0.67930. Took 0.08 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.68878/0.69273. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68694/0.69383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68654/0.69369. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68614/0.69291. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68586/0.69201. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.68560/0.69104. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68475/0.69026. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68439/0.68965. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68426/0.68872. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68302/0.68800. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68322/0.68764. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68333/0.68726. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68291/0.68651. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68253/0.68660. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68219/0.68630. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68260/0.68682. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68164/0.68646. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68135/0.68691. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68107/0.68644. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68026/0.68662. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68088/0.68716. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67977/0.68627. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68021/0.68640. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.68010/0.68658. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67972/0.68702. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67887/0.68659. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67873/0.68660. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67773/0.68641. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67739/0.68727. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67751/0.68742. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67732/0.68731. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67689/0.68828. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67521/0.68768. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67505/0.68812. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67500/0.68772. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67422/0.68851. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67263/0.68926. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67322/0.68989. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67185/0.68987. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67150/0.69011. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66928/0.69051. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66826/0.69104. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66826/0.69140. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66714/0.69058. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66499/0.69195. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66477/0.69389. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66256/0.69481. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66398/0.69538. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66067/0.69605. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66006/0.69742. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65961/0.69699. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65697/0.69698. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65439/0.69801. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65696/0.69874. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65321/0.70024. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65145/0.70159. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65131/0.70280. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64890/0.70493. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64492/0.70645. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64629/0.70645. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64470/0.70795. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64107/0.70798. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64139/0.71041. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64139/0.70949. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64061/0.70881. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63859/0.71139. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63849/0.71396. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63741/0.71325. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63817/0.71534. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63506/0.71534. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63578/0.71694. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63385/0.71603. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63252/0.71895. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62925/0.71775. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62794/0.72125. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63057/0.72031. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62717/0.72146. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62172/0.72264. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62160/0.72355. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.62217/0.72396. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62286/0.72512. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62038/0.72706. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61822/0.72792. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61878/0.72840. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.61438/0.73033. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.61854/0.73058. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61309/0.73318. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61410/0.73730. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60722/0.73918. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60790/0.73951. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60978/0.74053. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60972/0.74518. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.60970/0.74487. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60666/0.74306. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.59981/0.74525. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.60275/0.74453. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60095/0.75040. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.59639/0.75448. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59544/0.75480. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59755/0.75682. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69005/0.69715. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68679/0.70117. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68657/0.70323. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68634/0.70369. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68597/0.70400. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68596/0.70425. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68498/0.70471. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68473/0.70543. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68436/0.70598. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68426/0.70647. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68394/0.70674. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68301/0.70743. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68326/0.70802. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68247/0.70870. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68114/0.70960. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68158/0.71042. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68119/0.71050. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68063/0.71119. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68066/0.71095. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67980/0.71183. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68016/0.71191. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67876/0.71316. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67735/0.71290. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67730/0.71373. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67718/0.71448. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67544/0.71567. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67561/0.71574. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67468/0.71543. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67366/0.71587. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67373/0.71561. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67383/0.71672. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67285/0.71633. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67127/0.71565. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67022/0.71601. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66964/0.71718. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66696/0.71708. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66578/0.71635. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66521/0.71734. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66413/0.71819. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66251/0.71776. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66112/0.71668. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65751/0.71754. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65789/0.71778. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65691/0.71905. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65543/0.71864. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65255/0.71856. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65011/0.71796. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65057/0.71871. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64704/0.71617. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64594/0.71478. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64583/0.71668. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64230/0.71698. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64169/0.71684. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63944/0.71914. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63895/0.71404. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.63398/0.71401. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63262/0.71151. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63283/0.71154. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63241/0.71249. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62888/0.71313. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62705/0.70976. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62765/0.71286. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.62165/0.71186. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62018/0.71094. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62103/0.71725. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61335/0.71281. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61424/0.71994. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61560/0.72160. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61003/0.72180. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61020/0.72293. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.60783/0.72678. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60921/0.72579. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60307/0.72218. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.60475/0.73031. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60336/0.72836. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59922/0.73766. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59750/0.73678. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59914/0.73520. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59416/0.74075. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59513/0.73227. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58544/0.74215. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58363/0.75238. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58537/0.75584. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57905/0.75722. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58249/0.76634. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58237/0.75870. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57463/0.76576. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57679/0.76714. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.57107/0.77523. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57276/0.77574. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56852/0.77748. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56938/0.78347. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56477/0.79554. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56186/0.79383. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56411/0.79290. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55798/0.80281. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56514/0.80453. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55422/0.81290. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55162/0.81322. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55368/0.81288. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69007/0.68832. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68810/0.68797. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68810/0.68810. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68715/0.68786. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68633/0.68764. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68536/0.68748. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68485/0.68748. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68486/0.68750. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68412/0.68721. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68399/0.68702. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68217/0.68681. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68234/0.68656. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68275/0.68662. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68101/0.68621. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68112/0.68619. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67981/0.68572. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67922/0.68542. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67966/0.68585. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67952/0.68562. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67797/0.68543. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67704/0.68542. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67763/0.68565. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67890/0.68579. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67687/0.68554. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67681/0.68566. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67533/0.68549. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67608/0.68599. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67461/0.68653. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67549/0.68681. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67432/0.68684. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67312/0.68670. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67348/0.68721. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67159/0.68800. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67185/0.68840. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66962/0.68904. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66966/0.68922. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67106/0.68983. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66959/0.69033. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66656/0.69064. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66700/0.69147. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66562/0.69156. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66562/0.69226. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66467/0.69236. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66430/0.69290. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66408/0.69400. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66408/0.69462. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66075/0.69563. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66029/0.69653. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65873/0.69849. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65837/0.69865. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65794/0.70053. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65522/0.70163. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65584/0.70407. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65356/0.70470. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65125/0.70720. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65003/0.70967. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64981/0.70953. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64849/0.71162. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64808/0.71195. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64468/0.71570. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64346/0.71690. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64268/0.71818. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64371/0.72256. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63983/0.72680. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63955/0.72715. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63684/0.72812. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63618/0.73349. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63514/0.73645. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63399/0.73675. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63081/0.73882. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62990/0.74154. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62794/0.74535. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62573/0.74925. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.62673/0.74913. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62664/0.75064. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62342/0.75253. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.62190/0.75473. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62051/0.75886. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62085/0.75757. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61748/0.75660. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61720/0.76268. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61169/0.76369. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61184/0.76426. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61497/0.76855. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61623/0.76603. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60703/0.76959. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60381/0.77327. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60824/0.77316. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.60683/0.77745. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.60268/0.78020. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60396/0.77585. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.60194/0.78125. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59306/0.78543. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59331/0.78422. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60020/0.78378. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59595/0.78990. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59480/0.79126. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58771/0.79281. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.59287/0.79071. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.59430/0.79264. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69138/0.69524. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.69504. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69477. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69159/0.69464. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69113/0.69451. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69064/0.69442. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69031/0.69438. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68966/0.69436. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68894/0.69428. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68853/0.69436. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68840/0.69449. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68828/0.69439. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68738/0.69438. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68706/0.69417. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68660/0.69397. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68471/0.69325. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68497/0.69254. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68409/0.69193. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68306/0.69131. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68035/0.68998. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67953/0.68890. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67831/0.68794. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67660/0.68656. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67439/0.68563. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67228/0.68460. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66953/0.68322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66850/0.68207. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66633/0.68125. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66219/0.67951. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65919/0.67899. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65542/0.67791. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.65690/0.67768. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.65235/0.67708. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.65342/0.67703. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.64823/0.67810. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64449/0.67788. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.64210/0.67908. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.64342/0.68124. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.63870/0.68201. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63621/0.68662. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63411/0.68521. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.63934/0.68578. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63247/0.68885. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.62857/0.69212. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62802/0.69050. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62803/0.69499. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62380/0.69787. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.62524/0.69899. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.62536/0.70239. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.61752/0.70370. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61631/0.70429. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61474/0.70787. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.61174/0.70969. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.61350/0.71129. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60772/0.71400. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.60736/0.71808. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60498/0.72126. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.60102/0.72100. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60165/0.72726. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.60020/0.72952. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.59364/0.73215. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.59733/0.73104. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.59191/0.73847. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58818/0.73878. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.59179/0.73715. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58564/0.74737. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58461/0.74546. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.58349/0.74015. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.58344/0.75129. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57351/0.75690. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58017/0.76174. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57804/0.75700. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56849/0.76283. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.57380/0.76435. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.56917/0.76832. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56458/0.76564. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56421/0.77815. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56383/0.77794. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.55795/0.77802. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.55687/0.77670. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.55911/0.77386. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.54836/0.78098. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55463/0.79060. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54828/0.79239. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.54168/0.79591. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54307/0.80616. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.53999/0.81016. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54057/0.82110. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.53979/0.81770. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53986/0.80584. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.52970/0.82558. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.52507/0.83549. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52908/0.81771. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.51592/0.83251. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.51702/0.83361. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.51871/0.83377. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51611/0.83259. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.51606/0.83786. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51098/0.84222. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.50510/0.85410. Took 0.08 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69183/0.69391. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69089/0.69452. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69024/0.69520. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69024/0.69583. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68980/0.69649. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68887/0.69700. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68922/0.69748. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68844/0.69795. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68709/0.69839. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68679/0.69879. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68639/0.69892. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68650/0.69896. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68524/0.69959. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68425/0.69996. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68315/0.70022. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68193/0.70141. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68219/0.70217. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67978/0.70321. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67928/0.70469. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67936/0.70492. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67847/0.70608. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67576/0.70798. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67481/0.70897. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67448/0.71153. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67317/0.71237. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67460/0.71312. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67211/0.71504. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67140/0.71735. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67095/0.71808. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66815/0.72068. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66895/0.72189. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66777/0.72277. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66528/0.72413. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66411/0.72698. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66488/0.72706. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66137/0.73016. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65836/0.73103. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65947/0.73349. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65836/0.73713. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65708/0.73881. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65509/0.73773. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65693/0.74002. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65152/0.74306. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64852/0.74357. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64988/0.74590. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64912/0.74614. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64522/0.74834. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64486/0.75241. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64475/0.75175. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64589/0.74993. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64249/0.75377. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63992/0.75388. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63880/0.75609. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63772/0.75770. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63760/0.75943. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63752/0.76043. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63350/0.75840. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63424/0.76339. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63138/0.76456. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63064/0.76507. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62765/0.76679. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62509/0.76653. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62522/0.77208. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62113/0.76827. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61909/0.76921. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62010/0.77414. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61565/0.77502. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61286/0.77849. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.60761/0.77638. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61158/0.77803. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60968/0.78029. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60830/0.77938. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60293/0.77911. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60546/0.77833. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60369/0.78222. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.60363/0.78300. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59575/0.78325. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59344/0.78709. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58999/0.78570. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58963/0.78856. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59098/0.78824. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58837/0.78488. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58633/0.78969. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58471/0.79069. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58172/0.79159. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57523/0.79548. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57279/0.79703. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57398/0.79414. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57055/0.79631. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56803/0.80008. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.56475/0.80725. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56921/0.80437. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.56536/0.80118. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56200/0.80083. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55642/0.80418. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54953/0.81040. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55494/0.80847. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54861/0.81074. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.54185/0.81767. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54506/0.81554. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69737/0.69347. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69274/0.69339. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69368. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69134/0.69401. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69143/0.69412. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69104/0.69422. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69114/0.69419. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69019/0.69419. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68988/0.69405. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68982/0.69381. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68940/0.69361. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68981/0.69349. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68862/0.69346. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68837/0.69336. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68818/0.69326. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68725/0.69325. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68649/0.69300. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68579/0.69285. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68580/0.69296. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68490/0.69302. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68406/0.69292. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68245/0.69316. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68248/0.69335. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68243/0.69334. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68192/0.69363. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67984/0.69428. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68050/0.69468. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68090/0.69491. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68039/0.69471. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67834/0.69487. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67904/0.69489. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67748/0.69475. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67830/0.69506. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67668/0.69543. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67349/0.69533. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67338/0.69536. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67296/0.69544. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67269/0.69558. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67085/0.69572. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67115/0.69558. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67085/0.69580. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66671/0.69528. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66994/0.69552. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66912/0.69531. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66666/0.69580. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66492/0.69663. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66178/0.69724. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66120/0.69704. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66225/0.69614. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65761/0.69850. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65969/0.69691. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65910/0.69712. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65379/0.69828. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65410/0.69894. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65448/0.69984. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65108/0.70034. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65233/0.70067. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64980/0.70123. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64996/0.70065. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64898/0.70117. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64568/0.70258. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64486/0.70234. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64103/0.70386. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64088/0.70272. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63868/0.70495. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63868/0.70600. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63335/0.70495. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63825/0.70499. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63417/0.70614. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63262/0.70859. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.63043/0.70724. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62784/0.71010. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63090/0.71154. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62272/0.71201. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62405/0.71151. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62172/0.71090. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62362/0.70979. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.61563/0.71506. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61441/0.71267. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.61247/0.71312. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61517/0.71390. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61075/0.71722. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60679/0.71712. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60562/0.71657. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60783/0.71651. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.60162/0.72047. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60412/0.71918. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60046/0.71515. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59439/0.72017. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.59514/0.72194. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59747/0.72108. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59343/0.72274. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.58578/0.72208. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58694/0.72488. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58569/0.71611. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58106/0.72198. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.57993/0.72017. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58113/0.72490. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57913/0.72006. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57853/0.72639. Took 0.08 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69289/0.69291. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69216/0.69285. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69131/0.69311. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69116/0.69356. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69111/0.69413. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69099/0.69461. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.69522. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68983/0.69580. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69026/0.69635. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68956/0.69708. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68875/0.69778. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68875/0.69840. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68902/0.69871. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68730/0.69945. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68721/0.70043. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68736/0.70122. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68625/0.70155. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68572/0.70271. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68585/0.70372. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68559/0.70423. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68479/0.70492. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68471/0.70520. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68419/0.70544. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68423/0.70588. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68353/0.70613. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68231/0.70655. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68171/0.70667. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68100/0.70744. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68051/0.70813. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67932/0.70830. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68049/0.70796. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67918/0.70865. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67821/0.70912. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67559/0.70983. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67493/0.70928. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67303/0.70939. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67234/0.70962. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67079/0.71022. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67060/0.70980. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66966/0.70945. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66652/0.70961. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66506/0.71099. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66138/0.71116. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66162/0.71139. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65954/0.71079. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65537/0.71117. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65109/0.71034. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64913/0.70937. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64912/0.71081. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64644/0.71164. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64250/0.71060. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64178/0.71086. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63927/0.70899. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63820/0.71181. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63715/0.71436. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63192/0.71401. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62851/0.71436. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63200/0.71466. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62814/0.71501. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62888/0.71834. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62553/0.71818. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62320/0.71741. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62307/0.71646. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.61587/0.71865. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61848/0.71956. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61319/0.72216. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.61528/0.72380. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61198/0.72394. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60809/0.72739. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60410/0.72809. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60975/0.72625. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.60028/0.73185. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.60463/0.73598. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59778/0.73455. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60099/0.73544. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59672/0.73766. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.59081/0.74123. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59281/0.74133. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58811/0.74481. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58518/0.74424. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58346/0.74826. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58198/0.74963. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58050/0.74785. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57844/0.75503. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.57546/0.75669. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.56748/0.75753. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56716/0.75989. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57665/0.75870. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56573/0.76159. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.56499/0.76315. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.56183/0.76425. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.56268/0.76510. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55983/0.76960. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55147/0.77536. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.55319/0.78043. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55351/0.78019. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55077/0.77924. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55199/0.77697. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54704/0.78199. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.54096/0.79055. Took 0.08 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69336/0.69945. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69124/0.69816. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69087/0.69756. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69003/0.69685. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69020/0.69731. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.69699. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.68960/0.69644. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68797/0.69667. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68793/0.69585. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68690/0.69562. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68645/0.69623. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68629/0.69555. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68591/0.69456. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68478/0.69518. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68564/0.69485. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68404/0.69555. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68492/0.69519. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68363/0.69439. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68038/0.69455. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68068/0.69411. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67914/0.69477. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67975/0.69498. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67862/0.69433. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67580/0.69575. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67582/0.69451. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67743/0.69485. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67422/0.69558. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67356/0.69418. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67386/0.69440. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67072/0.69391. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67199/0.69338. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67032/0.69426. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67081/0.69457. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66515/0.69376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66601/0.69353. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66493/0.69285. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66186/0.69194. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66252/0.69237. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65994/0.69328. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65866/0.69217. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65624/0.69483. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65419/0.69515. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65409/0.69438. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65115/0.69560. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64815/0.69674. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64683/0.69716. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64458/0.69708. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64547/0.69968. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64136/0.70066. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63913/0.70112. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63919/0.70259. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63575/0.70722. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63167/0.70700. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63138/0.70971. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63250/0.70855. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62679/0.71036. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62548/0.71384. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.62524/0.71411. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62164/0.71473. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61849/0.71683. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61973/0.71709. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61697/0.71685. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61904/0.72030. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61271/0.72127. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60518/0.72133. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60313/0.72612. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.60531/0.72566. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60416/0.72447. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60315/0.72933. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60087/0.72707. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60343/0.72847. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59282/0.73294. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59406/0.72927. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59201/0.72874. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58666/0.73196. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.59061/0.73424. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58672/0.73323. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.58193/0.73181. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57747/0.73934. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57888/0.73809. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.58043/0.73752. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57342/0.73911. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57272/0.73941. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57461/0.73932. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.56431/0.74000. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.56432/0.73948. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56602/0.74196. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.56126/0.74003. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56369/0.74495. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.55664/0.74662. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.55565/0.74619. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55383/0.74701. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54879/0.75467. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54827/0.75286. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.54020/0.75254. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54071/0.75154. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54220/0.75023. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53837/0.75250. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54122/0.75755. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53272/0.75686. Took 0.09 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69484/0.69169. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69177/0.69406. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68988/0.69347. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68881/0.69305. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68815/0.69303. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68689/0.69206. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68632/0.69165. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68616/0.69136. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68477/0.69082. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68404/0.69019. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68366/0.68972. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68292/0.68887. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68280/0.68826. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68088/0.68859. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68086/0.68844. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68009/0.68788. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.67976/0.68687. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67881/0.68752. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67765/0.68676. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67600/0.68620. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67607/0.68554. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67426/0.68447. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67477/0.68488. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67202/0.68434. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67434/0.68424. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67133/0.68456. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67005/0.68462. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.66904/0.68410. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66939/0.68495. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66708/0.68448. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66710/0.68362. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66453/0.68487. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66395/0.68345. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66338/0.68422. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66037/0.68579. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66030/0.68616. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65953/0.68780. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65839/0.68734. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65623/0.68967. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65385/0.68967. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65369/0.69019. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65345/0.69140. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64734/0.69245. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64590/0.69359. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65185/0.69275. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64729/0.69595. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.64467/0.69784. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64178/0.69912. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64286/0.70189. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63661/0.70348. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63972/0.70473. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63257/0.70714. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.62965/0.70970. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63198/0.71376. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62703/0.71763. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63054/0.71501. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62569/0.71606. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62382/0.71547. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.62185/0.72173. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.61663/0.72323. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61559/0.72779. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61212/0.72523. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.61548/0.72921. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60822/0.73215. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.60837/0.73176. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60628/0.73032. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60042/0.74005. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.59818/0.74480. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59779/0.74135. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59866/0.74480. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59381/0.74389. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59111/0.74583. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.58916/0.75163. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.59116/0.74452. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59175/0.75091. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58430/0.74741. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.58536/0.75151. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58267/0.75000. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58052/0.75971. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57624/0.75779. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57792/0.75669. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57531/0.75788. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56833/0.75837. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.57012/0.76416. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56575/0.77787. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55865/0.77320. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.55423/0.77904. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55991/0.77857. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.55376/0.78128. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55238/0.77149. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55283/0.78757. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55080/0.78163. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.54806/0.78547. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.54451/0.79554. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54475/0.78829. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54028/0.79399. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.53685/0.80051. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52856/0.79795. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53285/0.80532. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53514/0.80661. Took 0.09 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69415. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69382/0.69353. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69232/0.69310. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69180/0.69284. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69127/0.69243. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.69212. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68989/0.69182. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68976/0.69155. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68981/0.69124. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68908/0.69095. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68791/0.69062. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.69034. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68771/0.68994. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68633/0.68955. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68703/0.68917. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68630/0.68880. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68501/0.68846. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68433/0.68789. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68513/0.68723. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68395/0.68660. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68405/0.68614. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68188/0.68558. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68276/0.68525. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68111/0.68466. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68065/0.68453. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68094/0.68426. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67793/0.68386. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67871/0.68343. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67592/0.68317. Took 0.11 sec\n",
      "Epoch 29, Loss(train/val) 0.67488/0.68295. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67570/0.68302. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67706/0.68252. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67304/0.68215. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67364/0.68197. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67018/0.68193. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66767/0.68192. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66645/0.68171. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66675/0.68195. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66478/0.68260. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66425/0.68248. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66244/0.68328. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66463/0.68404. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65858/0.68564. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65697/0.68488. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65775/0.68663. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65232/0.68780. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65448/0.68725. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65152/0.68802. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64979/0.69190. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64941/0.69185. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64498/0.69517. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64563/0.69629. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64138/0.69821. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63965/0.69924. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63903/0.70243. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63851/0.70332. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63558/0.70717. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63585/0.70706. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63043/0.70786. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63350/0.70855. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62914/0.71279. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63214/0.70987. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62662/0.71302. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62418/0.71215. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62493/0.71820. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.61623/0.71742. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61822/0.71719. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61495/0.72370. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61098/0.72341. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61633/0.72196. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61169/0.72895. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60608/0.73053. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61265/0.72883. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60559/0.72759. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60164/0.73696. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60309/0.74185. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60277/0.73919. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60005/0.73408. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.59509/0.73928. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59171/0.73746. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59362/0.73655. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58691/0.74052. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58965/0.74038. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58480/0.74575. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58360/0.74766. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58124/0.74635. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57592/0.74781. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58189/0.75091. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57598/0.74306. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.58016/0.75596. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57544/0.75601. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57130/0.75486. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57328/0.75588. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.56790/0.76112. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57313/0.76223. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56550/0.76422. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56972/0.77110. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56326/0.76576. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55671/0.77076. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.55642/0.77165. Took 0.08 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69377/0.69518. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69354/0.69505. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69403/0.69469. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69302/0.69439. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69224/0.69398. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69226/0.69381. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69191/0.69363. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69111/0.69363. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69031/0.69372. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68969/0.69386. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68925/0.69444. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68871/0.69495. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68767/0.69536. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68654/0.69648. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68498/0.69760. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68541/0.69807. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68432/0.69938. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68275/0.70046. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68369/0.70182. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68087/0.70288. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67981/0.70414. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68012/0.70492. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67857/0.70586. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67833/0.70680. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67647/0.70785. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67650/0.70914. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67610/0.71055. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67587/0.71195. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67237/0.71341. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67096/0.71458. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67228/0.71622. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67009/0.71756. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67091/0.71905. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66946/0.71982. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66681/0.72088. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66876/0.72176. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66389/0.72235. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66432/0.72296. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66374/0.72293. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66194/0.72441. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66195/0.72509. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65972/0.72643. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65792/0.72700. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65724/0.72793. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65501/0.72982. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65361/0.73002. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65082/0.73058. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65167/0.73127. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65250/0.73186. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64535/0.73196. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64507/0.73303. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64479/0.73425. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64283/0.73388. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64303/0.73536. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64031/0.73634. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63642/0.73557. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.63585/0.73549. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63348/0.73701. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63232/0.73760. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62781/0.73771. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62969/0.73621. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62328/0.73746. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62124/0.73731. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62040/0.74038. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61838/0.74095. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61403/0.74210. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61645/0.74281. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.61344/0.74639. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61135/0.74660. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60671/0.74709. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60661/0.75203. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59905/0.74998. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60055/0.75741. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59444/0.75545. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59418/0.75500. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59599/0.75909. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59199/0.75817. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.59411/0.76553. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58701/0.76583. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.58331/0.77237. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57997/0.77479. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.57693/0.77704. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.57726/0.77581. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57375/0.77905. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57787/0.78232. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57727/0.78110. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.57134/0.78372. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56788/0.78446. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56828/0.78845. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56527/0.79029. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55983/0.79758. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55545/0.79450. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56292/0.80524. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55838/0.80371. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55303/0.80383. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.55135/0.80857. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.54581/0.81529. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.53931/0.81581. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.54317/0.81666. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.53900/0.82045. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69660/0.70696. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69518/0.70496. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69423/0.70260. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69426/0.69946. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69360/0.69646. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69294/0.69395. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69209/0.69231. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69151/0.69129. Took 0.11 sec\n",
      "Epoch 8, Loss(train/val) 0.69142/0.69054. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.69018. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69112/0.68976. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69041/0.68950. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69128/0.68984. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69007/0.69023. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68951/0.69017. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68857/0.68955. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68880/0.69002. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.69032. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68617/0.69066. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68784/0.69072. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68624/0.69027. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68532/0.68936. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68574/0.68861. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68483/0.68960. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68292/0.68932. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68192/0.68818. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68307/0.68773. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68069/0.68823. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68129/0.68798. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68116/0.68708. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67972/0.68694. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67885/0.68804. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67861/0.68805. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67794/0.68847. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67627/0.68927. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67584/0.68985. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67475/0.68920. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67336/0.68985. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67160/0.69039. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66880/0.68818. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66984/0.68997. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66890/0.69164. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66889/0.69134. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66523/0.68958. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66438/0.69136. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66816/0.69154. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66387/0.69206. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66232/0.69159. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66068/0.69271. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65798/0.69342. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65828/0.69413. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65793/0.69327. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65501/0.69336. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65069/0.69239. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65310/0.69526. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64884/0.69351. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64694/0.69414. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64998/0.69600. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.64768/0.69366. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64511/0.69403. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64449/0.69681. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.64040/0.69638. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63866/0.69663. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64069/0.69604. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63466/0.69678. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.63366/0.69734. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.62958/0.69792. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63304/0.69691. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63202/0.69763. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62348/0.69740. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62616/0.69805. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.62493/0.70089. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62228/0.70089. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61847/0.70302. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61720/0.70313. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.61192/0.70746. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61342/0.70598. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.62038/0.70611. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61207/0.70611. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.60788/0.70895. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60872/0.70957. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61025/0.70857. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59915/0.70886. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59900/0.70985. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60282/0.71152. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.59337/0.71367. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.59396/0.71148. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58731/0.71153. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58844/0.71751. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.58472/0.71803. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.58690/0.71994. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.58101/0.72123. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.57138/0.72329. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57463/0.72408. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.57261/0.72160. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.57266/0.72175. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56637/0.71829. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56451/0.72706. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.56277/0.72748. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55596/0.72981. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69223/0.69035. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69093/0.68971. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69043/0.68930. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68957/0.68909. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.68817/0.68900. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68847/0.68882. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68875/0.68898. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68764/0.68905. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.68898. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68603/0.68978. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68510/0.69024. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68353/0.69062. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68374/0.69104. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68365/0.69126. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68338/0.69188. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68132/0.69258. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68062/0.69308. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68091/0.69329. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67881/0.69439. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68005/0.69371. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67879/0.69465. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67715/0.69434. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67810/0.69476. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67623/0.69509. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67374/0.69594. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67453/0.69567. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67359/0.69607. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67364/0.69585. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67331/0.69592. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67332/0.69618. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66996/0.69615. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67166/0.69565. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66940/0.69620. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66846/0.69642. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66584/0.69659. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66659/0.69541. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66626/0.69653. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66559/0.69703. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66374/0.69782. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66231/0.69657. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66306/0.69699. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65933/0.69650. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65926/0.69607. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65860/0.69470. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65628/0.69493. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65548/0.69546. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65199/0.69457. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65229/0.69339. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65040/0.69353. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64733/0.69383. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64608/0.69305. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64291/0.69298. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64222/0.69345. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64174/0.69382. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63734/0.69138. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63841/0.69262. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63569/0.69232. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63236/0.69221. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63024/0.69248. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62823/0.69486. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62200/0.69734. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.62580/0.69433. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.61982/0.69511. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61972/0.69594. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61519/0.69859. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61021/0.69850. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60633/0.70123. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.60784/0.70272. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60381/0.70364. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.60209/0.70655. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.59545/0.70605. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59417/0.71056. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59731/0.71299. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59089/0.71382. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59148/0.71467. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58599/0.71322. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.58501/0.71541. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.57343/0.71990. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57606/0.72389. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57642/0.72640. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57332/0.72489. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.56573/0.72993. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57120/0.73098. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.55895/0.73476. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55748/0.73560. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.55095/0.73786. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55246/0.74418. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.55095/0.74604. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54832/0.74207. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.54335/0.75360. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53860/0.75756. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.53974/0.75864. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52869/0.76147. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52739/0.76287. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52928/0.76453. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.52525/0.77019. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52135/0.77233. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52320/0.77780. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.52097/0.78203. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.52613/0.77680. Took 0.10 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69552/0.69444. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69480/0.69457. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69406/0.69471. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69340/0.69490. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69159/0.69526. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69078/0.69584. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69024/0.69662. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68871/0.69753. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68887/0.69848. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68771/0.69959. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68799/0.70060. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68593/0.70177. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68421/0.70328. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68449/0.70477. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68389/0.70649. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68236/0.70847. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68176/0.71049. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67973/0.71231. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68005/0.71388. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67897/0.71551. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67757/0.71767. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67683/0.71956. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67602/0.72163. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67343/0.72379. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67478/0.72563. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66995/0.72691. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67076/0.72888. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67145/0.73054. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67009/0.73244. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66659/0.73336. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66581/0.73486. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66393/0.73613. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66357/0.73827. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66125/0.73934. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65909/0.73969. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.65839/0.74119. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.65952/0.74300. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.65626/0.74223. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65370/0.74329. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65122/0.74307. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65040/0.74523. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.64659/0.74597. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.64590/0.74626. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64376/0.74767. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.64506/0.74807. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.63995/0.74806. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64310/0.74648. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64001/0.74550. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63917/0.74590. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63919/0.74713. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63152/0.74775. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.62862/0.74828. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63089/0.74694. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62604/0.74673. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62210/0.74783. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62221/0.74937. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62022/0.74846. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.61834/0.74853. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61148/0.75108. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.61590/0.75145. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.60967/0.75253. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.60690/0.75490. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60731/0.75405. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.60467/0.75615. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.60227/0.75735. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60033/0.75744. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.59697/0.75970. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.59214/0.76254. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.59266/0.76652. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59037/0.76819. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58640/0.76858. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58377/0.76828. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.57724/0.77156. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58099/0.77233. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.57901/0.77425. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.57513/0.77831. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57305/0.77901. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.56928/0.78098. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56644/0.78047. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.56074/0.78609. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55913/0.79180. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.55524/0.79142. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.55622/0.79579. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55266/0.80052. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55131/0.79970. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.54862/0.80331. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54510/0.80931. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.54222/0.81223. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.54304/0.81269. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53417/0.81753. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.53456/0.81831. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.53344/0.81817. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.52959/0.82800. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52563/0.83101. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.52393/0.83497. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.52677/0.83106. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.52098/0.83624. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.51279/0.83907. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.51401/0.84518. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51961/0.84425. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69286/0.69291. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68981/0.69299. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68844/0.69245. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68790/0.69169. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68688/0.69084. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68514/0.69052. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68415/0.68992. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68295/0.68941. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68237/0.68839. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68042/0.68786. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.67989/0.68740. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67905/0.68715. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67896/0.68700. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.67746/0.68683. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67479/0.68690. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.67389/0.68628. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67333/0.68602. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67113/0.68663. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67103/0.68627. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.66937/0.68642. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.66660/0.68553. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66595/0.68584. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.66342/0.68515. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66324/0.68512. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.66138/0.68437. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.65755/0.68356. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65924/0.68241. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65642/0.68128. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.65355/0.68081. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65332/0.68008. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64938/0.67922. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.64677/0.67943. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.64530/0.67850. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.64515/0.67957. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64394/0.67960. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.64521/0.67998. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63620/0.67914. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63771/0.67881. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.63358/0.67929. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63351/0.67955. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63377/0.67961. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.63198/0.68017. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63082/0.68128. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.62430/0.68089. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.62943/0.68000. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62492/0.68038. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.62219/0.68129. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.61993/0.68117. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61550/0.67928. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61764/0.67895. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.61631/0.67945. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.61198/0.68058. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.61011/0.68218. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.61082/0.68247. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60890/0.68327. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60243/0.68500. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.60131/0.68421. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60201/0.68420. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.60315/0.68472. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59103/0.68550. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58908/0.68716. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58794/0.68677. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.58832/0.68667. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.58575/0.68525. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.58283/0.68773. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.57889/0.68591. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57560/0.68858. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57413/0.68737. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57370/0.69182. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57157/0.69375. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56576/0.69848. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.56457/0.69634. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.56203/0.69780. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.55954/0.70138. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.55845/0.69939. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55392/0.70551. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55117/0.70503. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55188/0.70809. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.54184/0.71256. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.54298/0.71314. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.53919/0.71656. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.53747/0.71983. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.53279/0.71801. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.53365/0.72415. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.52888/0.72619. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.52515/0.72729. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.52128/0.73286. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.52053/0.73527. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.51374/0.74172. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.51741/0.73361. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.50850/0.74810. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.51432/0.73925. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.50778/0.74440. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.50535/0.74998. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.50763/0.75724. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.50744/0.75786. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.49969/0.76095. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48858/0.76131. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.48814/0.77111. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48360/0.77708. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69173/0.69909. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68996/0.70100. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68923/0.70229. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68907/0.70265. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68816/0.70291. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68801/0.70286. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68742/0.70297. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68561/0.70376. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68586/0.70464. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68569/0.70405. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68421/0.70498. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68360/0.70532. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68351/0.70568. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68206/0.70665. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68178/0.70715. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68189/0.70790. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68151/0.70721. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.67840/0.70831. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67814/0.71025. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67904/0.71024. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67651/0.71101. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67666/0.71085. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67487/0.71210. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67396/0.71270. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67374/0.71338. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67361/0.71494. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67216/0.71491. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67176/0.71502. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66941/0.71570. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66846/0.71475. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66855/0.71795. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66725/0.71851. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66462/0.71942. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66382/0.71836. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66433/0.72089. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66236/0.72262. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66229/0.72186. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66011/0.72551. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65835/0.72283. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65899/0.72726. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65613/0.72697. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.65424/0.72743. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65519/0.72665. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65187/0.72779. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65150/0.73043. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64793/0.73064. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64734/0.72969. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64572/0.73323. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64494/0.73417. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64374/0.73160. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64191/0.73488. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63940/0.73676. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63814/0.73458. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63472/0.73886. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.63357/0.73565. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62852/0.73783. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62791/0.73886. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62432/0.74212. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62690/0.74571. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.62055/0.73970. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62124/0.74311. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61694/0.74829. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61591/0.74993. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61097/0.74742. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61201/0.75677. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60879/0.75821. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.60834/0.75506. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60541/0.75697. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60205/0.76012. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.59765/0.75932. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.58914/0.76922. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.59482/0.76528. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59938/0.77550. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.58366/0.77585. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.58821/0.77028. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.58156/0.77662. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58173/0.78010. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.58113/0.77390. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.57890/0.78475. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.57470/0.78319. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.57290/0.79055. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.56735/0.79633. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.56905/0.79578. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.56550/0.80191. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56410/0.80072. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.55420/0.80389. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.55736/0.81516. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.55447/0.80466. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.55209/0.80835. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.55046/0.81715. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.54474/0.81823. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.54011/0.81763. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54197/0.82931. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54031/0.82715. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.53706/0.82283. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.52610/0.82943. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.52941/0.83105. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.52948/0.83936. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52474/0.84702. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.52641/0.84231. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69342/0.69237. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69329/0.69259. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69326/0.69267. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69310/0.69257. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69256/0.69270. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69258/0.69293. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69214/0.69304. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69216/0.69323. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69185/0.69368. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69151/0.69392. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69134/0.69429. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69106/0.69464. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69048/0.69510. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68994/0.69575. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68949/0.69632. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68958/0.69669. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68923/0.69752. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68837/0.69827. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68794/0.69879. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68732/0.69928. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68677/0.69967. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68620/0.70032. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68479/0.70116. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.70209. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68510/0.70179. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68347/0.70141. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68397/0.70161. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68237/0.70167. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68178/0.70247. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68092/0.70276. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67938/0.70343. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68015/0.70393. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67797/0.70429. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67585/0.70495. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67752/0.70498. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67570/0.70597. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67421/0.70564. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67553/0.70538. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67264/0.70557. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67257/0.70562. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67101/0.70606. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66802/0.70675. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66790/0.70718. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66731/0.70821. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66673/0.70921. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66456/0.70876. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66265/0.71080. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66289/0.71174. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66122/0.71226. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66095/0.71413. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65858/0.71490. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65755/0.71409. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65653/0.71701. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65545/0.71735. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65126/0.71878. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65007/0.71848. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65020/0.72134. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64797/0.72287. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64821/0.72435. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64517/0.72281. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.64473/0.72475. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64151/0.72591. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63783/0.72776. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63755/0.72950. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63740/0.72724. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63280/0.73183. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63011/0.73223. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63249/0.73216. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62815/0.73179. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.62828/0.73236. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62444/0.73386. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62016/0.73551. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61859/0.73451. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61709/0.73563. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.61518/0.73881. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60910/0.74092. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60663/0.73848. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60526/0.74129. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.60102/0.74076. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59824/0.74073. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59939/0.74012. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59713/0.74120. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59096/0.74373. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59222/0.73973. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58763/0.74489. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57753/0.75307. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58214/0.75541. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57987/0.74898. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57621/0.75855. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.57716/0.75275. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57064/0.75917. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.56407/0.76405. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.57122/0.77223. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.55499/0.77048. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55426/0.77336. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55026/0.77465. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55182/0.77147. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54341/0.76868. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55051/0.77596. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54046/0.79020. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69533/0.69354. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69159. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69159/0.69097. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69153/0.69060. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69117/0.69076. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69022/0.69077. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68981/0.69081. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68934/0.69092. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68899/0.69115. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68816/0.69130. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68734/0.69168. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68753/0.69196. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68648/0.69273. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68637/0.69341. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68519/0.69432. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68501/0.69479. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68357/0.69491. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68315/0.69558. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68303/0.69621. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68101/0.69688. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68121/0.69787. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68045/0.69866. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67795/0.69953. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67815/0.69930. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67567/0.70013. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67647/0.70202. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67462/0.70183. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67203/0.70322. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67419/0.70428. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67232/0.70438. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67039/0.70622. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67242/0.70820. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66808/0.71015. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66875/0.71166. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66925/0.71138. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66706/0.71320. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66541/0.71374. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66419/0.71495. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66430/0.71551. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66493/0.71687. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66347/0.71938. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66322/0.71960. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66042/0.72182. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65774/0.72221. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65842/0.72339. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65647/0.72583. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65466/0.72933. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65589/0.72927. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65191/0.73037. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65192/0.73207. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65011/0.73438. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64788/0.73672. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64824/0.73731. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64661/0.73829. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64571/0.73974. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.64767/0.73791. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64876/0.74207. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64520/0.74233. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63963/0.74384. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.64144/0.74676. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64002/0.74791. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63768/0.74754. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.63649/0.74982. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63479/0.75002. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.63364/0.75268. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63308/0.75541. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62892/0.75719. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.63286/0.75775. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.62578/0.75788. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62749/0.75808. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.62435/0.75930. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62185/0.75980. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62148/0.76396. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61818/0.76496. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62220/0.76688. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62240/0.76751. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.61854/0.76670. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61443/0.77160. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61931/0.77428. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61265/0.77138. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61324/0.77663. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60782/0.77922. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.61201/0.77801. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.60970/0.78194. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60640/0.78138. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60474/0.78099. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60060/0.78432. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.59966/0.78721. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.59890/0.79249. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59323/0.79068. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59608/0.78877. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.59473/0.79129. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.59296/0.79859. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.58978/0.79705. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.58710/0.79414. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.58667/0.79688. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59208/0.79683. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.58756/0.80557. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58209/0.80804. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58327/0.80814. Took 0.08 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69356/0.68587. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69119/0.68348. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68967/0.68343. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69048/0.68398. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.68462. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69001/0.68532. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68904/0.68570. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68791/0.68632. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.68703. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68722/0.68757. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68652/0.68791. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68681/0.68905. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68494/0.68941. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68383/0.68967. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68362/0.69069. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68324/0.69036. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68264/0.69018. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68096/0.69119. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67947/0.69180. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67872/0.69202. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67834/0.69253. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67682/0.69306. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67557/0.69422. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67501/0.69323. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67364/0.69304. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67146/0.69479. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66913/0.69513. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67138/0.69487. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.66921/0.69561. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66953/0.69523. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66849/0.69476. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66612/0.69530. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66443/0.69451. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66343/0.69425. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66209/0.69467. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66182/0.69436. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66010/0.69399. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66049/0.69559. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65773/0.69685. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66045/0.69837. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.65451/0.69628. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65498/0.69845. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65563/0.69706. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65216/0.69830. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65108/0.69629. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65126/0.69807. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64799/0.69822. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64738/0.69684. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64714/0.69792. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64241/0.69764. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64167/0.69887. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64283/0.69816. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63999/0.69937. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.63877/0.69971. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63753/0.70233. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63521/0.70279. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.63515/0.70082. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63333/0.70162. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63595/0.70543. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.63230/0.70284. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63343/0.70404. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63284/0.70534. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62229/0.70514. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62769/0.70746. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62551/0.70756. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.61794/0.70685. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62195/0.70603. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62333/0.71147. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61093/0.71223. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61685/0.71479. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61160/0.71153. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.61404/0.71524. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61025/0.71415. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60829/0.71490. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60208/0.71649. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60506/0.71763. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60316/0.71794. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.60030/0.72015. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60191/0.72349. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59785/0.72668. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59670/0.72534. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59342/0.72740. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.59020/0.72804. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.59757/0.73048. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59085/0.73306. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.58534/0.73889. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58448/0.73441. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58147/0.73745. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.57987/0.74235. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57794/0.74265. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.57592/0.74628. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57090/0.74956. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57096/0.75120. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56797/0.75387. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.57055/0.75738. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.56096/0.75588. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56451/0.76009. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.55944/0.76310. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55641/0.76032. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.56653/0.76842. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69587/0.69437. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69085/0.69730. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69107/0.69824. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69103/0.69911. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69053/0.69990. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68982/0.70061. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68800/0.70180. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68831/0.70273. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68716/0.70351. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68730/0.70437. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68667/0.70494. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68606/0.70540. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68489/0.70580. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68417/0.70626. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68387/0.70702. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68240/0.70784. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68246/0.70812. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68061/0.70912. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68059/0.70938. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67916/0.71115. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67820/0.71110. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67731/0.71261. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67694/0.71347. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67636/0.71436. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67565/0.71560. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67503/0.71604. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67391/0.71741. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67311/0.71698. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67177/0.71886. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67046/0.71956. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66958/0.72042. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66943/0.72184. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66812/0.72273. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66746/0.72326. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66623/0.72469. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66657/0.72447. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66535/0.72482. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66222/0.72379. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66090/0.72578. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66007/0.72535. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65838/0.72501. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65828/0.72467. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65472/0.72708. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.65250/0.72830. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64982/0.72933. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65299/0.72797. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65009/0.72936. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64502/0.72992. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.64506/0.73015. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64464/0.72931. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63956/0.73033. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63915/0.73023. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63748/0.73308. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63515/0.73170. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.63622/0.73451. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63129/0.73659. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62878/0.73673. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63045/0.73637. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62910/0.73600. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62807/0.73892. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62313/0.73894. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62246/0.73902. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61740/0.73711. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61865/0.74154. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.61945/0.74065. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.61668/0.74285. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60914/0.74458. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60772/0.75157. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.60568/0.74665. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60725/0.74941. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.60872/0.75278. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60030/0.75331. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60298/0.75496. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60058/0.75155. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.59519/0.76019. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.59331/0.76276. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.58878/0.76392. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.59207/0.76593. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59058/0.76924. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58786/0.77031. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58036/0.77229. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.58127/0.77369. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58032/0.76968. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57472/0.77327. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.57743/0.77434. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57446/0.77544. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57333/0.77375. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.56798/0.77722. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.56538/0.78344. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56247/0.78947. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55783/0.79508. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.55544/0.80139. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.55432/0.79818. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.55597/0.79885. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55408/0.80152. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.54869/0.81575. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55039/0.80774. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.54745/0.81503. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.54387/0.80854. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.53905/0.80445. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69838/0.69260. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69145/0.69691. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68922/0.69921. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68889/0.70040. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68757/0.70100. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68826/0.70128. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68911/0.70136. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68777/0.70091. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68696/0.70187. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68735/0.70207. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68592/0.70227. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68502/0.70288. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68355/0.70320. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68421/0.70390. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68274/0.70458. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68133/0.70457. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68194/0.70535. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68137/0.70577. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67998/0.70572. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67913/0.70653. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67828/0.70609. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67887/0.70565. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67759/0.70623. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67679/0.70650. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67403/0.70559. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67323/0.70713. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67385/0.70691. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67133/0.70709. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67035/0.70758. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66903/0.70688. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66728/0.70900. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66872/0.70911. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66777/0.70842. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66461/0.70984. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66188/0.70886. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66225/0.71108. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66019/0.71166. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65675/0.71286. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65737/0.71471. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65399/0.71283. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65384/0.71746. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.65251/0.71780. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64747/0.71521. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.64919/0.71906. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64211/0.71929. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.64470/0.72057. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64524/0.72097. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64136/0.72532. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.63901/0.72724. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.63503/0.72306. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.63123/0.72857. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63075/0.72566. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63044/0.72849. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.63068/0.72827. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62930/0.73133. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.62509/0.72936. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.62296/0.73294. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.61597/0.73262. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.61658/0.73489. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61238/0.73995. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.61994/0.73815. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.61085/0.73894. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.60566/0.74122. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.60992/0.74381. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61255/0.74737. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.60428/0.74570. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.60127/0.74059. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.59850/0.75125. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.60302/0.75057. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.59847/0.75279. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.59386/0.74597. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.58954/0.75734. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.59464/0.75398. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.59364/0.76125. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.58739/0.76085. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.58785/0.75769. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.57986/0.76371. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.58453/0.76263. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.58167/0.75802. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.57975/0.76199. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.57700/0.76699. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.57350/0.77472. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.57262/0.77780. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.57367/0.77071. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.56948/0.77331. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.56266/0.78594. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.56079/0.79970. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56373/0.78673. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.56221/0.78642. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56115/0.78563. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.55471/0.78866. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55585/0.79878. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.54806/0.80310. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.54289/0.78950. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.54869/0.80124. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.54799/0.81300. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.54120/0.81744. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54250/0.80845. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.53647/0.81458. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54556/0.81592. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69060/0.68345. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.68861/0.68238. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68772/0.68148. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68745/0.68087. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68663/0.68036. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68645/0.67986. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68458/0.67912. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68515/0.67897. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68400/0.67853. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68346/0.67834. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68262/0.67798. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68312/0.67790. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68307/0.67794. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68167/0.67691. Took 0.11 sec\n",
      "Epoch 14, Loss(train/val) 0.68086/0.67686. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68084/0.67679. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67951/0.67728. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67820/0.67760. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67779/0.67797. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.67781/0.67802. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67623/0.67801. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67715/0.67838. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67706/0.67830. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67514/0.67818. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67369/0.67917. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67210/0.67923. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67019/0.67967. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67089/0.68016. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.66987/0.68148. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66730/0.68128. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66629/0.68257. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66501/0.68316. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66433/0.68242. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66417/0.68268. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.66001/0.68302. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65983/0.68402. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65676/0.68426. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65615/0.68478. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.65611/0.68428. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65561/0.68498. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65162/0.68777. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64832/0.68781. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.64741/0.68653. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.64685/0.68860. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64161/0.69082. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64218/0.69246. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.64014/0.69365. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.63978/0.69529. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.63623/0.69717. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.63560/0.69836. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63490/0.69997. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63193/0.70025. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63070/0.70113. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.62563/0.70708. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.62377/0.70807. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.62385/0.71066. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.62321/0.71069. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.62236/0.71108. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.62034/0.71606. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.61953/0.71804. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.61408/0.71880. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.61099/0.72191. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.61586/0.72283. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.61150/0.72362. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.61135/0.72464. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.60798/0.72454. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.61173/0.72680. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.60556/0.72950. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.60472/0.73269. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60583/0.73232. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.60201/0.73246. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.59949/0.73068. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.59980/0.73389. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.59479/0.73653. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.59716/0.73512. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60000/0.74032. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.59224/0.73598. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.59318/0.73649. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.58988/0.73835. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.58824/0.74237. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.58870/0.74537. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58503/0.74321. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58678/0.74723. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58373/0.74835. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58187/0.74971. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.57802/0.74853. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.58423/0.75073. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.57839/0.75149. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57937/0.75235. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57729/0.75316. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.57431/0.75022. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.56899/0.75808. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57005/0.76120. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56857/0.75978. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56757/0.75867. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56675/0.76396. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.55671/0.76415. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56339/0.76610. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55568/0.76749. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55615/0.77500. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69052/0.70594. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.68884/0.70416. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68779/0.70326. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68750/0.70214. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68681/0.70160. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68612/0.70085. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68546/0.69992. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68492/0.69963. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68446/0.69884. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68416/0.69876. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.68461/0.69882. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68274/0.69892. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68224/0.69847. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68212/0.69882. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68103/0.69848. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68072/0.69891. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67996/0.69946. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68066/0.70014. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67894/0.70052. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67853/0.70008. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67837/0.70087. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67716/0.70052. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67731/0.70104. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67520/0.70245. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67384/0.70170. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67390/0.70197. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67358/0.70237. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67378/0.70255. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67100/0.70211. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67063/0.70331. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66927/0.70332. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66851/0.70329. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.66708/0.70293. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66575/0.70358. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66413/0.70468. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66413/0.70426. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66263/0.70439. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66258/0.70550. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66121/0.70515. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65848/0.70714. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65745/0.70759. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65821/0.70626. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.65604/0.70750. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65519/0.70879. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65296/0.71021. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65288/0.71211. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65073/0.71203. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64805/0.71326. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64657/0.71393. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64682/0.71435. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64611/0.71283. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64367/0.71608. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64424/0.71659. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.64016/0.71657. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64029/0.71735. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.63949/0.71975. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63510/0.71649. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63244/0.71804. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63049/0.71894. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63041/0.72352. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62342/0.72021. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62496/0.72506. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62593/0.72587. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62169/0.72531. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62234/0.72474. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.62131/0.72774. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.61777/0.72548. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.61564/0.72844. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61168/0.72619. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.60922/0.72995. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61262/0.73146. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.60662/0.72790. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60590/0.73248. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.60472/0.73303. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.60036/0.73127. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60089/0.73523. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.59697/0.73520. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.59194/0.73830. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59059/0.74201. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.59059/0.74135. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59243/0.73768. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.58825/0.74427. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.58604/0.74270. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.58356/0.74669. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.58373/0.74137. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.57950/0.74873. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.57580/0.74723. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.56933/0.74855. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57175/0.75178. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.56693/0.75082. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56380/0.75400. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.55999/0.76158. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.56174/0.75527. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56137/0.75914. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56190/0.76779. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.56244/0.76394. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.55619/0.76256. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.54972/0.76579. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55012/0.77203. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.54209/0.77298. Took 0.08 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69169/0.69332. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69065/0.69432. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68948/0.69528. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68890/0.69597. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68821/0.69679. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68715/0.69743. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68716/0.69787. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68630/0.69862. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68361/0.69985. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68314/0.70154. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68151/0.70252. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.67931/0.70418. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.67722/0.70598. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67607/0.70718. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67381/0.70873. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.66877/0.70996. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.66946/0.71040. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.66595/0.71000. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.66505/0.70996. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.66010/0.71043. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.65613/0.71043. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.65254/0.71019. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.65012/0.70993. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.64523/0.70918. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.64002/0.70917. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.63965/0.70887. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.63389/0.70933. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.63290/0.70937. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.63066/0.70958. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.62802/0.71047. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.62330/0.71165. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.62365/0.71344. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.61555/0.71414. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.61783/0.71446. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.60848/0.71570. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.61427/0.71650. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.61104/0.71886. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.60361/0.71977. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.59782/0.72266. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.59858/0.72499. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.60138/0.72777. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.59345/0.72934. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.59346/0.72707. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.59068/0.73053. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.58656/0.73207. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.58388/0.73782. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.57850/0.74251. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.57901/0.74158. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.57530/0.74393. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.57075/0.74933. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.56883/0.75090. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.56633/0.75415. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.56624/0.75588. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.56289/0.75664. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.55914/0.75635. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.56231/0.75563. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.56091/0.75660. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.55833/0.75862. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.55201/0.76096. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.54831/0.76634. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.55213/0.77054. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.54864/0.76891. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.54755/0.77066. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.53999/0.77049. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.54086/0.77580. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.53592/0.77978. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.53507/0.77656. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.52490/0.79085. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.53001/0.78950. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.52877/0.79913. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.52826/0.79810. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.52503/0.79851. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.52287/0.80155. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.51562/0.80277. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.51780/0.80775. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.51667/0.80305. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.51457/0.81049. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.51120/0.81358. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.51088/0.81746. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.50256/0.81513. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.50597/0.82159. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.50087/0.82137. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.49485/0.82903. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.49646/0.83717. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.49438/0.83738. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.49019/0.83830. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.48164/0.84299. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.48484/0.84456. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.47987/0.85060. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.47491/0.85267. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.47285/0.86186. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.47027/0.86753. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.47308/0.86982. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.46767/0.87312. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.46602/0.87762. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.45718/0.88440. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.45823/0.89428. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.45834/0.90094. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.45630/0.90506. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.45270/0.90685. Took 0.08 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69527/0.69581. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69062/0.70088. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68997/0.70236. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68982/0.70311. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68850/0.70387. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68832/0.70408. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68753/0.70449. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68529/0.70463. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68565/0.70529. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68556/0.70573. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68405/0.70631. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68369/0.70682. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68159/0.70742. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68292/0.70794. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.67945/0.70937. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67877/0.70933. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67894/0.70862. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67779/0.71024. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.67559/0.70971. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67484/0.71088. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67249/0.71011. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67273/0.71158. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67085/0.71176. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.66692/0.70959. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.66580/0.71117. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.66448/0.71007. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.66027/0.71088. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.65855/0.71000. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65340/0.71132. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65281/0.70957. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.65208/0.70882. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.64807/0.71090. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.64838/0.71225. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.64355/0.71197. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.64102/0.71223. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63871/0.71422. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.63969/0.71388. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.63514/0.71378. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.63682/0.71831. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63042/0.72140. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63323/0.72410. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62456/0.72420. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.62735/0.72791. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.62294/0.72738. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.62029/0.72965. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.62035/0.73225. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.61814/0.73501. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61363/0.73712. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.61157/0.74362. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.60784/0.74853. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.60845/0.74966. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.60971/0.75263. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60872/0.75476. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.60285/0.75940. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.60319/0.75770. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.60161/0.76330. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.60314/0.76635. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.59583/0.77024. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.59478/0.77219. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.59665/0.77536. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59039/0.77485. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.58968/0.77741. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.59024/0.78292. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.59165/0.78493. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58416/0.78993. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.58727/0.79230. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.58170/0.79558. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.58358/0.79762. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.57681/0.79891. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.57419/0.80247. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.57803/0.81000. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.57202/0.81363. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.57278/0.81830. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.56928/0.81931. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.56419/0.82396. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.56664/0.83198. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.56289/0.83309. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.55878/0.83181. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.56329/0.83398. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.55167/0.83691. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.55665/0.84048. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.55648/0.84059. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.55460/0.84906. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.55143/0.85485. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.55103/0.85599. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.54490/0.85754. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.54473/0.86474. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.54002/0.86522. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.54222/0.87257. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.53786/0.86977. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.53803/0.87692. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.53537/0.87969. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.53381/0.88899. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.52494/0.89470. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.52206/0.89715. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.53253/0.89750. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.51747/0.89699. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.52776/0.90631. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.52138/0.90971. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.51406/0.91157. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69286/0.69081. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69197/0.69152. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69052/0.69218. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69064/0.69287. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.68884/0.69362. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68832/0.69456. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68780/0.69557. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68673/0.69655. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68519/0.69783. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68434/0.69896. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68254/0.70019. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68149/0.70204. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68096/0.70383. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.67938/0.70553. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.67822/0.70677. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67707/0.70765. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.67241/0.70937. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.67179/0.71019. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67166/0.71093. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.66716/0.71184. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.66591/0.71240. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.66483/0.71333. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.65970/0.71481. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.65898/0.71558. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.65864/0.71603. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.65381/0.71597. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.65016/0.71666. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.65126/0.71873. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.64628/0.71822. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.64117/0.71957. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.64278/0.72096. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.63978/0.72248. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.63665/0.72227. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.63670/0.72419. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.63383/0.72530. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.63494/0.72403. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.62801/0.72556. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.62530/0.72628. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.62579/0.72926. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.62501/0.72985. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.61926/0.73227. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.62041/0.73250. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.61279/0.73355. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.61394/0.73714. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.61860/0.73671. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.60829/0.73834. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.60972/0.74239. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.60909/0.74370. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.60361/0.74584. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.60187/0.74659. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.60233/0.74583. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.59544/0.74949. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.59184/0.75273. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.58867/0.75130. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.59341/0.75323. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.58861/0.75815. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.59215/0.75909. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.58272/0.76003. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.58129/0.76423. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.57798/0.76157. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.58201/0.76178. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.57643/0.76453. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.56716/0.76378. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.56925/0.76765. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.55971/0.77155. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.56676/0.77434. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.55862/0.77944. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.55897/0.78288. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.55788/0.78230. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.55321/0.78199. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.55216/0.78470. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.55053/0.78296. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.54442/0.79174. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.54037/0.78938. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.54410/0.79528. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.54004/0.79817. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.53524/0.80499. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.53263/0.80668. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.52769/0.80525. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.53256/0.80857. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.52159/0.81161. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.52521/0.81525. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.52418/0.81798. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.51760/0.82507. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.51525/0.82899. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.51477/0.82613. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.51281/0.83030. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.50477/0.83062. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.49947/0.83136. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.50390/0.83442. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.50227/0.83570. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.49472/0.83957. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.49565/0.83857. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.48948/0.84672. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.48641/0.85160. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.48511/0.86106. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.48072/0.85647. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.48150/0.85702. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.47712/0.86547. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.47537/0.87303. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69418/0.69628. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69246/0.69493. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.69422. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69367. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.69305. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69140/0.69242. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69079/0.69187. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68981/0.69111. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68938/0.69083. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68944/0.69081. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68836/0.69023. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68748/0.68982. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68710/0.68993. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68685/0.68962. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68551/0.68952. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68458/0.68951. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68533/0.69024. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68391/0.69013. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68322/0.69023. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68432/0.69084. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68353/0.69108. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.69178. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68328/0.69256. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68108/0.69290. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68058/0.69365. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68196/0.69456. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68007/0.69537. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67872/0.69612. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67819/0.69716. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67762/0.69793. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67858/0.69885. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67833/0.69969. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67938/0.70045. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67380/0.70115. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67716/0.70178. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67494/0.70311. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67439/0.70393. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67295/0.70469. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67240/0.70505. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67289/0.70585. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67210/0.70778. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67161/0.70816. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66857/0.70897. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66990/0.70965. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66733/0.71088. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66573/0.71160. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66739/0.71237. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66475/0.71312. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66847/0.71398. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66345/0.71590. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66335/0.71606. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66399/0.71694. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65932/0.71947. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66165/0.71911. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66188/0.72000. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65847/0.72110. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65996/0.72137. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65651/0.72314. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65593/0.72348. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65632/0.72504. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65539/0.72605. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65517/0.72645. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65457/0.72735. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65002/0.72976. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.64746/0.73161. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64646/0.73235. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64666/0.73221. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64833/0.73512. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64422/0.73653. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64712/0.73677. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64223/0.73961. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63941/0.73983. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63904/0.74205. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63786/0.74523. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.63774/0.74573. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63594/0.74563. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63606/0.74724. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63555/0.74864. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63390/0.75109. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62976/0.75293. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62782/0.75666. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62684/0.75764. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62530/0.75977. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62405/0.76234. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62356/0.76379. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62496/0.76329. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.62358/0.76579. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61632/0.76966. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61316/0.77449. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61640/0.77353. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61677/0.77782. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61416/0.77761. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61376/0.78058. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.60739/0.78136. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60483/0.78667. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60816/0.78794. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.60400/0.79004. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60448/0.79314. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.60389/0.79429. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60242/0.79667. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69552/0.68577. Took 0.12 sec\n",
      "Epoch 1, Loss(train/val) 0.69292/0.68818. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69293/0.68860. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69225/0.68827. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69212/0.68854. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69209/0.68790. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69160/0.68721. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69136/0.68718. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69071/0.68698. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69109/0.68676. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69039/0.68622. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69024/0.68673. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69005/0.68591. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68965/0.68536. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68870/0.68586. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68743/0.68553. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68727/0.68561. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68797/0.68435. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68690/0.68482. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68533/0.68471. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68467/0.68466. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68499/0.68476. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68505/0.68445. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68374/0.68500. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68294/0.68499. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68263/0.68516. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68148/0.68480. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68026/0.68513. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68003/0.68663. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67833/0.68721. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67920/0.68700. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67678/0.68865. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67537/0.68729. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67290/0.68950. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67580/0.69039. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67214/0.69006. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67255/0.69084. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67190/0.69257. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66778/0.69111. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66874/0.69223. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66751/0.69368. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66497/0.69473. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66404/0.69514. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66371/0.69683. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66156/0.69787. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66257/0.69744. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65637/0.69860. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65625/0.70097. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65389/0.69952. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65051/0.70074. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65027/0.70176. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.64973/0.70176. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64810/0.70107. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64246/0.70339. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64323/0.70398. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64462/0.70343. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64209/0.70572. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.63515/0.70750. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63334/0.70774. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63310/0.71008. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.62986/0.71123. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.62895/0.71371. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.62943/0.71218. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.62758/0.71368. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62409/0.71429. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62522/0.71615. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62423/0.71742. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.61792/0.71907. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61851/0.71998. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.61489/0.72246. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61347/0.72631. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61310/0.72904. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.60824/0.73033. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61118/0.72995. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60786/0.73311. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.60297/0.73212. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.60358/0.73681. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60410/0.73870. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59588/0.73886. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60068/0.74105. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.59152/0.74382. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.59759/0.74594. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.59023/0.74922. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58667/0.75383. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.59004/0.75523. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.58020/0.75418. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.58048/0.75830. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.57768/0.75911. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.57944/0.76220. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.56894/0.76022. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.56462/0.76586. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.57213/0.77151. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.57092/0.77268. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.56536/0.77326. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.55867/0.77207. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.55320/0.78290. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56259/0.77969. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.55214/0.78069. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.55616/0.78630. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.55484/0.78186. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69465/0.69660. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69309/0.69735. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69235/0.69797. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69219/0.69857. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69911. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69079/0.69988. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69098/0.70066. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68992/0.70148. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68880/0.70215. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.70287. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68817/0.70345. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.70393. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68702/0.70399. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68535/0.70399. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68526/0.70383. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68367/0.70400. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68298/0.70399. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68250/0.70432. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68052/0.70421. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68007/0.70345. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.67660/0.70315. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67561/0.70310. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67367/0.70365. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67116/0.70363. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67052/0.70336. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.66734/0.70457. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.66461/0.70697. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66096/0.70719. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.65883/0.70881. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.65762/0.70816. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.65627/0.70854. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.65451/0.71035. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.64913/0.71206. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.65033/0.71424. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.64704/0.71571. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.64458/0.71754. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.64306/0.71692. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.64102/0.71565. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.64026/0.71849. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.63997/0.71993. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.63316/0.72201. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.63104/0.72417. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.63138/0.72445. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.62797/0.72649. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.62614/0.72871. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.62596/0.72788. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.62316/0.72999. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.61840/0.73069. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.61534/0.73197. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.61908/0.73173. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.61526/0.73228. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.61230/0.73176. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.60941/0.73777. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.60722/0.73917. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.60701/0.74040. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.60203/0.74160. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.60120/0.74319. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.60155/0.74115. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.60015/0.74484. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.59734/0.75130. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.59288/0.75742. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.58607/0.75818. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.58525/0.76314. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.58579/0.76467. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.58087/0.76375. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.57592/0.76416. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.57524/0.76929. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.57853/0.77165. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.57563/0.77699. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.56950/0.78166. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.56986/0.78244. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.56662/0.78609. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.55899/0.78839. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.56443/0.79474. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.55371/0.79811. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.55705/0.79991. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.55827/0.79828. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.54790/0.80602. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.54560/0.81695. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.54667/0.82230. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.55018/0.82442. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.54071/0.82177. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.54569/0.82416. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.54339/0.82673. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.53753/0.83089. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.53073/0.83600. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.52893/0.84315. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.53150/0.84921. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.52462/0.85026. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.51989/0.85254. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.52197/0.85725. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.52013/0.86951. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.50719/0.86724. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.51400/0.87763. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.51231/0.87949. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.51074/0.88428. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.50732/0.88587. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.50901/0.89773. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.49706/0.89451. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.48952/0.90676. Took 0.08 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69410/0.69339. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69241/0.69193. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.69185. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69180/0.69194. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69076/0.69213. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69053/0.69230. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.69295. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68922/0.69334. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69004/0.69374. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68895/0.69416. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68885/0.69456. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68755/0.69486. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68792/0.69532. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68789/0.69578. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68658/0.69647. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68689/0.69680. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68569/0.69696. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68539/0.69729. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68494/0.69791. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68482/0.69846. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68302/0.69842. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68383/0.69894. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68363/0.69884. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68264/0.69945. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68187/0.70017. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68234/0.70073. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68160/0.70096. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68005/0.70125. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67922/0.70118. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67815/0.70168. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67864/0.70172. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67818/0.70258. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67766/0.70236. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67599/0.70282. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67492/0.70344. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67490/0.70522. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67394/0.70425. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67307/0.70555. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67375/0.70609. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67034/0.70545. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67022/0.70621. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66847/0.70575. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66818/0.70586. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66809/0.70689. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66938/0.70630. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66864/0.70769. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66484/0.70843. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66846/0.70817. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66449/0.70796. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66611/0.70850. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66281/0.70912. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66148/0.70993. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66165/0.71083. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65838/0.70907. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66048/0.71050. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65866/0.71123. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65813/0.71226. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65663/0.71212. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65765/0.71273. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65720/0.71177. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65273/0.71116. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65430/0.71317. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65078/0.71387. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65334/0.71583. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65012/0.71344. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64890/0.71313. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64745/0.71492. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.64897/0.71431. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64448/0.71437. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64605/0.71489. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64539/0.71574. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64287/0.71684. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64216/0.71863. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64048/0.71592. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63924/0.71588. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63999/0.71661. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63993/0.71731. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63631/0.71833. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63515/0.72067. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63492/0.72014. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63191/0.72119. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63099/0.72267. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62877/0.72308. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62673/0.72287. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62723/0.72281. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62755/0.72519. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62486/0.72241. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62141/0.72624. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62188/0.72520. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62113/0.72601. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61818/0.72602. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62143/0.72604. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61822/0.72599. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.61641/0.72599. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61548/0.72745. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61425/0.72656. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61671/0.72780. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61402/0.72972. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61297/0.73032. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60819/0.73029. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69853/0.69784. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69454/0.69533. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69313/0.69392. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69194/0.69384. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69106/0.69315. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.69071/0.69233. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69014/0.69227. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68914/0.69239. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68915/0.69175. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68878/0.69170. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68878/0.69119. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68811/0.69168. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68798/0.69077. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68809/0.69093. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68711/0.69080. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68681/0.69082. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68702/0.69067. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.68670/0.69071. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68656/0.69061. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68596/0.69105. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68592/0.69105. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68553/0.69010. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68445/0.69103. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68569/0.69129. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68509/0.69093. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68472/0.69163. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68434/0.69149. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68415/0.69217. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68360/0.69245. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68303/0.69279. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68339/0.69328. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68307/0.69385. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68288/0.69340. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68260/0.69472. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68156/0.69429. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68141/0.69554. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68092/0.69594. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68066/0.69541. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68026/0.69623. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67933/0.69751. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67949/0.69777. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67810/0.69913. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68030/0.69870. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67871/0.69960. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67777/0.70082. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67664/0.70138. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67710/0.70196. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67766/0.70241. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67491/0.70477. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67341/0.70435. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67503/0.70640. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67440/0.70766. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67507/0.70671. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67140/0.70662. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67318/0.70990. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67348/0.71060. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67209/0.70979. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67133/0.71079. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67037/0.71197. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67033/0.71431. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66928/0.71397. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66867/0.71575. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66949/0.71496. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66680/0.71585. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66742/0.71643. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66767/0.71776. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66718/0.71904. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66540/0.71965. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66452/0.71994. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66514/0.72165. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66342/0.71903. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66241/0.72185. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66185/0.71947. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.66070/0.72267. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66145/0.72053. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66022/0.72197. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65998/0.72411. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65959/0.72559. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.65697/0.72508. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65583/0.72576. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65874/0.72361. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65387/0.72626. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.65231/0.72732. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65260/0.72646. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65098/0.72929. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64952/0.72909. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.65126/0.73044. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65109/0.73218. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64859/0.73219. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64748/0.73184. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64622/0.73288. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.64289/0.73217. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64243/0.73571. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.64530/0.73415. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.63986/0.73530. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64030/0.73865. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63951/0.73617. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63745/0.73977. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63923/0.73394. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63378/0.73472. Took 0.08 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69453/0.69226. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69277. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69266/0.69327. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69189/0.69377. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69117/0.69432. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69244/0.69500. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69062/0.69562. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69048/0.69628. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69123/0.69689. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69052/0.69738. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69010/0.69800. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68909/0.69857. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68901/0.69943. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68808/0.70019. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.70108. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.70183. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68673/0.70255. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68749/0.70373. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68644/0.70408. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68520/0.70465. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68407/0.70527. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68400/0.70598. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68439/0.70646. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68223/0.70649. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68279/0.70666. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68098/0.70758. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68263/0.70757. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68143/0.70790. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67979/0.70824. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67841/0.70855. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67621/0.70871. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67643/0.70891. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67747/0.70929. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67551/0.70912. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67492/0.70977. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67653/0.70979. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67388/0.70940. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67112/0.70948. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67085/0.70911. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67099/0.70927. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66902/0.70907. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67053/0.70891. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66665/0.70963. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66774/0.70983. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66317/0.70926. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66299/0.70959. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66250/0.71045. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66157/0.71060. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66083/0.70962. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66107/0.70928. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65598/0.70964. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.65987/0.70967. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65624/0.70924. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65366/0.71051. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65136/0.71055. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65041/0.71051. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65163/0.71029. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64795/0.71004. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64753/0.71045. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64482/0.71042. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64158/0.71059. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64502/0.71137. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64085/0.71255. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.63839/0.71366. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63701/0.71450. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63921/0.71203. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63632/0.71350. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63338/0.71320. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.63129/0.71367. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62850/0.71524. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62840/0.71560. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63028/0.71692. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.62732/0.71727. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62815/0.71744. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.62377/0.71949. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62071/0.72124. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62206/0.72128. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61938/0.72102. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.61802/0.72113. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61370/0.72201. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.61347/0.72337. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61884/0.72466. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61120/0.72290. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60705/0.72452. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.60841/0.72477. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60559/0.72741. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.60712/0.72876. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.60437/0.72905. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59863/0.73206. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59873/0.73056. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59225/0.73325. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.59370/0.73632. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59039/0.73198. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59137/0.73554. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.59189/0.73808. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58322/0.73774. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58914/0.73820. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58729/0.73843. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.58824/0.74176. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.58286/0.74235. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69679/0.69585. Took 0.09 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69848. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69192/0.69904. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69095/0.69937. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69187/0.69978. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69146/0.70006. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69079/0.70052. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69058/0.70087. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68967/0.70142. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68893/0.70188. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68931/0.70266. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68849/0.70327. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68870/0.70408. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68888/0.70507. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68753/0.70594. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68720/0.70687. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68762/0.70721. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68634/0.70789. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68652/0.70909. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68418/0.71017. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68552/0.71042. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68475/0.71149. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68512/0.71211. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68398/0.71287. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68329/0.71355. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68234/0.71433. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68372/0.71497. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68210/0.71515. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68208/0.71682. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68028/0.71608. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68091/0.71713. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67986/0.71731. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67867/0.71817. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67925/0.71963. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67664/0.72144. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67472/0.72252. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67456/0.72252. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67586/0.72305. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67421/0.72494. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67288/0.72517. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66860/0.72673. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66988/0.72762. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66852/0.72720. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66717/0.73023. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66830/0.72961. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66413/0.73073. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66671/0.73285. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66658/0.73453. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66191/0.73449. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66228/0.73653. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66021/0.73670. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66166/0.73694. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65812/0.73822. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66465/0.73781. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65980/0.73886. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65622/0.73966. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65683/0.74083. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65451/0.74135. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.65615/0.74112. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65335/0.74112. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65089/0.74230. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65361/0.74390. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64831/0.74544. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64853/0.74541. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65008/0.74449. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64927/0.74612. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64281/0.74626. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64291/0.74759. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64555/0.74834. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64138/0.74916. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.64168/0.75069. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.63977/0.75110. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63878/0.75184. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.63927/0.75176. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.63644/0.75247. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63182/0.75367. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.63704/0.75703. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63315/0.75791. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63127/0.75663. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.63209/0.75898. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63184/0.76065. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62642/0.76043. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.62718/0.76065. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62376/0.76013. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62400/0.75722. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62034/0.75898. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61600/0.76080. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.61957/0.76043. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.61542/0.76225. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61751/0.76375. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61822/0.76462. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.61271/0.76796. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.61070/0.76973. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.60761/0.76845. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.60637/0.76912. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.60694/0.77275. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60986/0.76908. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.60041/0.77268. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.60689/0.77216. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60556/0.77513. Took 0.08 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69480/0.68937. Took 0.11 sec\n",
      "Epoch 1, Loss(train/val) 0.69398/0.69291. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69659. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69215/0.69919. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69134/0.70132. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.70245. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69165/0.70315. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69072/0.70405. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69040/0.70513. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69021/0.70630. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68964/0.70664. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68945/0.70727. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68854/0.70822. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68860/0.70892. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68759/0.71055. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68759/0.71260. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68585/0.71378. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68715/0.71504. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68560/0.71607. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68651/0.71790. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68547/0.71880. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68548/0.71950. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68505/0.72025. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68596/0.72204. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68560/0.72242. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68485/0.72296. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68378/0.72220. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68383/0.72313. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68360/0.72245. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68246/0.72386. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68269/0.72490. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68254/0.72434. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68081/0.72399. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68096/0.72397. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68169/0.72471. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68017/0.72505. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67950/0.72471. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67979/0.72579. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67943/0.72496. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67831/0.72534. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67819/0.72403. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67855/0.72434. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67712/0.72610. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67645/0.72228. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67537/0.72508. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67402/0.72556. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67450/0.72334. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67457/0.72586. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67243/0.72210. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67013/0.72315. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67148/0.72318. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67151/0.72372. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66888/0.72145. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66946/0.72241. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66976/0.72390. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66768/0.72060. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66578/0.72165. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66545/0.72073. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66319/0.72053. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66471/0.72135. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66286/0.71961. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66233/0.72015. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66261/0.72020. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66087/0.72225. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65831/0.72132. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65640/0.72132. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65697/0.72201. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65515/0.71838. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65489/0.71993. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65485/0.72618. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65231/0.72768. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64895/0.72208. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64844/0.72557. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.64856/0.72392. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65006/0.72342. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64656/0.73110. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64579/0.73130. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.64131/0.73066. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64352/0.73473. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63971/0.73423. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63668/0.73319. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63716/0.73485. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64179/0.73478. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63350/0.73753. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63102/0.74300. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.63402/0.73995. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63209/0.74076. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.63147/0.74342. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62756/0.74711. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62226/0.75007. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61997/0.75540. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.62134/0.76167. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62039/0.75753. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62113/0.76006. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61888/0.75368. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61877/0.76158. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61449/0.76960. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61139/0.77830. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61332/0.77339. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.60879/0.78306. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69615/0.69100. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69337/0.69035. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69270/0.69003. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69147/0.68964. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.68936. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69079/0.68902. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.68872. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.68845. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68855/0.68826. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68822/0.68832. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68775/0.68843. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68646/0.68859. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68649/0.68885. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68638/0.68919. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68547/0.68954. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68378/0.68984. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68481/0.69008. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68423/0.69039. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68344/0.69072. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68297/0.69119. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68239/0.69163. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68245/0.69211. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68296/0.69228. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68213/0.69232. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68159/0.69256. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68081/0.69255. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68161/0.69281. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68018/0.69316. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67965/0.69345. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67857/0.69397. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67907/0.69418. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67803/0.69444. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67819/0.69452. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67652/0.69452. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67593/0.69445. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67579/0.69445. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67484/0.69453. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67508/0.69483. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67482/0.69511. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67242/0.69508. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67325/0.69513. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67156/0.69547. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67081/0.69552. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66970/0.69527. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66981/0.69551. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66855/0.69570. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66845/0.69565. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66690/0.69553. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66705/0.69521. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66423/0.69518. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66491/0.69493. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66407/0.69471. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66242/0.69510. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66038/0.69540. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65974/0.69518. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65887/0.69531. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65974/0.69461. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65759/0.69551. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65875/0.69627. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65540/0.69493. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.65457/0.69464. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65342/0.69539. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65076/0.69512. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.64904/0.69622. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65059/0.69604. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.64966/0.69480. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.64746/0.69835. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64767/0.69694. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.64487/0.69724. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.64215/0.69811. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63998/0.69970. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64175/0.69774. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.63783/0.69861. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63953/0.70101. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64101/0.69926. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.63656/0.69444. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63722/0.69835. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63492/0.69871. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.63604/0.69863. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63294/0.70059. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63167/0.69606. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.63179/0.69963. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63099/0.69850. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62732/0.70123. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.62689/0.70039. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62726/0.69945. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62597/0.69786. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.62120/0.70006. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62359/0.70168. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.61954/0.70247. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.62335/0.70285. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62089/0.70329. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62175/0.70381. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61828/0.71147. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61456/0.70680. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.61255/0.70748. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.61406/0.70893. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61420/0.71034. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61238/0.71175. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.60586/0.70853. Took 0.10 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69651/0.69527. Took 0.10 sec\n",
      "Epoch 1, Loss(train/val) 0.69319/0.69543. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69313/0.69532. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69233/0.69506. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69174/0.69527. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69107/0.69525. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69093/0.69532. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69058/0.69544. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68987/0.69545. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68976/0.69524. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68765/0.69607. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68877/0.69587. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68848/0.69640. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68748/0.69705. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68676/0.69697. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68544/0.69703. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68676/0.69785. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68532/0.69735. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68482/0.69822. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68398/0.69782. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68388/0.69901. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68306/0.69825. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68178/0.69899. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68244/0.69978. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68179/0.70047. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68065/0.69992. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68009/0.70032. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67873/0.70142. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67951/0.70126. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67644/0.70036. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67388/0.70199. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67633/0.70218. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67608/0.70250. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67426/0.70222. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67285/0.70263. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67298/0.70351. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67177/0.70512. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66950/0.70541. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66954/0.70582. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66704/0.70628. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66754/0.70863. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66639/0.70574. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66353/0.70850. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66399/0.70852. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66156/0.70922. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65845/0.70998. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.65957/0.71121. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65883/0.71301. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.65643/0.71311. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.65188/0.71131. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65328/0.71452. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65136/0.71503. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.65121/0.71739. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64734/0.71511. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.64687/0.71836. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64262/0.72045. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64091/0.72034. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.64268/0.72184. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.63926/0.72196. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.63740/0.72671. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.63602/0.72297. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.63227/0.72613. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63366/0.72292. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63070/0.72767. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.62814/0.72755. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62691/0.72997. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.62609/0.72913. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.62555/0.73333. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.61928/0.73489. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61723/0.73798. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.61784/0.73962. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61420/0.74110. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.61330/0.74291. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.61093/0.74459. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.60948/0.74764. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60894/0.75039. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60162/0.74990. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.60273/0.75424. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.59891/0.75771. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.59967/0.76084. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59496/0.76606. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59166/0.77208. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.58781/0.77366. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.58991/0.77592. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.58780/0.78054. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58861/0.77684. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58359/0.78265. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.58467/0.78180. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.58707/0.78660. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.57856/0.78735. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.57565/0.79144. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.57295/0.79739. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.56921/0.79353. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.57075/0.79750. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.56019/0.80622. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.56558/0.81487. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.56006/0.81066. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.56292/0.81634. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.55389/0.82203. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.55250/0.82800. Took 0.08 sec\n",
      "ACC: 0.5\n"
     ]
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\MM_Transformer_early-main\\data\\kdd17\\price_long_50\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Multimodal_transformer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Multimodal_transformer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "\n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model\n",
    "                Transformer = args.Transformer(args.input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout, args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Transformer_optimizer = optim.Adam(Transformer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Transformer, train_loss = train(Transformer,Transformer_optimizer, args, partition)\n",
    "                    Transformer, validation_loss = validation(Transformer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Transformer = args.Transformer(args.input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout,args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_transformer' + '.pt'))\n",
    "\n",
    "                ACC = test(Transformer, args, partition)\n",
    "                www.writerow([ACC])\n",
    "                print('ACC: {}'.format(ACC))\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "                \n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"MM_Transformer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
