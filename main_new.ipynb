{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def train(Transformer, Transformer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Transformer.zero_grad()\n",
    "        Transformer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "\n",
    "        Transf_out, attn_prob = Transformer(x)\n",
    "\n",
    "        loss = args.loss_fn(Transf_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Transformer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Transformer, train_loss\n",
    "\n",
    "def validation(Transformer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Transformer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out, attn_prob= Transformer(x)\n",
    "                        \n",
    "            loss = args.loss_fn(Transf_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Transformer, val_loss\n",
    "\n",
    "def test(Transformer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    AP_list = []\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out, attn_prob = Transformer(x)\n",
    "            \n",
    "            AP_list.append(attn_prob)\n",
    "            output_ = torch.where(Transf_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric, AP_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\MM_results_map_ent\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from Transformer_Encoder import Transformer\n",
    "\n",
    "args.Transformer = Transformer\n",
    "args.input_feature_size = 40\n",
    "\n",
    "args.Transformer_feature_size = 64\n",
    "\n",
    "args.nhead = 4\n",
    "\n",
    "#args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "Epoch 0, Loss(train/val) 0.70191/0.70028. Took 0.63 sec\n",
      "Epoch 1, Loss(train/val) 0.69948/0.69873. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69696/0.69723. Took 0.11 sec\n",
      "Epoch 3, Loss(train/val) 0.69428/0.69612. Took 0.12 sec\n",
      "Epoch 4, Loss(train/val) 0.69231/0.69555. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69106/0.69537. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69035/0.69535. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69020/0.69535. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68943/0.69531. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68915/0.69524. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68876/0.69517. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68880/0.69507. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68851/0.69495. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68826/0.69485. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68797/0.69474. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68834/0.69467. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68775/0.69455. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68764/0.69448. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68750/0.69441. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68742/0.69435. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68725/0.69430. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68710/0.69424. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68700/0.69422. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68661/0.69418. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68690/0.69416. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68628/0.69413. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68625/0.69412. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68590/0.69409. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68578/0.69411. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68567/0.69413. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68502/0.69414. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68515/0.69418. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68514/0.69421. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68480/0.69425. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68491/0.69431. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68441/0.69437. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68407/0.69443. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68386/0.69456. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68326/0.69468. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68320/0.69479. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68279/0.69491. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68286/0.69505. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68221/0.69518. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68194/0.69534. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68167/0.69552. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68108/0.69569. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68070/0.69585. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68049/0.69604. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67976/0.69624. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67910/0.69643. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67911/0.69662. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67831/0.69679. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67819/0.69699. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67794/0.69715. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67726/0.69734. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67690/0.69751. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67602/0.69766. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67608/0.69782. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67533/0.69800. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67492/0.69816. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67477/0.69830. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67443/0.69849. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67357/0.69865. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67294/0.69881. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67249/0.69891. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67237/0.69902. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67157/0.69915. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67112/0.69928. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67036/0.69935. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67039/0.69947. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66930/0.69962. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66880/0.69976. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66800/0.69995. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66751/0.70009. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66693/0.70018. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66642/0.70031. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66566/0.70046. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66496/0.70060. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66442/0.70078. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66329/0.70092. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66266/0.70118. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66221/0.70132. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66090/0.70152. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66041/0.70175. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65904/0.70185. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65826/0.70210. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65721/0.70242. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65653/0.70266. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65528/0.70282. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65431/0.70319. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65315/0.70358. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65258/0.70392. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65125/0.70429. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65087/0.70467. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64897/0.70511. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64783/0.70558. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64651/0.70606. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64589/0.70651. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64373/0.70710. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64211/0.70776. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69101/0.68984. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69002/0.68941. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68988/0.68907. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68944/0.68877. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68907/0.68855. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68888/0.68839. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68870/0.68830. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68820/0.68827. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68799/0.68829. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68783/0.68836. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68783/0.68845. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68737/0.68856. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68724/0.68870. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68732/0.68885. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68695/0.68904. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68708/0.68922. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68636/0.68941. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68625/0.68964. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68631/0.68985. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68607/0.69011. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68583/0.69038. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68541/0.69064. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68524/0.69092. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68527/0.69123. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68491/0.69153. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68484/0.69185. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68428/0.69218. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68414/0.69249. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68385/0.69282. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68324/0.69313. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68331/0.69341. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68309/0.69371. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68312/0.69395. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68261/0.69421. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68224/0.69444. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68193/0.69470. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68176/0.69487. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68158/0.69516. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68120/0.69535. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68100/0.69552. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68081/0.69565. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68035/0.69580. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68016/0.69597. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67969/0.69607. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67952/0.69616. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67874/0.69634. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67874/0.69647. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67862/0.69665. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67797/0.69691. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67782/0.69700. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67782/0.69716. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67727/0.69729. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67674/0.69735. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67665/0.69748. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67639/0.69768. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67587/0.69789. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67536/0.69812. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67504/0.69819. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67499/0.69851. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67452/0.69861. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67404/0.69871. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67396/0.69889. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67339/0.69919. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67319/0.69936. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67275/0.69950. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67256/0.69967. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67163/0.70008. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67172/0.70023. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67126/0.70050. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67117/0.70075. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67074/0.70110. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67019/0.70122. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67011/0.70147. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66928/0.70195. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66875/0.70211. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66912/0.70258. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66786/0.70287. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66781/0.70326. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66720/0.70382. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66726/0.70411. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66644/0.70440. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66612/0.70488. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66556/0.70512. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66480/0.70573. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66455/0.70610. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66442/0.70647. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66385/0.70673. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66334/0.70740. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66284/0.70765. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66234/0.70829. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66162/0.70845. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66192/0.70888. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66110/0.70962. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66071/0.70999. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66015/0.71037. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65941/0.71089. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65915/0.71093. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65909/0.71135. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65776/0.71199. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65799/0.71219. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68761/0.69284. Took 0.30 sec\n",
      "Epoch 1, Loss(train/val) 0.68722/0.69287. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68695/0.69289. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68668/0.69291. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68669/0.69291. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68660/0.69294. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68587/0.69294. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68570/0.69294. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68550/0.69293. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68501/0.69290. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68523/0.69287. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68481/0.69281. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68449/0.69274. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68411/0.69265. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68406/0.69259. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68361/0.69254. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68322/0.69244. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68255/0.69238. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68249/0.69233. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68164/0.69233. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68132/0.69240. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68072/0.69248. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68027/0.69259. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67965/0.69275. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67925/0.69298. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67860/0.69327. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67814/0.69361. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67732/0.69402. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67689/0.69446. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67609/0.69495. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67556/0.69545. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67506/0.69603. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67446/0.69666. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67407/0.69728. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67294/0.69797. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67288/0.69858. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67214/0.69923. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67131/0.70000. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67102/0.70075. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67044/0.70146. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66981/0.70217. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66883/0.70301. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66838/0.70373. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66788/0.70441. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66757/0.70515. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66624/0.70596. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66593/0.70667. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66517/0.70736. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66496/0.70817. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66365/0.70884. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66318/0.70962. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66219/0.71015. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66174/0.71086. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66130/0.71147. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66007/0.71217. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65898/0.71285. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65844/0.71347. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65716/0.71388. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65686/0.71446. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65625/0.71488. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65532/0.71538. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65439/0.71584. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65292/0.71657. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65263/0.71717. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65106/0.71755. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65040/0.71825. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64983/0.71883. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64880/0.71914. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64794/0.71964. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64684/0.72018. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64601/0.72084. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64452/0.72131. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64366/0.72182. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64254/0.72246. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64152/0.72300. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64065/0.72360. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63967/0.72417. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63854/0.72481. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63713/0.72543. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63534/0.72605. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63441/0.72658. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63350/0.72721. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63211/0.72806. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63063/0.72899. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63008/0.72992. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62832/0.73053. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62739/0.73142. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62564/0.73241. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62444/0.73359. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62293/0.73432. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62196/0.73526. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62064/0.73657. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61934/0.73744. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61768/0.73850. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61621/0.73958. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61455/0.74078. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61384/0.74191. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61277/0.74304. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61018/0.74456. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60903/0.74573. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.70032/0.68617. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69791/0.68892. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69568/0.69191. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69369/0.69546. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69953. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69057/0.70391. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68966/0.70793. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68882/0.71127. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68871/0.71343. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68868/0.71488. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68808/0.71570. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68770/0.71637. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68751/0.71676. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68716/0.71707. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68716/0.71720. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68655/0.71736. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68665/0.71751. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68620/0.71761. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68649/0.71781. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68566/0.71783. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68593/0.71789. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68551/0.71801. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68543/0.71817. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68524/0.71826. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68474/0.71838. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68448/0.71844. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68432/0.71849. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68403/0.71860. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68387/0.71868. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68381/0.71874. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68340/0.71881. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68322/0.71879. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68286/0.71897. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68255/0.71894. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68273/0.71888. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68209/0.71898. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68175/0.71911. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68152/0.71925. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68120/0.71916. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68066/0.71927. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68029/0.71954. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67973/0.71959. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67954/0.71948. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67915/0.71937. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67861/0.71955. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67858/0.71972. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67760/0.71995. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67706/0.72005. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67684/0.72006. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67614/0.71999. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67554/0.72017. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67490/0.72034. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67472/0.72085. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67421/0.72073. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67334/0.72074. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67305/0.72062. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67218/0.72084. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67179/0.72113. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67099/0.72086. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67041/0.72141. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66967/0.72102. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66860/0.72095. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66797/0.72131. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66714/0.72188. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66693/0.72207. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66555/0.72205. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66529/0.72242. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66425/0.72241. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66342/0.72274. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66301/0.72323. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66169/0.72273. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66126/0.72363. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66022/0.72345. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65978/0.72352. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65900/0.72440. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65780/0.72398. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65650/0.72500. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65597/0.72503. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65509/0.72488. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65486/0.72572. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65367/0.72603. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65299/0.72704. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65218/0.72739. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65135/0.72799. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65038/0.72890. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64979/0.72927. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64882/0.72949. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64762/0.73116. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64661/0.73139. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64664/0.73221. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64512/0.73302. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64494/0.73324. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64376/0.73426. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64244/0.73489. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64251/0.73681. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64174/0.73686. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63959/0.73828. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63881/0.73925. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63851/0.74045. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63799/0.74088. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.70911/0.70271. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.70003/0.69841. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69629/0.69696. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69450/0.69651. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69337/0.69622. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69330/0.69593. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69289/0.69564. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69268/0.69534. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69201/0.69507. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69162/0.69480. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69160/0.69454. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69170/0.69429. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69104/0.69407. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69093/0.69384. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69064/0.69361. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69021/0.69340. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69015/0.69322. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69013/0.69300. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69008/0.69280. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68963/0.69261. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68957/0.69241. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68926/0.69221. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68874/0.69200. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68889/0.69181. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68873/0.69160. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68862/0.69139. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68838/0.69118. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68796/0.69096. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68751/0.69073. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68756/0.69050. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68711/0.69029. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68721/0.69005. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68720/0.68979. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68690/0.68958. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68650/0.68932. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68632/0.68911. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68610/0.68887. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68589/0.68864. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68550/0.68841. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68516/0.68821. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68503/0.68798. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68489/0.68774. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68439/0.68749. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68447/0.68729. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68425/0.68706. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68354/0.68683. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68318/0.68667. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68322/0.68650. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68269/0.68630. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68278/0.68613. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68207/0.68591. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68207/0.68570. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68174/0.68550. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68125/0.68533. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68097/0.68522. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68090/0.68500. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68027/0.68486. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68005/0.68472. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67965/0.68460. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67880/0.68444. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67885/0.68425. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67875/0.68407. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67821/0.68387. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67765/0.68371. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67746/0.68356. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67664/0.68346. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67671/0.68332. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67533/0.68321. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67571/0.68303. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67494/0.68292. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67448/0.68288. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67353/0.68273. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67314/0.68275. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67253/0.68248. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67237/0.68247. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67177/0.68232. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67136/0.68220. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67060/0.68210. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66979/0.68214. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66964/0.68195. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66841/0.68191. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66816/0.68181. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66766/0.68189. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66655/0.68180. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66576/0.68156. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66546/0.68156. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66484/0.68151. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66440/0.68154. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66337/0.68167. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66266/0.68164. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66249/0.68147. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66188/0.68136. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66100/0.68155. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66072/0.68123. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65973/0.68123. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65941/0.68125. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65851/0.68133. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65808/0.68138. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65699/0.68119. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65630/0.68123. Took 0.09 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69606/0.70210. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69499/0.70157. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69447/0.70109. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69348/0.70065. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69293/0.70023. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69217/0.69982. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69161/0.69941. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69111/0.69902. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69067/0.69861. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69014/0.69816. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68978/0.69769. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.69715. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68884/0.69659. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68861/0.69601. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68839/0.69538. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68806/0.69472. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68736/0.69402. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68745/0.69328. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68660/0.69248. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68619/0.69168. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68582/0.69083. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68512/0.68994. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68480/0.68905. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68445/0.68812. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68375/0.68715. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68308/0.68622. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68279/0.68520. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68175/0.68419. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68139/0.68317. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68096/0.68224. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68045/0.68125. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67983/0.68028. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67931/0.67933. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67870/0.67840. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67789/0.67748. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67746/0.67654. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67730/0.67583. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67666/0.67501. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67551/0.67415. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67498/0.67338. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67451/0.67265. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67398/0.67198. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67360/0.67138. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67282/0.67065. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67187/0.67006. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67174/0.66957. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67106/0.66896. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67047/0.66852. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66980/0.66814. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66919/0.66756. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66900/0.66715. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66812/0.66681. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66793/0.66674. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66707/0.66647. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66635/0.66620. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66573/0.66604. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66461/0.66573. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66445/0.66564. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66390/0.66566. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66277/0.66565. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66253/0.66579. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66122/0.66568. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66103/0.66579. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66017/0.66586. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.65954/0.66603. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65889/0.66611. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65814/0.66642. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65765/0.66661. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65659/0.66696. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65586/0.66739. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65472/0.66761. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65397/0.66809. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65331/0.66854. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65230/0.66904. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65176/0.66963. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65083/0.67017. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65000/0.67093. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64913/0.67159. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64822/0.67237. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64692/0.67311. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64609/0.67395. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64549/0.67467. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64459/0.67563. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64409/0.67666. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64291/0.67745. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64189/0.67819. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64081/0.67906. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64021/0.68019. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63893/0.68140. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63820/0.68246. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63737/0.68355. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63672/0.68477. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63500/0.68582. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63472/0.68699. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63397/0.68826. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63306/0.68968. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63191/0.69075. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63115/0.69209. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63076/0.69316. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.62910/0.69439. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69074/0.68750. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69046/0.68753. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69040/0.68755. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69022/0.68759. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69009/0.68763. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68979/0.68767. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.68773. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68934/0.68780. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68934/0.68787. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68896/0.68797. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68865/0.68808. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68814/0.68821. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68797/0.68836. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68764/0.68851. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68706/0.68867. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68674/0.68889. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68608/0.68919. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68539/0.68942. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68466/0.68974. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68438/0.69010. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68328/0.69055. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68304/0.69104. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68223/0.69153. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68162/0.69210. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68096/0.69276. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68011/0.69340. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67979/0.69409. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67886/0.69490. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67786/0.69568. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67756/0.69651. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67660/0.69730. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67654/0.69822. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67525/0.69911. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67520/0.69998. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67445/0.70086. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67372/0.70184. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67360/0.70266. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67313/0.70362. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67222/0.70462. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67140/0.70550. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67063/0.70652. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67038/0.70767. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66936/0.70851. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66909/0.70956. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66857/0.71049. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66783/0.71157. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66685/0.71243. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66644/0.71354. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66578/0.71459. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66484/0.71571. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66490/0.71696. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66412/0.71791. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66366/0.71888. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66243/0.72000. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66208/0.72115. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66186/0.72236. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66096/0.72335. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66015/0.72443. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66001/0.72547. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65917/0.72646. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65878/0.72752. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.65769/0.72876. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65715/0.72982. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65685/0.73101. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65632/0.73218. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65531/0.73316. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65468/0.73448. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65470/0.73519. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65424/0.73663. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.65283/0.73770. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65248/0.73861. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65210/0.73957. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65073/0.74078. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65068/0.74190. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64998/0.74297. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64943/0.74411. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64884/0.74538. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64760/0.74594. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64762/0.74710. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64646/0.74814. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.64631/0.74878. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64577/0.74972. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64462/0.75099. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64433/0.75197. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64340/0.75294. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64365/0.75364. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64188/0.75428. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64208/0.75526. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64171/0.75653. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.64063/0.75730. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.63995/0.75838. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63909/0.75960. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63887/0.76010. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63792/0.76096. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63756/0.76214. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63617/0.76341. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63613/0.76364. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63545/0.76491. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63483/0.76565. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.63391/0.76628. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69346/0.69568. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69319/0.69571. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69303/0.69573. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69296/0.69574. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69255/0.69575. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69222/0.69576. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69190/0.69577. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69174/0.69578. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69120/0.69578. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69082/0.69577. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69046/0.69577. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69005/0.69579. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68967/0.69581. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68912/0.69585. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68876/0.69593. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68775/0.69601. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68696/0.69614. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68615/0.69633. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68497/0.69656. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68381/0.69687. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68269/0.69732. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68105/0.69780. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67987/0.69845. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67808/0.69919. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67686/0.70011. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67499/0.70099. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67330/0.70198. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67217/0.70303. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67064/0.70403. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66913/0.70505. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66797/0.70607. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66692/0.70700. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66553/0.70788. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66433/0.70864. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66337/0.70938. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66224/0.71023. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66160/0.71076. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66006/0.71147. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.65950/0.71220. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65870/0.71269. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65747/0.71324. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65647/0.71394. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65600/0.71415. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65458/0.71470. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65372/0.71506. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65295/0.71548. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65193/0.71587. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65116/0.71638. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64984/0.71689. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.64966/0.71729. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.64776/0.71762. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64709/0.71805. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.64636/0.71842. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64561/0.71864. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64465/0.71922. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64352/0.71971. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64283/0.72018. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64163/0.72054. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64020/0.72089. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63935/0.72124. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63812/0.72200. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63747/0.72269. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63641/0.72300. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63554/0.72371. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63411/0.72425. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63325/0.72466. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63239/0.72532. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63060/0.72608. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63035/0.72687. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62883/0.72758. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62776/0.72846. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62650/0.72914. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62524/0.73044. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62401/0.73159. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62334/0.73218. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62195/0.73263. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62118/0.73349. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61974/0.73462. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61797/0.73568. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61766/0.73663. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61731/0.73764. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61477/0.73846. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61353/0.73956. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61217/0.74056. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61190/0.74111. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61016/0.74246. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60868/0.74337. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60763/0.74421. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60663/0.74519. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60580/0.74636. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60379/0.74749. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60260/0.74895. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60128/0.74939. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59992/0.75051. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59866/0.75084. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59729/0.75181. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59558/0.75301. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59477/0.75424. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59288/0.75495. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59157/0.75562. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69652/0.69491. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69568/0.69614. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69486/0.69719. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69435/0.69808. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69388/0.69878. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69378/0.69931. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69338/0.69966. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69306/0.69991. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69287/0.70009. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69243/0.70015. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69225/0.70018. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69210/0.70014. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69159/0.70009. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69124/0.70001. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69133/0.69991. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69085/0.69976. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69057/0.69962. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69020/0.69956. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68988/0.69947. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68925/0.69929. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68917/0.69913. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68904/0.69896. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68871/0.69889. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68795/0.69884. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68755/0.69872. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68753/0.69861. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68731/0.69853. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68644/0.69846. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68635/0.69846. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68603/0.69828. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68548/0.69842. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68504/0.69848. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68451/0.69855. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68450/0.69872. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68411/0.69872. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68357/0.69879. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68308/0.69894. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68263/0.69907. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68228/0.69927. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68191/0.69933. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68153/0.69954. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68077/0.69967. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68055/0.69985. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67980/0.70020. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67917/0.70036. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67875/0.70057. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67803/0.70099. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67776/0.70120. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67729/0.70153. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67645/0.70185. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67568/0.70210. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67501/0.70278. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67419/0.70303. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67328/0.70362. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67277/0.70406. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67182/0.70430. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67132/0.70497. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67011/0.70558. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66955/0.70618. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66835/0.70671. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66785/0.70722. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66654/0.70773. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66576/0.70826. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66474/0.70894. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66417/0.70936. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66288/0.70960. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66174/0.71012. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66088/0.71090. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65991/0.71116. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65882/0.71197. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65885/0.71204. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65720/0.71310. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65677/0.71348. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65564/0.71350. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65472/0.71378. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65384/0.71386. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65305/0.71426. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65209/0.71465. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65127/0.71557. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65069/0.71488. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65010/0.71591. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64906/0.71600. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64830/0.71614. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64763/0.71646. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64657/0.71659. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64596/0.71699. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64566/0.71725. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64544/0.71769. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64474/0.71764. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64383/0.71829. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64299/0.71872. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64290/0.71866. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64231/0.71891. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64087/0.71979. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64120/0.71956. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64040/0.72017. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63954/0.72061. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63885/0.72098. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63784/0.72116. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63720/0.72212. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69876/0.69543. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69739/0.69484. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69641/0.69447. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69589/0.69424. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69508/0.69408. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69433/0.69396. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69388/0.69390. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69356/0.69386. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69318/0.69384. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69287/0.69385. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69248/0.69386. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69149/0.69389. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69124/0.69394. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69088/0.69401. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69045/0.69410. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69054/0.69420. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69014/0.69430. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68977/0.69445. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68940/0.69463. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68891/0.69482. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68864/0.69502. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68841/0.69524. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68807/0.69548. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68764/0.69574. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68731/0.69600. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68690/0.69630. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68674/0.69657. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68633/0.69687. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68612/0.69723. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68590/0.69752. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68527/0.69787. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68542/0.69822. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68459/0.69859. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68446/0.69892. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68433/0.69923. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68407/0.69956. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68348/0.69988. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68318/0.70021. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68282/0.70058. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68261/0.70087. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68205/0.70116. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68178/0.70148. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68144/0.70179. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68112/0.70206. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68075/0.70231. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68012/0.70253. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68023/0.70274. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67976/0.70297. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67920/0.70323. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67895/0.70344. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67860/0.70366. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67837/0.70384. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67763/0.70404. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67745/0.70424. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67664/0.70435. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67654/0.70453. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67587/0.70467. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67551/0.70484. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67463/0.70497. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67444/0.70507. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67440/0.70517. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67343/0.70532. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67365/0.70535. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67259/0.70551. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67191/0.70555. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67138/0.70567. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67090/0.70575. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67076/0.70587. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66983/0.70607. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66914/0.70613. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66849/0.70636. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66844/0.70656. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66778/0.70668. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66719/0.70696. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66625/0.70718. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66587/0.70737. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66508/0.70759. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66482/0.70785. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66483/0.70804. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66366/0.70826. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66320/0.70857. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66296/0.70886. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66217/0.70919. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66146/0.70944. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66123/0.70977. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66019/0.71030. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65998/0.71063. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65834/0.71117. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65855/0.71174. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65797/0.71227. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65822/0.71263. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65703/0.71289. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65646/0.71338. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65635/0.71411. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65588/0.71456. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65496/0.71512. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65429/0.71574. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65370/0.71627. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65328/0.71699. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65328/0.71762. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69339/0.68864. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.68867. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69299/0.68870. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69302/0.68873. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69271/0.68876. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69238/0.68876. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69224/0.68879. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69230/0.68883. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69226/0.68885. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69192/0.68889. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69214/0.68892. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69181/0.68895. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69152/0.68898. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69125/0.68898. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69113/0.68896. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69099/0.68893. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69069/0.68889. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69039/0.68880. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69014/0.68869. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69001/0.68860. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68955/0.68841. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68961/0.68823. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68910/0.68802. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68871/0.68771. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68803/0.68735. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68767/0.68706. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68720/0.68667. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68663/0.68631. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68610/0.68603. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68575/0.68592. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68481/0.68553. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68442/0.68540. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68364/0.68522. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68323/0.68502. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68267/0.68492. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68236/0.68494. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68125/0.68476. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68099/0.68485. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68054/0.68502. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68006/0.68506. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67945/0.68516. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67833/0.68544. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67854/0.68551. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67749/0.68576. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67697/0.68589. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67685/0.68625. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67592/0.68620. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67546/0.68642. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67471/0.68660. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67383/0.68695. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67304/0.68725. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67296/0.68758. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67206/0.68800. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67139/0.68844. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67046/0.68863. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67000/0.68896. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66930/0.68946. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66878/0.69012. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66811/0.69030. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66711/0.69089. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66595/0.69154. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66549/0.69227. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66422/0.69277. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66393/0.69355. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66290/0.69471. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66182/0.69518. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66114/0.69604. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66009/0.69715. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65930/0.69774. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65795/0.69871. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65737/0.69944. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65561/0.70020. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65540/0.70120. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65378/0.70245. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65284/0.70311. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65282/0.70462. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65166/0.70545. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65093/0.70633. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64888/0.70758. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64843/0.70927. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64738/0.71022. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64523/0.71123. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64546/0.71279. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64405/0.71372. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64263/0.71471. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64195/0.71627. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64054/0.71763. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63956/0.71920. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63934/0.72054. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63686/0.72182. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63670/0.72348. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63513/0.72530. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63415/0.72627. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63232/0.72851. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63185/0.72989. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63035/0.73142. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62929/0.73279. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62796/0.73409. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62664/0.73611. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62390/0.73799. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69593/0.69209. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69561/0.69208. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69528/0.69206. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69521/0.69203. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69467/0.69201. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69435/0.69200. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69382/0.69200. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69362/0.69202. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69346/0.69206. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69309/0.69214. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69298/0.69221. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69268/0.69228. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69265/0.69236. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69232/0.69242. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69228/0.69247. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69245/0.69251. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69220/0.69256. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69204/0.69260. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69216/0.69263. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69161/0.69267. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69099/0.69271. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69132/0.69273. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69110/0.69276. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69053/0.69281. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.69043/0.69286. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69019/0.69292. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69017/0.69299. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68987/0.69309. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68941/0.69317. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68914/0.69327. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68889/0.69341. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68800/0.69357. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68767/0.69380. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68729/0.69403. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68694/0.69429. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68641/0.69461. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68605/0.69497. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68535/0.69528. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68517/0.69567. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68431/0.69613. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68390/0.69660. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68330/0.69708. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68251/0.69761. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68243/0.69817. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68129/0.69878. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68092/0.69942. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68042/0.69994. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67959/0.70057. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67953/0.70118. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67878/0.70184. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67849/0.70244. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67811/0.70301. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67708/0.70353. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67648/0.70409. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67571/0.70465. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67599/0.70524. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67486/0.70578. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67390/0.70620. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67336/0.70681. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67253/0.70723. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67183/0.70763. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67148/0.70804. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67022/0.70843. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67024/0.70889. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66903/0.70921. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66834/0.70978. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66769/0.71017. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66694/0.71046. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66685/0.71088. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66551/0.71119. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66514/0.71152. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66386/0.71194. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66340/0.71231. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66229/0.71250. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66231/0.71294. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66083/0.71318. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66032/0.71352. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66014/0.71391. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65841/0.71430. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65806/0.71476. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65702/0.71479. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65550/0.71501. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65528/0.71537. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65439/0.71560. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65412/0.71588. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65219/0.71600. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65151/0.71648. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65032/0.71664. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64871/0.71696. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64818/0.71698. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64672/0.71717. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64605/0.71757. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64464/0.71777. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64385/0.71809. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64261/0.71810. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64048/0.71824. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63978/0.71804. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63776/0.71821. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63670/0.71869. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63573/0.71914. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69061. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69367/0.69050. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69320/0.69040. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69254/0.69027. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69222/0.69013. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69172/0.68995. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69129/0.68975. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69110/0.68955. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69091/0.68935. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69045/0.68916. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69004/0.68899. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68993/0.68881. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68970/0.68866. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68920/0.68852. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68907/0.68839. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68869/0.68827. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68863/0.68813. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.68801. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68791/0.68789. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68764/0.68775. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68710/0.68757. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68687/0.68744. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68642/0.68727. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68616/0.68711. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68564/0.68696. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68533/0.68677. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68485/0.68661. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68424/0.68646. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68396/0.68629. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68342/0.68615. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68264/0.68599. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68170/0.68586. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68139/0.68573. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68057/0.68560. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67975/0.68549. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67927/0.68539. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67865/0.68536. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67794/0.68535. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67695/0.68532. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67595/0.68524. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67535/0.68527. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67433/0.68549. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67334/0.68565. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67199/0.68582. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67167/0.68608. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67039/0.68639. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66976/0.68678. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66874/0.68708. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66754/0.68754. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66602/0.68811. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66505/0.68885. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66399/0.68958. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66264/0.69034. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66140/0.69117. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66035/0.69201. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65918/0.69298. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65800/0.69395. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65605/0.69512. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65503/0.69631. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65440/0.69752. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65278/0.69860. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65134/0.69978. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65083/0.70100. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64869/0.70227. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64785/0.70355. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64609/0.70490. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64522/0.70625. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64407/0.70760. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64239/0.70878. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64125/0.70995. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63991/0.71127. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63855/0.71253. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63703/0.71400. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63593/0.71549. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63451/0.71686. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63386/0.71799. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63212/0.71903. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63014/0.72035. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62974/0.72162. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62805/0.72259. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62762/0.72359. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62649/0.72462. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62569/0.72571. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62277/0.72651. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62231/0.72754. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62181/0.72843. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62060/0.72926. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61922/0.72993. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61835/0.73091. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61623/0.73182. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61539/0.73221. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61372/0.73286. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61214/0.73370. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61156/0.73421. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61038/0.73450. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60821/0.73522. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60711/0.73590. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60628/0.73664. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60411/0.73705. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60331/0.73825. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69204/0.69252. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69201/0.69245. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69190/0.69237. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69174/0.69229. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69156/0.69222. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69134/0.69214. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69143/0.69205. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69196. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69105/0.69185. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69080/0.69172. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69062/0.69159. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69038/0.69146. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69009/0.69131. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68966/0.69117. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68918/0.69101. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68885/0.69085. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68845/0.69070. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68783/0.69058. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68723/0.69049. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68662/0.69041. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68605/0.69037. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68500/0.69036. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68419/0.69039. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68318/0.69048. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68243/0.69056. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68154/0.69071. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68052/0.69091. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67952/0.69115. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67849/0.69141. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67726/0.69177. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67631/0.69210. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67547/0.69261. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67415/0.69295. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67310/0.69352. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67179/0.69409. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67109/0.69455. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66987/0.69503. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66872/0.69551. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66717/0.69600. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66685/0.69657. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66524/0.69707. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66444/0.69753. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66324/0.69814. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66211/0.69858. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66113/0.69904. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66018/0.69945. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65916/0.69983. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65835/0.70024. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65755/0.70068. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65605/0.70115. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65524/0.70140. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65506/0.70152. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65313/0.70165. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65290/0.70178. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65144/0.70231. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65060/0.70243. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64942/0.70280. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64885/0.70279. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64794/0.70294. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64662/0.70334. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64581/0.70374. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64464/0.70391. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64318/0.70424. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64270/0.70454. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64186/0.70484. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64062/0.70518. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64086/0.70548. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63899/0.70570. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63828/0.70604. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63763/0.70635. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63726/0.70654. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63598/0.70665. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63469/0.70693. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63393/0.70723. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63290/0.70753. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63135/0.70786. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63123/0.70809. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63021/0.70823. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62864/0.70841. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62856/0.70871. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62700/0.70924. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62616/0.70953. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62603/0.70967. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62439/0.70968. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62337/0.70988. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62264/0.71012. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62133/0.71032. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62031/0.71062. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62022/0.71066. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61866/0.71101. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61813/0.71115. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61720/0.71124. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61603/0.71161. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61397/0.71181. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61428/0.71249. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61269/0.71298. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61199/0.71320. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61112/0.71335. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60948/0.71320. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60907/0.71346. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69209/0.69779. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69190/0.69776. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69155/0.69774. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.69772. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69149/0.69771. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69129/0.69772. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69774. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69115/0.69778. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69059/0.69786. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69049/0.69795. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69017/0.69806. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69011/0.69822. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68980/0.69838. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68993/0.69855. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68942/0.69871. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68942/0.69895. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68872/0.69923. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69958. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68844/0.69994. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68841/0.70033. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68783/0.70076. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68745/0.70121. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68673/0.70177. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68610/0.70232. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68581/0.70287. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68551/0.70349. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68469/0.70419. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68435/0.70477. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68343/0.70554. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68341/0.70621. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68279/0.70695. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68219/0.70774. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68159/0.70843. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68106/0.70923. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68052/0.71009. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67951/0.71082. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67872/0.71170. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67816/0.71243. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67738/0.71333. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67700/0.71417. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67633/0.71483. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67542/0.71560. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67506/0.71636. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67441/0.71706. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67383/0.71786. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67277/0.71852. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67255/0.71930. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67124/0.72011. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67067/0.72069. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67000/0.72133. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66918/0.72190. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66876/0.72256. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66779/0.72311. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66687/0.72362. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66608/0.72397. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66499/0.72461. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66393/0.72519. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66350/0.72573. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66308/0.72607. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66195/0.72640. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66102/0.72661. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65995/0.72699. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65868/0.72731. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65784/0.72759. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65649/0.72810. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65548/0.72857. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65425/0.72911. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65361/0.72927. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65308/0.72949. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65144/0.73010. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65030/0.73042. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64900/0.73065. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64735/0.73103. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64576/0.73163. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64531/0.73226. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64398/0.73264. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64286/0.73312. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64223/0.73383. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64107/0.73445. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63955/0.73500. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63803/0.73535. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63596/0.73605. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63572/0.73662. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63431/0.73740. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63313/0.73803. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63235/0.73870. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63066/0.73981. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62951/0.74032. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62758/0.74093. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62700/0.74192. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62684/0.74269. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62563/0.74339. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62469/0.74409. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62300/0.74436. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62124/0.74545. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62023/0.74619. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61915/0.74648. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61772/0.74739. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61596/0.74812. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61569/0.74905. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70099/0.70179. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69536/0.69747. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69286/0.69597. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69222/0.69569. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69157/0.69583. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69142/0.69616. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69101/0.69649. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69077/0.69685. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69031/0.69721. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69019/0.69758. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69007/0.69795. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68968/0.69828. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68958/0.69863. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68931/0.69900. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68909/0.69935. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68871/0.69967. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68856/0.70001. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68857/0.70036. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68846/0.70073. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68808/0.70107. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68770/0.70145. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68749/0.70179. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68741/0.70213. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68723/0.70249. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68718/0.70284. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68674/0.70318. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68651/0.70358. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68683/0.70388. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68615/0.70424. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68612/0.70459. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68578/0.70492. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68546/0.70525. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68561/0.70562. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68532/0.70595. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68471/0.70627. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68464/0.70660. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68455/0.70693. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68427/0.70726. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68390/0.70757. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68366/0.70785. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68366/0.70818. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68309/0.70850. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68242/0.70881. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68243/0.70914. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68230/0.70944. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68240/0.70968. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68183/0.70994. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68149/0.71024. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68104/0.71050. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68113/0.71070. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68024/0.71096. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68019/0.71121. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67973/0.71143. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67970/0.71162. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67924/0.71182. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67830/0.71204. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67831/0.71221. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67805/0.71233. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67752/0.71247. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67735/0.71269. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67688/0.71290. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67611/0.71307. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67540/0.71328. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67512/0.71346. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67496/0.71354. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67435/0.71364. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67419/0.71379. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67325/0.71386. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67316/0.71395. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67240/0.71406. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67223/0.71420. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67128/0.71423. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67091/0.71432. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67000/0.71451. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66958/0.71460. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66925/0.71465. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66773/0.71493. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66808/0.71505. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66742/0.71530. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66653/0.71549. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66639/0.71565. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66542/0.71588. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66505/0.71616. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66445/0.71638. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66411/0.71659. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66310/0.71667. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66287/0.71694. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66175/0.71742. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66139/0.71767. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66032/0.71800. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65958/0.71821. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65938/0.71861. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65859/0.71892. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65893/0.71925. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65805/0.71936. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65764/0.71990. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65740/0.72008. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65637/0.72024. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65625/0.72052. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65553/0.72085. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69505/0.69171. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69462/0.69173. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69435/0.69177. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69384/0.69182. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69332/0.69190. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69302/0.69201. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69273/0.69217. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69230/0.69236. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69188/0.69260. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69127/0.69287. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.69316. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69047/0.69347. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69023/0.69380. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68974/0.69417. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68919/0.69457. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68833/0.69498. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68820/0.69542. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68778/0.69590. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68693/0.69645. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68652/0.69706. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68599/0.69771. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68548/0.69841. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68486/0.69916. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68398/0.69999. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68336/0.70083. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68285/0.70173. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68200/0.70259. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68144/0.70348. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68066/0.70441. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68032/0.70540. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67951/0.70638. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67900/0.70734. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67791/0.70834. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67732/0.70932. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67703/0.71034. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67567/0.71131. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67519/0.71231. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67462/0.71327. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67345/0.71419. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67298/0.71516. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67234/0.71602. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67163/0.71689. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67099/0.71781. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67005/0.71870. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66906/0.71965. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66851/0.72055. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66791/0.72145. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66759/0.72227. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66707/0.72317. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66596/0.72401. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66533/0.72479. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66474/0.72566. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66459/0.72652. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66382/0.72734. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66348/0.72815. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66232/0.72890. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66180/0.72985. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66114/0.73072. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66087/0.73154. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66043/0.73226. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65929/0.73300. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65886/0.73378. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65739/0.73455. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.65751/0.73533. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65677/0.73608. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65619/0.73696. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65556/0.73769. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65466/0.73849. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65395/0.73932. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65402/0.74009. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65335/0.74083. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65230/0.74153. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65145/0.74244. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65121/0.74324. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.65010/0.74403. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65012/0.74481. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64920/0.74547. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64812/0.74630. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64749/0.74710. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64697/0.74796. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64609/0.74874. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.64524/0.74955. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64437/0.75035. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64393/0.75094. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64330/0.75166. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64192/0.75233. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.64109/0.75304. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64016/0.75375. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63950/0.75456. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63920/0.75549. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63814/0.75631. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63688/0.75709. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63599/0.75788. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.63464/0.75874. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63381/0.75934. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63306/0.76017. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63273/0.76075. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63171/0.76158. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63129/0.76227. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62930/0.76297. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69610/0.69123. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69533/0.69135. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69500/0.69153. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69476/0.69176. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69432/0.69211. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69364/0.69260. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69315/0.69314. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69280/0.69375. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69247/0.69427. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69199/0.69472. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69184/0.69503. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69149/0.69529. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69133/0.69543. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69124/0.69550. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69088/0.69549. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69073/0.69547. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69046/0.69530. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68975/0.69509. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68966/0.69490. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68930/0.69460. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68880/0.69431. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68856/0.69404. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68805/0.69371. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68747/0.69334. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68712/0.69288. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68637/0.69254. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68587/0.69220. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68534/0.69201. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68505/0.69173. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68451/0.69163. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68416/0.69140. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68356/0.69126. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68311/0.69112. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68267/0.69099. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68230/0.69084. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68178/0.69082. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68145/0.69069. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68094/0.69059. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68068/0.69053. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68009/0.69040. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67971/0.69031. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67923/0.69023. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67851/0.69015. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67829/0.69004. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67776/0.68987. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67736/0.68988. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67682/0.68967. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67621/0.68954. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67559/0.68933. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67497/0.68926. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67422/0.68918. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67350/0.68908. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67326/0.68890. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67275/0.68881. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67159/0.68879. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67140/0.68871. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67062/0.68865. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66989/0.68850. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66963/0.68848. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66840/0.68862. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66724/0.68854. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66697/0.68864. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66649/0.68878. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66513/0.68876. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66453/0.68901. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66354/0.68919. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66301/0.68934. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66229/0.68967. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66145/0.68981. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66018/0.69023. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65996/0.69058. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65827/0.69086. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65764/0.69143. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65708/0.69160. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65566/0.69226. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65450/0.69272. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65331/0.69327. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65299/0.69380. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65198/0.69442. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65143/0.69501. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64937/0.69563. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64868/0.69613. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64745/0.69674. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64683/0.69747. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64501/0.69807. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64416/0.69881. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64345/0.69963. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64205/0.70030. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64081/0.70101. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63918/0.70168. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63904/0.70251. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63751/0.70318. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63576/0.70382. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63580/0.70448. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63436/0.70526. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63379/0.70602. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63203/0.70683. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63042/0.70772. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63014/0.70871. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62870/0.70930. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69359/0.69794. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69290/0.69787. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.69782. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69172/0.69779. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.69774. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69089/0.69766. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69058/0.69757. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68978/0.69747. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68903/0.69739. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68840/0.69735. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68805/0.69733. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68747/0.69737. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68735/0.69740. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68705/0.69749. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68593/0.69766. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68582/0.69780. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68526/0.69801. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68443/0.69827. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68365/0.69858. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68326/0.69893. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68210/0.69937. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68178/0.69982. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68073/0.70039. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68002/0.70102. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67902/0.70176. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67785/0.70258. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67705/0.70352. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67560/0.70454. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67473/0.70566. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67375/0.70685. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67286/0.70805. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67163/0.70941. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67091/0.71082. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66975/0.71227. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66888/0.71361. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66792/0.71516. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66667/0.71665. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66592/0.71818. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66495/0.71962. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66401/0.72110. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66350/0.72261. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66235/0.72419. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66161/0.72562. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66006/0.72694. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66001/0.72828. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65907/0.72969. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65831/0.73086. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65722/0.73199. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65645/0.73342. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65552/0.73471. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65466/0.73592. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65451/0.73710. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65360/0.73828. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65296/0.73934. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65175/0.74035. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65053/0.74160. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65066/0.74275. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65011/0.74375. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64959/0.74472. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64834/0.74570. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64814/0.74665. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64679/0.74754. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64625/0.74842. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64527/0.74925. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64438/0.75024. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64364/0.75126. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64325/0.75211. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64210/0.75328. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64156/0.75434. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64029/0.75543. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63966/0.75628. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63881/0.75737. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63809/0.75855. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63668/0.75951. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63574/0.76057. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63577/0.76153. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63421/0.76242. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63367/0.76308. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63313/0.76447. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63204/0.76550. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63074/0.76616. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62968/0.76747. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62813/0.76864. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62867/0.76957. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62745/0.77036. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62539/0.77149. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62474/0.77244. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62371/0.77362. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62331/0.77451. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62193/0.77580. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62053/0.77721. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61866/0.77814. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61781/0.77941. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61656/0.78077. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61481/0.78195. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61404/0.78274. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61334/0.78426. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61120/0.78500. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61010/0.78608. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60918/0.78685. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69366/0.69706. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69321/0.69697. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69303/0.69692. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69244/0.69691. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69694. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69698. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69143/0.69704. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69112/0.69713. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.69726. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.69740. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69006/0.69756. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68994/0.69774. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68956/0.69794. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68932/0.69815. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68879/0.69837. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68828/0.69862. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68813/0.69889. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68752/0.69919. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68741/0.69950. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68656/0.69984. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68605/0.70022. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68532/0.70060. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68459/0.70107. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68409/0.70156. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68328/0.70209. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68307/0.70263. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68214/0.70321. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68170/0.70377. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68111/0.70434. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68047/0.70492. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67990/0.70547. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67944/0.70601. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67871/0.70658. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67815/0.70710. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67786/0.70764. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67695/0.70816. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67647/0.70868. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67605/0.70920. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67543/0.70969. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67507/0.71015. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67440/0.71061. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67362/0.71110. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67348/0.71157. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67247/0.71204. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67228/0.71249. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67113/0.71292. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67096/0.71333. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67078/0.71382. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66977/0.71435. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66891/0.71479. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66818/0.71538. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66775/0.71593. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66737/0.71650. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66663/0.71700. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66636/0.71760. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66562/0.71817. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 0.66459/0.71880. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66417/0.71944. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66331/0.72004. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66299/0.72067. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66227/0.72128. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66124/0.72197. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66083/0.72266. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66005/0.72335. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65930/0.72408. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65823/0.72495. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65803/0.72576. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65723/0.72639. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65722/0.72723. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65576/0.72792. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65549/0.72867. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65561/0.72950. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65474/0.73006. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65372/0.73076. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65299/0.73148. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65266/0.73207. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65235/0.73272. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65157/0.73323. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65069/0.73400. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64971/0.73473. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64924/0.73539. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64896/0.73608. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64822/0.73667. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64666/0.73725. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64635/0.73803. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64571/0.73869. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64499/0.73917. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64470/0.73994. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64410/0.74059. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64235/0.74130. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64236/0.74194. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64227/0.74247. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64076/0.74300. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64101/0.74359. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63921/0.74420. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63896/0.74480. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63790/0.74534. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63758/0.74581. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63602/0.74649. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63609/0.74700. Took 0.09 sec\n",
      "ACC: 0.3645833333333333\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69678. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69398/0.69660. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69392/0.69641. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69368/0.69620. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69320/0.69596. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69308/0.69573. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69259/0.69549. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69248/0.69527. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69242/0.69507. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69212/0.69487. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69193/0.69472. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69177/0.69458. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69174/0.69447. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69165/0.69440. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69129/0.69435. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69101/0.69430. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69068/0.69428. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69081/0.69426. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69057/0.69425. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69030/0.69425. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69012/0.69428. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68991/0.69432. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68970/0.69437. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68947/0.69441. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68924/0.69444. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68891/0.69449. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68866/0.69454. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68815/0.69460. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68790/0.69463. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68781/0.69468. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68716/0.69478. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68723/0.69484. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68664/0.69492. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68611/0.69502. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68593/0.69507. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68578/0.69514. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68525/0.69523. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68497/0.69529. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68470/0.69529. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68412/0.69536. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68368/0.69546. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68315/0.69550. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68243/0.69558. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68211/0.69564. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68196/0.69568. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68171/0.69564. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68098/0.69567. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68016/0.69567. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67981/0.69564. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67934/0.69566. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67882/0.69570. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67833/0.69575. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67762/0.69578. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67730/0.69578. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67657/0.69586. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67610/0.69594. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67511/0.69603. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67468/0.69626. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67404/0.69633. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67303/0.69652. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67284/0.69674. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67162/0.69697. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67105/0.69723. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67049/0.69756. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66944/0.69798. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66871/0.69840. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66751/0.69886. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66703/0.69951. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66653/0.70021. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66549/0.70080. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66461/0.70154. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66358/0.70229. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66303/0.70313. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66166/0.70393. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66094/0.70491. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65987/0.70584. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65907/0.70683. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65862/0.70761. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65714/0.70867. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65641/0.70988. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65493/0.71102. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65396/0.71196. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65356/0.71299. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65222/0.71416. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65136/0.71553. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65011/0.71676. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64872/0.71791. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64820/0.71917. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64679/0.72054. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64647/0.72171. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64548/0.72302. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64461/0.72395. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64266/0.72494. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64270/0.72617. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64201/0.72741. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64060/0.72826. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63920/0.72929. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63837/0.73053. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63837/0.73164. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63612/0.73297. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69373/0.69671. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69329/0.69662. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69349/0.69653. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69331/0.69644. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69327/0.69639. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69319/0.69631. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69288/0.69627. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69289/0.69623. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69286/0.69622. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69289/0.69622. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69254/0.69621. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69241/0.69625. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69208/0.69629. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69209/0.69634. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69180/0.69642. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69187/0.69651. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69150/0.69662. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69124/0.69677. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69113/0.69694. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69073/0.69715. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69055/0.69736. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69023/0.69769. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68993/0.69801. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68931/0.69845. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68899/0.69894. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68840/0.69945. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68785/0.70008. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68755/0.70065. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68700/0.70129. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68609/0.70199. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68592/0.70278. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68482/0.70364. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68428/0.70435. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68373/0.70543. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68274/0.70629. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68198/0.70712. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68168/0.70791. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68078/0.70895. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68006/0.71029. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67899/0.71088. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67839/0.71189. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67764/0.71290. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67658/0.71415. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67554/0.71537. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67511/0.71619. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67425/0.71734. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67307/0.71865. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67225/0.71974. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67142/0.72092. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67025/0.72222. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66927/0.72341. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66824/0.72493. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66685/0.72631. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66597/0.72772. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66496/0.72878. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66480/0.73016. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66341/0.73156. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66193/0.73293. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66133/0.73452. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65986/0.73545. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65890/0.73706. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65737/0.73777. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65643/0.73940. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65570/0.74104. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65381/0.74230. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65272/0.74329. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65145/0.74480. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64985/0.74580. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64965/0.74712. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64762/0.74816. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64661/0.74987. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64552/0.75135. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64373/0.75242. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64268/0.75449. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64126/0.75537. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63966/0.75675. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63895/0.75845. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63737/0.75923. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63499/0.75983. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63490/0.76264. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63296/0.76312. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63225/0.76450. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63049/0.76595. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62910/0.76766. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62747/0.76942. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62679/0.77091. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62508/0.77259. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62325/0.77306. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62235/0.77419. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62157/0.77595. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61943/0.77738. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61863/0.77921. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61606/0.78147. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61511/0.78482. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61390/0.78553. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61195/0.78660. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61183/0.78798. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61183/0.79105. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60774/0.79219. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60696/0.79550. Took 0.10 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69673/0.68695. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69479/0.68697. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69361/0.68721. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69298/0.68758. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69233/0.68790. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69230/0.68812. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69187/0.68824. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69187/0.68825. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69185/0.68824. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69175/0.68817. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69164/0.68812. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69138/0.68802. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69132/0.68793. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69117/0.68786. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69113/0.68780. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69084/0.68769. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69080/0.68759. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69085/0.68753. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69063/0.68743. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69056/0.68732. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69034/0.68722. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69039/0.68717. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69018/0.68710. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68996/0.68703. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69013/0.68695. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68980/0.68687. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68990/0.68680. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68968/0.68678. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68964/0.68676. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68966/0.68668. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68918/0.68662. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68898/0.68658. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68918/0.68654. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68920/0.68650. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68878/0.68646. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68871/0.68641. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68841/0.68639. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68831/0.68639. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68822/0.68639. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68805/0.68634. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68783/0.68634. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68784/0.68634. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68773/0.68630. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68722/0.68630. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68739/0.68637. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68690/0.68638. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68658/0.68645. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68648/0.68649. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68634/0.68658. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68638/0.68665. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68582/0.68669. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68562/0.68670. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68538/0.68682. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68546/0.68692. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68527/0.68703. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68483/0.68719. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68445/0.68736. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68422/0.68746. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68391/0.68757. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68351/0.68778. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68325/0.68795. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68292/0.68825. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68268/0.68852. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68233/0.68873. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68190/0.68899. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68152/0.68934. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68086/0.68969. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68090/0.69002. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68050/0.69043. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68004/0.69093. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67931/0.69133. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67887/0.69176. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67841/0.69221. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67801/0.69281. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67738/0.69320. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67718/0.69376. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67653/0.69434. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67644/0.69484. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67558/0.69533. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67483/0.69602. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67456/0.69670. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67434/0.69734. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67342/0.69806. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67261/0.69864. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67218/0.69940. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67140/0.70016. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67035/0.70102. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67014/0.70177. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66968/0.70257. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66812/0.70344. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66800/0.70419. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66700/0.70513. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66628/0.70620. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66590/0.70715. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66472/0.70801. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66417/0.70884. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66344/0.70975. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66268/0.71062. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66215/0.71171. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66107/0.71291. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69313/0.68685. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69293/0.68708. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69289/0.68730. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69283/0.68753. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69298/0.68775. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69277/0.68794. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69288/0.68812. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69266/0.68828. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69268/0.68845. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69258/0.68860. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69268/0.68878. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69245/0.68897. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69240/0.68916. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69240/0.68935. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69239/0.68951. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69211/0.68967. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69230/0.68985. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69227/0.68998. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69211/0.69012. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69194/0.69024. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69197/0.69037. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69182/0.69052. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69191/0.69070. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69164/0.69086. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69158/0.69101. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69153/0.69114. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69152/0.69130. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.69140/0.69141. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69122/0.69158. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.69126/0.69171. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.69105/0.69189. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69094/0.69198. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.69078/0.69209. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.69062/0.69222. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69039/0.69235. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.69030/0.69256. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.69007/0.69271. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.69022/0.69288. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68978/0.69302. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68964/0.69315. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68951/0.69337. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68944/0.69353. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68929/0.69388. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68908/0.69415. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68895/0.69435. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68880/0.69450. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68873/0.69488. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68839/0.69518. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68828/0.69534. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68790/0.69554. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68786/0.69586. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68777/0.69610. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68733/0.69645. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68728/0.69673. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68718/0.69704. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68689/0.69723. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68675/0.69758. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68653/0.69795. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68626/0.69820. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68596/0.69841. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68588/0.69849. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68553/0.69881. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68557/0.69908. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68503/0.69950. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68502/0.69962. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68464/0.69955. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68431/0.69993. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68424/0.70030. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68362/0.70035. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68366/0.70049. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.68344/0.70069. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68297/0.70101. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.68242/0.70096. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68255/0.70126. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.68213/0.70151. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.68179/0.70151. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68160/0.70142. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68095/0.70150. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.68075/0.70192. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.68013/0.70161. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.68006/0.70215. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67932/0.70208. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67925/0.70202. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67905/0.70206. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.67852/0.70223. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67812/0.70241. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67775/0.70233. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67707/0.70205. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67651/0.70227. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67622/0.70235. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67553/0.70205. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67489/0.70227. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.67480/0.70212. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67398/0.70197. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67324/0.70185. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67319/0.70202. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.67269/0.70180. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.67199/0.70186. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.67145/0.70190. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.67071/0.70116. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69269/0.68689. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69209/0.68716. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69187/0.68744. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69157/0.68767. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.68792. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.68818. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.68846. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.68872. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68990/0.68898. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68989/0.68929. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68922/0.68956. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68886/0.68990. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68837/0.69022. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68773/0.69052. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68756/0.69082. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68700/0.69116. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68677/0.69157. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68623/0.69196. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68566/0.69237. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68492/0.69284. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68464/0.69333. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68385/0.69375. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68343/0.69416. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68325/0.69475. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68284/0.69542. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68198/0.69593. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68140/0.69661. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68092/0.69708. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68053/0.69777. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67948/0.69809. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67978/0.69885. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67887/0.69908. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67836/0.69967. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67726/0.70034. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67719/0.70068. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67680/0.70128. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67583/0.70236. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67538/0.70278. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67578/0.70314. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67488/0.70398. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67425/0.70465. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67453/0.70509. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67343/0.70570. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67200/0.70628. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67200/0.70701. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67144/0.70813. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67102/0.70867. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67002/0.70928. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66931/0.71043. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66872/0.71114. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66907/0.71179. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66846/0.71372. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66718/0.71370. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66673/0.71451. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66565/0.71595. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66511/0.71634. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66473/0.71819. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66434/0.71869. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66308/0.71926. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66317/0.72041. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66188/0.72206. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66189/0.72267. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66095/0.72337. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65989/0.72486. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65910/0.72623. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65763/0.72743. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65757/0.72775. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65708/0.72901. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65634/0.73069. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65505/0.73160. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65447/0.73286. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65387/0.73429. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65313/0.73506. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65276/0.73663. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65093/0.73780. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65124/0.73847. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64925/0.73962. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64882/0.74049. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64721/0.74260. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64650/0.74306. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64540/0.74463. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64511/0.74600. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64364/0.74609. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64298/0.74762. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64251/0.74972. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64095/0.74972. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63975/0.75203. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63888/0.75209. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63828/0.75450. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63700/0.75511. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63587/0.75646. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63403/0.75786. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63276/0.75825. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63435/0.75983. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63141/0.76107. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63104/0.76234. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62962/0.76428. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62909/0.76522. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62889/0.76584. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62592/0.76738. Took 0.12 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69248/0.69741. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69224/0.69714. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69688. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.69666. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69160/0.69644. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69623. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69152/0.69603. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69149/0.69586. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69566. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69107/0.69548. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69128/0.69531. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69087/0.69512. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69045/0.69495. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69073/0.69479. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69032/0.69462. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69020/0.69441. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69011/0.69419. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68958/0.69396. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68971/0.69375. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68954/0.69350. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68915/0.69327. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68946/0.69300. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68913/0.69273. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68872/0.69245. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68865/0.69216. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68832/0.69186. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68777/0.69154. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68756/0.69120. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68697/0.69084. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68671/0.69047. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68644/0.69013. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68618/0.68977. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68567/0.68945. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68532/0.68910. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68488/0.68871. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68468/0.68843. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68410/0.68817. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68388/0.68787. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68319/0.68763. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68264/0.68717. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68260/0.68703. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68150/0.68681. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68121/0.68642. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68082/0.68616. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68043/0.68590. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67980/0.68567. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67882/0.68543. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67826/0.68532. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67782/0.68505. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67698/0.68476. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67666/0.68475. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67581/0.68452. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67558/0.68438. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67422/0.68403. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67383/0.68383. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67316/0.68377. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67240/0.68351. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67173/0.68330. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67065/0.68312. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66956/0.68312. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66877/0.68290. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66818/0.68251. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66700/0.68255. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66619/0.68252. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66516/0.68223. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66320/0.68187. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66325/0.68163. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66180/0.68179. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66071/0.68137. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65902/0.68106. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65814/0.68071. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65653/0.68030. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65510/0.68045. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65367/0.67977. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65206/0.67969. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65061/0.67926. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64973/0.67905. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64798/0.67916. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64696/0.67871. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64479/0.67848. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64401/0.67841. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64206/0.67857. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64002/0.67841. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63887/0.67867. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63705/0.67850. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63498/0.67849. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63361/0.67797. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63216/0.67868. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63087/0.67822. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62924/0.67872. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62767/0.67866. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62497/0.67915. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62275/0.67915. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.62247/0.67917. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62124/0.67980. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61911/0.67984. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61695/0.67982. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61620/0.68070. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.61448/0.68069. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61216/0.68103. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69556/0.68923. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69500/0.68962. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69469/0.68999. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69445/0.69033. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69435/0.69064. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69391/0.69094. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69363/0.69118. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69322/0.69143. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69308/0.69168. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69275/0.69191. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69266/0.69215. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69257/0.69236. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69260/0.69256. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69207/0.69276. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69197/0.69298. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69169/0.69320. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69169/0.69340. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69122/0.69360. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69137/0.69382. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69069/0.69403. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69077/0.69428. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.69054/0.69452. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69042/0.69478. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68990/0.69505. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68957/0.69534. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68960/0.69565. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68915/0.69596. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68908/0.69634. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68890/0.69674. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68806/0.69722. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68787/0.69767. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68774/0.69817. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68719/0.69875. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68714/0.69938. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68616/0.70011. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68569/0.70088. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68514/0.70169. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68494/0.70251. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68393/0.70338. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68394/0.70419. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68347/0.70510. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68262/0.70598. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68237/0.70688. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68185/0.70782. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68117/0.70869. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68078/0.70948. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68042/0.71018. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67994/0.71085. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67983/0.71156. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67881/0.71228. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67848/0.71306. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67790/0.71388. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67736/0.71446. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67707/0.71515. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67644/0.71589. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67576/0.71661. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67554/0.71734. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67476/0.71799. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67452/0.71862. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67368/0.71933. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67362/0.71996. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67290/0.72052. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67223/0.72118. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67172/0.72183. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67103/0.72254. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67094/0.72312. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67049/0.72374. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66992/0.72446. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66910/0.72506. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66951/0.72585. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66840/0.72648. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66835/0.72708. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66748/0.72773. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66745/0.72834. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66636/0.72893. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66607/0.72965. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66517/0.73024. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66428/0.73078. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66528/0.73151. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66334/0.73228. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66345/0.73291. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66334/0.73347. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66244/0.73410. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66147/0.73467. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66105/0.73509. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66048/0.73584. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65991/0.73634. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65928/0.73698. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65882/0.73746. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65746/0.73800. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65729/0.73853. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65674/0.73916. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65608/0.73962. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65657/0.74026. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65490/0.74090. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65416/0.74128. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65319/0.74172. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65234/0.74246. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65196/0.74296. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65093/0.74343. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69008/0.69672. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68998/0.69636. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68944/0.69603. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68946/0.69574. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68899/0.69545. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68891/0.69522. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68890/0.69500. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68846/0.69480. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.69464. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68808/0.69447. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68768/0.69434. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68716/0.69425. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68726/0.69415. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68672/0.69404. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68648/0.69394. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68584/0.69383. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68554/0.69373. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68488/0.69364. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68478/0.69360. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68406/0.69356. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68369/0.69351. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68338/0.69351. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68238/0.69353. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68194/0.69358. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68136/0.69373. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68057/0.69392. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68015/0.69412. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67954/0.69436. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67876/0.69465. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67822/0.69495. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67737/0.69539. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67733/0.69590. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67653/0.69639. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67602/0.69678. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67533/0.69727. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67472/0.69775. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67411/0.69828. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67354/0.69876. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67258/0.69928. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67207/0.69985. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67186/0.70047. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67099/0.70085. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67072/0.70139. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66978/0.70184. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66958/0.70235. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66874/0.70293. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66793/0.70340. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66705/0.70404. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66624/0.70450. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66578/0.70514. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66571/0.70540. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66469/0.70616. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66413/0.70664. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66329/0.70716. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66229/0.70781. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66186/0.70845. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66150/0.70907. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66056/0.70974. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65991/0.71054. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65875/0.71099. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65840/0.71174. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65754/0.71236. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65643/0.71310. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65559/0.71416. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65550/0.71476. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65440/0.71558. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65298/0.71649. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65237/0.71754. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65202/0.71859. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65091/0.71944. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64967/0.72011. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64928/0.72115. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64786/0.72229. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64701/0.72349. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64665/0.72455. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64501/0.72558. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64503/0.72645. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64357/0.72786. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64250/0.72875. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64138/0.73025. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64140/0.73152. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63917/0.73276. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63885/0.73394. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63751/0.73523. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63636/0.73675. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63488/0.73797. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63418/0.73914. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63280/0.74102. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63142/0.74271. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63077/0.74450. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62976/0.74588. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62860/0.74772. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62724/0.74912. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62627/0.75073. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62541/0.75204. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62392/0.75381. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62325/0.75531. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62202/0.75705. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61989/0.75906. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61942/0.76091. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69855/0.69081. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69575/0.69040. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69371/0.69045. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69196/0.69092. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69105/0.69169. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69056/0.69247. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69025/0.69308. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69011/0.69351. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68980/0.69383. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68977/0.69408. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68953/0.69432. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68928/0.69452. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68910/0.69472. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68894/0.69492. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68870/0.69512. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68865/0.69529. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68856/0.69548. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68805/0.69570. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68790/0.69591. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68776/0.69613. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68763/0.69638. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68725/0.69665. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68703/0.69695. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68701/0.69721. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68650/0.69753. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68619/0.69783. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68603/0.69813. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68593/0.69845. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68555/0.69877. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68515/0.69915. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68492/0.69954. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68459/0.69986. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68412/0.70023. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68404/0.70060. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68362/0.70101. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68301/0.70141. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68275/0.70184. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68223/0.70219. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68196/0.70259. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68148/0.70309. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68132/0.70345. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68048/0.70388. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68035/0.70431. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68003/0.70467. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67955/0.70507. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67915/0.70546. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67859/0.70579. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67799/0.70616. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67769/0.70661. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67710/0.70697. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67669/0.70734. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67592/0.70778. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67566/0.70810. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67517/0.70861. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67468/0.70894. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67407/0.70938. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67384/0.70969. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67292/0.71013. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67237/0.71046. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67206/0.71089. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67112/0.71133. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67105/0.71160. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67014/0.71204. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66957/0.71244. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66923/0.71303. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66850/0.71349. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66850/0.71389. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66725/0.71429. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66686/0.71463. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66638/0.71508. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66565/0.71561. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66516/0.71602. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66423/0.71661. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66374/0.71702. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66277/0.71770. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66249/0.71809. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66200/0.71873. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66132/0.71931. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66080/0.71979. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65970/0.72066. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65960/0.72122. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65884/0.72176. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65843/0.72207. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65731/0.72294. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65608/0.72348. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65616/0.72420. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65544/0.72467. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65464/0.72547. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65437/0.72610. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65295/0.72683. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65173/0.72749. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65198/0.72825. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65099/0.72883. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65023/0.72956. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64966/0.73036. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64824/0.73124. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64756/0.73230. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64715/0.73284. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64664/0.73370. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64569/0.73440. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69705/0.70273. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69657/0.70249. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69632/0.70226. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69610/0.70195. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69557/0.70148. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69523/0.70067. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69438/0.69935. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69342/0.69749. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69220/0.69529. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69141/0.69340. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69071/0.69215. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69012/0.69144. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68976/0.69109. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68942/0.69104. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68925/0.69110. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68891/0.69126. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68840/0.69148. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68852/0.69171. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68817/0.69206. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68793/0.69244. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68772/0.69284. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68756/0.69324. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68723/0.69370. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68713/0.69421. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68676/0.69472. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68647/0.69527. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68615/0.69586. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68598/0.69653. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68556/0.69724. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68528/0.69797. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68493/0.69864. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68470/0.69942. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68441/0.70028. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68378/0.70117. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68362/0.70197. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68313/0.70287. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68269/0.70368. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68243/0.70467. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68209/0.70555. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68166/0.70640. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68140/0.70732. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68094/0.70833. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68039/0.70927. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67985/0.71043. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67970/0.71139. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67910/0.71230. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67863/0.71340. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67803/0.71446. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67755/0.71571. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67691/0.71697. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67661/0.71814. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67596/0.71921. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67527/0.72022. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67429/0.72140. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67397/0.72246. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67316/0.72370. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67237/0.72474. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67159/0.72604. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67167/0.72710. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67075/0.72819. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66982/0.72892. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66923/0.73031. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66838/0.73147. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66799/0.73269. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66707/0.73365. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66602/0.73449. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66566/0.73601. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66493/0.73726. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66371/0.73855. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66312/0.73920. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66241/0.74078. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66128/0.74203. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66096/0.74285. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65972/0.74401. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65848/0.74526. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65809/0.74666. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65630/0.74832. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65605/0.74942. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65515/0.75087. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65478/0.75209. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65316/0.75315. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65197/0.75503. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65134/0.75633. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65041/0.75712. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64944/0.75884. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64792/0.75970. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64721/0.76069. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64594/0.76300. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64520/0.76459. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64363/0.76607. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64337/0.76792. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64204/0.76906. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64084/0.76993. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63972/0.77107. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63932/0.77352. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63801/0.77531. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63704/0.77648. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63499/0.77795. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63458/0.78013. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63383/0.78109. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69445/0.69209. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69400/0.69184. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69384/0.69154. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69388/0.69125. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69376/0.69100. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69330/0.69075. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69341/0.69046. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69316/0.69018. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69275/0.68981. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69270/0.68942. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69260/0.68902. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69258/0.68858. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69234/0.68814. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69226/0.68773. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69176/0.68734. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69181/0.68691. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69131/0.68651. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69108/0.68610. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69106/0.68573. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69088/0.68535. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69057/0.68496. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69022/0.68468. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68989/0.68424. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68939/0.68393. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68917/0.68366. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68864/0.68327. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68823/0.68309. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68744/0.68269. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68700/0.68233. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68623/0.68199. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68546/0.68156. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68475/0.68108. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68402/0.68077. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68368/0.68040. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68247/0.68000. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68197/0.67964. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68131/0.67918. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68055/0.67879. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67982/0.67829. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67931/0.67762. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67809/0.67756. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67724/0.67717. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67673/0.67638. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67596/0.67596. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67548/0.67568. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67437/0.67532. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67375/0.67488. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67306/0.67412. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67247/0.67406. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67136/0.67326. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67102/0.67255. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66983/0.67187. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66903/0.67144. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66859/0.67119. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66790/0.67054. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66776/0.67039. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66710/0.66927. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66597/0.66864. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66576/0.66876. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66435/0.66826. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66419/0.66694. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66379/0.66719. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66278/0.66706. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66218/0.66670. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66084/0.66649. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66035/0.66584. Took 0.12 sec\n",
      "Epoch 66, Loss(train/val) 0.65963/0.66547. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65896/0.66514. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65833/0.66513. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65774/0.66451. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65739/0.66428. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65608/0.66406. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65584/0.66317. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65476/0.66333. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65353/0.66293. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65276/0.66236. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65229/0.66217. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65135/0.66217. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65055/0.66188. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65043/0.66125. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64868/0.66123. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64836/0.66127. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64660/0.66045. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64583/0.66030. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64639/0.65994. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64343/0.66013. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64273/0.65957. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64215/0.65994. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64140/0.65917. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64057/0.65951. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63957/0.65858. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63787/0.65773. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63755/0.65846. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63580/0.65763. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63569/0.65716. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63345/0.65817. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63229/0.65750. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63132/0.65744. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62972/0.65729. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62870/0.65764. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69409/0.69285. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69392/0.69307. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69326/0.69325. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69267/0.69341. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69301/0.69354. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69242/0.69366. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69210/0.69380. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69183/0.69398. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69152/0.69421. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69134/0.69453. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69084/0.69501. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69008/0.69565. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68958/0.69643. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68901/0.69730. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68884/0.69821. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68835/0.69911. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68801/0.69993. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68704/0.70051. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68668/0.70113. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68668/0.70164. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68618/0.70204. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68523/0.70234. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68536/0.70269. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68506/0.70315. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68464/0.70359. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68401/0.70382. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68370/0.70417. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68332/0.70457. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68305/0.70498. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68200/0.70535. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68231/0.70578. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68135/0.70624. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68075/0.70662. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68068/0.70708. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67982/0.70745. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67932/0.70803. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67859/0.70849. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67827/0.70897. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67835/0.70934. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67753/0.70964. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67705/0.70997. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67674/0.71032. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67628/0.71057. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67605/0.71088. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67546/0.71130. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67500/0.71169. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67482/0.71199. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67430/0.71224. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67348/0.71245. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67316/0.71285. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67267/0.71305. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67234/0.71327. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67160/0.71352. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67148/0.71355. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67048/0.71385. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67037/0.71408. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66975/0.71424. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66942/0.71445. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66844/0.71470. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66863/0.71490. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66777/0.71495. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66749/0.71500. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66673/0.71509. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66656/0.71525. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66610/0.71531. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66536/0.71538. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66465/0.71551. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66466/0.71550. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66348/0.71561. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66344/0.71573. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66286/0.71588. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66211/0.71609. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66086/0.71610. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66049/0.71623. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66030/0.71623. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65908/0.71649. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65789/0.71670. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65728/0.71696. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65675/0.71718. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65582/0.71727. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65581/0.71749. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65459/0.71783. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65419/0.71809. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65307/0.71832. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65237/0.71852. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65130/0.71880. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65070/0.71899. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64916/0.71911. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64837/0.71955. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64799/0.71999. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64708/0.72043. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64604/0.72076. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64567/0.72114. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64340/0.72170. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64367/0.72227. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64179/0.72224. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64076/0.72287. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63987/0.72339. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63969/0.72367. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63789/0.72400. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69705/0.70897. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69533/0.70625. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69425/0.70435. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69393/0.70294. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69324/0.70190. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69301/0.70115. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69271/0.70067. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69231/0.70032. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69228/0.70005. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69196/0.69988. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69186/0.69975. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69159/0.69966. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69128/0.69961. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69125/0.69959. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69110/0.69960. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69065/0.69963. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69050/0.69965. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69037/0.69972. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69017/0.69980. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69000/0.69991. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68994/0.70001. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68957/0.70017. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68958/0.70033. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68932/0.70049. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68919/0.70062. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68900/0.70076. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68890/0.70097. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68862/0.70114. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68842/0.70136. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68858/0.70161. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68827/0.70184. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68822/0.70204. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68796/0.70221. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68781/0.70241. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68757/0.70265. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68746/0.70285. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68728/0.70304. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68721/0.70324. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68693/0.70350. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68695/0.70372. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68686/0.70397. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68670/0.70416. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68666/0.70435. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68640/0.70453. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68632/0.70471. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68629/0.70491. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68602/0.70515. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68587/0.70529. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68574/0.70551. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68566/0.70569. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68527/0.70587. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68553/0.70610. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68525/0.70624. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68487/0.70639. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68485/0.70658. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68459/0.70675. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68463/0.70687. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68463/0.70709. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68437/0.70723. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68423/0.70739. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68427/0.70760. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 0.68366/0.70779. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68374/0.70791. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68354/0.70806. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68374/0.70820. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68335/0.70845. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68300/0.70859. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68295/0.70868. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68281/0.70886. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68262/0.70897. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68248/0.70917. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68213/0.70940. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.68207/0.70957. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68218/0.70986. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68177/0.70998. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.68134/0.71007. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68104/0.71023. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68102/0.71043. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.68088/0.71062. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.68059/0.71070. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.68035/0.71095. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.68009/0.71104. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67996/0.71118. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67967/0.71128. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67930/0.71149. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67928/0.71170. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67889/0.71192. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67851/0.71221. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67822/0.71222. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67776/0.71229. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67772/0.71250. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67734/0.71271. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67692/0.71277. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67672/0.71304. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67635/0.71343. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67600/0.71348. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.67549/0.71388. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.67505/0.71404. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.67461/0.71413. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.67434/0.71454. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69341/0.69132. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69292/0.69140. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69291/0.69136. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69282/0.69137. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69131. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69216/0.69126. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69191/0.69122. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69225/0.69114. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69156/0.69106. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69175/0.69097. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69142/0.69084. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69106/0.69068. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69151/0.69047. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69093/0.69033. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69067/0.69017. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69053/0.68997. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68992/0.68969. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68991/0.68946. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68953/0.68928. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68907/0.68901. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68877/0.68868. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68834/0.68838. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68781/0.68793. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68755/0.68746. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68690/0.68700. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68661/0.68649. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68628/0.68597. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68557/0.68536. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68509/0.68470. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68456/0.68408. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68369/0.68353. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68303/0.68301. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68257/0.68237. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68170/0.68165. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68104/0.68107. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68092/0.68029. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68029/0.67979. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67947/0.67932. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67893/0.67884. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67848/0.67833. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67761/0.67793. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67693/0.67744. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67626/0.67697. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67580/0.67663. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67494/0.67596. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67449/0.67580. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67406/0.67554. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67268/0.67491. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67192/0.67476. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67161/0.67466. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67122/0.67450. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67036/0.67420. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66985/0.67417. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66824/0.67389. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66794/0.67407. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66770/0.67383. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66669/0.67371. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66610/0.67385. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66532/0.67397. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66464/0.67387. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66355/0.67360. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66338/0.67412. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66152/0.67396. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66191/0.67414. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66054/0.67443. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65950/0.67415. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65924/0.67540. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65809/0.67544. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65820/0.67564. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65643/0.67588. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65566/0.67610. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65547/0.67683. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65404/0.67707. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65353/0.67724. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65198/0.67781. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65108/0.67859. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65070/0.67889. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64981/0.67992. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64909/0.68046. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64800/0.68113. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64710/0.68122. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64664/0.68299. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64598/0.68279. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64495/0.68320. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64389/0.68388. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64248/0.68497. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64240/0.68516. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64148/0.68657. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64026/0.68698. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63916/0.68751. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63858/0.68903. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63778/0.68984. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63586/0.69062. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63582/0.69121. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63486/0.69221. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.63436/0.69326. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63352/0.69454. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63211/0.69481. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63194/0.69667. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62990/0.69738. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69377/0.69072. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69385/0.69060. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69360/0.69045. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69358/0.69031. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69320/0.69015. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69302/0.68999. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69313/0.68985. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69295/0.68972. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69270/0.68963. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69258/0.68954. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69270/0.68946. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69235/0.68936. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69220/0.68928. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69222/0.68918. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69207/0.68909. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69167/0.68901. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69199/0.68891. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69148/0.68882. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69135/0.68868. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69134/0.68857. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69117/0.68847. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69081/0.68835. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69087/0.68821. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69070/0.68810. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69051/0.68799. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69042/0.68786. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69030/0.68769. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68988/0.68756. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68962/0.68734. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68946/0.68722. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68917/0.68707. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68877/0.68690. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68862/0.68667. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68832/0.68640. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68812/0.68624. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68788/0.68602. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68762/0.68567. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68716/0.68544. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68708/0.68520. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68672/0.68494. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68634/0.68448. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68575/0.68430. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68538/0.68398. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68522/0.68366. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68481/0.68322. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68467/0.68303. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68439/0.68260. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68421/0.68226. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68356/0.68202. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68304/0.68163. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68278/0.68106. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68230/0.68073. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68169/0.68027. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68124/0.67982. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68189/0.67946. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68124/0.67932. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68047/0.67898. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67988/0.67857. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67946/0.67833. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67900/0.67812. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67895/0.67785. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67819/0.67760. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67776/0.67721. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67750/0.67687. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67690/0.67660. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67654/0.67639. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67565/0.67630. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67566/0.67610. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67482/0.67586. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67433/0.67569. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67381/0.67560. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67332/0.67549. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67299/0.67523. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67180/0.67529. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67169/0.67495. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67117/0.67498. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67027/0.67492. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66940/0.67513. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66923/0.67500. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66875/0.67514. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66804/0.67507. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66737/0.67506. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66695/0.67534. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66599/0.67507. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66539/0.67515. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66382/0.67541. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66312/0.67575. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66197/0.67577. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66223/0.67595. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66090/0.67661. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65979/0.67663. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65918/0.67698. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65786/0.67731. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65663/0.67800. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65549/0.67831. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65516/0.67896. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65382/0.67904. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65222/0.67977. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65167/0.67993. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65003/0.68032. Took 0.09 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69498/0.68685. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69436/0.68675. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69408/0.68665. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69397/0.68655. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69387/0.68644. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69343/0.68637. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69319/0.68632. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69267/0.68629. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69273/0.68627. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69225/0.68626. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69239/0.68628. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69191/0.68626. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69186/0.68623. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69175/0.68618. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69155/0.68610. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69159/0.68601. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69138/0.68588. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69150/0.68574. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69093/0.68559. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69085/0.68545. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69081/0.68530. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69068/0.68515. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69054/0.68499. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69030/0.68479. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69019/0.68458. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68975/0.68434. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68985/0.68412. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68974/0.68390. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68953/0.68369. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68914/0.68345. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68905/0.68322. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68874/0.68296. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68896/0.68274. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68857/0.68254. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68832/0.68232. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68809/0.68211. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68749/0.68184. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68760/0.68161. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68698/0.68143. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68695/0.68121. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68664/0.68103. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68633/0.68081. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68625/0.68064. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68572/0.68049. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68522/0.68034. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68511/0.68020. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68462/0.68010. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68415/0.68000. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68397/0.67993. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68340/0.67988. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68328/0.67984. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68272/0.67980. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68192/0.67981. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68163/0.67982. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.68130/0.67987. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68048/0.67993. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68019/0.68004. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67951/0.68015. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67903/0.68028. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67810/0.68047. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67783/0.68067. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67700/0.68090. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67657/0.68113. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67577/0.68149. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67531/0.68184. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67406/0.68227. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67338/0.68272. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67303/0.68329. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67186/0.68393. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67046/0.68454. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67006/0.68539. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66926/0.68621. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66820/0.68705. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66731/0.68780. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66670/0.68882. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66501/0.68990. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66450/0.69097. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66313/0.69210. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66121/0.69329. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66098/0.69448. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66031/0.69591. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65852/0.69697. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65762/0.69805. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65658/0.69925. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65557/0.70080. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65509/0.70213. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65346/0.70365. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65222/0.70545. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65100/0.70677. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65012/0.70840. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64896/0.70988. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64774/0.71141. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64675/0.71313. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64534/0.71475. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64415/0.71664. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64336/0.71797. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64215/0.71984. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64107/0.72147. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63954/0.72321. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63866/0.72503. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69311/0.69138. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69301/0.69169. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69202. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69278/0.69234. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69259/0.69265. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69239/0.69294. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69246/0.69324. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69232/0.69351. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69227/0.69382. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69221/0.69410. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69191/0.69440. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69168/0.69472. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69165/0.69504. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69158/0.69531. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69132/0.69563. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69127/0.69591. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69106/0.69618. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69108/0.69648. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69090/0.69682. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69080/0.69713. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69075/0.69743. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69054/0.69778. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69034/0.69813. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69013/0.69848. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68996/0.69887. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68990/0.69924. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68970/0.69966. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68947/0.70009. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68934/0.70052. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68905/0.70099. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68899/0.70143. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68883/0.70186. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68866/0.70235. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68859/0.70279. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68824/0.70323. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68812/0.70368. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68802/0.70416. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68777/0.70464. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68764/0.70502. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68731/0.70545. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68730/0.70584. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68697/0.70618. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68688/0.70658. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68642/0.70701. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68644/0.70746. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68638/0.70789. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68570/0.70822. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68567/0.70858. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68557/0.70896. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68526/0.70936. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68516/0.70970. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68482/0.70992. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68455/0.71023. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68423/0.71058. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68388/0.71097. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68343/0.71130. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68342/0.71173. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68307/0.71202. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68248/0.71253. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.68252/0.71292. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68197/0.71314. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68147/0.71343. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.68111/0.71360. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68054/0.71388. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68018/0.71414. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67990/0.71450. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67927/0.71473. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67870/0.71501. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67851/0.71545. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67794/0.71566. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67699/0.71588. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67689/0.71607. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67637/0.71636. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67577/0.71665. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67484/0.71693. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67407/0.71725. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67377/0.71753. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67286/0.71765. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67251/0.71808. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67170/0.71848. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67157/0.71883. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67026/0.71893. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66942/0.71915. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66926/0.71957. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66812/0.72011. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66780/0.72067. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.66735/0.72088. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66678/0.72163. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66547/0.72200. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66454/0.72247. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66402/0.72301. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.66299/0.72338. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66293/0.72442. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66156/0.72491. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66053/0.72544. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65924/0.72639. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65946/0.72731. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65886/0.72763. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65838/0.72884. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65790/0.72991. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69337/0.69035. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69308/0.69027. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69019. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69299/0.69012. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69268/0.69005. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69249/0.68998. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69248/0.68990. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69215/0.68982. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69182/0.68973. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.68964. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69163/0.68953. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69147/0.68942. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69125/0.68931. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69093/0.68921. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69050/0.68911. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69036/0.68901. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69012/0.68893. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68975/0.68886. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68960/0.68880. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68942/0.68879. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68855/0.68878. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68849/0.68880. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68803/0.68885. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68741/0.68893. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68741/0.68904. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68720/0.68921. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68627/0.68943. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68615/0.68965. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68601/0.68995. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68513/0.69024. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68450/0.69060. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68379/0.69099. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68358/0.69145. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68294/0.69185. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68224/0.69231. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68200/0.69282. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68140/0.69343. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68017/0.69417. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67969/0.69479. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67898/0.69551. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67841/0.69622. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67739/0.69700. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67685/0.69779. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67648/0.69852. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67520/0.69952. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67412/0.70024. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67381/0.70127. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67270/0.70213. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67250/0.70302. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67109/0.70399. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66996/0.70475. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66968/0.70558. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66845/0.70680. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66801/0.70785. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66723/0.70858. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66658/0.70934. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66553/0.71058. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66521/0.71086. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66410/0.71197. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66326/0.71301. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66220/0.71418. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66155/0.71492. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66108/0.71616. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65982/0.71673. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65926/0.71815. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65811/0.71876. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65701/0.72026. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65622/0.72112. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65612/0.72160. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65479/0.72265. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65393/0.72374. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65264/0.72480. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65208/0.72601. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65114/0.72699. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65087/0.72841. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64949/0.72879. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64813/0.73052. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64817/0.73140. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64652/0.73246. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64620/0.73300. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64463/0.73438. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64432/0.73517. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64262/0.73679. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64143/0.73773. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64038/0.73932. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63973/0.74041. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63802/0.74232. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63709/0.74194. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63631/0.74454. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63499/0.74389. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63304/0.74619. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63271/0.74719. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63182/0.74764. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63141/0.74836. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62881/0.75016. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62859/0.75156. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62661/0.75279. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62587/0.75385. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62446/0.75406. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62310/0.75635. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70069/0.69967. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69763/0.69762. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69571/0.69622. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69430/0.69535. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69366/0.69490. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69276/0.69471. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69245/0.69462. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69233/0.69458. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69245/0.69455. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69194/0.69453. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.69197/0.69450. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69148/0.69445. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69174/0.69443. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69156/0.69440. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69117/0.69436. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69129/0.69432. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69109/0.69430. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69110/0.69427. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69071/0.69420. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69080/0.69415. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69062/0.69413. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69014/0.69409. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69043/0.69405. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69011/0.69403. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68971/0.69402. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68970/0.69397. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68955/0.69396. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68955/0.69393. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68940/0.69388. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68904/0.69388. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68893/0.69387. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68855/0.69384. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68864/0.69379. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68856/0.69374. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68807/0.69368. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68814/0.69361. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68820/0.69361. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68746/0.69356. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68703/0.69353. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68735/0.69344. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68675/0.69336. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68659/0.69324. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68637/0.69314. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68660/0.69303. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68590/0.69291. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68544/0.69271. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68559/0.69255. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68463/0.69243. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68526/0.69221. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68471/0.69200. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68448/0.69181. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68373/0.69161. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68372/0.69138. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68361/0.69111. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68332/0.69085. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68250/0.69051. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68253/0.69019. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68209/0.68988. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68199/0.68955. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68177/0.68924. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68101/0.68890. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68080/0.68851. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68072/0.68817. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67998/0.68777. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68008/0.68735. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67931/0.68693. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67887/0.68663. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67906/0.68619. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67814/0.68592. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67769/0.68551. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67737/0.68513. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67733/0.68480. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67676/0.68453. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67642/0.68411. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67611/0.68375. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67608/0.68341. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67558/0.68306. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67452/0.68287. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67426/0.68253. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67411/0.68201. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67365/0.68185. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67366/0.68156. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67329/0.68130. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67258/0.68099. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67196/0.68067. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67164/0.68054. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67125/0.68019. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67142/0.68010. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67050/0.68001. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67005/0.67974. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66955/0.67963. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66977/0.67951. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66969/0.67933. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66881/0.67920. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66819/0.67902. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66815/0.67892. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66712/0.67893. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66704/0.67894. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66692/0.67885. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66639/0.67876. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69208/0.69315. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69151/0.69365. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69072/0.69416. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69075/0.69465. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69037/0.69512. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.69556. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.69595. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68950/0.69632. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68952/0.69670. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68926/0.69708. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68896/0.69750. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68830/0.69795. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68828/0.69840. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68814/0.69887. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68771/0.69930. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68768/0.69980. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68718/0.70037. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68719/0.70097. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68660/0.70159. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68669/0.70222. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68650/0.70284. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68582/0.70353. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68552/0.70428. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68523/0.70498. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68524/0.70571. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68496/0.70640. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68465/0.70711. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68439/0.70780. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68412/0.70850. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68411/0.70919. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68352/0.70986. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68377/0.71046. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68317/0.71106. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68305/0.71168. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68265/0.71225. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68214/0.71282. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68197/0.71341. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68218/0.71393. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68200/0.71443. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68163/0.71496. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68118/0.71543. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68116/0.71588. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68093/0.71631. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68066/0.71667. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68051/0.71708. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68016/0.71752. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68020/0.71783. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67954/0.71826. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67955/0.71861. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67933/0.71892. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67885/0.71925. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67888/0.71952. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67821/0.71985. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67788/0.72018. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67782/0.72052. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67777/0.72076. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67743/0.72111. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67716/0.72141. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67668/0.72172. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67662/0.72202. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67660/0.72229. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67606/0.72257. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67574/0.72279. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67573/0.72303. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67573/0.72329. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67517/0.72357. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67477/0.72372. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67481/0.72400. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67423/0.72440. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67441/0.72456. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67365/0.72479. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67352/0.72499. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67275/0.72520. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67264/0.72547. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67256/0.72563. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67204/0.72590. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67230/0.72615. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67133/0.72634. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67142/0.72660. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67056/0.72686. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67052/0.72710. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67036/0.72735. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66982/0.72751. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66906/0.72778. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66891/0.72817. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66842/0.72826. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66850/0.72849. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66812/0.72873. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66775/0.72888. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66685/0.72912. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66685/0.72935. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66597/0.72953. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66587/0.72972. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66519/0.72989. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66503/0.73018. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66456/0.73038. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66470/0.73048. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66369/0.73088. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66360/0.73108. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66284/0.73129. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69352/0.69346. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69188/0.69490. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69054/0.69656. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68980/0.69825. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68902/0.69971. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68866/0.70083. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68866/0.70162. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68828/0.70212. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68795/0.70246. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68777/0.70266. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68788/0.70279. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68750/0.70282. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68743/0.70286. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68725/0.70299. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68713/0.70307. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68701/0.70314. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68689/0.70313. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68690/0.70309. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68676/0.70311. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68650/0.70307. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68652/0.70307. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.70303. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68613/0.70302. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68575/0.70300. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68567/0.70291. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68549/0.70291. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68539/0.70285. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68543/0.70284. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68508/0.70282. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68489/0.70273. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68468/0.70258. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68468/0.70252. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68447/0.70257. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68427/0.70257. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68396/0.70247. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68372/0.70241. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68348/0.70232. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68342/0.70233. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68295/0.70225. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68285/0.70225. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68282/0.70222. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68235/0.70211. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68232/0.70212. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68214/0.70208. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68187/0.70214. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68173/0.70220. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68181/0.70217. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68141/0.70230. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68132/0.70214. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68094/0.70216. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68085/0.70209. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68091/0.70213. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68040/0.70216. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68029/0.70207. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68025/0.70221. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67989/0.70214. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67971/0.70206. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67963/0.70208. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67947/0.70221. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67941/0.70196. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67919/0.70199. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67893/0.70202. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67852/0.70186. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67873/0.70191. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67834/0.70181. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67784/0.70176. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67774/0.70172. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67762/0.70178. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67740/0.70174. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67723/0.70171. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67663/0.70163. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67692/0.70152. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67611/0.70168. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67624/0.70157. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67591/0.70147. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67594/0.70155. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67532/0.70134. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67531/0.70141. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67522/0.70125. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67503/0.70121. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67486/0.70117. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67411/0.70118. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67426/0.70095. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67379/0.70092. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67363/0.70089. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67364/0.70082. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67324/0.70074. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67290/0.70079. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67269/0.70062. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67241/0.70056. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67223/0.70041. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67167/0.70028. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67129/0.70025. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67123/0.70044. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67084/0.70001. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67050/0.70006. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.67021/0.69989. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66995/0.70006. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66951/0.69988. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66944/0.69985. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69065/0.69473. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69038/0.69491. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69024/0.69509. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68982/0.69527. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68968/0.69546. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68953/0.69565. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.69585. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68879/0.69605. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68885/0.69622. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68851/0.69638. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68862/0.69654. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68812/0.69670. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68774/0.69685. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68728/0.69700. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68720/0.69713. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68718/0.69729. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68694/0.69743. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68661/0.69757. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68643/0.69768. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68614/0.69781. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68588/0.69794. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68535/0.69806. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68537/0.69817. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68511/0.69827. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68474/0.69841. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68461/0.69855. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68422/0.69860. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68393/0.69868. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68350/0.69881. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68329/0.69894. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68304/0.69902. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68285/0.69919. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68239/0.69936. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68249/0.69947. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68176/0.69956. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68140/0.69971. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68108/0.69992. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68094/0.70007. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68045/0.70023. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68053/0.70045. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68035/0.70052. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67957/0.70074. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67972/0.70106. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67901/0.70125. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67859/0.70153. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67820/0.70173. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67806/0.70206. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67785/0.70229. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67740/0.70248. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67717/0.70273. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67662/0.70312. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67614/0.70330. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67622/0.70364. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67590/0.70388. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67494/0.70417. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67465/0.70444. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67444/0.70477. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67411/0.70518. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67360/0.70547. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67307/0.70577. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67244/0.70605. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67236/0.70642. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67161/0.70672. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67168/0.70691. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67110/0.70715. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66993/0.70757. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67012/0.70783. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66971/0.70804. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66914/0.70847. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66833/0.70868. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66774/0.70907. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66673/0.70920. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66689/0.70941. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66645/0.70969. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66537/0.71002. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66492/0.71030. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66426/0.71061. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66392/0.71078. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66354/0.71083. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66235/0.71113. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66124/0.71146. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66048/0.71174. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65983/0.71195. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65969/0.71198. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65831/0.71214. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65776/0.71237. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65691/0.71258. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65569/0.71264. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65575/0.71300. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65466/0.71315. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65388/0.71308. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65256/0.71347. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65166/0.71368. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65118/0.71380. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65026/0.71406. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64963/0.71407. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64876/0.71436. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64803/0.71445. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64735/0.71461. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64562/0.71508. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69377/0.69049. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69069. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.69099. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.69136. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69188. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69145/0.69245. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69130/0.69307. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69104/0.69369. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69066/0.69422. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69060/0.69462. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69027/0.69492. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69010/0.69511. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69016/0.69517. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68981/0.69523. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68980/0.69519. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68962/0.69514. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68952/0.69508. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68945/0.69502. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68947/0.69491. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68918/0.69485. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68892/0.69475. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68889/0.69463. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68872/0.69453. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68853/0.69440. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68855/0.69430. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68836/0.69418. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68810/0.69408. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68812/0.69397. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68801/0.69386. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68767/0.69377. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68758/0.69371. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68741/0.69360. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68733/0.69348. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68712/0.69345. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68692/0.69340. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68704/0.69340. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68673/0.69338. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68659/0.69335. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68632/0.69340. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68602/0.69348. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68593/0.69346. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68572/0.69363. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68564/0.69373. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68539/0.69384. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68502/0.69400. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68482/0.69420. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68457/0.69433. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68443/0.69457. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68406/0.69490. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.68400/0.69518. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68376/0.69543. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68346/0.69581. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68304/0.69600. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68328/0.69644. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.68259/0.69678. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68225/0.69703. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68210/0.69744. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.68185/0.69781. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68161/0.69799. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68128/0.69849. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68105/0.69900. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68070/0.69924. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.68049/0.69961. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68002/0.69990. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67949/0.70007. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67938/0.70058. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67922/0.70127. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67873/0.70164. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67782/0.70191. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67743/0.70242. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67719/0.70280. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67657/0.70334. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67650/0.70381. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67586/0.70415. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67540/0.70467. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67463/0.70507. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67449/0.70551. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67411/0.70616. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67347/0.70642. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67302/0.70738. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67205/0.70741. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67145/0.70853. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67076/0.70888. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67033/0.70954. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66990/0.70995. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66903/0.71031. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66900/0.71083. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66769/0.71122. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66708/0.71197. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66693/0.71245. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66592/0.71290. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66485/0.71365. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66482/0.71448. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66395/0.71529. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66334/0.71550. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66270/0.71654. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66200/0.71724. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66050/0.71779. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66063/0.71865. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66025/0.71917. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69301/0.69183. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.69172. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69234/0.69162. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69247/0.69152. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69219/0.69140. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69240/0.69129. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69118. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69192/0.69104. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69146/0.69091. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69122/0.69077. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69071/0.69064. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69052/0.69052. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69011/0.69044. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69036/0.69038. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68966/0.69033. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68933/0.69032. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68893/0.69031. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68884/0.69031. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68853/0.69032. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68850/0.69033. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68781/0.69032. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68757/0.69034. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68739/0.69035. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68685/0.69036. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68653/0.69040. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68607/0.69046. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68557/0.69054. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68541/0.69061. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68528/0.69072. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68469/0.69082. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68439/0.69090. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68381/0.69102. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68328/0.69116. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68271/0.69130. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68239/0.69139. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68170/0.69153. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68142/0.69163. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68079/0.69169. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68018/0.69180. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67961/0.69189. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67880/0.69193. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67803/0.69199. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67702/0.69205. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67675/0.69207. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67572/0.69208. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67499/0.69215. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67418/0.69222. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67371/0.69215. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67262/0.69208. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67158/0.69196. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67086/0.69203. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67010/0.69203. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66951/0.69206. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66828/0.69220. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66753/0.69199. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66666/0.69211. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66580/0.69202. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66441/0.69251. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66389/0.69256. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66247/0.69288. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66171/0.69277. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66078/0.69300. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65919/0.69325. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65899/0.69334. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65785/0.69362. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65685/0.69350. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65509/0.69380. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65441/0.69408. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65398/0.69445. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65265/0.69438. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65153/0.69513. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65057/0.69511. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64985/0.69594. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64805/0.69609. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64748/0.69684. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64654/0.69748. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64591/0.69779. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64493/0.69813. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64354/0.69899. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64246/0.69954. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64280/0.69971. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64024/0.70018. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63941/0.70070. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63863/0.70133. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63819/0.70184. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63682/0.70244. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63434/0.70316. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63465/0.70398. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63317/0.70498. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63229/0.70546. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63064/0.70615. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63074/0.70687. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62864/0.70750. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62768/0.70830. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62797/0.70884. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62743/0.70971. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62495/0.71048. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62380/0.71195. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62252/0.71278. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62267/0.71322. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69538/0.69231. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69448/0.69350. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69370/0.69476. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69348/0.69597. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69708. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69276/0.69802. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69208/0.69873. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69218/0.69935. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69173/0.69979. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.70014. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69132/0.70051. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69122/0.70086. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69071/0.70116. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69090/0.70141. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69070/0.70171. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69011/0.70197. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69004/0.70227. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68988/0.70251. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68963/0.70282. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68937/0.70309. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68893/0.70340. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68896/0.70378. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68876/0.70415. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68824/0.70440. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68809/0.70482. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68789/0.70521. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68774/0.70567. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68717/0.70605. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68711/0.70646. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68690/0.70693. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68649/0.70733. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68629/0.70776. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68613/0.70816. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68584/0.70860. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68556/0.70898. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68561/0.70937. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68488/0.70987. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68510/0.71027. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68441/0.71073. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68443/0.71113. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68401/0.71146. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68368/0.71187. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68352/0.71229. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68349/0.71259. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68310/0.71286. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68284/0.71317. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68262/0.71361. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68215/0.71391. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.68203/0.71427. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68174/0.71455. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68138/0.71473. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68109/0.71498. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68067/0.71526. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68077/0.71571. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68037/0.71591. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68038/0.71631. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67970/0.71657. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67932/0.71677. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67884/0.71710. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67867/0.71738. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67824/0.71740. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67817/0.71780. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67744/0.71781. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67738/0.71817. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67704/0.71860. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67692/0.71853. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67625/0.71847. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67650/0.71857. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67552/0.71861. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67519/0.71864. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67453/0.71887. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67424/0.71894. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67381/0.71898. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67366/0.71930. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67300/0.71922. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67293/0.71932. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67235/0.71911. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67169/0.71930. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67155/0.71904. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67070/0.71928. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67052/0.71922. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66983/0.71888. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66977/0.71910. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66896/0.71910. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66833/0.71879. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66800/0.71892. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66765/0.71869. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66722/0.71851. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66645/0.71813. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66652/0.71844. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66604/0.71822. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66533/0.71829. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66482/0.71777. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66355/0.71767. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66418/0.71802. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66335/0.71728. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66297/0.71741. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66234/0.71748. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66195/0.71743. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66143/0.71767. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69242/0.68461. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69204/0.68466. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69167/0.68481. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69112/0.68497. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69116/0.68514. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69068/0.68534. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69130/0.68554. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69063/0.68569. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69052/0.68587. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69033/0.68606. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69035/0.68625. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69002/0.68643. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69026/0.68663. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68967/0.68679. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68934/0.68695. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68946/0.68714. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68914/0.68732. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68940/0.68748. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68893/0.68761. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68902/0.68776. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68852/0.68795. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68868/0.68811. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68821/0.68831. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68838/0.68845. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68794/0.68858. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68791/0.68871. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68783/0.68884. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68774/0.68891. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68758/0.68904. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68750/0.68913. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68743/0.68922. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68695/0.68934. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68704/0.68940. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68682/0.68946. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68672/0.68956. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68666/0.68965. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68659/0.68969. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68651/0.68975. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68604/0.68976. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68560/0.68982. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68591/0.68986. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68548/0.68991. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68548/0.68998. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68521/0.69005. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68513/0.69007. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68497/0.69009. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68441/0.69013. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68446/0.69017. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68422/0.69022. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68408/0.69017. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68408/0.69015. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68361/0.69020. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68363/0.69017. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68326/0.69015. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68330/0.69021. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68288/0.69028. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68264/0.69022. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68268/0.69019. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68217/0.69019. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68174/0.69019. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68156/0.69023. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68136/0.69017. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68142/0.69032. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68102/0.69033. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68067/0.69033. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68060/0.69032. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68021/0.69037. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67982/0.69042. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67947/0.69047. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67949/0.69054. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67903/0.69062. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67857/0.69063. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67826/0.69078. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67801/0.69081. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67759/0.69080. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67744/0.69077. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67699/0.69092. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67640/0.69099. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67642/0.69115. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67590/0.69116. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67559/0.69121. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67515/0.69130. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67462/0.69151. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67423/0.69161. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67378/0.69176. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67361/0.69183. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67295/0.69205. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67281/0.69220. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67206/0.69246. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.67162/0.69268. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67138/0.69295. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67142/0.69302. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.67044/0.69331. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67018/0.69354. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.66966/0.69375. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66921/0.69436. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.66904/0.69466. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66849/0.69497. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.66799/0.69496. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66794/0.69521. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69774/0.69937. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69680/0.69927. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69590/0.69916. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69519/0.69904. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69421/0.69892. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69367/0.69888. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69302/0.69890. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69261/0.69895. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69214/0.69902. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69184/0.69907. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69161/0.69912. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69155/0.69915. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69098/0.69919. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69067/0.69924. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69037/0.69930. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69047/0.69934. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69031/0.69941. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68986/0.69947. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68993/0.69955. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68928/0.69964. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68912/0.69974. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68905/0.69986. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68865/0.69996. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68839/0.70008. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68787/0.70022. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68763/0.70040. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68719/0.70062. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68698/0.70083. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68685/0.70102. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68642/0.70123. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68616/0.70151. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68566/0.70179. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68514/0.70211. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68494/0.70246. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68486/0.70281. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68409/0.70318. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68388/0.70357. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68359/0.70395. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68287/0.70431. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68264/0.70471. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68273/0.70515. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68220/0.70558. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68152/0.70598. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68159/0.70643. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68092/0.70684. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68037/0.70725. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67961/0.70767. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67942/0.70802. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67942/0.70844. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67861/0.70888. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67818/0.70927. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67767/0.70967. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67708/0.71011. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67667/0.71052. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67643/0.71092. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67542/0.71133. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67498/0.71176. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67414/0.71221. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67421/0.71264. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67312/0.71304. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67258/0.71344. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67233/0.71383. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.67191/0.71426. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67109/0.71464. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67047/0.71507. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66978/0.71555. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66910/0.71603. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.66823/0.71650. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66804/0.71698. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66705/0.71744. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.66620/0.71790. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66537/0.71834. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66457/0.71882. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66394/0.71938. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66323/0.71995. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66300/0.72049. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66226/0.72104. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.66135/0.72160. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66040/0.72230. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65936/0.72294. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65870/0.72349. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65803/0.72413. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65715/0.72476. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65620/0.72537. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65521/0.72602. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65517/0.72661. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65437/0.72731. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65321/0.72808. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65195/0.72868. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65166/0.72948. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65104/0.73014. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65063/0.73085. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64914/0.73167. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64812/0.73257. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64678/0.73348. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64712/0.73435. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64639/0.73528. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64511/0.73612. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64471/0.73685. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64317/0.73773. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69487/0.69878. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69450/0.69835. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69426/0.69798. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69401/0.69760. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69406/0.69727. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69374/0.69692. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69356/0.69662. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69351/0.69636. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69342/0.69613. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69323/0.69590. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69303/0.69569. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69290/0.69552. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69280/0.69538. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69258/0.69528. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69270/0.69524. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69227/0.69519. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69225/0.69516. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69198/0.69520. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69165/0.69529. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69168/0.69542. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69134/0.69559. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69109/0.69578. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69082/0.69602. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69047/0.69628. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69006/0.69664. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68987/0.69709. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68956/0.69747. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68930/0.69786. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68895/0.69827. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68848/0.69862. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68814/0.69900. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68770/0.69941. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68730/0.69989. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68708/0.70009. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68654/0.70045. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68621/0.70084. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68593/0.70092. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68558/0.70108. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68542/0.70128. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68502/0.70150. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68470/0.70140. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68450/0.70180. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68418/0.70209. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68371/0.70208. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68353/0.70218. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68304/0.70228. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68254/0.70250. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68230/0.70279. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68207/0.70293. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.68149/0.70336. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.68127/0.70340. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68101/0.70355. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.68047/0.70396. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67986/0.70430. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67987/0.70454. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67916/0.70491. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67836/0.70546. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67840/0.70584. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67768/0.70629. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67773/0.70652. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67692/0.70728. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.67668/0.70758. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67623/0.70833. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67540/0.70906. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67476/0.70947. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67430/0.71016. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.67414/0.71066. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67312/0.71170. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67288/0.71219. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67236/0.71286. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67191/0.71375. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67137/0.71399. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67113/0.71472. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67045/0.71602. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66971/0.71633. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66936/0.71750. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.66858/0.71750. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66813/0.71854. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66826/0.71914. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66704/0.71991. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66672/0.72115. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66644/0.72219. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66536/0.72256. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66519/0.72350. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66469/0.72372. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.66438/0.72504. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66376/0.72543. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66266/0.72619. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66184/0.72751. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66167/0.72879. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.66107/0.72905. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66033/0.72972. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65984/0.73150. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65960/0.73170. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65878/0.73269. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65807/0.73430. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65756/0.73490. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65590/0.73645. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65617/0.73735. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65552/0.73771. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69320/0.69137. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69358/0.69135. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69307/0.69135. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69308/0.69134. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69331/0.69136. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69288/0.69136. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69271/0.69137. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69269/0.69139. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69278/0.69142. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69249/0.69145. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69274/0.69149. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69244/0.69156. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69238/0.69162. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69212/0.69166. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69228/0.69174. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69199/0.69182. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69201/0.69191. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69176/0.69200. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69167/0.69210. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.69164/0.69221. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69170/0.69234. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69133/0.69249. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69132/0.69264. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69086/0.69280. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69056/0.69299. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69055/0.69320. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69053/0.69340. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.69017/0.69366. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68956/0.69396. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68960/0.69423. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68927/0.69450. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68957/0.69480. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68884/0.69511. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68886/0.69547. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68838/0.69583. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68822/0.69621. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68794/0.69659. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68752/0.69703. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68704/0.69749. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68692/0.69789. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68687/0.69829. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68623/0.69871. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68600/0.69914. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68559/0.69956. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68530/0.70002. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68537/0.70048. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68517/0.70087. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68451/0.70126. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.68408/0.70165. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.68389/0.70200. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68366/0.70239. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.68296/0.70268. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68270/0.70305. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.68245/0.70331. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68213/0.70365. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.68169/0.70395. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.68129/0.70409. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68093/0.70431. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.68045/0.70459. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68057/0.70484. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67992/0.70502. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67956/0.70521. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67904/0.70547. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67883/0.70566. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67818/0.70587. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67804/0.70612. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67747/0.70620. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67685/0.70635. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67639/0.70654. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67653/0.70666. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67575/0.70685. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67549/0.70710. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67499/0.70730. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67432/0.70747. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67412/0.70775. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67358/0.70793. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67286/0.70816. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67288/0.70835. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67244/0.70834. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67150/0.70876. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67100/0.70879. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67076/0.70902. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67021/0.70939. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66931/0.70979. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66913/0.71037. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66867/0.71050. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66809/0.71079. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66784/0.71139. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66719/0.71152. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66703/0.71204. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66599/0.71228. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66543/0.71264. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66584/0.71327. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66508/0.71358. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66488/0.71407. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66393/0.71479. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66339/0.71522. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66301/0.71596. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66262/0.71645. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66215/0.71729. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69383/0.69740. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69370/0.69730. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69326/0.69719. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69313/0.69707. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69312/0.69695. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69300/0.69682. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.69667. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69235/0.69651. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69236/0.69633. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69227/0.69614. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69171/0.69595. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69171/0.69580. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69165/0.69564. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69138/0.69549. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69129/0.69538. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69098/0.69531. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69079/0.69521. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69064/0.69514. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69038/0.69509. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68996/0.69505. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68967/0.69500. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68950/0.69493. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68885/0.69487. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68875/0.69483. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68826/0.69480. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68794/0.69474. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68698/0.69463. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68685/0.69450. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68626/0.69432. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68553/0.69410. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68477/0.69388. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68388/0.69360. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68345/0.69333. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68230/0.69302. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68153/0.69270. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68077/0.69238. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67972/0.69196. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67867/0.69156. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67786/0.69117. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67695/0.69078. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67583/0.69043. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67471/0.69009. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67391/0.68979. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67263/0.68945. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67165/0.68914. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67065/0.68891. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66943/0.68882. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66868/0.68879. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66753/0.68871. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66647/0.68865. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66550/0.68862. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66407/0.68860. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66312/0.68868. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66279/0.68879. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66169/0.68894. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66013/0.68904. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65947/0.68927. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65824/0.68946. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65700/0.68965. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65646/0.68968. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65483/0.68984. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65433/0.69005. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65332/0.69037. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65244/0.69102. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65177/0.69136. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65064/0.69151. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64950/0.69202. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64835/0.69255. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64760/0.69308. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64630/0.69379. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64590/0.69422. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64504/0.69489. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64365/0.69566. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64367/0.69629. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64112/0.69696. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64153/0.69770. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64004/0.69857. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63834/0.69938. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63777/0.70015. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63712/0.70093. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63626/0.70195. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63476/0.70302. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63427/0.70379. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63338/0.70451. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63216/0.70562. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63135/0.70650. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63021/0.70728. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62837/0.70857. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62826/0.70955. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62719/0.71084. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62641/0.71189. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62588/0.71322. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62519/0.71456. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62363/0.71550. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62151/0.71665. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62159/0.71801. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62012/0.71929. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61869/0.72026. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61891/0.72186. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61687/0.72275. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69184/0.69774. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69144/0.69781. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69094/0.69795. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69085/0.69813. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69060/0.69834. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69027/0.69857. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.69881. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68922/0.69910. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68930/0.69940. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68884/0.69973. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68858/0.70009. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68823/0.70048. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68785/0.70087. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68793/0.70131. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68717/0.70182. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68709/0.70237. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68678/0.70297. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68605/0.70363. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68572/0.70432. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68525/0.70503. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.70585. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68455/0.70666. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68390/0.70754. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68343/0.70843. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68299/0.70939. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68310/0.71031. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68206/0.71124. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68168/0.71225. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68140/0.71326. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68136/0.71417. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68100/0.71504. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68023/0.71594. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68006/0.71685. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67911/0.71781. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67878/0.71882. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67854/0.71984. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67812/0.72078. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67749/0.72163. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67706/0.72257. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67676/0.72349. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67612/0.72437. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67582/0.72535. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67437/0.72636. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67459/0.72732. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67363/0.72837. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67341/0.72938. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67294/0.73034. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67245/0.73129. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67149/0.73225. Took 0.12 sec\n",
      "Epoch 49, Loss(train/val) 0.67091/0.73321. Took 0.16 sec\n",
      "Epoch 50, Loss(train/val) 0.66990/0.73433. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66980/0.73528. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66906/0.73622. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66810/0.73720. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66789/0.73812. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66698/0.73911. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66695/0.74015. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66558/0.74124. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66535/0.74220. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66462/0.74308. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66352/0.74405. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66375/0.74498. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66272/0.74585. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66246/0.74674. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66211/0.74756. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66138/0.74857. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66088/0.74961. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66029/0.75039. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65970/0.75121. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65946/0.75204. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65913/0.75280. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65881/0.75370. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65740/0.75462. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65683/0.75556. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65622/0.75650. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65585/0.75744. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65639/0.75829. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65466/0.75922. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65528/0.76006. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65487/0.76089. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65387/0.76172. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65252/0.76258. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65288/0.76339. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65309/0.76403. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65178/0.76485. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65231/0.76550. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65138/0.76604. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65081/0.76677. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65030/0.76762. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65039/0.76840. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64996/0.76888. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64961/0.76963. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64858/0.77061. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64845/0.77122. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64748/0.77193. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64731/0.77260. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64613/0.77326. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64664/0.77395. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64617/0.77488. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64514/0.77539. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69624/0.69835. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69472/0.69784. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69400/0.69744. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69324/0.69712. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69688. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69182/0.69668. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69126/0.69653. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.69638. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69047/0.69621. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69011/0.69601. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68971/0.69571. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68881/0.69538. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68841/0.69500. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68788/0.69459. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68721/0.69413. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68701/0.69366. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68607/0.69327. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68512/0.69288. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68481/0.69253. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68424/0.69230. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68343/0.69208. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68276/0.69190. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68203/0.69180. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68162/0.69178. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68090/0.69182. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67985/0.69186. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67939/0.69194. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67855/0.69212. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67807/0.69227. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67741/0.69249. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67611/0.69277. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67553/0.69309. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67493/0.69342. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67425/0.69376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67286/0.69420. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67277/0.69457. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67138/0.69496. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67080/0.69535. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66971/0.69589. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66919/0.69637. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66744/0.69694. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66720/0.69732. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66572/0.69792. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66505/0.69832. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66438/0.69867. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66256/0.69911. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66120/0.69976. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66051/0.70050. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66022/0.70090. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65909/0.70129. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65720/0.70188. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65554/0.70271. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65535/0.70309. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65386/0.70383. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65223/0.70458. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65136/0.70519. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65042/0.70592. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64948/0.70685. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64875/0.70738. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64757/0.70846. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64638/0.70915. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64657/0.71000. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.64468/0.71090. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64364/0.71166. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64230/0.71255. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64122/0.71353. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64125/0.71421. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64001/0.71539. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63954/0.71615. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63809/0.71717. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63738/0.71797. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63684/0.71906. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63568/0.72006. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63460/0.72087. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63352/0.72174. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63276/0.72294. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63308/0.72372. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63174/0.72455. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63145/0.72534. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63067/0.72631. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62944/0.72722. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62895/0.72789. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62843/0.72875. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62683/0.72995. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62572/0.73105. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62588/0.73185. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62426/0.73296. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62416/0.73387. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62340/0.73464. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62322/0.73542. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62285/0.73632. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62136/0.73719. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61989/0.73824. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61900/0.73906. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61832/0.73961. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61868/0.74067. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61668/0.74156. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61640/0.74250. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61525/0.74351. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61464/0.74425. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69228/0.69519. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69536. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69158/0.69556. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69130/0.69580. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69142/0.69604. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69084/0.69632. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69071/0.69661. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.69697. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68982/0.69738. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68936/0.69785. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68887/0.69836. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.69893. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68790/0.69955. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68709/0.70021. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68642/0.70101. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68563/0.70186. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68475/0.70277. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68387/0.70364. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68301/0.70449. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68197/0.70535. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68102/0.70614. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68008/0.70688. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67949/0.70740. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67890/0.70794. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67773/0.70838. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67674/0.70885. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67612/0.70923. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67518/0.70961. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67451/0.71013. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67324/0.71049. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67227/0.71094. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67213/0.71142. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67122/0.71186. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67000/0.71216. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66958/0.71266. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66882/0.71309. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66820/0.71354. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66721/0.71395. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66639/0.71425. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66568/0.71487. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66491/0.71524. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66401/0.71558. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66366/0.71596. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66265/0.71627. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66160/0.71675. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66056/0.71724. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66034/0.71748. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65925/0.71773. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65823/0.71809. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65758/0.71863. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65666/0.71901. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65584/0.71935. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65504/0.71985. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65384/0.72003. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65290/0.72037. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65251/0.72077. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65158/0.72088. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65051/0.72134. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65019/0.72159. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64966/0.72211. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64853/0.72236. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64802/0.72267. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64760/0.72320. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64631/0.72308. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64555/0.72360. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64503/0.72379. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64421/0.72453. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64320/0.72458. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64295/0.72470. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64282/0.72521. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64171/0.72562. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64050/0.72589. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64034/0.72653. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63985/0.72678. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63892/0.72701. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63801/0.72759. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63790/0.72801. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63677/0.72841. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63602/0.72855. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63570/0.72901. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63509/0.72968. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63376/0.72998. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63316/0.73040. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63284/0.73095. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63214/0.73125. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63059/0.73209. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63128/0.73249. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63030/0.73306. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62888/0.73341. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62869/0.73439. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62861/0.73487. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62759/0.73550. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62654/0.73651. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62648/0.73715. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62523/0.73763. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62509/0.73823. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62436/0.73871. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62306/0.73932. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62292/0.74039. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62143/0.74145. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69582/0.68848. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69542/0.68857. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69504/0.68868. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69506/0.68879. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69496/0.68890. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69446/0.68905. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69444/0.68920. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69407/0.68938. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69350/0.68963. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69331/0.68998. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69259/0.69048. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69232/0.69119. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69136/0.69223. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69087/0.69377. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69000/0.69576. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68885/0.69794. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68801/0.69991. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68743/0.70163. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68643/0.70306. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68588/0.70438. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68503/0.70537. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68384/0.70614. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68291/0.70692. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68303/0.70800. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68146/0.70891. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68055/0.70976. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68005/0.71029. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67920/0.71116. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67816/0.71219. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67755/0.71317. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67660/0.71376. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67579/0.71501. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67511/0.71606. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67378/0.71705. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67277/0.71783. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67268/0.71913. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67117/0.71988. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67019/0.72102. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66937/0.72176. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66853/0.72249. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66739/0.72367. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66626/0.72523. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66548/0.72613. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66449/0.72705. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66374/0.72858. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66234/0.72986. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66126/0.73114. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66070/0.73211. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65925/0.73287. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65840/0.73459. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65715/0.73573. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65634/0.73700. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65494/0.73840. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65358/0.73972. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65262/0.74096. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65132/0.74259. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65020/0.74459. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64885/0.74538. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64770/0.74766. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64641/0.74907. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64512/0.75068. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64396/0.75197. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64360/0.75368. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64081/0.75560. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64028/0.75721. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63822/0.75887. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.63833/0.76027. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63601/0.76306. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63455/0.76388. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63362/0.76619. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63118/0.76822. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63076/0.77045. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62881/0.77218. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62723/0.77367. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62665/0.77637. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62536/0.77805. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62232/0.77951. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62133/0.78211. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62022/0.78447. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61969/0.78708. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61688/0.78888. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.61616/0.79131. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61509/0.79292. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.61300/0.79533. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61157/0.79736. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61026/0.80046. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60961/0.80292. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60683/0.80586. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60650/0.80777. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60485/0.80995. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60349/0.81267. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60183/0.81430. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60027/0.81654. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.59910/0.81871. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59765/0.82070. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59631/0.82366. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.59525/0.82611. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59388/0.82940. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59198/0.83213. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59168/0.83453. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69614/0.70408. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69590/0.70364. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69559/0.70316. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69511/0.70260. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69510/0.70184. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69489/0.70078. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69432/0.69949. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69409/0.69795. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69357/0.69637. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69311/0.69484. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69282/0.69367. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69254/0.69273. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69247/0.69207. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69227/0.69158. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69228/0.69108. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69199/0.69070. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69197/0.69027. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69165/0.68997. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69100/0.68983. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69112/0.68954. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69093/0.68935. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69067/0.68911. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69031/0.68880. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69012/0.68851. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69001/0.68820. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68972/0.68789. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68929/0.68771. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68872/0.68738. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68870/0.68698. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68806/0.68672. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68758/0.68651. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68721/0.68608. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68668/0.68564. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68634/0.68528. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68605/0.68501. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68553/0.68474. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68462/0.68423. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68391/0.68378. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68360/0.68356. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68345/0.68323. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68270/0.68287. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68233/0.68253. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68145/0.68220. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68081/0.68184. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68086/0.68153. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68024/0.68096. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67947/0.68070. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67877/0.68046. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67861/0.67997. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67813/0.67983. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67743/0.67930. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67687/0.67886. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67637/0.67874. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67613/0.67805. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67537/0.67800. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67489/0.67782. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67449/0.67759. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67313/0.67752. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67248/0.67711. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67191/0.67672. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67171/0.67642. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67117/0.67635. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67010/0.67624. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66944/0.67610. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66919/0.67611. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66822/0.67564. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66799/0.67550. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66789/0.67547. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66693/0.67539. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66626/0.67535. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66510/0.67522. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66512/0.67571. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66468/0.67526. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66360/0.67558. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66274/0.67556. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66225/0.67596. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66193/0.67579. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66098/0.67627. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66002/0.67589. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65991/0.67633. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65900/0.67630. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65853/0.67642. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65706/0.67673. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65702/0.67706. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65597/0.67756. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65470/0.67749. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65521/0.67796. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65445/0.67812. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65380/0.67824. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65233/0.67879. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65149/0.67889. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65144/0.67918. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65048/0.67901. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64952/0.67958. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64899/0.68014. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64905/0.68050. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64803/0.68162. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64664/0.68101. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64604/0.68060. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64526/0.68252. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69594/0.68965. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69525/0.68967. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69465/0.68973. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69440/0.68985. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69366/0.69001. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69347/0.69022. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69304/0.69048. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69241/0.69073. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69214/0.69099. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69174/0.69122. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69171/0.69141. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69107/0.69158. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69102/0.69174. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69057/0.69184. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69025/0.69192. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69002/0.69194. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68996/0.69198. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68958/0.69199. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68932/0.69201. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68880/0.69197. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68858/0.69188. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68811/0.69183. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68792/0.69176. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68753/0.69175. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68727/0.69172. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68683/0.69165. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68625/0.69165. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68576/0.69161. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68520/0.69161. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68457/0.69169. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68435/0.69175. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68359/0.69176. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68317/0.69187. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68275/0.69205. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68195/0.69218. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68130/0.69240. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68126/0.69261. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68041/0.69283. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67968/0.69308. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67934/0.69341. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67844/0.69377. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67808/0.69395. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67750/0.69428. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67699/0.69454. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67668/0.69486. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67575/0.69516. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67545/0.69544. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67469/0.69590. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67412/0.69609. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67365/0.69623. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67297/0.69650. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67213/0.69670. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67159/0.69701. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67100/0.69738. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67016/0.69769. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66951/0.69790. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66892/0.69837. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66851/0.69873. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66784/0.69902. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66704/0.69938. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66657/0.69951. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66549/0.69973. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66477/0.69996. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66465/0.70020. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66380/0.70048. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66259/0.70069. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66211/0.70088. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66141/0.70091. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66069/0.70177. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65948/0.70170. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65879/0.70199. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65761/0.70214. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65733/0.70255. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65625/0.70288. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65550/0.70338. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65461/0.70377. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65337/0.70389. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65234/0.70432. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65167/0.70463. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64964/0.70472. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64949/0.70498. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64816/0.70537. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64739/0.70572. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64582/0.70613. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64530/0.70670. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64312/0.70746. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64181/0.70785. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64082/0.70789. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63982/0.70836. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63879/0.70896. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63792/0.70948. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63644/0.71013. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63381/0.71097. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63294/0.71143. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63232/0.71209. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63108/0.71309. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62980/0.71321. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62937/0.71516. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62681/0.71482. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62512/0.71601. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69578/0.69899. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69536/0.69847. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69497/0.69792. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69470/0.69729. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69446/0.69653. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69368/0.69559. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69336/0.69455. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69272/0.69350. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69234/0.69256. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69197/0.69181. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69159/0.69123. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69171/0.69077. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69153/0.69037. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69102/0.69011. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69093/0.68992. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69105/0.68970. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69085/0.68954. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69067/0.68936. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69035/0.68915. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69029/0.68896. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69019/0.68881. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68972/0.68862. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68960/0.68844. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68929/0.68825. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68914/0.68805. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68915/0.68780. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68841/0.68755. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68843/0.68736. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68806/0.68718. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68768/0.68689. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68735/0.68658. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68717/0.68630. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68689/0.68602. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68633/0.68575. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68598/0.68536. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68549/0.68502. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68492/0.68462. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68430/0.68425. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68403/0.68383. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68363/0.68350. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68308/0.68314. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68258/0.68283. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68186/0.68244. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68100/0.68211. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68060/0.68173. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68023/0.68143. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67937/0.68113. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67860/0.68087. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67811/0.68067. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67725/0.68047. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67687/0.68030. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67610/0.68013. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67542/0.67998. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67459/0.67998. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67428/0.67998. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67329/0.68009. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67252/0.68025. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67146/0.68043. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67145/0.68070. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67018/0.68107. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66971/0.68135. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66938/0.68181. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66784/0.68231. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66754/0.68283. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66671/0.68350. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66578/0.68422. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66530/0.68496. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66440/0.68566. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66350/0.68652. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66293/0.68732. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66184/0.68825. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66068/0.68924. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66015/0.69028. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65930/0.69128. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65878/0.69234. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65738/0.69332. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65667/0.69441. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65580/0.69556. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65512/0.69687. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65411/0.69798. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65260/0.69932. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65162/0.70076. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65088/0.70227. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64966/0.70352. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64907/0.70494. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64784/0.70644. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64695/0.70801. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64539/0.70969. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64454/0.71119. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64367/0.71280. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64220/0.71456. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64073/0.71625. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63960/0.71809. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63932/0.71966. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63813/0.72136. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63675/0.72314. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63525/0.72493. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63397/0.72659. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63243/0.72842. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63202/0.73030. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69351/0.69653. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.69754. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69180/0.69858. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69140/0.69959. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69105/0.70052. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69089/0.70134. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69054/0.70196. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69022/0.70245. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69009/0.70282. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68984/0.70308. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68964/0.70332. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.70350. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68923/0.70364. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68886/0.70381. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68862/0.70391. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68866/0.70406. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68811/0.70419. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68787/0.70431. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68754/0.70445. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68713/0.70462. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68684/0.70478. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68665/0.70499. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68650/0.70516. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68580/0.70541. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68555/0.70564. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68510/0.70584. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68521/0.70608. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68455/0.70638. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68411/0.70669. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68388/0.70686. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68348/0.70720. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68353/0.70752. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68290/0.70772. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68281/0.70793. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68235/0.70818. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68216/0.70838. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68188/0.70862. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68174/0.70894. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68123/0.70914. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68082/0.70943. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68094/0.70969. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68037/0.70993. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68044/0.71005. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67974/0.71036. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67967/0.71063. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67901/0.71086. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67870/0.71099. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67867/0.71121. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67808/0.71147. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67802/0.71183. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67740/0.71192. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67708/0.71223. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67684/0.71242. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67654/0.71275. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67618/0.71321. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67573/0.71326. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67511/0.71348. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67446/0.71368. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67449/0.71400. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67407/0.71414. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67356/0.71473. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67306/0.71483. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67233/0.71501. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67225/0.71530. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67157/0.71557. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67133/0.71597. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67076/0.71594. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67006/0.71641. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66957/0.71664. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66899/0.71710. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66866/0.71731. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66812/0.71745. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66717/0.71789. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66679/0.71813. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66611/0.71844. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66544/0.71883. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66501/0.71913. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66437/0.71959. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66409/0.71991. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66350/0.71993. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66285/0.72044. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66194/0.72102. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66171/0.72152. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66082/0.72193. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66006/0.72235. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65998/0.72314. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65937/0.72328. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65885/0.72374. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65818/0.72419. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65726/0.72460. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65660/0.72508. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65605/0.72594. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65534/0.72654. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65548/0.72682. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65502/0.72745. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65418/0.72772. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65320/0.72826. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65323/0.72910. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65236/0.72974. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65193/0.73027. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69222/0.70903. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69186/0.70898. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69163/0.70895. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69144/0.70891. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.70887. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69134/0.70886. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69118/0.70890. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69123/0.70890. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69092/0.70889. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69095/0.70888. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69066/0.70890. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69033/0.70895. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69041/0.70898. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69016/0.70899. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68984/0.70903. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68990/0.70904. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68955/0.70910. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68937/0.70907. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68888/0.70915. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68868/0.70928. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68831/0.70940. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68803/0.70958. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68743/0.70998. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68738/0.71028. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68661/0.71058. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68662/0.71080. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68562/0.71116. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68533/0.71143. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68483/0.71163. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68417/0.71218. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68391/0.71250. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68347/0.71298. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68257/0.71318. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68199/0.71357. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68156/0.71377. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68120/0.71396. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68034/0.71431. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68030/0.71448. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67906/0.71470. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67851/0.71494. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67812/0.71519. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67762/0.71541. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67657/0.71577. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67659/0.71563. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67568/0.71596. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67478/0.71613. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67456/0.71638. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67334/0.71645. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67252/0.71662. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67173/0.71678. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67151/0.71703. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67079/0.71712. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67003/0.71749. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67006/0.71754. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66851/0.71789. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66819/0.71834. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66694/0.71920. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66698/0.71917. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66638/0.71946. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66557/0.71961. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66443/0.72023. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66451/0.72066. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66324/0.72066. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66297/0.72109. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66217/0.72140. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66184/0.72130. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66096/0.72185. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66061/0.72279. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66017/0.72273. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65923/0.72321. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65810/0.72354. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65826/0.72383. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65706/0.72410. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65661/0.72437. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65598/0.72478. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65475/0.72509. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65440/0.72537. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65366/0.72562. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65385/0.72598. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65278/0.72599. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65187/0.72589. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65074/0.72698. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65065/0.72680. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64944/0.72692. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64911/0.72705. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64750/0.72736. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64717/0.72702. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64721/0.72821. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64676/0.72862. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64511/0.72782. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64516/0.72807. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64385/0.72730. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64283/0.72863. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64274/0.72818. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64175/0.72789. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64023/0.72839. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64032/0.72876. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63940/0.72841. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63827/0.72844. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63691/0.72809. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69335/0.68792. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69298/0.68806. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.68823. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69267/0.68840. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69286/0.68855. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69296/0.68871. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69253/0.68888. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69269/0.68906. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69225/0.68925. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69210/0.68945. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69204/0.68966. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69199/0.68987. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69178/0.69011. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69175/0.69036. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69162/0.69063. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69128/0.69092. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69099/0.69123. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69100/0.69156. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69058/0.69192. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69007/0.69225. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69016/0.69263. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68989/0.69303. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68940/0.69349. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68923/0.69396. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68892/0.69449. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68853/0.69506. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68786/0.69562. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68756/0.69628. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68734/0.69692. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68693/0.69754. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68655/0.69823. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68624/0.69894. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68616/0.69955. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68583/0.70013. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68530/0.70069. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68476/0.70119. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68429/0.70168. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68408/0.70221. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68367/0.70266. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68323/0.70322. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68299/0.70355. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68245/0.70395. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68248/0.70437. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68234/0.70470. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68172/0.70523. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68119/0.70561. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68050/0.70591. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68008/0.70628. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68020/0.70663. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67947/0.70711. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67906/0.70752. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67851/0.70800. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67794/0.70843. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67724/0.70880. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67697/0.70919. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67645/0.70987. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67552/0.71034. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67477/0.71110. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67469/0.71163. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67391/0.71246. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67304/0.71321. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67195/0.71387. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.67128/0.71457. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.67101/0.71543. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67033/0.71618. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66933/0.71717. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66809/0.71801. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66766/0.71914. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66773/0.71999. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66606/0.72117. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66507/0.72208. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.66432/0.72311. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66414/0.72383. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66346/0.72497. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66191/0.72600. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66131/0.72676. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.66078/0.72823. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65972/0.72914. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65883/0.73018. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.65843/0.73085. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65738/0.73193. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65625/0.73315. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65602/0.73382. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65497/0.73473. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.65406/0.73608. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65330/0.73668. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65150/0.73817. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65086/0.73898. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65065/0.74007. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.64918/0.74119. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64882/0.74191. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64790/0.74319. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64727/0.74419. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.64729/0.74482. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64533/0.74599. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64399/0.74714. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64334/0.74822. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64265/0.74919. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64178/0.75068. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64161/0.75108. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69583/0.69392. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69474/0.69367. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69400/0.69364. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69349/0.69373. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69321/0.69389. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69284/0.69408. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69244/0.69429. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69210/0.69453. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69186/0.69477. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69148/0.69505. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69128/0.69535. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69067/0.69566. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69039/0.69599. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69043/0.69632. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69028/0.69666. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68979/0.69700. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68946/0.69734. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68925/0.69770. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68899/0.69805. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68879/0.69840. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68883/0.69874. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68847/0.69906. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68830/0.69937. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68786/0.69968. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68800/0.69999. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68770/0.70026. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68757/0.70054. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68729/0.70081. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68703/0.70105. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68714/0.70128. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68694/0.70148. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68641/0.70169. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68634/0.70189. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68619/0.70209. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68620/0.70228. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68575/0.70246. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68582/0.70263. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68551/0.70281. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68538/0.70299. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68493/0.70314. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68486/0.70331. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68474/0.70343. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68462/0.70356. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68448/0.70371. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68414/0.70384. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68384/0.70396. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68348/0.70409. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68345/0.70423. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68327/0.70437. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.68331/0.70445. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.68280/0.70457. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68245/0.70470. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.68209/0.70484. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.68203/0.70493. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68192/0.70501. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68125/0.70511. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68136/0.70520. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68094/0.70529. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68074/0.70536. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68025/0.70547. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67984/0.70557. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67949/0.70569. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67936/0.70582. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67877/0.70593. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67854/0.70602. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67766/0.70613. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67793/0.70624. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67738/0.70631. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67699/0.70642. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67623/0.70660. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67588/0.70673. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67525/0.70687. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67497/0.70707. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67436/0.70721. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67383/0.70742. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67309/0.70759. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67262/0.70778. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67233/0.70795. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67182/0.70808. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67079/0.70829. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67017/0.70854. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66931/0.70883. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66854/0.70908. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66812/0.70938. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66733/0.70973. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66657/0.71009. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66577/0.71044. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66509/0.71079. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66374/0.71127. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66311/0.71171. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66226/0.71219. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66187/0.71270. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66122/0.71309. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65984/0.71360. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65946/0.71408. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65855/0.71449. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65712/0.71491. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65713/0.71544. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65541/0.71601. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65459/0.71654. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69667/0.69549. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69616/0.69499. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69493/0.69462. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69455/0.69432. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69455/0.69402. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69442/0.69376. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69342/0.69348. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69308/0.69321. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69274/0.69294. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69219/0.69267. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69208/0.69238. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69180/0.69211. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69135/0.69185. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69078/0.69162. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69079/0.69137. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69027/0.69111. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68984/0.69088. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68960/0.69067. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68908/0.69046. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68835/0.69022. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68820/0.69004. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68766/0.68994. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68723/0.68982. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68658/0.68977. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68612/0.68973. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68567/0.68972. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68602/0.68970. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68517/0.68973. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68466/0.68972. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68464/0.68977. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68357/0.68987. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68355/0.68991. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68296/0.69001. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68325/0.69003. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68278/0.69004. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68176/0.69013. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68143/0.69021. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68114/0.69029. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68056/0.69035. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68031/0.69040. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68001/0.69044. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67958/0.69051. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67961/0.69050. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67866/0.69055. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67848/0.69061. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67799/0.69067. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67755/0.69073. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67747/0.69078. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67711/0.69077. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67664/0.69076. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67616/0.69082. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67495/0.69082. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67482/0.69076. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67429/0.69085. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67390/0.69091. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67343/0.69088. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67304/0.69090. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67285/0.69091. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67187/0.69083. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67102/0.69088. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67006/0.69101. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67023/0.69101. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67003/0.69102. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66917/0.69105. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66853/0.69110. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66800/0.69122. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66678/0.69136. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66607/0.69137. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66543/0.69154. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66536/0.69168. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66546/0.69180. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66422/0.69189. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66391/0.69194. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66280/0.69218. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66213/0.69235. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66200/0.69267. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66126/0.69286. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65944/0.69310. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65912/0.69332. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65829/0.69368. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65788/0.69414. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65708/0.69440. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65668/0.69500. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65498/0.69545. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65454/0.69602. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65372/0.69639. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65293/0.69684. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65254/0.69752. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65209/0.69802. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65098/0.69871. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65020/0.69924. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64931/0.70003. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64816/0.70046. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64739/0.70130. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64663/0.70227. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64633/0.70300. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64565/0.70336. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.64490/0.70398. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64376/0.70475. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64277/0.70553. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69443/0.68672. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69451/0.68682. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69438/0.68692. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69417/0.68707. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69405/0.68722. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69411/0.68740. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69393/0.68762. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69384/0.68784. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69386/0.68810. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69357/0.68843. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69330/0.68878. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69331/0.68920. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69317/0.68962. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69287/0.69009. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69264/0.69056. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69262/0.69099. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69238/0.69137. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69227/0.69174. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69195/0.69201. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.69167/0.69223. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69158/0.69240. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.69126/0.69251. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69125/0.69259. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69077/0.69263. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69054/0.69256. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69023/0.69253. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68963/0.69244. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68913/0.69234. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68899/0.69220. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68818/0.69219. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68799/0.69217. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68721/0.69227. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68651/0.69225. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68626/0.69232. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68554/0.69227. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68485/0.69242. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68399/0.69235. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68361/0.69280. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68292/0.69318. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68210/0.69343. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68164/0.69363. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68095/0.69385. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68051/0.69408. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68000/0.69445. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67931/0.69456. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67879/0.69477. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67808/0.69503. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67754/0.69516. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67699/0.69555. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67632/0.69606. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67573/0.69616. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67528/0.69662. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67468/0.69665. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67415/0.69703. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67314/0.69720. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67299/0.69758. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67217/0.69788. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67161/0.69848. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67064/0.69825. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66991/0.69883. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66910/0.69895. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66827/0.69979. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66823/0.70006. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66699/0.70061. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66626/0.70085. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66607/0.70159. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66530/0.70196. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66486/0.70264. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66357/0.70303. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66314/0.70418. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66245/0.70458. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66180/0.70529. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66087/0.70587. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66015/0.70746. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65917/0.70785. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65860/0.70886. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65826/0.70974. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65699/0.71001. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65657/0.71111. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65564/0.71164. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65474/0.71387. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65455/0.71455. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65348/0.71493. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65239/0.71593. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65151/0.71657. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65133/0.71889. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65113/0.71894. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64958/0.72145. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64874/0.72123. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64839/0.72280. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64772/0.72421. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64650/0.72499. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64616/0.72542. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.64516/0.72719. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64494/0.72780. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64449/0.72879. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64350/0.72968. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64274/0.73119. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64178/0.73129. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64053/0.73242. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69618/0.70167. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69560/0.70029. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69462/0.69908. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69431/0.69804. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69368/0.69706. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69327/0.69617. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69295/0.69534. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69255/0.69461. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69215/0.69394. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69187/0.69333. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69193/0.69277. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69124/0.69226. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69124/0.69178. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69074/0.69133. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69053/0.69088. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69012/0.69050. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68988/0.69013. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68923/0.68976. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68911/0.68936. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68858/0.68902. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68810/0.68865. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68780/0.68828. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68731/0.68795. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68693/0.68764. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68616/0.68736. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68551/0.68708. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68502/0.68688. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68476/0.68674. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68366/0.68667. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68293/0.68652. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68232/0.68639. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68157/0.68638. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68085/0.68650. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67991/0.68656. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67921/0.68657. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67814/0.68679. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67751/0.68675. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67665/0.68712. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.68740. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67516/0.68770. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67465/0.68804. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67306/0.68863. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67261/0.68885. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67152/0.68928. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67134/0.68982. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67046/0.69046. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66949/0.69080. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66878/0.69141. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66818/0.69207. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66734/0.69253. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66615/0.69298. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66606/0.69339. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66544/0.69403. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66513/0.69417. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66363/0.69493. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66340/0.69532. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66273/0.69521. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66200/0.69589. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66131/0.69626. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66046/0.69662. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65992/0.69720. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65941/0.69751. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65895/0.69773. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65764/0.69810. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65736/0.69799. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65652/0.69849. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65611/0.69906. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65526/0.69949. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65500/0.69941. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65401/0.69994. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65296/0.70041. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65263/0.70078. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65183/0.70059. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65154/0.70101. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65059/0.70104. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65013/0.70138. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64927/0.70202. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64807/0.70240. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64782/0.70244. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64696/0.70262. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64625/0.70312. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64615/0.70353. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64511/0.70392. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64424/0.70431. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64382/0.70457. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64374/0.70489. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64226/0.70559. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64174/0.70609. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64099/0.70646. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63984/0.70682. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63961/0.70705. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63883/0.70762. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63806/0.70811. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63740/0.70843. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63683/0.70872. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63670/0.70960. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63570/0.71033. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63458/0.71084. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63429/0.71173. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63425/0.71151. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69383/0.69236. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69359/0.69218. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69338/0.69202. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69296/0.69188. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69275/0.69174. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69259/0.69164. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69247/0.69154. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69242/0.69143. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69223/0.69133. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69184/0.69123. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69172/0.69113. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69145/0.69104. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69109/0.69092. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69069/0.69083. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69043/0.69074. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69035/0.69064. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68984/0.69056. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68961/0.69049. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68919/0.69043. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68886/0.69040. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68831/0.69038. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68787/0.69039. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68769/0.69044. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68697/0.69050. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68656/0.69060. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68589/0.69073. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68549/0.69086. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68488/0.69106. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68462/0.69128. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68402/0.69151. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68341/0.69178. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68268/0.69203. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68203/0.69232. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68196/0.69259. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68126/0.69289. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68051/0.69317. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68018/0.69350. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67951/0.69378. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67897/0.69415. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67854/0.69451. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67784/0.69485. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67730/0.69519. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67682/0.69556. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67633/0.69593. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67566/0.69634. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67532/0.69672. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67453/0.69701. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67425/0.69736. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67374/0.69775. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67305/0.69803. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67278/0.69845. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67179/0.69885. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67162/0.69928. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67075/0.69968. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67066/0.70014. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66940/0.70044. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66919/0.70094. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66838/0.70131. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66826/0.70171. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66767/0.70219. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66725/0.70266. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66601/0.70312. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66570/0.70376. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66502/0.70424. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66465/0.70481. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66355/0.70539. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66280/0.70596. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66268/0.70644. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66184/0.70715. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66162/0.70783. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66072/0.70858. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65976/0.70939. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65934/0.71017. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65914/0.71085. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65788/0.71165. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65703/0.71254. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65652/0.71348. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65576/0.71442. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65428/0.71538. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65345/0.71627. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65298/0.71726. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65256/0.71822. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65127/0.71920. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65080/0.72016. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64943/0.72118. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64831/0.72213. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64832/0.72307. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64741/0.72410. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64646/0.72514. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64579/0.72618. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64510/0.72716. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64347/0.72826. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64205/0.72942. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64124/0.73052. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64101/0.73164. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64031/0.73255. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63847/0.73348. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63911/0.73450. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63671/0.73558. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63519/0.73679. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69307/0.68956. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69320/0.68966. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.68975. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69286/0.68984. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69234/0.68993. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69180/0.69006. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69151/0.69021. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69122/0.69040. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69085/0.69063. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69031/0.69087. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69017/0.69118. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68960/0.69143. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68905/0.69176. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.69209. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68771/0.69236. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68712/0.69254. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68612/0.69266. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68597/0.69279. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68529/0.69288. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68439/0.69298. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68371/0.69313. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68326/0.69317. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68251/0.69335. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68165/0.69327. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68056/0.69323. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68030/0.69327. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67951/0.69329. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67838/0.69330. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67762/0.69323. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67709/0.69331. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67588/0.69320. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67552/0.69336. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67445/0.69336. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67389/0.69351. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67264/0.69355. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67182/0.69386. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67142/0.69421. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67016/0.69438. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66923/0.69497. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66860/0.69531. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66757/0.69583. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66686/0.69617. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66625/0.69698. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66563/0.69742. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66444/0.69791. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66367/0.69897. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66311/0.69959. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66209/0.70039. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66150/0.70126. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66029/0.70214. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65990/0.70259. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65945/0.70390. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65840/0.70472. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65793/0.70526. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65705/0.70625. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65637/0.70718. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65602/0.70780. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65483/0.70853. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65519/0.70981. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65393/0.71060. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65265/0.71156. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65267/0.71259. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65169/0.71336. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65040/0.71430. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64989/0.71561. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64978/0.71597. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64931/0.71746. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64847/0.71775. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64829/0.71871. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64729/0.72005. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64669/0.72045. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64574/0.72177. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64556/0.72268. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64427/0.72396. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64468/0.72460. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64300/0.72579. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64220/0.72649. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64156/0.72735. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64124/0.72865. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64044/0.72946. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64012/0.73074. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63975/0.73089. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63810/0.73245. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63809/0.73354. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63783/0.73459. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63576/0.73528. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63626/0.73691. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63485/0.73805. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63541/0.73882. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63336/0.73984. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63274/0.74084. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.63164/0.74200. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63219/0.74300. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62949/0.74398. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62986/0.74596. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62886/0.74708. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62886/0.74823. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62703/0.74914. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62647/0.75032. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62657/0.75170. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69932/0.72218. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69725/0.71716. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69556/0.71364. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69454/0.71110. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69371/0.70905. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69308/0.70724. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69227/0.70561. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.70418. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69101/0.70286. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69073/0.70167. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69032/0.70054. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68964/0.69951. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68915/0.69852. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68876/0.69766. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68850/0.69681. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68826/0.69614. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68784/0.69549. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68726/0.69490. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68725/0.69443. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68680/0.69390. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68647/0.69350. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68603/0.69323. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68640/0.69294. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68600/0.69268. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68589/0.69250. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68545/0.69233. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68513/0.69229. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68490/0.69226. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68517/0.69224. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68461/0.69218. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68456/0.69221. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68448/0.69231. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68391/0.69235. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68393/0.69240. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68400/0.69251. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68357/0.69271. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68350/0.69276. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68292/0.69292. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68289/0.69305. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68267/0.69322. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68288/0.69343. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68222/0.69360. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68229/0.69378. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68181/0.69403. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68174/0.69424. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68129/0.69439. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68136/0.69456. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68096/0.69488. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68101/0.69500. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68055/0.69520. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68034/0.69547. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68016/0.69569. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67990/0.69592. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67947/0.69617. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67953/0.69644. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67924/0.69681. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67864/0.69697. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67881/0.69718. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67814/0.69743. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67813/0.69784. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67773/0.69808. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67757/0.69836. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67737/0.69861. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67692/0.69889. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67669/0.69936. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67698/0.69959. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67645/0.69987. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67594/0.70028. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67533/0.70065. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67522/0.70081. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67482/0.70125. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67488/0.70162. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67439/0.70201. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67410/0.70224. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67386/0.70269. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67333/0.70320. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67286/0.70349. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67295/0.70398. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67228/0.70437. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67247/0.70479. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67157/0.70536. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67168/0.70581. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67122/0.70627. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67035/0.70675. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67023/0.70724. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67007/0.70786. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66949/0.70825. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66957/0.70879. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66840/0.70922. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66851/0.70979. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66835/0.71043. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66768/0.71109. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66784/0.71173. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66670/0.71237. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66682/0.71286. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66614/0.71356. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66608/0.71414. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66531/0.71480. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66501/0.71556. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66514/0.71613. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69668/0.69348. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69401/0.69319. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69242/0.69358. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69153/0.69416. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69081/0.69471. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69011/0.69520. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68983/0.69562. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68959/0.69599. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68951/0.69636. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68907/0.69675. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.69717. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68821/0.69761. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68796/0.69803. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68779/0.69850. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68744/0.69896. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68677/0.69946. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68690/0.69996. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68639/0.70050. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68620/0.70097. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68617/0.70148. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68585/0.70202. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68557/0.70253. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68528/0.70306. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68520/0.70357. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68468/0.70407. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68455/0.70455. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68440/0.70503. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68415/0.70555. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68385/0.70605. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68377/0.70650. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68322/0.70696. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68310/0.70746. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68334/0.70791. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68267/0.70835. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68263/0.70875. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68253/0.70920. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68224/0.70965. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68195/0.71007. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68195/0.71050. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68163/0.71093. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68161/0.71126. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68110/0.71167. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68089/0.71206. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68086/0.71247. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68044/0.71288. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68011/0.71326. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67989/0.71366. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67978/0.71407. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67908/0.71444. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67882/0.71478. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67915/0.71518. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67847/0.71553. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67838/0.71590. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67796/0.71626. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67795/0.71658. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67712/0.71695. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67713/0.71733. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67693/0.71764. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67653/0.71792. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67621/0.71831. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67566/0.71868. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67532/0.71912. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67507/0.71948. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67472/0.71990. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67466/0.72018. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67388/0.72051. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67336/0.72085. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67341/0.72118. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67291/0.72158. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67244/0.72198. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67218/0.72233. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67155/0.72270. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67114/0.72307. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67091/0.72345. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67035/0.72379. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66999/0.72423. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66995/0.72470. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66941/0.72513. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66905/0.72561. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66819/0.72602. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66781/0.72649. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66727/0.72697. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66686/0.72750. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66650/0.72801. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66595/0.72852. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66639/0.72898. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66527/0.72948. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66511/0.72982. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66459/0.73041. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66480/0.73085. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66367/0.73126. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66308/0.73188. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66263/0.73249. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66216/0.73301. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66176/0.73353. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66133/0.73404. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66170/0.73464. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66077/0.73501. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66038/0.73556. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66049/0.73593. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69639/0.69876. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69530/0.69719. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69434/0.69613. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69382/0.69539. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69345/0.69489. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69267/0.69453. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69269/0.69436. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69202/0.69428. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69197/0.69424. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69145/0.69422. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69077/0.69417. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69097/0.69417. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69055/0.69420. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69040/0.69428. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69006/0.69435. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68991/0.69436. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68927/0.69443. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68896/0.69452. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68848/0.69463. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68791/0.69472. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68811/0.69473. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68769/0.69472. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68688/0.69472. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68667/0.69463. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68633/0.69449. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68572/0.69426. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68567/0.69398. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68479/0.69365. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68475/0.69325. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68416/0.69284. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68327/0.69239. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68333/0.69195. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68271/0.69138. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68234/0.69092. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68218/0.69028. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68156/0.68960. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68097/0.68903. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68062/0.68837. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68012/0.68776. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67997/0.68725. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67946/0.68672. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67898/0.68632. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67857/0.68575. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67809/0.68509. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67771/0.68462. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67712/0.68406. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67719/0.68372. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67656/0.68318. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67620/0.68280. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67569/0.68228. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67523/0.68198. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67411/0.68146. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67390/0.68123. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67330/0.68093. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67270/0.68048. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67214/0.68017. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67142/0.67979. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67104/0.67927. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67087/0.67919. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67017/0.67880. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66953/0.67860. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66915/0.67848. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66849/0.67845. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66756/0.67827. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66709/0.67826. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66645/0.67813. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66606/0.67826. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66583/0.67792. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66498/0.67817. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66397/0.67833. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66347/0.67838. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66281/0.67859. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66178/0.67922. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66187/0.67926. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66135/0.67924. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66004/0.67973. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66000/0.67991. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65900/0.68040. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65878/0.68117. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65731/0.68139. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65758/0.68161. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65686/0.68202. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65694/0.68258. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65566/0.68301. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65435/0.68442. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65447/0.68434. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65355/0.68491. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65255/0.68558. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65191/0.68661. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65226/0.68674. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65118/0.68759. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65035/0.68787. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64966/0.68902. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64908/0.68950. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64802/0.69103. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64765/0.69143. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64718/0.69180. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64628/0.69257. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64616/0.69326. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64525/0.69393. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69456/0.69148. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69362/0.69112. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69087. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69239/0.69076. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.69080. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69095/0.69099. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.69128. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69000/0.69162. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68942/0.69196. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68922/0.69231. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68913/0.69267. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68853/0.69307. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68877/0.69350. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68810/0.69391. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68781/0.69432. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68792/0.69479. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68736/0.69527. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68697/0.69578. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68678/0.69629. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68649/0.69681. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68630/0.69735. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68587/0.69792. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68575/0.69852. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68528/0.69905. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68482/0.69957. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68512/0.70010. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68459/0.70064. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68432/0.70117. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68455/0.70168. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68401/0.70215. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68364/0.70256. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68395/0.70299. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68369/0.70339. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68332/0.70380. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68308/0.70416. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68286/0.70454. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68227/0.70489. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68235/0.70520. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68213/0.70552. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68201/0.70586. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68162/0.70616. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68149/0.70645. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68138/0.70675. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68113/0.70700. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68092/0.70730. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68074/0.70763. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68008/0.70789. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68005/0.70810. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67943/0.70841. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67948/0.70859. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67916/0.70876. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67885/0.70900. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67857/0.70924. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67849/0.70946. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.67807/0.70961. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67768/0.70974. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67687/0.70987. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67648/0.70994. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67630/0.71003. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67641/0.71006. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67532/0.71016. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67524/0.71023. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67447/0.71027. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67384/0.71025. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67382/0.71018. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67309/0.71009. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67255/0.70997. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67218/0.70977. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67177/0.70958. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67084/0.70933. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67045/0.70904. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66955/0.70879. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66914/0.70845. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66850/0.70798. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66738/0.70748. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66651/0.70712. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66540/0.70675. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66469/0.70615. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66384/0.70579. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66354/0.70522. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66237/0.70475. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66129/0.70431. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66016/0.70375. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65948/0.70326. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65860/0.70288. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65743/0.70253. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65692/0.70228. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.65512/0.70205. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65481/0.70145. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65361/0.70103. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65252/0.70113. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65165/0.70080. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.65007/0.70052. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64976/0.70057. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64812/0.70044. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64818/0.70006. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64636/0.70039. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64498/0.70062. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64490/0.70066. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.64346/0.70054. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69727/0.69002. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69661/0.69002. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69629/0.69008. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69581/0.69016. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69520/0.69027. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69507/0.69041. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69477/0.69058. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69454/0.69077. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69418/0.69098. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69387/0.69121. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69387/0.69145. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69323/0.69169. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69299/0.69194. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69280/0.69221. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69266/0.69247. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69241/0.69276. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69210/0.69305. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69217/0.69335. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69179/0.69368. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69172/0.69404. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69104/0.69447. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69083/0.69492. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69070/0.69540. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69014/0.69591. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68963/0.69651. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68945/0.69722. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68876/0.69797. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68867/0.69886. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68814/0.69983. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68755/0.70087. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68700/0.70206. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68673/0.70334. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68557/0.70471. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68534/0.70613. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68472/0.70767. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68410/0.70929. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68365/0.71088. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68284/0.71258. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68229/0.71426. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68183/0.71588. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68122/0.71744. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68133/0.71890. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68067/0.72033. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67987/0.72167. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67969/0.72304. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67914/0.72433. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67880/0.72539. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67837/0.72651. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67752/0.72766. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67770/0.72872. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67696/0.72971. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67674/0.73071. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67627/0.73154. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67583/0.73234. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67558/0.73308. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67481/0.73384. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67424/0.73454. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67357/0.73531. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67359/0.73595. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67293/0.73663. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67268/0.73715. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67203/0.73772. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67129/0.73835. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67141/0.73894. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67069/0.73955. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67031/0.73998. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66988/0.74037. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66921/0.74099. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66845/0.74146. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66739/0.74181. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66768/0.74219. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66711/0.74268. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66616/0.74331. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66538/0.74381. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66499/0.74447. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66475/0.74510. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66347/0.74555. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66361/0.74601. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66247/0.74645. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66224/0.74685. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66111/0.74722. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66055/0.74765. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66043/0.74820. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65961/0.74890. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65882/0.74934. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65780/0.74986. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65736/0.75042. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65674/0.75119. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65646/0.75188. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65552/0.75246. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65520/0.75320. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65392/0.75374. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65306/0.75438. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65254/0.75503. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65239/0.75557. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.65129/0.75648. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65102/0.75690. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65020/0.75779. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.64860/0.75827. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64837/0.75881. Took 0.08 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69463/0.68757. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69422/0.68807. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69396/0.68858. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69356/0.68913. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69341/0.68972. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69289/0.69035. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69305/0.69103. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69258/0.69176. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69201/0.69245. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69172/0.69306. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69159/0.69367. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69117/0.69426. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69100/0.69477. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69019/0.69526. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68992/0.69568. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68948/0.69613. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68901/0.69657. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68892/0.69693. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68815/0.69729. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68780/0.69761. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68685/0.69804. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68651/0.69839. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68585/0.69888. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68506/0.69944. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68465/0.70002. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68406/0.70070. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68372/0.70136. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68305/0.70195. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68269/0.70258. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68231/0.70342. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68146/0.70418. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68090/0.70495. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68061/0.70579. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68029/0.70655. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67933/0.70729. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67877/0.70802. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67809/0.70891. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67801/0.70994. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67714/0.71072. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67645/0.71133. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67593/0.71230. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67562/0.71317. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67488/0.71393. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67474/0.71481. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67380/0.71552. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67343/0.71645. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67251/0.71697. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67233/0.71778. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67189/0.71880. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67094/0.71952. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67010/0.72000. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66923/0.72093. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66885/0.72150. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66847/0.72224. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66762/0.72313. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66659/0.72369. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66634/0.72459. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66524/0.72491. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66500/0.72584. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66391/0.72667. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66293/0.72734. Took 0.08 sec\n",
      "Epoch 61, Loss(train/val) 0.66203/0.72801. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66136/0.72891. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66138/0.72921. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66020/0.73003. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65964/0.73098. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65801/0.73104. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65745/0.73222. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.65657/0.73259. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65597/0.73280. Took 0.08 sec\n",
      "Epoch 70, Loss(train/val) 0.65580/0.73407. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65457/0.73460. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.65359/0.73514. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65288/0.73555. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.65167/0.73583. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65062/0.73677. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.64974/0.73680. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64856/0.73775. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.64726/0.73852. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.64614/0.73848. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64536/0.73952. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64434/0.73981. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64354/0.74046. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64182/0.74047. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64141/0.74111. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64046/0.74113. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.63848/0.74142. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63651/0.74193. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.63653/0.74225. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63479/0.74275. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63285/0.74326. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63233/0.74402. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62989/0.74393. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62879/0.74399. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62781/0.74511. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62584/0.74552. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62493/0.74610. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62281/0.74608. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.62163/0.74747. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62094/0.74828. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69254/0.69635. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69233/0.69659. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.69686. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.69720. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69170/0.69759. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69141/0.69803. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69137/0.69850. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.69899. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69109/0.69950. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69065/0.70003. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69062/0.70059. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69035/0.70116. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69035/0.70169. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69038/0.70222. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69001/0.70276. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68992/0.70327. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68967/0.70380. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68993/0.70429. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68963/0.70480. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68909/0.70529. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68908/0.70577. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68902/0.70622. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68878/0.70669. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68873/0.70714. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68855/0.70754. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68858/0.70797. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68841/0.70840. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68788/0.70881. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68807/0.70917. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68778/0.70952. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68771/0.70987. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68733/0.71020. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68719/0.71048. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68691/0.71074. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68651/0.71100. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68658/0.71122. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68620/0.71146. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68605/0.71164. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68586/0.71175. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68565/0.71199. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68547/0.71210. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68536/0.71218. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68517/0.71221. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68461/0.71233. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68439/0.71242. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68427/0.71250. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68416/0.71251. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68383/0.71244. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68364/0.71237. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68337/0.71234. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68303/0.71227. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68284/0.71222. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68255/0.71211. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68215/0.71196. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68168/0.71186. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68145/0.71183. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68120/0.71164. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68077/0.71142. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68037/0.71124. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67986/0.71118. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67991/0.71090. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67951/0.71077. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67904/0.71043. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67839/0.71018. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67824/0.70995. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67790/0.70971. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67735/0.70940. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67660/0.70924. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67632/0.70910. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67607/0.70870. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67599/0.70867. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67542/0.70849. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67469/0.70838. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67432/0.70806. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67423/0.70806. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67355/0.70787. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67338/0.70785. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67287/0.70759. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67205/0.70745. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67193/0.70699. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67169/0.70679. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67050/0.70697. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67081/0.70653. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67032/0.70677. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66951/0.70633. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66945/0.70654. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66899/0.70611. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66815/0.70619. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66779/0.70600. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66771/0.70612. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66725/0.70644. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66621/0.70640. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66641/0.70658. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66570/0.70666. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66546/0.70677. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66509/0.70672. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66444/0.70658. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66368/0.70696. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66351/0.70701. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66270/0.70703. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69937/0.70083. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69772/0.70068. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69687/0.70058. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69602/0.70052. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69542/0.70047. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69474/0.70035. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69466/0.70020. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69427/0.70000. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69375/0.69977. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69368/0.69954. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69347/0.69926. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69327/0.69896. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69274/0.69861. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69235/0.69828. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69223/0.69791. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69178/0.69752. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69135/0.69717. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69063/0.69677. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69073/0.69643. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69049/0.69609. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68981/0.69581. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68950/0.69552. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68905/0.69528. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68876/0.69501. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68830/0.69488. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68801/0.69481. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68793/0.69472. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68726/0.69467. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68701/0.69466. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68678/0.69463. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68627/0.69470. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68613/0.69476. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68594/0.69486. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68545/0.69500. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68518/0.69512. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68485/0.69531. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68466/0.69544. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68436/0.69565. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68402/0.69589. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68360/0.69614. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68313/0.69634. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68308/0.69654. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68303/0.69678. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68233/0.69699. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68213/0.69719. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68185/0.69743. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68139/0.69767. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68109/0.69787. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68091/0.69816. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68082/0.69838. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68029/0.69862. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67991/0.69886. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67958/0.69912. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67918/0.69935. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67918/0.69959. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.67844/0.69996. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67846/0.70020. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67770/0.70044. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67744/0.70086. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67679/0.70110. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67624/0.70144. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67645/0.70171. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67589/0.70197. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67542/0.70229. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67531/0.70252. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67451/0.70291. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67418/0.70327. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67368/0.70363. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67325/0.70400. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67294/0.70443. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67285/0.70484. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67217/0.70506. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67136/0.70549. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67123/0.70582. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67076/0.70618. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67000/0.70667. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67001/0.70704. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66942/0.70737. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66809/0.70800. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66793/0.70845. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66807/0.70881. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66689/0.70939. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66664/0.70973. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66575/0.71044. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66625/0.71072. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66567/0.71111. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66507/0.71147. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66407/0.71188. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66310/0.71248. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66273/0.71285. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66214/0.71371. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66225/0.71402. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66175/0.71408. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66058/0.71504. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66050/0.71536. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66013/0.71569. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65877/0.71634. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65810/0.71675. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65863/0.71718. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65737/0.71771. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69318/0.68875. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69291/0.68889. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69246/0.68903. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69245/0.68917. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69215/0.68930. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69202/0.68943. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69179/0.68956. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69167/0.68968. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69156/0.68980. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69147/0.68992. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69118/0.69002. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69116/0.69012. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69093/0.69021. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69101/0.69029. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69093/0.69037. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69082/0.69045. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69054/0.69052. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69046/0.69059. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69041/0.69065. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.69033/0.69070. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69022/0.69074. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69005/0.69078. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68973/0.69082. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68955/0.69085. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68930/0.69089. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68913/0.69092. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68872/0.69094. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68846/0.69099. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68817/0.69102. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68770/0.69107. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68732/0.69112. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68682/0.69121. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68634/0.69130. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68576/0.69143. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68514/0.69161. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68432/0.69182. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68370/0.69209. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68309/0.69232. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68247/0.69263. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68201/0.69295. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68104/0.69331. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68038/0.69365. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67981/0.69407. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67927/0.69439. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67842/0.69474. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67756/0.69512. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67685/0.69562. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67656/0.69595. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67555/0.69637. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67479/0.69682. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67362/0.69741. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67334/0.69799. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67201/0.69851. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67121/0.69910. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67050/0.69970. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66945/0.70040. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66873/0.70126. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66727/0.70214. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66639/0.70312. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66544/0.70426. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66412/0.70525. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66322/0.70640. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66235/0.70765. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66106/0.70870. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65971/0.71000. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65880/0.71141. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65781/0.71276. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65611/0.71433. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65509/0.71561. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65376/0.71729. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65224/0.71841. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65101/0.71999. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64941/0.72153. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64840/0.72293. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64734/0.72460. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.64648/0.72609. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64498/0.72746. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64353/0.72887. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64274/0.73047. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64138/0.73223. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64023/0.73346. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63834/0.73490. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63744/0.73630. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63640/0.73745. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63508/0.73925. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63343/0.74064. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63297/0.74219. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63194/0.74407. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63047/0.74496. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62932/0.74647. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62837/0.74756. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62719/0.74929. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62633/0.75056. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62546/0.75195. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62345/0.75386. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62288/0.75495. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62149/0.75595. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62053/0.75741. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61934/0.75897. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61849/0.76045. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70253/0.69637. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69846/0.69093. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69584/0.68770. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69461/0.68605. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69362/0.68550. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69305/0.68545. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69246/0.68564. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69175/0.68601. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69127/0.68647. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.68699. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69039/0.68756. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69020/0.68810. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68962/0.68866. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.68922. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68906/0.68977. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68915/0.69033. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68835/0.69092. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68798/0.69153. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68782/0.69206. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68763/0.69264. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69324. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68708/0.69379. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68713/0.69432. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.69492. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68642/0.69545. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68624/0.69592. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68608/0.69633. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68585/0.69681. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68567/0.69731. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68540/0.69773. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68538/0.69807. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68532/0.69833. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68482/0.69868. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68469/0.69906. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68494/0.69934. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68466/0.69962. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68447/0.69984. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68448/0.70002. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68417/0.70013. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68410/0.70039. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68380/0.70071. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68404/0.70085. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68387/0.70093. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68342/0.70108. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68314/0.70132. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68284/0.70147. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68298/0.70164. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68267/0.70185. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68256/0.70189. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68215/0.70209. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68211/0.70223. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68243/0.70227. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68194/0.70233. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68168/0.70253. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68133/0.70270. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68141/0.70273. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68141/0.70293. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68084/0.70309. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68100/0.70315. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68043/0.70322. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68068/0.70337. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67988/0.70349. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67939/0.70361. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67953/0.70370. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67912/0.70373. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67938/0.70384. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67959/0.70380. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67888/0.70386. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67831/0.70412. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67829/0.70417. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67865/0.70430. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67793/0.70424. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67769/0.70438. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67743/0.70436. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67686/0.70445. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67677/0.70466. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67678/0.70459. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67613/0.70447. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67577/0.70467. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67607/0.70488. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67511/0.70493. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67486/0.70507. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67438/0.70530. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67443/0.70527. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67411/0.70524. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67374/0.70526. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67370/0.70541. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67254/0.70552. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67272/0.70563. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67202/0.70560. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67105/0.70575. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67107/0.70591. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67135/0.70593. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67118/0.70589. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67080/0.70613. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66995/0.70629. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66950/0.70644. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66912/0.70679. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66910/0.70693. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66839/0.70703. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69797/0.69754. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69727/0.69654. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69634/0.69556. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69539/0.69435. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69369/0.69276. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.69110. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.68971. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69008/0.68873. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68971/0.68813. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68941/0.68775. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68915/0.68751. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68902/0.68736. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68873/0.68722. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68879/0.68710. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68846/0.68704. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68826/0.68697. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68795/0.68689. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68792/0.68681. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68792/0.68676. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68780/0.68665. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68740/0.68656. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68691/0.68647. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68692/0.68639. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68707/0.68624. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68684/0.68609. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68653/0.68592. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68646/0.68578. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68583/0.68560. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68593/0.68544. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68588/0.68523. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68551/0.68503. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68525/0.68480. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68496/0.68453. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68461/0.68424. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68473/0.68397. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68446/0.68365. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68376/0.68332. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68374/0.68302. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68351/0.68270. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68287/0.68237. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68294/0.68204. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68262/0.68164. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68201/0.68127. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68198/0.68087. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68127/0.68047. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68108/0.68011. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68080/0.67968. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68033/0.67926. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67940/0.67893. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67945/0.67864. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67885/0.67825. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67825/0.67793. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67775/0.67758. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67707/0.67727. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67640/0.67694. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67626/0.67665. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67561/0.67635. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67468/0.67613. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67452/0.67589. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67356/0.67563. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67320/0.67528. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67286/0.67517. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67166/0.67500. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67124/0.67478. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67072/0.67460. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66994/0.67447. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66920/0.67450. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66857/0.67429. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66792/0.67406. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66692/0.67402. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66634/0.67369. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66597/0.67354. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66525/0.67328. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66430/0.67326. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66341/0.67323. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66278/0.67320. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66240/0.67311. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66123/0.67322. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66130/0.67344. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66018/0.67344. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65986/0.67347. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65874/0.67373. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65852/0.67397. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65752/0.67409. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65685/0.67426. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65606/0.67412. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65583/0.67440. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65521/0.67458. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65407/0.67500. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65337/0.67503. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65286/0.67550. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65201/0.67595. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65118/0.67625. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65025/0.67683. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65026/0.67750. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64963/0.67812. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64836/0.67855. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64810/0.67917. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64738/0.68002. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64686/0.68079. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69335/0.69647. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69271/0.69601. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69260/0.69563. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69194/0.69532. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69152/0.69504. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.69479. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69056/0.69454. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69001/0.69434. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68975/0.69424. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68921/0.69425. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68891/0.69433. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68856/0.69451. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68851/0.69478. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.69509. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68779/0.69548. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68747/0.69593. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68722/0.69644. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.69700. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68672/0.69758. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68638/0.69815. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68587/0.69889. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68548/0.69964. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68534/0.70047. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68468/0.70137. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68428/0.70234. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68395/0.70337. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68334/0.70449. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68270/0.70564. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68228/0.70681. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68133/0.70799. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68116/0.70921. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68035/0.71035. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67987/0.71150. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67903/0.71260. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67922/0.71362. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67823/0.71469. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67746/0.71576. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67671/0.71673. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67620/0.71751. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67557/0.71828. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67526/0.71898. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67465/0.71956. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67355/0.72021. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67337/0.72077. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67243/0.72147. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67182/0.72188. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67107/0.72231. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67044/0.72290. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66981/0.72341. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66928/0.72394. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66860/0.72448. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66766/0.72499. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66707/0.72569. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66639/0.72626. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66573/0.72672. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66505/0.72723. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66385/0.72798. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66349/0.72872. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66238/0.72958. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66151/0.73056. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66095/0.73159. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66038/0.73256. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65847/0.73388. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65780/0.73516. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65771/0.73600. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65681/0.73729. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65524/0.73831. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65501/0.73968. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65380/0.74109. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65318/0.74225. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65220/0.74381. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65112/0.74497. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65048/0.74676. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64960/0.74812. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64969/0.75002. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64797/0.75127. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64679/0.75305. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64549/0.75426. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64521/0.75590. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64356/0.75722. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64313/0.75891. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64215/0.76060. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64093/0.76254. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64053/0.76397. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63921/0.76550. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63798/0.76731. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63629/0.76917. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63712/0.77071. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63578/0.77249. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63438/0.77406. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63358/0.77551. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63270/0.77675. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63118/0.77829. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63112/0.77945. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62942/0.78176. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62829/0.78322. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62799/0.78507. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62731/0.78660. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62645/0.78826. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62492/0.78989. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69034/0.69886. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68997/0.69846. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.68941/0.69806. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68955/0.69767. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.69731. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68885/0.69696. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68840/0.69662. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68865/0.69626. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68792/0.69593. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68759/0.69563. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68706/0.69530. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68687/0.69502. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68651/0.69477. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68583/0.69454. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68561/0.69437. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68511/0.69424. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68480/0.69418. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68443/0.69417. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68381/0.69419. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68290/0.69425. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68234/0.69438. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68219/0.69457. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68108/0.69484. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68106/0.69509. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68027/0.69537. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67945/0.69564. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67847/0.69586. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67811/0.69618. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67740/0.69647. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67674/0.69668. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67611/0.69688. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67499/0.69713. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67423/0.69727. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67369/0.69745. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67254/0.69767. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67202/0.69783. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67149/0.69793. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67082/0.69802. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67010/0.69814. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66931/0.69829. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66921/0.69838. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66798/0.69853. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66701/0.69867. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66654/0.69894. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66623/0.69910. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66520/0.69936. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66462/0.69954. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66351/0.69980. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66289/0.70004. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66273/0.70044. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66107/0.70072. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66054/0.70104. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65970/0.70149. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65963/0.70183. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.65778/0.70232. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.65754/0.70272. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.65652/0.70307. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65614/0.70361. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65494/0.70404. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65377/0.70458. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65240/0.70530. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65233/0.70570. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65088/0.70644. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65005/0.70687. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64915/0.70768. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64828/0.70831. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64779/0.70885. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64701/0.70971. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64553/0.71057. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64561/0.71103. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64404/0.71172. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64274/0.71254. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64196/0.71353. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64113/0.71408. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64032/0.71489. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63963/0.71534. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63841/0.71607. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63794/0.71699. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63694/0.71767. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63557/0.71852. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63441/0.71922. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63380/0.72010. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63292/0.72069. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63265/0.72138. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63059/0.72231. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63047/0.72302. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62896/0.72380. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62784/0.72465. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62691/0.72511. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62572/0.72621. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62490/0.72678. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62341/0.72784. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62237/0.72841. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62132/0.72885. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62090/0.72944. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61968/0.73019. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61855/0.73119. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61683/0.73160. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61642/0.73239. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61508/0.73368. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69921/0.69603. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69669/0.69293. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69468/0.68971. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.68673. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69139/0.68455. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69057/0.68317. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68972/0.68231. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68973/0.68184. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68953/0.68154. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.68134. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68909/0.68118. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68926/0.68106. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68879/0.68096. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68861/0.68086. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68851/0.68078. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68859/0.68070. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68852/0.68062. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68828/0.68054. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68809/0.68046. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68779/0.68039. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68775/0.68030. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68770/0.68022. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68769/0.68014. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68748/0.68003. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68725/0.67994. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68717/0.67987. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68690/0.67982. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68668/0.67973. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68658/0.67964. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68646/0.67957. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68607/0.67950. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68619/0.67944. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68590/0.67935. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68560/0.67930. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68574/0.67925. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68513/0.67920. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68507/0.67920. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68482/0.67914. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68459/0.67910. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68421/0.67907. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68395/0.67904. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68376/0.67899. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68333/0.67896. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68323/0.67895. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68340/0.67898. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68271/0.67901. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68236/0.67899. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68208/0.67899. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68187/0.67897. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68174/0.67895. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68164/0.67893. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68114/0.67893. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68123/0.67890. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68062/0.67887. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68036/0.67881. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68015/0.67875. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67987/0.67872. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67912/0.67865. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67905/0.67855. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67888/0.67848. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67849/0.67836. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67798/0.67821. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67786/0.67807. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67763/0.67791. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67701/0.67768. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67650/0.67746. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67621/0.67723. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67594/0.67701. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67562/0.67680. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67564/0.67655. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67482/0.67627. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67426/0.67601. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67396/0.67570. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67395/0.67544. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67345/0.67508. Took 0.12 sec\n",
      "Epoch 75, Loss(train/val) 0.67223/0.67474. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67225/0.67439. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67162/0.67399. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67087/0.67365. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67056/0.67332. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67018/0.67302. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66973/0.67260. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66915/0.67226. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66877/0.67198. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66806/0.67167. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66702/0.67135. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66698/0.67097. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66630/0.67061. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66621/0.67018. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66526/0.66987. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66522/0.66961. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66467/0.66927. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66416/0.66897. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66346/0.66874. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66298/0.66844. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.66279/0.66818. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66181/0.66804. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66185/0.66775. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66052/0.66749. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66003/0.66739. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69389/0.68882. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69260/0.68897. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.68925. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.68964. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69010/0.69015. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68952/0.69074. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68882/0.69129. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68823/0.69168. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68796/0.69194. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68761/0.69208. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68724/0.69213. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68682/0.69208. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68667/0.69200. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68620/0.69187. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68606/0.69171. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68562/0.69145. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68524/0.69121. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68480/0.69104. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68462/0.69088. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68458/0.69075. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68389/0.69060. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68361/0.69045. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68355/0.69026. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68345/0.69013. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68273/0.69000. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68217/0.68989. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68200/0.68980. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68172/0.68971. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68141/0.68973. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68155/0.68988. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68083/0.68991. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68098/0.69000. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68059/0.69005. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68037/0.69003. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67980/0.69017. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68000/0.69022. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67927/0.69036. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67903/0.69050. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67912/0.69061. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67860/0.69085. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67865/0.69094. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67847/0.69108. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67797/0.69120. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67758/0.69146. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67779/0.69167. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67685/0.69184. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67734/0.69211. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67706/0.69238. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67669/0.69263. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67587/0.69282. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67600/0.69307. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67621/0.69330. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67567/0.69348. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67540/0.69372. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67568/0.69399. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67474/0.69414. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67430/0.69433. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67429/0.69459. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67407/0.69477. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67337/0.69501. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67300/0.69508. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67266/0.69539. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67276/0.69568. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67269/0.69587. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67236/0.69625. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67234/0.69651. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67118/0.69673. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67100/0.69705. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67040/0.69724. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66994/0.69745. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67046/0.69768. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66959/0.69793. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66932/0.69823. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66867/0.69855. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66804/0.69880. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66785/0.69907. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66708/0.69943. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66645/0.69974. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66642/0.70003. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66603/0.70032. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66546/0.70058. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66504/0.70087. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66462/0.70129. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66421/0.70147. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66363/0.70167. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66293/0.70193. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66249/0.70221. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66195/0.70240. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66110/0.70271. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65986/0.70305. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65992/0.70322. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65910/0.70344. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65913/0.70364. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65858/0.70385. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65768/0.70396. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65632/0.70419. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65532/0.70434. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65520/0.70456. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65505/0.70462. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65397/0.70477. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69834/0.69612. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69681/0.69487. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69553/0.69349. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69401/0.69205. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69080. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69136/0.68991. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69027/0.68937. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68944/0.68912. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68919/0.68900. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.68897. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68841/0.68897. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68835/0.68899. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68821/0.68902. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68805/0.68906. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68770/0.68909. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68754/0.68914. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68743/0.68918. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68726/0.68924. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68714/0.68931. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68685/0.68940. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68661/0.68948. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68648/0.68957. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68631/0.68965. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68595/0.68975. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68571/0.68986. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68543/0.69000. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68535/0.69016. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68481/0.69035. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68476/0.69055. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68429/0.69075. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68424/0.69091. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68391/0.69107. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68358/0.69127. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68338/0.69147. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68318/0.69168. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68290/0.69189. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68242/0.69210. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68213/0.69231. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68149/0.69253. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68158/0.69273. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68137/0.69294. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68102/0.69315. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68079/0.69338. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68036/0.69361. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68038/0.69382. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67971/0.69400. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67950/0.69421. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67929/0.69445. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67889/0.69469. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67847/0.69494. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67827/0.69516. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67800/0.69539. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67751/0.69561. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67732/0.69584. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67703/0.69607. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67739/0.69623. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67657/0.69647. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67599/0.69675. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67588/0.69699. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67551/0.69725. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67547/0.69757. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67499/0.69779. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67437/0.69800. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67431/0.69825. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67402/0.69855. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67366/0.69887. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67295/0.69925. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67244/0.69953. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67246/0.69982. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67212/0.70014. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67166/0.70046. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.67158/0.70078. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67060/0.70112. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67064/0.70145. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.67001/0.70186. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66977/0.70223. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66908/0.70257. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.66859/0.70300. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66854/0.70338. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66792/0.70367. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66716/0.70404. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66707/0.70457. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66678/0.70500. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66640/0.70538. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66577/0.70573. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66533/0.70618. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66535/0.70663. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66471/0.70698. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66397/0.70737. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.66376/0.70781. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66301/0.70817. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66224/0.70878. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66229/0.70930. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66128/0.70984. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66095/0.71035. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66052/0.71089. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.65977/0.71140. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65923/0.71211. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65850/0.71268. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.65836/0.71320. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69385/0.69429. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69358/0.69416. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69308/0.69390. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69259/0.69354. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69204/0.69309. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.69244. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69084/0.69173. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69041/0.69100. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68956/0.69028. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.68980. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68820/0.68953. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68827/0.68939. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68763/0.68939. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68709/0.68948. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.68967. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68642/0.68990. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68629/0.69018. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68608/0.69047. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68557/0.69083. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68491/0.69121. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68448/0.69159. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68435/0.69188. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68408/0.69216. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68366/0.69243. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68291/0.69279. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68291/0.69311. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68260/0.69350. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68230/0.69372. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68192/0.69397. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68173/0.69430. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68120/0.69462. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68100/0.69483. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.69513. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67981/0.69546. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67958/0.69562. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67859/0.69581. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67884/0.69604. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67814/0.69626. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67791/0.69640. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67773/0.69670. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67745/0.69682. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67709/0.69709. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67670/0.69740. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67590/0.69767. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67585/0.69763. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67549/0.69789. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67479/0.69813. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67438/0.69819. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67444/0.69845. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67379/0.69869. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67350/0.69896. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67328/0.69907. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67246/0.69919. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67250/0.69938. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67171/0.69971. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67128/0.69971. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67121/0.69992. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67036/0.70034. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66984/0.70040. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66898/0.70057. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66910/0.70084. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66865/0.70108. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66847/0.70076. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66797/0.70120. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66668/0.70129. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66625/0.70149. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66583/0.70118. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66577/0.70144. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66534/0.70167. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66388/0.70140. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66338/0.70148. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66379/0.70195. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66245/0.70206. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66222/0.70207. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66134/0.70141. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66094/0.70168. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66049/0.70225. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65990/0.70166. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65921/0.70189. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65845/0.70184. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65792/0.70168. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65719/0.70162. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65649/0.70142. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65546/0.70168. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65486/0.70182. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65440/0.70033. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65350/0.70097. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.65272/0.70072. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65226/0.70052. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65165/0.70006. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65004/0.70010. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64950/0.70022. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64849/0.70080. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64813/0.69979. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64710/0.69930. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64635/0.69941. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64532/0.69872. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64432/0.69987. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64317/0.69920. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64365/0.69921. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69240/0.68386. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69196/0.68384. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.68384. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69103/0.68383. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69089/0.68381. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69109/0.68379. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.68382. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68999/0.68385. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68958/0.68393. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68901/0.68396. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68867/0.68390. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68830/0.68388. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68765/0.68386. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68707/0.68370. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68628/0.68364. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68585/0.68365. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68456/0.68351. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68399/0.68355. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68335/0.68355. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68265/0.68383. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68225/0.68428. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68180/0.68473. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68111/0.68516. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68061/0.68570. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68016/0.68642. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67969/0.68705. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67945/0.68773. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67937/0.68846. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67878/0.68932. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67843/0.69020. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67796/0.69106. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67724/0.69181. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67704/0.69252. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67709/0.69345. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67669/0.69443. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67595/0.69533. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67584/0.69601. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67520/0.69690. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67500/0.69787. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67444/0.69864. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67430/0.69963. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67412/0.70056. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67396/0.70137. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67346/0.70230. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67313/0.70322. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67337/0.70403. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67212/0.70465. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67193/0.70553. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67120/0.70650. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67133/0.70727. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67070/0.70834. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67032/0.70910. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67012/0.70994. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66960/0.71068. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66929/0.71141. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66912/0.71218. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66822/0.71294. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66812/0.71369. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66764/0.71468. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66719/0.71540. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66698/0.71617. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66619/0.71694. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66588/0.71798. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66570/0.71865. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66535/0.71956. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66475/0.72039. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66361/0.72117. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66311/0.72200. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66282/0.72294. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66225/0.72367. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66209/0.72448. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66146/0.72541. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66178/0.72615. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66122/0.72713. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66014/0.72795. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66062/0.72861. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65962/0.72946. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65953/0.73026. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65885/0.73109. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65837/0.73191. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65707/0.73266. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65780/0.73364. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65704/0.73438. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65613/0.73487. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65601/0.73598. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65578/0.73665. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65513/0.73724. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65448/0.73804. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65356/0.73904. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65308/0.73959. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65307/0.74017. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65182/0.74108. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65169/0.74185. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65119/0.74324. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65085/0.74363. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65031/0.74469. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64967/0.74497. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64934/0.74610. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64925/0.74664. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64797/0.74749. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69234/0.70060. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69992. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69155/0.69936. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69112/0.69887. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69067/0.69840. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69048/0.69798. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.69757. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68960/0.69717. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68907/0.69680. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68870/0.69642. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68848/0.69603. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68775/0.69566. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68784/0.69530. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68748/0.69492. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68716/0.69451. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68682/0.69406. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68621/0.69369. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68593/0.69330. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68561/0.69287. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68463/0.69243. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68464/0.69203. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68422/0.69159. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68386/0.69116. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68349/0.69077. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68286/0.69037. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68283/0.68998. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68191/0.68960. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68186/0.68932. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68145/0.68904. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68119/0.68881. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68081/0.68856. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68045/0.68838. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68034/0.68823. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67966/0.68803. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67964/0.68792. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67891/0.68778. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67854/0.68763. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67838/0.68739. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67829/0.68741. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67732/0.68731. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67714/0.68720. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67701/0.68715. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67639/0.68702. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67579/0.68689. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67600/0.68688. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67549/0.68686. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67511/0.68679. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67471/0.68672. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67451/0.68673. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67343/0.68662. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67336/0.68661. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67260/0.68645. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67243/0.68631. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67206/0.68633. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67205/0.68615. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67096/0.68617. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67039/0.68613. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67024/0.68608. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66942/0.68590. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66892/0.68585. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66834/0.68573. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66815/0.68564. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66728/0.68558. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66732/0.68548. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66629/0.68550. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66546/0.68533. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66543/0.68526. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66464/0.68541. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66472/0.68522. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66371/0.68525. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66307/0.68517. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66234/0.68522. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66171/0.68513. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66181/0.68524. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66064/0.68516. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66014/0.68509. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65991/0.68537. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65928/0.68520. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65824/0.68534. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65781/0.68549. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65671/0.68545. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65685/0.68534. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65682/0.68528. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65507/0.68548. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65507/0.68569. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65414/0.68557. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65362/0.68566. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65332/0.68575. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65337/0.68587. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65219/0.68593. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.65192/0.68591. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65183/0.68626. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65137/0.68618. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64931/0.68643. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64915/0.68656. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64830/0.68679. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64892/0.68682. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64770/0.68700. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64687/0.68701. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64676/0.68674. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69580/0.69514. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69522/0.69372. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69435/0.69207. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69324/0.68999. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69216/0.68757. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69120/0.68504. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69014/0.68268. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68939/0.68075. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68882/0.67923. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68853/0.67806. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68798/0.67713. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68766/0.67631. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68757/0.67558. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68714/0.67494. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68669/0.67428. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68621/0.67366. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68603/0.67300. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68574/0.67238. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68538/0.67170. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68461/0.67099. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68470/0.67030. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68382/0.66962. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68358/0.66888. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68314/0.66813. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68233/0.66743. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68200/0.66667. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68129/0.66597. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68060/0.66529. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68040/0.66466. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67949/0.66411. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67934/0.66353. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67843/0.66297. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67807/0.66253. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67773/0.66216. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67687/0.66174. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67644/0.66148. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67600/0.66119. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67533/0.66086. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67500/0.66058. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67427/0.66035. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67341/0.66026. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67299/0.66004. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67277/0.65987. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67213/0.65972. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67190/0.65956. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67102/0.65944. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67030/0.65940. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66963/0.65939. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66934/0.65933. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66852/0.65926. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66784/0.65927. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66721/0.65915. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66692/0.65911. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66603/0.65909. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66602/0.65910. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66481/0.65920. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66425/0.65923. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66335/0.65932. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66273/0.65921. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66191/0.65940. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66154/0.65933. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66105/0.65960. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66003/0.65951. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65901/0.65957. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65812/0.65999. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65790/0.66014. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65682/0.66033. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65620/0.66029. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65551/0.66053. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65467/0.66070. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65409/0.66093. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65294/0.66126. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65248/0.66158. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65200/0.66206. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65057/0.66248. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64995/0.66280. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64898/0.66315. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64892/0.66345. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64749/0.66398. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64682/0.66463. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64607/0.66506. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64532/0.66568. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64423/0.66599. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64400/0.66621. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64240/0.66664. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64220/0.66727. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64137/0.66787. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64057/0.66827. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63957/0.66857. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63881/0.66906. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63866/0.66993. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63684/0.67017. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63671/0.67078. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63559/0.67118. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63504/0.67167. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63475/0.67258. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63327/0.67317. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63256/0.67371. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63149/0.67447. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63136/0.67491. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69720/0.69487. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69554/0.69478. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69438/0.69489. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.69526. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69081/0.69595. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68965/0.69686. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68888/0.69786. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68804/0.69877. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.69951. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68752/0.70002. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68684/0.70045. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68666/0.70071. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68655/0.70092. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68617/0.70101. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68621/0.70103. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68584/0.70103. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68599/0.70108. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68551/0.70107. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68566/0.70100. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68506/0.70099. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68443/0.70096. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68471/0.70086. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68450/0.70087. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68410/0.70085. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68431/0.70076. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68363/0.70065. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68370/0.70059. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68294/0.70053. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68266/0.70045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68237/0.70039. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68208/0.70022. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68189/0.70017. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.70004. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68118/0.69991. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68069/0.69975. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68066/0.69968. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67966/0.69942. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68000/0.69923. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67919/0.69920. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67899/0.69902. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67842/0.69896. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67812/0.69894. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67758/0.69851. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67721/0.69824. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67706/0.69807. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67676/0.69800. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67561/0.69783. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67586/0.69762. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67469/0.69732. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67438/0.69744. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67416/0.69737. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67383/0.69726. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67349/0.69708. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67332/0.69706. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67279/0.69704. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67179/0.69686. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67177/0.69695. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67114/0.69682. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67078/0.69669. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67066/0.69682. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67050/0.69676. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66985/0.69639. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66847/0.69664. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66806/0.69655. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66771/0.69635. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66720/0.69646. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66727/0.69680. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66615/0.69648. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66528/0.69675. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66542/0.69698. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66479/0.69700. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66402/0.69692. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66371/0.69695. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66345/0.69722. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66241/0.69725. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66213/0.69755. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66140/0.69815. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66085/0.69777. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66030/0.69828. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65970/0.69866. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65850/0.69891. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65851/0.69916. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65738/0.69932. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65699/0.69970. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65661/0.69978. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65660/0.70016. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65483/0.70043. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65477/0.70054. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65377/0.70121. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65329/0.70182. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65242/0.70232. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65155/0.70256. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65094/0.70327. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65088/0.70321. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64976/0.70353. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64904/0.70417. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64809/0.70511. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64687/0.70512. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64624/0.70549. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64602/0.70616. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69509/0.69981. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69268/0.70277. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69071/0.70607. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68942/0.70929. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68871/0.71191. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68812/0.71379. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68789/0.71498. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68779/0.71563. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68740/0.71606. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68705/0.71630. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68660/0.71650. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68655/0.71660. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68625/0.71680. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68602/0.71704. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68594/0.71726. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68552/0.71748. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68526/0.71765. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68500/0.71786. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68504/0.71816. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68483/0.71846. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68428/0.71865. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68435/0.71887. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68395/0.71919. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68366/0.71959. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68367/0.71990. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68350/0.72021. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68330/0.72053. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68301/0.72089. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68305/0.72119. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68288/0.72155. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68264/0.72191. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68240/0.72238. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68225/0.72264. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68208/0.72292. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68202/0.72317. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68185/0.72345. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68165/0.72374. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68102/0.72403. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68090/0.72435. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68081/0.72474. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68084/0.72499. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68021/0.72511. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68048/0.72540. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67963/0.72559. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67966/0.72586. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67965/0.72629. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67932/0.72635. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67902/0.72662. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67911/0.72690. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67863/0.72699. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67845/0.72723. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67788/0.72739. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67767/0.72747. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67744/0.72787. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67694/0.72789. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67650/0.72801. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67614/0.72808. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67595/0.72839. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67577/0.72833. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67548/0.72850. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67503/0.72888. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67421/0.72864. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67449/0.72888. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67366/0.72885. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67311/0.72904. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67244/0.72888. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67186/0.72911. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67160/0.72902. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67093/0.72926. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67061/0.72891. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66983/0.72899. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66940/0.72896. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66912/0.72934. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66804/0.72914. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66741/0.72909. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66709/0.72897. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66651/0.72911. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66560/0.72923. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66507/0.72908. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66390/0.72911. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66377/0.72936. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66294/0.72908. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66214/0.72925. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66100/0.72956. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66062/0.72907. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65989/0.72937. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65863/0.73014. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65819/0.73037. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65804/0.72997. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65666/0.73070. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65629/0.73102. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65550/0.73123. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65403/0.73163. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65361/0.73191. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65330/0.73259. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65188/0.73363. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65110/0.73331. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65070/0.73373. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64934/0.73453. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64950/0.73527. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69229/0.69832. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69212/0.69829. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69202/0.69823. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69182/0.69817. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69170/0.69807. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69141/0.69794. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69129/0.69780. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69096/0.69765. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.69749. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69056/0.69732. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69040/0.69717. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69004/0.69703. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69002/0.69692. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68953/0.69684. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68931/0.69686. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68886/0.69691. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68851/0.69701. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68822/0.69724. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68799/0.69751. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68726/0.69785. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68684/0.69829. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68655/0.69872. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68576/0.69925. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68537/0.69988. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68486/0.70052. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68440/0.70101. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68410/0.70165. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68379/0.70230. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68318/0.70294. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68298/0.70362. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68231/0.70423. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68213/0.70476. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68185/0.70533. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68120/0.70591. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68113/0.70650. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68063/0.70703. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68026/0.70755. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67988/0.70803. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68014/0.70856. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67953/0.70915. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67936/0.70965. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67888/0.71016. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67868/0.71067. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67823/0.71119. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67776/0.71174. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67750/0.71225. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67722/0.71284. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67702/0.71331. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67669/0.71375. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67678/0.71425. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67580/0.71472. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67575/0.71523. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67524/0.71575. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67496/0.71628. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67445/0.71671. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67437/0.71724. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67409/0.71774. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67374/0.71813. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67335/0.71863. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67283/0.71917. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67275/0.71968. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67222/0.72006. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67175/0.72061. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67136/0.72110. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67103/0.72151. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67103/0.72200. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67068/0.72240. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67028/0.72284. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66939/0.72334. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66907/0.72375. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66904/0.72423. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66904/0.72465. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66831/0.72518. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66750/0.72560. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66730/0.72610. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66678/0.72655. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66591/0.72696. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66604/0.72753. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66556/0.72806. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66538/0.72853. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66433/0.72889. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66381/0.72939. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66379/0.72986. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66335/0.73038. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66250/0.73089. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66157/0.73131. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66164/0.73189. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66093/0.73232. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66070/0.73278. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66005/0.73322. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65894/0.73383. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65861/0.73445. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65773/0.73487. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65691/0.73538. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65630/0.73600. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65601/0.73641. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65517/0.73690. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65456/0.73760. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65430/0.73823. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65327/0.73882. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69418/0.69571. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69411/0.69560. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69411/0.69549. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69382/0.69539. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69374/0.69529. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69342/0.69519. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69319/0.69509. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69299/0.69501. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69300/0.69492. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69276/0.69484. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69253/0.69474. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69208/0.69465. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69188/0.69456. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69183/0.69445. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69150/0.69434. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69113/0.69422. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69070/0.69414. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69028/0.69403. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68979/0.69392. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68947/0.69380. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68880/0.69369. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68811/0.69358. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68759/0.69347. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68638/0.69339. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68593/0.69335. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68482/0.69328. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68386/0.69324. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68261/0.69321. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68139/0.69327. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68019/0.69327. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67925/0.69330. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67852/0.69341. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67725/0.69332. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67637/0.69333. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67552/0.69347. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67428/0.69345. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67312/0.69364. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67210/0.69359. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67124/0.69361. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66988/0.69377. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66890/0.69398. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66778/0.69385. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66675/0.69400. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66612/0.69404. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66492/0.69421. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66451/0.69380. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66330/0.69411. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66229/0.69418. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66136/0.69367. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66024/0.69401. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65946/0.69400. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65859/0.69374. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65816/0.69391. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65690/0.69362. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65594/0.69388. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65495/0.69381. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65421/0.69341. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65371/0.69344. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65268/0.69350. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65232/0.69304. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65123/0.69295. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64987/0.69277. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64897/0.69283. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64892/0.69255. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64833/0.69245. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64631/0.69218. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64644/0.69214. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64569/0.69159. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.64568/0.69193. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64421/0.69151. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64373/0.69129. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64315/0.69115. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64129/0.69115. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64094/0.69071. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64001/0.69106. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63976/0.69061. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63850/0.69095. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.63750/0.69041. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63727/0.69088. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63614/0.69087. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63557/0.69020. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63492/0.69056. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.63428/0.69079. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63337/0.69021. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63251/0.69002. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63211/0.68976. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63107/0.68985. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62945/0.69014. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62913/0.68980. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62872/0.69043. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62760/0.68940. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62643/0.68979. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.62533/0.68996. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62477/0.68972. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62451/0.68979. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.62384/0.68913. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62269/0.68946. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62166/0.68963. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62125/0.68902. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61948/0.68913. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69305/0.69557. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69247/0.69547. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69254/0.69535. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69201/0.69521. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69179/0.69502. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69156/0.69477. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69139/0.69450. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69096/0.69423. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69042/0.69395. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69018/0.69377. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68981/0.69364. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68975/0.69366. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68912/0.69371. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68868/0.69392. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68817/0.69426. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68799/0.69462. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68745/0.69506. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68717/0.69563. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68664/0.69628. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68629/0.69699. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68585/0.69782. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68499/0.69872. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68472/0.69960. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68390/0.70073. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68342/0.70195. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68286/0.70305. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68202/0.70426. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68156/0.70546. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68092/0.70684. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68056/0.70810. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67977/0.70946. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67903/0.71084. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67827/0.71209. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67783/0.71352. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67733/0.71466. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67664/0.71605. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67585/0.71749. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67521/0.71866. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67431/0.71998. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67372/0.72118. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67304/0.72244. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67269/0.72333. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67184/0.72472. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67109/0.72587. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67046/0.72661. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66953/0.72776. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66922/0.72871. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66825/0.72955. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66806/0.73025. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66746/0.73168. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66652/0.73246. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66575/0.73342. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66524/0.73428. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66445/0.73491. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66422/0.73575. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66372/0.73672. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66268/0.73748. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66135/0.73848. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66071/0.73931. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65999/0.74015. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65900/0.74065. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65859/0.74201. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65760/0.74303. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65681/0.74378. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65688/0.74496. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65538/0.74567. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65431/0.74708. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65353/0.74753. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65293/0.74908. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65157/0.74996. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65055/0.75151. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64978/0.75263. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64918/0.75353. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64827/0.75531. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64680/0.75639. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64619/0.75752. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64579/0.75877. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64398/0.76017. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64354/0.76102. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64247/0.76274. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64167/0.76420. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64032/0.76511. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63941/0.76768. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63848/0.76807. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63729/0.76971. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63686/0.77139. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63592/0.77201. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63414/0.77419. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63394/0.77523. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63251/0.77578. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63310/0.77791. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63109/0.77866. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63096/0.78021. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62963/0.78115. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62922/0.78263. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62794/0.78401. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62663/0.78528. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62654/0.78642. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62560/0.78788. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62407/0.78951. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69251/0.69399. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69409. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.69417. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.69425. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.69432. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69178/0.69438. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69130/0.69446. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69132/0.69455. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69121/0.69464. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69116/0.69471. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.69480. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69041/0.69495. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69019/0.69509. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69011/0.69525. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68983/0.69545. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68981/0.69561. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68918/0.69570. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68925/0.69585. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68910/0.69595. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68838/0.69605. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68817/0.69605. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68793/0.69599. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68740/0.69594. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68723/0.69582. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68657/0.69567. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68642/0.69545. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68548/0.69520. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68518/0.69491. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68431/0.69435. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68352/0.69403. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68247/0.69379. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68165/0.69362. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68061/0.69351. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67958/0.69335. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67868/0.69317. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67805/0.69319. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67680/0.69330. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67585/0.69324. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67526/0.69349. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67427/0.69344. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67334/0.69326. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67310/0.69315. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67199/0.69304. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67153/0.69287. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67059/0.69273. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67024/0.69268. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66902/0.69253. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66889/0.69186. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66821/0.69191. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66779/0.69203. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66673/0.69184. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66599/0.69162. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66554/0.69116. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66483/0.69106. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66423/0.69101. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66360/0.69092. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66297/0.69091. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66251/0.69067. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66110/0.69041. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66098/0.69054. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66014/0.68999. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65909/0.69037. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65818/0.69036. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65829/0.69023. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65839/0.69023. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65680/0.69006. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65606/0.68984. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65557/0.68997. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65457/0.69021. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65479/0.68993. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65287/0.69037. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65265/0.69036. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65243/0.69046. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65063/0.69058. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65075/0.69045. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64938/0.69081. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64939/0.69084. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64813/0.69074. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64723/0.69063. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64648/0.69109. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64552/0.69071. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64548/0.69116. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64375/0.69133. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64376/0.69142. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64220/0.69132. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64133/0.69200. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64091/0.69202. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63918/0.69257. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63898/0.69284. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63860/0.69344. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63748/0.69393. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63702/0.69428. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63506/0.69490. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63523/0.69529. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63422/0.69585. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63264/0.69622. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63247/0.69670. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63077/0.69740. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62997/0.69837. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62930/0.69914. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69350/0.69615. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.69613. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69283/0.69615. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69244/0.69618. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69251/0.69623. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69208/0.69630. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.69641. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69129/0.69658. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69115/0.69678. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69067/0.69707. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69011/0.69744. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68973/0.69787. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68906/0.69839. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68862/0.69905. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68813/0.69985. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68734/0.70075. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68637/0.70180. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68561/0.70303. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68480/0.70441. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68392/0.70585. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68369/0.70727. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68295/0.70865. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68205/0.71002. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68131/0.71133. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68108/0.71250. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68032/0.71353. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68010/0.71446. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68007/0.71522. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67949/0.71584. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67929/0.71634. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67891/0.71680. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67865/0.71721. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67829/0.71759. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67817/0.71792. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67756/0.71821. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67731/0.71855. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67734/0.71872. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67688/0.71903. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67667/0.71927. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67614/0.71946. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67577/0.71967. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67569/0.71981. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67519/0.71992. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67507/0.72009. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67508/0.72019. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67481/0.72031. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67419/0.72038. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67431/0.72046. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67357/0.72062. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67312/0.72078. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67291/0.72092. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67302/0.72110. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67237/0.72127. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67224/0.72149. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67177/0.72161. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67153/0.72186. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67111/0.72205. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67081/0.72231. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67054/0.72250. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67016/0.72274. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66962/0.72296. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66946/0.72319. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66910/0.72349. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66878/0.72368. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66827/0.72385. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66853/0.72400. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66792/0.72418. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66738/0.72452. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66683/0.72484. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66632/0.72518. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66599/0.72560. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66607/0.72579. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66517/0.72608. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66522/0.72648. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66474/0.72666. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66423/0.72689. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66379/0.72725. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66333/0.72759. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66296/0.72778. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66239/0.72811. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66222/0.72844. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66198/0.72864. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66102/0.72893. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66145/0.72947. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66033/0.72977. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65959/0.73023. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65957/0.73047. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65904/0.73083. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65868/0.73133. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65796/0.73189. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65761/0.73217. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65670/0.73259. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65648/0.73304. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65621/0.73332. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65555/0.73366. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65514/0.73412. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65465/0.73471. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65409/0.73508. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65320/0.73551. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65253/0.73601. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69373/0.68614. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.68667. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69267/0.68717. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69247/0.68759. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69199/0.68793. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.68828. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.68856. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69088/0.68883. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69013/0.68905. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68997/0.68932. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68954/0.68952. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68894/0.68975. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68859/0.69004. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68832/0.69027. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68783/0.69045. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68721/0.69070. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68674/0.69099. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68606/0.69122. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68564/0.69150. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68485/0.69181. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68431/0.69213. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68398/0.69247. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68344/0.69279. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68307/0.69315. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68259/0.69345. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68215/0.69374. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68187/0.69406. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68121/0.69433. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68120/0.69454. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68068/0.69474. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68021/0.69486. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67971/0.69506. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67940/0.69531. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67910/0.69539. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67906/0.69554. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67852/0.69567. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67812/0.69574. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67804/0.69574. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67764/0.69584. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67706/0.69582. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67682/0.69586. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67668/0.69591. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67611/0.69606. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67615/0.69598. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67598/0.69589. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67513/0.69592. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67501/0.69602. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67471/0.69602. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67446/0.69600. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67410/0.69608. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67365/0.69591. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67363/0.69594. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67304/0.69598. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67252/0.69607. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67194/0.69608. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67175/0.69614. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67122/0.69613. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67092/0.69608. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67070/0.69626. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67042/0.69622. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67015/0.69613. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66976/0.69618. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66952/0.69621. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66890/0.69619. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66856/0.69630. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66820/0.69644. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66738/0.69654. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66753/0.69652. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66701/0.69661. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66629/0.69663. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66616/0.69681. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66570/0.69690. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66511/0.69696. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66473/0.69706. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66456/0.69706. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66383/0.69721. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66309/0.69741. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66294/0.69752. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66231/0.69765. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66219/0.69780. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66173/0.69808. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66161/0.69836. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66073/0.69842. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66047/0.69854. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66037/0.69883. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65922/0.69906. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65948/0.69913. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65888/0.69943. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65856/0.69972. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65839/0.69998. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65751/0.70034. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65716/0.70057. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65661/0.70073. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65626/0.70118. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65580/0.70126. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65556/0.70178. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65485/0.70194. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65491/0.70213. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65401/0.70246. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65432/0.70282. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69874/0.70229. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69752/0.70142. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69631/0.70001. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69451/0.69778. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69232/0.69507. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69046/0.69273. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68913/0.69114. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68818/0.69032. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68749/0.68998. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68686/0.68990. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68653/0.68998. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68569/0.69015. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68509/0.69042. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68458/0.69072. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68415/0.69110. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68367/0.69148. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68296/0.69189. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68244/0.69237. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68149/0.69298. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68113/0.69362. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68049/0.69433. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67993/0.69502. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67933/0.69579. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67831/0.69667. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67806/0.69756. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67747/0.69847. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67683/0.69945. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67622/0.70042. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67567/0.70133. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67492/0.70223. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67456/0.70322. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67432/0.70416. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67378/0.70512. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67321/0.70608. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67294/0.70695. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67241/0.70786. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67224/0.70861. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67163/0.70935. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67095/0.71008. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67088/0.71068. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67040/0.71143. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66999/0.71226. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66924/0.71299. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66889/0.71359. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66869/0.71429. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66839/0.71489. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66797/0.71545. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66739/0.71619. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66739/0.71686. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66684/0.71729. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66640/0.71793. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66588/0.71877. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66571/0.71921. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66556/0.71990. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66521/0.72043. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66422/0.72108. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66365/0.72163. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66368/0.72218. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66308/0.72303. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66332/0.72324. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66219/0.72380. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66211/0.72440. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66145/0.72513. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66133/0.72575. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66115/0.72652. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66074/0.72712. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66039/0.72755. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65938/0.72831. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65947/0.72900. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65866/0.72959. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65823/0.73040. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65863/0.73124. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65683/0.73172. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65751/0.73237. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65689/0.73320. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65648/0.73384. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65608/0.73422. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65490/0.73523. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65473/0.73623. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65433/0.73606. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65396/0.73714. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65384/0.73765. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65349/0.73810. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65281/0.73882. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65221/0.73954. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65155/0.74029. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65117/0.74080. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64967/0.74206. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65008/0.74249. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64887/0.74372. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64919/0.74419. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64822/0.74501. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64778/0.74558. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.64678/0.74621. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64653/0.74697. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64589/0.74780. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64596/0.74839. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64481/0.74916. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64392/0.75012. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64360/0.75106. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69718/0.69355. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69544/0.69373. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69362/0.69420. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69237/0.69497. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69107/0.69599. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69021/0.69697. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68984/0.69785. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68935/0.69836. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68879/0.69870. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68853/0.69879. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68821/0.69874. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68792/0.69860. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68789/0.69845. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68757/0.69821. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68747/0.69806. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68718/0.69788. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68690/0.69762. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68679/0.69735. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68638/0.69708. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68642/0.69682. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68621/0.69655. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68572/0.69625. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68542/0.69595. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68504/0.69568. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68486/0.69547. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68474/0.69517. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68423/0.69482. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68396/0.69449. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68380/0.69423. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68320/0.69397. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68320/0.69370. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68251/0.69351. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68261/0.69326. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68213/0.69314. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68172/0.69293. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68151/0.69276. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68123/0.69266. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68068/0.69257. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68083/0.69256. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68070/0.69260. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68048/0.69253. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67961/0.69271. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67934/0.69276. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67891/0.69291. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67904/0.69298. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67883/0.69312. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67822/0.69327. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67834/0.69344. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67785/0.69357. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67794/0.69382. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67724/0.69403. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67715/0.69419. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67725/0.69443. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67652/0.69463. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67639/0.69495. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67648/0.69520. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67593/0.69552. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67578/0.69580. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67518/0.69592. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67501/0.69614. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67476/0.69647. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67459/0.69664. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67428/0.69692. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67412/0.69713. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67394/0.69752. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67372/0.69773. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67300/0.69796. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67346/0.69824. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67279/0.69839. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67268/0.69860. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67241/0.69889. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67214/0.69916. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67191/0.69934. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67166/0.69969. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67116/0.69996. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.67097/0.70011. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67074/0.70032. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67069/0.70053. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67042/0.70087. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67041/0.70104. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66928/0.70131. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66962/0.70145. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66939/0.70182. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66903/0.70214. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66922/0.70232. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66830/0.70269. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66881/0.70275. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66750/0.70300. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66807/0.70337. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66727/0.70354. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66722/0.70376. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66631/0.70411. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66651/0.70431. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66627/0.70472. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66597/0.70491. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66539/0.70527. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66515/0.70536. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66530/0.70554. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66357/0.70586. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66371/0.70612. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69519/0.69534. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69490/0.69515. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69446/0.69498. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69448/0.69481. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69388/0.69462. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69360/0.69443. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69313/0.69423. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69311/0.69401. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69291/0.69378. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69257/0.69353. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69199/0.69327. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69153/0.69297. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69093/0.69265. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69022/0.69231. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68942/0.69196. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68860/0.69154. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68777/0.69113. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68686/0.69071. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68594/0.69033. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68482/0.68989. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68371/0.68960. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68308/0.68935. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68147/0.68919. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68010/0.68915. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67876/0.68925. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67806/0.68937. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67650/0.68950. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67513/0.68982. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67386/0.69024. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67258/0.69084. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67122/0.69148. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66998/0.69230. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66909/0.69288. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66739/0.69404. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66626/0.69505. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66520/0.69623. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66423/0.69736. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66340/0.69791. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66226/0.69905. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66146/0.70025. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66079/0.70111. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65970/0.70235. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65889/0.70306. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65803/0.70423. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65760/0.70499. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65606/0.70577. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65539/0.70644. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65550/0.70708. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65416/0.70809. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65348/0.70869. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65242/0.70915. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65196/0.70997. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65092/0.71034. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65025/0.71085. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64965/0.71154. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64943/0.71181. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64836/0.71279. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64726/0.71314. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64719/0.71366. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64670/0.71414. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64507/0.71494. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64434/0.71518. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64395/0.71564. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64339/0.71613. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64298/0.71642. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64120/0.71686. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64137/0.71772. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64001/0.71844. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63965/0.71839. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63938/0.71892. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63740/0.71926. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63715/0.72013. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63627/0.72048. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63524/0.72108. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63538/0.72176. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63455/0.72202. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63281/0.72248. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63253/0.72331. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63098/0.72338. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63115/0.72433. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62990/0.72482. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62951/0.72507. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62780/0.72579. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62727/0.72646. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.62566/0.72727. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62555/0.72826. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62538/0.72858. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62372/0.72933. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62272/0.72998. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62148/0.73067. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62133/0.73130. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62013/0.73228. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61900/0.73294. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61883/0.73344. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61694/0.73464. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61656/0.73526. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61658/0.73583. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61581/0.73619. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61375/0.73726. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61417/0.73746. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69565/0.69623. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69498/0.69536. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69414/0.69425. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69299/0.69300. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69183/0.69179. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69055/0.69082. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69023/0.69014. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.68971. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68949/0.68944. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68925/0.68925. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.68910. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.68898. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68840/0.68888. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.68878. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68785/0.68871. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68727/0.68866. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68685/0.68859. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68670/0.68854. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68583/0.68852. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68551/0.68848. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.68847. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68451/0.68851. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68395/0.68859. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68337/0.68866. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68268/0.68876. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68196/0.68887. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68121/0.68906. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68100/0.68924. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68012/0.68941. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67994/0.68961. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67925/0.68982. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67865/0.69001. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67804/0.69019. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67780/0.69041. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67728/0.69062. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67664/0.69077. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67630/0.69093. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67577/0.69112. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67506/0.69124. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67474/0.69134. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67437/0.69145. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67378/0.69152. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67313/0.69163. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67317/0.69171. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67228/0.69178. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67164/0.69187. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67124/0.69193. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67085/0.69203. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67053/0.69209. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66992/0.69215. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66966/0.69229. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66888/0.69236. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66822/0.69247. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66806/0.69252. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66714/0.69262. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66721/0.69260. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66645/0.69271. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66598/0.69280. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66509/0.69284. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66500/0.69297. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66463/0.69318. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66370/0.69320. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66330/0.69332. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66247/0.69344. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66150/0.69361. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66087/0.69388. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66070/0.69404. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65997/0.69431. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65934/0.69450. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65862/0.69476. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65842/0.69495. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65757/0.69518. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65647/0.69548. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65678/0.69580. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65614/0.69613. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65508/0.69654. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65421/0.69695. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65387/0.69734. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65355/0.69768. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65259/0.69813. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65198/0.69867. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65149/0.69921. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65093/0.69968. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64996/0.70029. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64975/0.70087. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64906/0.70141. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64787/0.70211. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64727/0.70273. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64697/0.70334. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64582/0.70401. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64647/0.70456. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64489/0.70534. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64418/0.70607. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64382/0.70682. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64349/0.70753. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64260/0.70836. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64173/0.70913. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64107/0.70988. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64006/0.71072. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64002/0.71153. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69570/0.68872. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69453/0.68580. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69356/0.68277. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.67975. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69165/0.67701. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.67486. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.67317. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69040/0.67190. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68978/0.67106. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68975/0.67043. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68968/0.67004. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68931/0.66969. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68926/0.66952. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68891/0.66935. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68889/0.66927. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68821/0.66905. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68847/0.66896. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68813/0.66887. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68788/0.66877. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68759/0.66870. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68736/0.66871. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68675/0.66864. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68630/0.66856. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68627/0.66858. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68573/0.66859. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68537/0.66863. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68532/0.66872. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68480/0.66891. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68444/0.66910. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68415/0.66935. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68345/0.66958. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68312/0.66978. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68273/0.67008. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68204/0.67015. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68160/0.67056. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68110/0.67100. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68061/0.67149. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68000/0.67199. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67960/0.67244. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67895/0.67292. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67852/0.67349. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67790/0.67391. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67752/0.67466. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67706/0.67529. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67646/0.67591. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67587/0.67668. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67581/0.67732. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67465/0.67776. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67463/0.67834. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67386/0.67924. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67311/0.67988. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67284/0.68074. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67163/0.68155. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67125/0.68208. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67061/0.68273. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67026/0.68344. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66982/0.68430. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66864/0.68517. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66819/0.68581. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66725/0.68674. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66612/0.68768. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66558/0.68872. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66525/0.68953. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66393/0.69032. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66362/0.69109. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66268/0.69250. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66201/0.69304. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66103/0.69414. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66018/0.69544. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65982/0.69625. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65832/0.69759. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65738/0.69876. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65616/0.70019. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65534/0.70087. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65477/0.70212. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65247/0.70320. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65269/0.70462. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65073/0.70598. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64950/0.70756. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64915/0.70822. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64763/0.70995. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64669/0.71132. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64580/0.71269. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64426/0.71417. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64347/0.71543. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64229/0.71661. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64067/0.71748. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63989/0.71910. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63909/0.72066. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63719/0.72120. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63574/0.72307. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63492/0.72410. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63417/0.72562. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63215/0.72697. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63174/0.72831. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63052/0.72960. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62974/0.73051. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62899/0.73220. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62675/0.73386. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62566/0.73522. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69566/0.69394. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69536/0.69358. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69533/0.69322. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69455/0.69281. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69437/0.69236. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69362/0.69185. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69367/0.69130. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69330/0.69072. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69281/0.69014. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69256/0.68963. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69239/0.68920. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69193/0.68890. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69154/0.68868. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69150/0.68856. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69143/0.68849. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69119/0.68848. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69133/0.68850. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69076/0.68859. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69059/0.68867. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69046/0.68879. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69006/0.68898. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68980/0.68923. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68964/0.68946. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68932/0.68978. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68915/0.69013. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68891/0.69056. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68855/0.69101. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68800/0.69158. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68763/0.69219. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68709/0.69290. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68666/0.69372. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68589/0.69457. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68524/0.69564. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68491/0.69680. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68401/0.69791. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68340/0.69919. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68265/0.70047. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68199/0.70170. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68164/0.70302. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68083/0.70441. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67991/0.70578. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67915/0.70701. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67927/0.70824. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67826/0.70947. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67772/0.71057. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67705/0.71136. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67638/0.71251. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67578/0.71350. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67521/0.71450. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67447/0.71526. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67441/0.71606. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67371/0.71662. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67334/0.71724. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67277/0.71792. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67255/0.71836. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67198/0.71915. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67134/0.71991. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67080/0.72048. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67017/0.72109. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67003/0.72170. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66883/0.72217. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66852/0.72286. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66819/0.72350. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66788/0.72394. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66709/0.72466. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66681/0.72508. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66580/0.72578. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66551/0.72659. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66468/0.72721. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66460/0.72770. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66429/0.72874. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66350/0.72921. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66254/0.72968. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66183/0.73043. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66228/0.73098. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66103/0.73141. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.66115/0.73231. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66028/0.73275. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65948/0.73345. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65874/0.73433. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65876/0.73488. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65748/0.73541. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65792/0.73609. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65731/0.73671. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65667/0.73719. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65597/0.73799. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65459/0.73857. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65446/0.73955. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65330/0.74028. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65374/0.74106. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65276/0.74170. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65204/0.74243. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65172/0.74292. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65029/0.74342. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65028/0.74480. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64945/0.74506. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64887/0.74547. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64806/0.74625. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64693/0.74700. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64652/0.74762. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69435/0.69792. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 0.69413/0.69755. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69402/0.69717. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69374/0.69673. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69327/0.69622. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69290/0.69565. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69284/0.69506. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69233/0.69452. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69180/0.69409. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69178/0.69375. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69137/0.69353. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69127/0.69342. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69104/0.69335. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69083/0.69335. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69068/0.69339. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69018/0.69347. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69011/0.69359. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68984/0.69376. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68935/0.69398. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68914/0.69421. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68891/0.69452. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68844/0.69491. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68814/0.69536. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68778/0.69586. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68728/0.69642. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68668/0.69705. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68635/0.69773. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68568/0.69841. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68532/0.69908. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68448/0.69980. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68412/0.70055. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68329/0.70136. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68287/0.70208. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68246/0.70277. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68181/0.70342. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68116/0.70408. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68087/0.70465. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68007/0.70526. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67944/0.70581. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67880/0.70626. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67844/0.70668. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67781/0.70702. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67696/0.70731. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67669/0.70766. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67630/0.70780. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67558/0.70792. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67488/0.70806. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67424/0.70814. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67357/0.70817. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67333/0.70809. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67235/0.70799. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67141/0.70796. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67094/0.70771. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67020/0.70757. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66965/0.70744. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66837/0.70736. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66796/0.70716. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66715/0.70698. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66646/0.70666. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66563/0.70629. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66424/0.70588. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66368/0.70565. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66300/0.70545. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66144/0.70520. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66104/0.70489. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65990/0.70461. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65943/0.70424. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65816/0.70405. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65807/0.70382. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65601/0.70344. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65538/0.70298. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65420/0.70285. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65262/0.70301. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65260/0.70296. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65188/0.70281. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65008/0.70296. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64963/0.70277. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64898/0.70302. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64791/0.70288. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64725/0.70307. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64614/0.70344. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64463/0.70356. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64472/0.70350. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64324/0.70381. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64272/0.70424. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64164/0.70465. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64049/0.70503. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63975/0.70537. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63893/0.70596. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63860/0.70652. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63800/0.70720. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63573/0.70740. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63452/0.70778. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63486/0.70879. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63327/0.70943. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63305/0.71015. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63224/0.71048. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63169/0.71102. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63049/0.71182. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63099/0.71246. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69394/0.69321. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.69367. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69288/0.69415. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69270/0.69466. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69517. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69181/0.69571. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69142/0.69628. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69135/0.69682. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69074/0.69740. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69079/0.69795. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69046/0.69852. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69018/0.69908. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69008/0.69963. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68962/0.70020. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68901/0.70090. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68890/0.70161. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68846/0.70237. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68817/0.70315. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68761/0.70391. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68724/0.70467. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68670/0.70547. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68679/0.70628. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68629/0.70700. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68595/0.70779. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68549/0.70854. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68507/0.70922. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68458/0.70990. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68474/0.71051. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68409/0.71112. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68370/0.71172. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68334/0.71220. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68330/0.71266. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68246/0.71311. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68230/0.71362. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68223/0.71400. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68129/0.71441. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68082/0.71489. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68096/0.71523. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68031/0.71564. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68023/0.71592. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67947/0.71631. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67910/0.71663. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67854/0.71696. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67840/0.71724. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67793/0.71750. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67704/0.71782. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67694/0.71811. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67648/0.71845. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67625/0.71872. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67528/0.71889. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67505/0.71908. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67501/0.71934. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67425/0.71957. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67365/0.71980. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67322/0.72000. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67235/0.72026. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67235/0.72045. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67123/0.72071. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67055/0.72090. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67083/0.72115. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67013/0.72131. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66946/0.72164. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66860/0.72173. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66805/0.72198. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66757/0.72217. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66702/0.72244. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66583/0.72249. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66605/0.72276. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66495/0.72269. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66473/0.72324. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66399/0.72337. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66269/0.72331. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66227/0.72357. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66163/0.72403. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66063/0.72391. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66023/0.72405. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65978/0.72455. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65903/0.72477. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65778/0.72503. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65694/0.72538. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65697/0.72546. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65582/0.72583. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65515/0.72651. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65470/0.72675. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65352/0.72705. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65287/0.72733. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65178/0.72806. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65091/0.72801. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65097/0.72872. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64978/0.72898. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64937/0.72957. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64835/0.72941. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64712/0.73030. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64612/0.73066. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64501/0.73170. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64438/0.73178. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64451/0.73275. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64301/0.73265. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64250/0.73396. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64183/0.73418. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69362/0.69200. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69361/0.69200. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69305/0.69199. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69285/0.69198. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69265/0.69197. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69262/0.69198. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69249/0.69198. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69204/0.69199. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69182/0.69198. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69161/0.69198. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69158/0.69198. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69126/0.69198. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69098/0.69199. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69055/0.69200. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69070/0.69201. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69024/0.69203. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69008/0.69204. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68957/0.69210. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68916/0.69216. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68904/0.69224. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68866/0.69232. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68827/0.69244. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68790/0.69260. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68722/0.69279. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68692/0.69306. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68630/0.69340. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68604/0.69379. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68552/0.69427. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68541/0.69483. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68465/0.69549. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68422/0.69622. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68346/0.69702. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68310/0.69786. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68256/0.69877. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68231/0.69980. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68144/0.70094. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68097/0.70211. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68000/0.70333. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68011/0.70463. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67927/0.70599. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67829/0.70735. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67785/0.70891. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67724/0.71041. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67747/0.71190. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67628/0.71359. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67565/0.71534. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67490/0.71705. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67482/0.71866. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67301/0.72046. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67349/0.72215. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67242/0.72407. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67168/0.72589. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67082/0.72796. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67002/0.72985. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66922/0.73194. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66870/0.73392. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66754/0.73588. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66748/0.73794. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66662/0.73999. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66614/0.74211. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66505/0.74405. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66479/0.74637. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66407/0.74814. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66265/0.75086. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66175/0.75270. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66152/0.75471. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66106/0.75695. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65947/0.75912. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65874/0.76127. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65786/0.76384. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65703/0.76562. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65625/0.76753. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65595/0.77025. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65492/0.77205. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65388/0.77400. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65325/0.77587. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65363/0.77820. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65214/0.77986. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65108/0.78120. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65024/0.78334. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64976/0.78477. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64897/0.78675. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64807/0.78808. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64782/0.79003. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64651/0.79207. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64584/0.79370. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64438/0.79532. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64438/0.79752. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64315/0.79944. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64287/0.80058. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64094/0.80247. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64130/0.80479. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63984/0.80606. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63974/0.80768. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63730/0.80836. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63802/0.81032. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63662/0.81218. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63622/0.81444. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63473/0.81610. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63402/0.81761. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69652/0.69641. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69572/0.69522. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69521/0.69424. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69476/0.69343. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69439/0.69271. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69394/0.69213. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69378/0.69165. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69389/0.69128. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69346/0.69094. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69350/0.69064. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69336/0.69042. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69298/0.69020. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69305/0.69006. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69286/0.68992. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69293/0.68977. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69282/0.68962. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69296/0.68951. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69248/0.68937. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69214/0.68921. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69256/0.68908. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69218/0.68896. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69191/0.68879. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69203/0.68863. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69199/0.68847. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69154/0.68831. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69173/0.68812. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69130/0.68794. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69123/0.68774. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69078/0.68754. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69109/0.68732. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69080/0.68707. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69074/0.68681. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69005/0.68649. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.69020/0.68619. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68981/0.68588. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68942/0.68553. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68924/0.68512. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68910/0.68475. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68877/0.68434. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68818/0.68396. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68793/0.68353. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68770/0.68309. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68737/0.68262. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68708/0.68215. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68651/0.68170. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68635/0.68127. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68556/0.68081. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68528/0.68032. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68475/0.67987. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68429/0.67943. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68379/0.67903. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68325/0.67865. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68266/0.67822. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68236/0.67782. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.68191/0.67739. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68133/0.67704. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68047/0.67673. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68033/0.67636. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67956/0.67607. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67867/0.67572. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67842/0.67539. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67738/0.67519. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67703/0.67493. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67615/0.67463. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67524/0.67442. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67452/0.67428. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67401/0.67392. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67290/0.67370. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67235/0.67346. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67148/0.67331. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67031/0.67316. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66956/0.67300. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66803/0.67286. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66731/0.67261. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66660/0.67235. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66503/0.67205. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66398/0.67216. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66237/0.67178. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66213/0.67178. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66085/0.67149. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65989/0.67148. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65808/0.67141. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65801/0.67106. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65631/0.67101. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65487/0.67091. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65333/0.67082. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65239/0.67089. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65134/0.67094. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64960/0.67100. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64887/0.67106. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64761/0.67115. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64628/0.67098. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64560/0.67146. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64431/0.67168. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64313/0.67191. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64153/0.67213. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64020/0.67238. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63934/0.67290. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63853/0.67310. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63667/0.67341. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69483/0.69326. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69454/0.69269. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69381/0.69219. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69363/0.69172. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69308/0.69126. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69291/0.69086. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69265/0.69051. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69228/0.69022. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69180/0.68995. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69168/0.68970. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69165/0.68944. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69139/0.68920. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69109/0.68895. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69071/0.68870. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69046/0.68843. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69053/0.68815. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69015/0.68785. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68974/0.68755. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68942/0.68723. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68915/0.68689. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68871/0.68653. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68855/0.68614. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68817/0.68575. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68771/0.68534. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68720/0.68491. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68679/0.68446. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68641/0.68400. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68569/0.68347. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68549/0.68296. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68479/0.68247. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68451/0.68196. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68390/0.68143. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68314/0.68090. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68286/0.68036. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68205/0.67990. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68135/0.67946. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68096/0.67895. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68082/0.67847. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67998/0.67799. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67932/0.67759. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67895/0.67718. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67778/0.67674. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67755/0.67619. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67711/0.67578. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67646/0.67542. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67550/0.67488. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67482/0.67454. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67467/0.67424. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67395/0.67393. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67316/0.67369. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67269/0.67333. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67204/0.67305. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67128/0.67283. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67032/0.67258. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66986/0.67251. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66941/0.67243. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66863/0.67233. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66859/0.67229. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.66737/0.67229. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66665/0.67220. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66529/0.67213. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66498/0.67219. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66437/0.67241. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66416/0.67259. Took 0.08 sec\n",
      "Epoch 64, Loss(train/val) 0.66251/0.67252. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66193/0.67301. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66199/0.67320. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66084/0.67348. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66062/0.67402. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65939/0.67413. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65814/0.67436. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65844/0.67461. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65715/0.67521. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.65677/0.67557. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65511/0.67597. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65489/0.67675. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65383/0.67732. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65412/0.67803. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65279/0.67857. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65180/0.67911. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65099/0.67981. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65068/0.68025. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65027/0.68098. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64854/0.68180. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64849/0.68258. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64733/0.68344. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64699/0.68409. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64631/0.68534. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64489/0.68566. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64469/0.68642. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64367/0.68700. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64298/0.68801. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64199/0.68846. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64073/0.68994. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64068/0.69006. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64000/0.69119. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.63897/0.69178. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63876/0.69272. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63783/0.69382. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63661/0.69472. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69570/0.69027. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69513/0.69040. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69486/0.69056. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69437/0.69077. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69428/0.69098. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69415/0.69122. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69356/0.69149. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69323/0.69178. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69311/0.69212. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69256/0.69245. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69252/0.69277. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69231/0.69311. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69190/0.69341. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69180/0.69368. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69155/0.69396. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69133/0.69412. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69103/0.69428. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69097/0.69440. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69056/0.69452. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69036/0.69457. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69008/0.69465. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68951/0.69470. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68919/0.69472. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68894/0.69484. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68834/0.69499. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68799/0.69513. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68763/0.69523. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68709/0.69543. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68626/0.69569. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68616/0.69587. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68546/0.69612. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68484/0.69641. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68432/0.69661. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68341/0.69701. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68330/0.69732. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68242/0.69773. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68209/0.69806. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68126/0.69832. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68067/0.69868. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67991/0.69884. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67971/0.69925. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67877/0.69956. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67831/0.69998. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67762/0.70021. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67702/0.70013. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67655/0.70076. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67622/0.70116. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67549/0.70108. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67458/0.70159. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67399/0.70204. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67338/0.70221. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67284/0.70264. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67199/0.70313. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67146/0.70343. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67050/0.70379. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66986/0.70420. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66942/0.70457. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66825/0.70499. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66812/0.70604. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66661/0.70687. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66655/0.70638. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66536/0.70722. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66514/0.70784. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66437/0.70828. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66353/0.70938. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66261/0.70996. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66156/0.71026. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66043/0.71109. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65999/0.71154. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65940/0.71179. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65892/0.71241. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65781/0.71335. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65678/0.71455. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65630/0.71469. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65523/0.71580. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65435/0.71661. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65351/0.71754. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65324/0.71815. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65230/0.71924. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65064/0.72002. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65013/0.72066. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64913/0.72163. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64847/0.72212. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64794/0.72322. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64661/0.72444. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64594/0.72493. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64546/0.72534. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64505/0.72664. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64380/0.72714. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64353/0.72796. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64244/0.72841. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64118/0.72903. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64072/0.72944. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63864/0.73119. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63803/0.73142. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63873/0.73268. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63793/0.73382. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63669/0.73473. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63566/0.73479. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63545/0.73621. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69402/0.68523. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69358/0.68625. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69354/0.68696. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69342/0.68739. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69296/0.68769. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69271/0.68787. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69282/0.68795. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69260/0.68804. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69223/0.68807. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.68805. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69176/0.68816. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69182/0.68813. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69152/0.68815. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69128/0.68819. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69100/0.68819. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69063/0.68823. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69061/0.68821. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69046/0.68829. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69034/0.68828. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69001/0.68834. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68983/0.68842. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68942/0.68854. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68929/0.68865. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68925/0.68869. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68889/0.68878. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68859/0.68892. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68853/0.68899. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68835/0.68914. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68798/0.68926. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68792/0.68944. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68759/0.68965. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68729/0.68982. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68703/0.68994. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68702/0.69011. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68668/0.69030. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68631/0.69049. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68590/0.69071. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68607/0.69082. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68550/0.69109. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68553/0.69123. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68532/0.69147. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68491/0.69193. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68459/0.69213. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68403/0.69210. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68426/0.69236. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68368/0.69271. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68332/0.69307. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68282/0.69325. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68250/0.69364. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68214/0.69385. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68156/0.69425. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68133/0.69458. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68091/0.69492. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68034/0.69518. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68009/0.69567. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67985/0.69600. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67925/0.69655. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67895/0.69683. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67789/0.69748. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67762/0.69792. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67686/0.69862. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67654/0.69911. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67555/0.69981. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67535/0.70042. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67462/0.70096. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67416/0.70199. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67345/0.70262. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67263/0.70359. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67181/0.70445. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67117/0.70529. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67079/0.70638. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.66944/0.70700. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66897/0.70826. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66819/0.70918. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66703/0.71054. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66622/0.71204. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66585/0.71317. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66480/0.71473. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66472/0.71589. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66340/0.71713. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66326/0.71842. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66194/0.71960. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66095/0.72120. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.66016/0.72267. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65934/0.72417. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65878/0.72574. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65721/0.72731. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65705/0.72863. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65596/0.73029. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65502/0.73160. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65470/0.73346. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65361/0.73468. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65289/0.73620. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65255/0.73846. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65156/0.73977. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65046/0.74070. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65010/0.74237. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64901/0.74451. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64806/0.74617. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64749/0.74708. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69635/0.69258. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69526/0.69161. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69483/0.69102. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69467/0.69062. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69438/0.69034. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69401/0.69014. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69380/0.69000. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69354/0.68989. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69344/0.68981. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69310/0.68976. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69312/0.68971. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69280/0.68968. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69285/0.68964. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69243/0.68962. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69207/0.68961. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69198/0.68960. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69203/0.68959. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69209/0.68957. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69163/0.68959. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69133/0.68959. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69123/0.68958. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69130/0.68962. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69100/0.68965. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69060/0.68967. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69070/0.68970. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.69051/0.68973. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69036/0.68975. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69003/0.68980. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69016/0.68982. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68984/0.68987. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68952/0.68991. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68924/0.68997. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68940/0.69003. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68925/0.69006. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68883/0.69009. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68872/0.69013. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68837/0.69020. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68821/0.69023. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68783/0.69027. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68775/0.69032. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68755/0.69038. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68740/0.69044. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68704/0.69048. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68697/0.69056. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68657/0.69063. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68645/0.69065. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68606/0.69073. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68591/0.69078. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68574/0.69083. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68533/0.69093. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68497/0.69103. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68470/0.69110. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68435/0.69116. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68406/0.69119. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68343/0.69127. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68321/0.69136. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68306/0.69145. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68258/0.69154. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68210/0.69167. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68168/0.69175. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.68116/0.69197. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68068/0.69209. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68021/0.69214. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67957/0.69223. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67914/0.69246. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67839/0.69265. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67762/0.69275. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67727/0.69295. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67667/0.69321. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67560/0.69342. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67495/0.69368. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67433/0.69393. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67351/0.69425. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67316/0.69458. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67201/0.69469. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67161/0.69508. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67063/0.69555. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66944/0.69597. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66864/0.69630. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66780/0.69671. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66675/0.69727. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66593/0.69771. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66506/0.69831. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66421/0.69889. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66327/0.69955. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66263/0.70020. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66208/0.70067. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66073/0.70148. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65955/0.70202. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65926/0.70292. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65821/0.70356. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65769/0.70410. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65694/0.70489. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65568/0.70569. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65455/0.70664. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65436/0.70734. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65304/0.70824. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65224/0.70909. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65115/0.70979. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65077/0.71098. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69324/0.69154. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69281/0.69114. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69252/0.69081. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69270/0.69050. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69246/0.69023. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69228/0.68995. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69202/0.68969. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69176/0.68942. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69154/0.68916. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69146/0.68887. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69112/0.68858. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69123/0.68827. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69105/0.68797. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69095/0.68765. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69053/0.68731. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69010/0.68694. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68993/0.68656. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68961/0.68615. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68929/0.68573. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68897/0.68531. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68875/0.68485. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68810/0.68440. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68777/0.68395. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68761/0.68348. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68705/0.68303. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68624/0.68259. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68596/0.68218. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68549/0.68182. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68482/0.68147. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68442/0.68116. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68384/0.68092. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68306/0.68072. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68255/0.68059. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68197/0.68052. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68117/0.68053. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68055/0.68058. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67993/0.68066. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67924/0.68080. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67845/0.68101. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67760/0.68126. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67679/0.68158. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67615/0.68194. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67539/0.68232. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67483/0.68275. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67379/0.68321. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67337/0.68365. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67249/0.68413. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67138/0.68466. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67099/0.68511. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67003/0.68569. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66978/0.68627. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66877/0.68691. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66778/0.68757. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66671/0.68827. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66611/0.68897. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66532/0.68968. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66488/0.69040. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66406/0.69115. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66349/0.69184. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66296/0.69260. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66238/0.69335. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66146/0.69407. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66102/0.69472. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66032/0.69545. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65998/0.69608. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65911/0.69684. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65874/0.69765. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65744/0.69843. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65731/0.69910. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65684/0.69971. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65607/0.70038. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65551/0.70109. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65497/0.70176. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65424/0.70223. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65417/0.70278. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65363/0.70332. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65325/0.70391. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65260/0.70453. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65195/0.70486. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65141/0.70543. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65086/0.70609. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65032/0.70671. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64955/0.70713. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64979/0.70773. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64900/0.70818. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64837/0.70862. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64783/0.70925. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64694/0.70979. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64671/0.71024. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64637/0.71069. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64557/0.71116. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64514/0.71159. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64427/0.71183. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64486/0.71244. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64425/0.71290. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64366/0.71324. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64285/0.71362. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64252/0.71410. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64173/0.71463. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64052/0.71506. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69148/0.68562. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69124/0.68570. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69112/0.68577. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69072/0.68589. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69027/0.68601. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69002/0.68616. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69008/0.68631. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68961/0.68648. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68931/0.68669. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68876/0.68688. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68860/0.68711. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68795/0.68730. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68743/0.68748. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68666/0.68768. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68633/0.68791. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68561/0.68806. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68467/0.68825. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68334/0.68842. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68277/0.68862. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68162/0.68884. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68062/0.68911. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67926/0.68953. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67805/0.69002. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67649/0.69043. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67523/0.69097. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67377/0.69179. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67227/0.69265. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67053/0.69349. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66841/0.69471. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.66672/0.69573. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66583/0.69710. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.66367/0.69869. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66218/0.69994. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66087/0.70187. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.65932/0.70363. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65762/0.70506. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65607/0.70741. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.65471/0.70876. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65309/0.71090. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.65107/0.71287. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.65042/0.71417. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.64914/0.71624. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.64811/0.71789. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64629/0.71841. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.64651/0.72061. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64425/0.72239. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.64353/0.72410. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.64184/0.72580. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64120/0.72865. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64045/0.72917. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.63808/0.73070. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.63796/0.73302. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.63655/0.73426. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63519/0.73496. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63482/0.73790. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63259/0.73904. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.63223/0.74059. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63126/0.74219. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63031/0.74336. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62896/0.74530. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62727/0.74630. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62729/0.74912. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62596/0.75001. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62496/0.75149. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62403/0.75248. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62324/0.75476. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62149/0.75523. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62163/0.75802. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61909/0.75963. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61914/0.75968. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61805/0.76126. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61643/0.76329. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61568/0.76485. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61543/0.76600. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61623/0.76812. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.61293/0.76906. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61273/0.76994. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61124/0.77261. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61038/0.77536. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60959/0.77524. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60747/0.77749. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60760/0.77861. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60623/0.77917. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60418/0.78107. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60283/0.78287. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60386/0.78327. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60204/0.78584. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59977/0.78850. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60054/0.78810. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59808/0.79023. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59676/0.79171. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59577/0.79373. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.59486/0.79404. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59499/0.79483. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59325/0.79686. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59208/0.79907. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.59045/0.80052. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59030/0.79969. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58726/0.80244. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.58725/0.80554. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69905/0.70210. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69645/0.69984. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69525/0.69814. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69409/0.69686. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69351/0.69594. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69274/0.69537. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69279/0.69494. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69256/0.69466. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69207/0.69450. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69190/0.69438. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69153/0.69431. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69151/0.69425. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69098/0.69421. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69114/0.69421. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69096/0.69424. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69064/0.69423. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69056/0.69427. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69010/0.69431. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69002/0.69434. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68977/0.69439. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68978/0.69442. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68931/0.69449. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68887/0.69459. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68882/0.69469. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68852/0.69477. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68862/0.69486. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68815/0.69496. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68758/0.69507. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68767/0.69518. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68737/0.69525. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68708/0.69544. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68687/0.69558. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68648/0.69571. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68634/0.69584. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68596/0.69595. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68565/0.69606. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68560/0.69621. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68535/0.69637. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68509/0.69647. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68443/0.69659. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68411/0.69672. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68427/0.69689. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68391/0.69701. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68369/0.69710. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68285/0.69724. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68325/0.69741. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68257/0.69757. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68234/0.69762. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68215/0.69779. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68187/0.69790. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68130/0.69795. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68144/0.69802. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68095/0.69815. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68029/0.69826. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68031/0.69837. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67960/0.69855. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67889/0.69864. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67874/0.69884. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67821/0.69897. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67804/0.69911. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67767/0.69924. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67722/0.69936. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67701/0.69937. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67646/0.69956. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67579/0.69969. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67552/0.69993. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67457/0.70004. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67418/0.70018. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67421/0.70042. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67312/0.70050. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67300/0.70068. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67262/0.70077. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67178/0.70103. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67101/0.70124. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.67043/0.70149. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67024/0.70145. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66950/0.70182. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66837/0.70196. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66815/0.70216. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66689/0.70235. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66583/0.70252. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66519/0.70277. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66474/0.70304. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66445/0.70329. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66295/0.70343. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66246/0.70379. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66142/0.70419. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66048/0.70475. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65874/0.70496. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65824/0.70549. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65721/0.70575. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65661/0.70638. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65561/0.70693. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65452/0.70751. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65326/0.70807. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65162/0.70873. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65113/0.70950. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64972/0.71028. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64919/0.71099. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64695/0.71221. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69349/0.69444. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69348/0.69408. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69330/0.69375. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69320/0.69343. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69297/0.69313. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69281/0.69284. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69279/0.69254. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69255/0.69228. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69238/0.69202. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69203/0.69178. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69183/0.69146. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69178/0.69121. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69171/0.69096. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69136/0.69074. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69090/0.69051. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69083/0.69029. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69064/0.69007. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69003/0.68991. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68990/0.68964. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68961/0.68954. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68926/0.68939. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68874/0.68912. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68837/0.68881. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68815/0.68858. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68763/0.68845. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68729/0.68824. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68668/0.68811. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68639/0.68793. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68594/0.68784. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68518/0.68777. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68484/0.68777. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68441/0.68763. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68395/0.68746. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68316/0.68753. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68266/0.68753. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68241/0.68737. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68166/0.68726. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68148/0.68728. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68075/0.68739. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68061/0.68742. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67954/0.68729. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67944/0.68733. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67893/0.68745. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67804/0.68753. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67784/0.68725. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67691/0.68746. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67603/0.68765. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67535/0.68762. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.67485/0.68761. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67407/0.68802. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67322/0.68827. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67208/0.68852. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67166/0.68880. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67104/0.68886. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66947/0.68949. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66891/0.68983. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66791/0.69033. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66687/0.69074. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66675/0.69084. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66537/0.69172. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66394/0.69272. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66305/0.69318. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66211/0.69359. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66138/0.69452. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65999/0.69535. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65878/0.69647. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65726/0.69723. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65633/0.69782. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65548/0.69885. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65428/0.69995. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65366/0.70041. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65271/0.70127. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65198/0.70208. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65003/0.70316. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64940/0.70441. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64741/0.70465. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64716/0.70645. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64562/0.70717. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64432/0.70851. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64299/0.70963. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64201/0.71089. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64149/0.71167. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64081/0.71286. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63943/0.71339. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63828/0.71558. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63756/0.71618. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63551/0.71792. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63448/0.71955. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63438/0.72036. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63316/0.72208. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63133/0.72296. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63121/0.72328. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62898/0.72506. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62887/0.72627. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62774/0.72736. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62531/0.72883. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62393/0.73081. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62412/0.73284. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62288/0.73305. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62104/0.73514. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69335/0.68742. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69332/0.68752. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69297/0.68754. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69299/0.68756. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69302/0.68758. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69273/0.68762. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69229/0.68762. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69227/0.68762. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69192/0.68757. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69175/0.68751. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69151/0.68742. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69159/0.68729. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69096/0.68715. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69073/0.68704. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69032/0.68688. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69026/0.68667. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68988/0.68637. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68925/0.68615. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68906/0.68591. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68890/0.68565. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68817/0.68535. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68776/0.68499. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68698/0.68470. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68693/0.68436. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68632/0.68405. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68610/0.68378. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68572/0.68371. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68542/0.68343. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68494/0.68321. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68439/0.68314. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68391/0.68304. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68375/0.68293. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68333/0.68280. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68287/0.68287. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68255/0.68269. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68216/0.68266. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68159/0.68271. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68101/0.68279. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68089/0.68284. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68062/0.68295. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68002/0.68305. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67933/0.68311. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67908/0.68311. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67821/0.68333. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67800/0.68352. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67736/0.68380. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67674/0.68396. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67645/0.68401. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67621/0.68437. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67528/0.68489. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67457/0.68494. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67428/0.68534. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67338/0.68551. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67313/0.68622. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67208/0.68664. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67193/0.68729. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67084/0.68781. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67044/0.68812. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66958/0.68872. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66893/0.68942. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66850/0.68996. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66749/0.69042. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66665/0.69148. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66650/0.69176. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66512/0.69301. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66430/0.69403. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66338/0.69482. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66298/0.69567. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66207/0.69707. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66119/0.69762. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66047/0.69859. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65904/0.69978. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65866/0.70113. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65765/0.70228. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65636/0.70343. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65574/0.70467. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65470/0.70616. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.65404/0.70723. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65305/0.70859. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65161/0.70976. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.65154/0.71129. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65110/0.71403. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64857/0.71339. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64817/0.71630. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64720/0.71717. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64680/0.71925. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64596/0.72033. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.64392/0.72199. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64376/0.72391. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64231/0.72555. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64167/0.72658. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64115/0.72805. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64022/0.72992. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63900/0.73092. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63863/0.73330. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.63690/0.73461. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63536/0.73639. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63504/0.73774. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63322/0.73966. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63229/0.74010. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69891/0.71203. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69679/0.70678. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69521/0.70162. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69344/0.69683. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69241/0.69285. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69173/0.69005. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69137/0.68827. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69122/0.68713. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69127/0.68649. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69089/0.68610. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69096/0.68590. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69065/0.68575. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69060/0.68568. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69059/0.68566. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69053/0.68569. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69040/0.68570. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69022/0.68571. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69019/0.68572. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68992/0.68568. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68977/0.68568. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69003/0.68578. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68961/0.68583. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68959/0.68587. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68935/0.68588. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68932/0.68595. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68941/0.68600. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68893/0.68601. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68884/0.68596. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68890/0.68594. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68833/0.68596. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68859/0.68599. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68825/0.68601. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68839/0.68608. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68818/0.68617. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68801/0.68617. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68767/0.68606. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68748/0.68608. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68739/0.68616. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68738/0.68620. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68722/0.68618. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68680/0.68610. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68682/0.68613. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68667/0.68611. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68648/0.68621. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68621/0.68624. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68563/0.68623. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68565/0.68628. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68563/0.68638. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68507/0.68638. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68483/0.68637. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68453/0.68667. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68404/0.68674. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68384/0.68683. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68348/0.68677. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68296/0.68664. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68265/0.68687. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68218/0.68704. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68189/0.68720. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68146/0.68735. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68117/0.68757. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68065/0.68765. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67998/0.68802. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67971/0.68818. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67924/0.68833. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67848/0.68839. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67815/0.68877. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67758/0.68899. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67701/0.68910. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67640/0.68943. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67607/0.68957. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67570/0.68989. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67471/0.69009. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67418/0.69050. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67358/0.69079. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67287/0.69097. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67269/0.69139. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67201/0.69177. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67107/0.69208. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67061/0.69251. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66976/0.69285. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66947/0.69361. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66860/0.69404. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66783/0.69489. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66700/0.69489. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66654/0.69561. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66639/0.69616. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66544/0.69674. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66464/0.69721. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66399/0.69802. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66345/0.69831. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66294/0.69898. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66249/0.69944. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66191/0.69998. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66135/0.70076. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66069/0.70129. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65956/0.70178. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65944/0.70230. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65856/0.70295. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65800/0.70339. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65697/0.70417. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.70040/0.70060. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69963/0.70013. Took 0.08 sec\n",
      "Epoch 2, Loss(train/val) 0.69848/0.69960. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69723/0.69877. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69571/0.69749. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69408/0.69591. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69218/0.69445. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69107/0.69328. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68997/0.69246. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.69193. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68908/0.69163. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68890/0.69143. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68846/0.69134. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68816/0.69128. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68792/0.69125. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68774/0.69126. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68767/0.69134. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68751/0.69145. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68714/0.69155. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68687/0.69166. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68653/0.69181. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68652/0.69197. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68606/0.69219. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68576/0.69245. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68532/0.69274. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68506/0.69306. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68470/0.69343. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68425/0.69384. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68354/0.69431. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68338/0.69481. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68270/0.69534. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68207/0.69595. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68140/0.69659. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68112/0.69728. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68028/0.69793. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67956/0.69863. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67949/0.69941. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67834/0.70018. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67763/0.70095. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67718/0.70176. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67637/0.70253. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67564/0.70334. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67508/0.70413. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67432/0.70492. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67326/0.70579. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67271/0.70666. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67202/0.70751. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67162/0.70828. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67084/0.70916. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67035/0.71004. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66952/0.71090. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66900/0.71170. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66853/0.71251. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66776/0.71340. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66708/0.71419. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66647/0.71497. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66577/0.71562. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66542/0.71643. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66423/0.71718. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66385/0.71793. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66320/0.71860. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66288/0.71934. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66185/0.72003. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66118/0.72078. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65989/0.72147. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65931/0.72216. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65904/0.72279. Took 0.08 sec\n",
      "Epoch 67, Loss(train/val) 0.65810/0.72355. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65797/0.72414. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65685/0.72463. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65658/0.72527. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65516/0.72602. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65444/0.72675. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65453/0.72731. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65354/0.72798. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65289/0.72842. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65215/0.72909. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65149/0.72970. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65065/0.73045. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64972/0.73127. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64845/0.73203. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64841/0.73264. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64743/0.73332. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64697/0.73413. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64659/0.73457. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.64502/0.73519. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64602/0.73595. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64419/0.73675. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64352/0.73760. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64280/0.73825. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64221/0.73908. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64199/0.73961. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64094/0.74020. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64026/0.74109. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.63922/0.74190. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63888/0.74225. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63830/0.74315. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63668/0.74374. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63557/0.74473. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63550/0.74506. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.68988/0.69126. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69000/0.69118. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.68951/0.69111. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68943/0.69104. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69096. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68878/0.69088. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68902/0.69079. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.69072. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68810/0.69069. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68796/0.69069. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68756/0.69069. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68730/0.69073. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68734/0.69082. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68691/0.69093. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68676/0.69107. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68635/0.69123. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68632/0.69144. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68619/0.69165. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68542/0.69189. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68567/0.69216. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68502/0.69241. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68484/0.69268. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68441/0.69296. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68429/0.69331. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68376/0.69362. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68351/0.69398. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68284/0.69437. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68290/0.69476. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68213/0.69517. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68210/0.69557. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68152/0.69598. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68112/0.69640. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68072/0.69681. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68067/0.69721. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68079/0.69758. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67972/0.69798. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67937/0.69835. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67889/0.69871. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67895/0.69909. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67823/0.69944. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67806/0.69977. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67733/0.70009. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67711/0.70038. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67665/0.70068. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67646/0.70098. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67564/0.70122. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67520/0.70154. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67506/0.70179. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67438/0.70212. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67374/0.70238. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67347/0.70263. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67300/0.70290. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67273/0.70317. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67173/0.70345. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67139/0.70373. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67090/0.70405. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67010/0.70441. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66944/0.70478. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66869/0.70520. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66868/0.70554. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66779/0.70584. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66772/0.70615. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66606/0.70651. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66563/0.70676. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66471/0.70718. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66519/0.70763. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66369/0.70813. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66253/0.70859. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66180/0.70908. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66161/0.70963. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66036/0.71022. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65971/0.71078. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65915/0.71138. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65822/0.71199. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65758/0.71237. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65627/0.71297. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65528/0.71371. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65429/0.71428. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65314/0.71510. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65203/0.71583. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65124/0.71678. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65050/0.71786. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64939/0.71858. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64893/0.71945. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64832/0.72020. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64704/0.72117. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64679/0.72210. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64530/0.72289. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64400/0.72421. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64306/0.72542. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64316/0.72647. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64181/0.72745. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64093/0.72827. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64021/0.72920. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63943/0.73039. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63808/0.73182. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63748/0.73284. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63601/0.73378. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63562/0.73460. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63372/0.73617. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.68910/0.69078. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68914/0.69091. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68910/0.69103. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68894/0.69117. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68884/0.69131. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68870/0.69146. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68858/0.69162. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68851/0.69179. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68849/0.69200. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68810/0.69222. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68823/0.69248. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68794/0.69277. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68779/0.69310. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68785/0.69347. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68726/0.69386. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68718/0.69427. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68702/0.69467. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68668/0.69510. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68652/0.69556. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68620/0.69599. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68595/0.69651. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68564/0.69701. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68549/0.69752. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68506/0.69801. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68496/0.69864. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68426/0.69926. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68388/0.69992. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68361/0.70060. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68311/0.70128. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68255/0.70202. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68240/0.70275. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68187/0.70347. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.70424. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68096/0.70502. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68046/0.70580. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67995/0.70652. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67918/0.70732. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67898/0.70818. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67823/0.70899. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67773/0.70964. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67727/0.71048. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67630/0.71113. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67592/0.71204. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67515/0.71273. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67444/0.71353. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67410/0.71423. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67360/0.71486. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67305/0.71560. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67219/0.71624. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67178/0.71702. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67088/0.71781. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67043/0.71850. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66977/0.71916. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66897/0.71981. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66831/0.72051. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66780/0.72121. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66704/0.72191. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66683/0.72265. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66568/0.72334. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66541/0.72398. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66467/0.72459. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66334/0.72531. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66305/0.72611. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66245/0.72669. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66222/0.72740. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66169/0.72803. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66086/0.72889. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66048/0.72938. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65939/0.73000. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65905/0.73052. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65821/0.73116. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65737/0.73177. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65679/0.73226. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65605/0.73269. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65566/0.73324. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65446/0.73370. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65438/0.73415. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65386/0.73452. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65277/0.73504. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65299/0.73562. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65188/0.73599. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65087/0.73638. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65043/0.73690. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64983/0.73733. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64876/0.73751. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64842/0.73792. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64797/0.73812. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64685/0.73869. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64646/0.73899. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64506/0.73917. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64455/0.73952. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64434/0.73989. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64419/0.74030. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64341/0.74055. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64230/0.74115. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64211/0.74131. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64166/0.74162. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64068/0.74196. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63930/0.74229. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63870/0.74275. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.68662/0.68350. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68634/0.68305. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.68603/0.68268. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68597/0.68236. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68554/0.68208. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68517/0.68182. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68493/0.68157. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68470/0.68134. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68449/0.68109. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68418/0.68085. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68390/0.68060. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68374/0.68036. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68340/0.68010. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68316/0.67982. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68312/0.67952. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68269/0.67920. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68254/0.67888. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68217/0.67856. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68143/0.67820. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68141/0.67782. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68088/0.67744. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68074/0.67702. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68001/0.67656. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67960/0.67610. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67922/0.67560. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67907/0.67510. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67818/0.67459. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67773/0.67405. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67709/0.67349. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67687/0.67295. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67653/0.67242. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67547/0.67187. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67498/0.67137. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67435/0.67091. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67421/0.67038. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67376/0.66990. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67251/0.66943. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67280/0.66900. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67191/0.66857. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67170/0.66820. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67126/0.66783. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67039/0.66748. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66995/0.66711. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66934/0.66676. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66862/0.66645. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66838/0.66620. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66807/0.66592. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66755/0.66565. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66641/0.66546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66601/0.66525. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66577/0.66504. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66523/0.66483. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66496/0.66461. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66365/0.66435. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66332/0.66408. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66326/0.66384. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66302/0.66369. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66186/0.66358. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66128/0.66347. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66096/0.66340. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66034/0.66329. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65968/0.66317. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65925/0.66305. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65872/0.66292. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65820/0.66280. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65785/0.66277. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65683/0.66268. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65640/0.66264. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65549/0.66261. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65517/0.66261. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65479/0.66262. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65412/0.66265. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65340/0.66273. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65309/0.66281. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65212/0.66267. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65216/0.66267. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65114/0.66270. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65059/0.66274. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64984/0.66289. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64944/0.66305. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64867/0.66313. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64816/0.66326. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64785/0.66334. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64703/0.66351. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64600/0.66368. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64613/0.66374. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64520/0.66380. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64476/0.66391. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64380/0.66401. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64314/0.66426. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64290/0.66446. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64198/0.66456. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64122/0.66453. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64113/0.66480. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64004/0.66501. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63962/0.66516. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63856/0.66553. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63818/0.66591. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63735/0.66616. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63732/0.66628. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69286/0.68668. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69027/0.68392. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68896/0.68187. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68793/0.68055. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68751/0.67985. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68717/0.67958. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68674/0.67946. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68641/0.67943. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68680/0.67946. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68663/0.67954. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68642/0.67960. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68626/0.67967. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68616/0.67975. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68595/0.67981. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68607/0.67988. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68582/0.67996. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68545/0.68003. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68557/0.68013. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.68022. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68516/0.68033. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68479/0.68045. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68492/0.68053. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68436/0.68063. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68422/0.68074. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68430/0.68089. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68403/0.68105. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68369/0.68117. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68368/0.68135. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68331/0.68156. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68299/0.68181. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68251/0.68204. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68232/0.68230. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68191/0.68259. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68159/0.68291. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68136/0.68325. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68067/0.68362. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68059/0.68400. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67981/0.68441. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67919/0.68488. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67895/0.68535. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67835/0.68581. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67769/0.68624. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67729/0.68671. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67644/0.68723. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67594/0.68775. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67535/0.68821. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67440/0.68879. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67370/0.68924. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67329/0.68973. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67241/0.69022. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67157/0.69063. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67090/0.69112. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66993/0.69156. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66924/0.69195. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66853/0.69237. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66774/0.69266. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66698/0.69299. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66692/0.69333. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66538/0.69368. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66481/0.69409. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66466/0.69448. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66325/0.69486. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66295/0.69519. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66175/0.69557. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66140/0.69597. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65987/0.69647. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65967/0.69694. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65851/0.69738. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65826/0.69791. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65710/0.69845. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65621/0.69891. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65606/0.69940. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65443/0.69989. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65353/0.70043. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65305/0.70098. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65199/0.70157. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65137/0.70218. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65066/0.70307. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64994/0.70365. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64901/0.70410. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64794/0.70464. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64773/0.70530. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64686/0.70602. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64581/0.70689. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64495/0.70770. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64475/0.70845. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64338/0.70919. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64258/0.71000. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64173/0.71080. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64072/0.71160. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64049/0.71221. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63894/0.71303. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63774/0.71379. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63833/0.71441. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.63722/0.71522. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63618/0.71600. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63478/0.71700. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63496/0.71779. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63332/0.71827. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63370/0.71918. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70154/0.69032. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69408/0.69106. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68857/0.69389. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68547/0.69753. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68398/0.70022. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68373/0.70170. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68386/0.70239. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68356/0.70272. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68337/0.70299. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68356/0.70314. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68327/0.70313. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68310/0.70314. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68338/0.70321. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68310/0.70331. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68299/0.70340. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68283/0.70342. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68239/0.70352. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68258/0.70346. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68272/0.70342. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68247/0.70345. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68255/0.70361. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68217/0.70360. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68208/0.70351. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68209/0.70357. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68200/0.70354. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68185/0.70352. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68185/0.70351. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68146/0.70354. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68165/0.70356. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68143/0.70353. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68147/0.70346. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68139/0.70334. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68120/0.70334. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68106/0.70330. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68080/0.70331. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68069/0.70324. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68045/0.70313. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68015/0.70323. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68010/0.70316. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68014/0.70317. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67987/0.70303. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67970/0.70304. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67963/0.70295. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67915/0.70294. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67907/0.70279. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67880/0.70266. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67833/0.70246. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67837/0.70233. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67809/0.70235. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67798/0.70234. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67784/0.70201. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67719/0.70181. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67718/0.70181. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67687/0.70178. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67652/0.70168. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67612/0.70143. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67569/0.70123. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67559/0.70109. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67548/0.70109. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67501/0.70116. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67411/0.70087. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67379/0.70066. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67363/0.70060. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67316/0.70028. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67256/0.70050. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67220/0.70024. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67219/0.69994. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67144/0.69983. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67098/0.69970. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67014/0.69937. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66989/0.69950. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66937/0.69942. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66879/0.69923. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66796/0.69887. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66772/0.69892. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66696/0.69886. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66664/0.69869. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66619/0.69829. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66540/0.69833. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66470/0.69814. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66379/0.69838. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66316/0.69795. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66275/0.69775. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66208/0.69810. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66119/0.69766. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66053/0.69786. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65987/0.69813. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65920/0.69833. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65871/0.69844. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65822/0.69843. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65702/0.69862. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65677/0.69887. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65542/0.69890. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65460/0.69945. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65406/0.69974. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65314/0.70048. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65275/0.70057. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65186/0.70077. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65070/0.70083. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65011/0.70180. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69219. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69436/0.69208. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69424/0.69199. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69397/0.69192. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69378/0.69186. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69375/0.69182. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69344/0.69179. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69336/0.69179. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69334/0.69181. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69289/0.69187. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69278/0.69194. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69245/0.69206. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69215/0.69223. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69191/0.69241. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69153/0.69262. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69127/0.69280. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69135/0.69296. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69075/0.69308. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69062/0.69317. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69038/0.69326. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69017/0.69333. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68994/0.69337. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68977/0.69338. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68962/0.69335. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68928/0.69335. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68893/0.69336. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68879/0.69334. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68867/0.69331. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68843/0.69328. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68802/0.69328. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68789/0.69330. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68750/0.69328. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68733/0.69326. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68700/0.69324. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68683/0.69324. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68649/0.69328. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68613/0.69328. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68577/0.69325. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68548/0.69334. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68510/0.69331. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68502/0.69331. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68471/0.69335. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68448/0.69341. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68399/0.69344. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68364/0.69347. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68353/0.69357. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68280/0.69363. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68246/0.69368. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68244/0.69372. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68187/0.69378. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68155/0.69386. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68139/0.69388. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68058/0.69400. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68049/0.69395. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68005/0.69407. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67942/0.69422. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67878/0.69422. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67836/0.69433. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67784/0.69452. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.67728/0.69457. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67662/0.69453. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67622/0.69460. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67571/0.69469. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67469/0.69485. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67450/0.69488. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67353/0.69511. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67342/0.69508. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67209/0.69522. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67151/0.69534. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67073/0.69555. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66988/0.69574. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66867/0.69583. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66836/0.69596. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66761/0.69619. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66605/0.69635. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66512/0.69651. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66378/0.69690. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66299/0.69705. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66167/0.69742. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66016/0.69739. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65905/0.69802. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65788/0.69826. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65694/0.69846. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65495/0.69908. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65401/0.69914. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65321/0.69957. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65123/0.70031. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64972/0.70094. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64854/0.70110. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64674/0.70171. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64626/0.70219. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64405/0.70291. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64253/0.70334. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64107/0.70416. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63925/0.70482. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63790/0.70560. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63651/0.70625. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63473/0.70693. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63332/0.70763. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63058/0.70891. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69531/0.69837. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69509/0.69828. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69492/0.69816. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69477/0.69801. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69469/0.69780. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69447/0.69752. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69411/0.69719. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69401/0.69677. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69343/0.69635. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69320/0.69593. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69298/0.69542. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69253/0.69491. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69244/0.69449. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69210/0.69414. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69167/0.69388. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69148/0.69365. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69145/0.69351. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69111/0.69345. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69101/0.69347. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69093/0.69348. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69062/0.69359. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69086/0.69360. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69044/0.69368. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68991/0.69386. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69007/0.69406. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68982/0.69423. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68972/0.69443. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68924/0.69474. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68907/0.69500. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68896/0.69535. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68836/0.69575. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68844/0.69612. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68847/0.69640. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68772/0.69672. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68738/0.69705. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68735/0.69755. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68684/0.69800. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68681/0.69836. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68634/0.69872. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68624/0.69912. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68588/0.69952. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68578/0.70003. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68532/0.70046. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68475/0.70081. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68462/0.70072. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68366/0.70133. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68334/0.70188. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68334/0.70216. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68289/0.70238. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68267/0.70296. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68183/0.70351. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68201/0.70364. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68102/0.70411. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68087/0.70431. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68064/0.70464. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67989/0.70500. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67924/0.70536. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67901/0.70576. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67846/0.70573. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67806/0.70640. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67788/0.70658. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67743/0.70681. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67627/0.70736. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67637/0.70779. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67575/0.70768. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67535/0.70806. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67501/0.70836. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67392/0.70898. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67331/0.70963. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67318/0.70978. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67260/0.71080. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67177/0.71119. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67155/0.71116. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67103/0.71150. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66982/0.71222. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66966/0.71300. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66912/0.71370. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66871/0.71439. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66803/0.71450. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66712/0.71580. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66628/0.71615. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66566/0.71665. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66491/0.71832. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66488/0.71844. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66355/0.71994. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66321/0.71981. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66153/0.72200. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66199/0.72272. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66135/0.72282. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65990/0.72460. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65974/0.72562. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65881/0.72608. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65774/0.72721. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65793/0.72838. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65567/0.72957. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65451/0.72975. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65442/0.73178. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65404/0.73256. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65257/0.73439. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65135/0.73509. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69622/0.69133. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69535/0.69168. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69517/0.69205. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69426/0.69248. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69387/0.69300. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69350/0.69362. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69286/0.69437. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69264/0.69516. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69226/0.69591. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.69663. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69164/0.69730. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69168/0.69787. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69121/0.69832. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69137/0.69871. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69094/0.69904. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69070/0.69920. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69066/0.69937. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69037/0.69952. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69036/0.69959. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69005/0.69967. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69002/0.69978. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68977/0.69982. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68973/0.69975. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68963/0.69969. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68916/0.69963. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68901/0.69960. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68898/0.69955. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68881/0.69951. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68838/0.69949. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68825/0.69933. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68781/0.69934. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68761/0.69930. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68737/0.69928. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68683/0.69918. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68662/0.69909. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68653/0.69895. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68599/0.69885. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68570/0.69869. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68574/0.69846. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68537/0.69840. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68504/0.69805. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68477/0.69794. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68449/0.69801. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68402/0.69794. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68353/0.69794. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68357/0.69757. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68312/0.69765. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68262/0.69760. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68256/0.69743. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68219/0.69730. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68184/0.69705. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68136/0.69703. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68123/0.69691. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68107/0.69677. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68026/0.69672. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68028/0.69659. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67999/0.69647. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67962/0.69645. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67943/0.69628. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67864/0.69629. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67818/0.69603. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67811/0.69600. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67769/0.69603. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67729/0.69610. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67647/0.69598. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67651/0.69572. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67592/0.69550. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67573/0.69526. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67487/0.69547. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67454/0.69518. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67385/0.69514. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.67383/0.69541. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67290/0.69513. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67290/0.69486. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67197/0.69451. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67153/0.69409. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67104/0.69419. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67070/0.69439. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66971/0.69419. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66941/0.69390. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66873/0.69366. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66749/0.69378. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66714/0.69307. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66658/0.69300. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66600/0.69293. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66528/0.69236. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66413/0.69252. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66310/0.69277. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66212/0.69246. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.66141/0.69214. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66090/0.69209. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65953/0.69257. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65864/0.69158. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65744/0.69177. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65670/0.69210. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65561/0.69198. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65449/0.69184. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65341/0.69198. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65178/0.69115. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65035/0.69203. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69868/0.71999. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69751/0.71696. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69627/0.71405. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69571/0.71126. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69529/0.70863. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69456/0.70611. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69407/0.70382. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69388/0.70194. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69341/0.70036. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69327/0.69919. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69330/0.69833. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69322/0.69761. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69284/0.69721. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69287/0.69686. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69263/0.69664. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69259/0.69656. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.69226/0.69654. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69246/0.69648. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69218/0.69651. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69206/0.69645. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69221/0.69644. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69164/0.69649. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69188/0.69671. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69178/0.69680. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.69144/0.69696. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69148/0.69706. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.69146/0.69712. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.69117/0.69723. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69090/0.69736. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69112/0.69748. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69096/0.69755. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69072/0.69774. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.69054/0.69786. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69033/0.69798. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69049/0.69816. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.69007/0.69843. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.69001/0.69867. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68998/0.69891. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68980/0.69918. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68934/0.69934. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68929/0.69951. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68919/0.69981. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68891/0.69998. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68887/0.70014. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68878/0.70038. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68856/0.70064. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68814/0.70099. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68827/0.70134. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68808/0.70155. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68758/0.70188. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68753/0.70220. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68723/0.70230. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68715/0.70253. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68690/0.70286. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68651/0.70311. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68642/0.70344. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68584/0.70367. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68588/0.70372. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68561/0.70393. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68543/0.70401. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68538/0.70429. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68486/0.70439. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68443/0.70446. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68456/0.70475. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68399/0.70473. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68367/0.70491. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68360/0.70508. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68297/0.70515. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68275/0.70524. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68231/0.70548. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68189/0.70559. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68165/0.70583. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.68117/0.70597. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68075/0.70605. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68048/0.70568. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67973/0.70589. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67947/0.70595. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67889/0.70611. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67844/0.70630. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67796/0.70631. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67709/0.70662. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67682/0.70647. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67584/0.70643. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67545/0.70663. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67500/0.70647. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67421/0.70646. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67347/0.70627. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67266/0.70616. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67190/0.70616. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67127/0.70621. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67089/0.70572. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67021/0.70577. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66854/0.70596. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66796/0.70565. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66680/0.70564. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66642/0.70629. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66462/0.70608. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66457/0.70643. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66316/0.70606. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66166/0.70670. Took 0.11 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69841/0.69043. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69714/0.69089. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69581/0.69157. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69443/0.69257. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69331/0.69384. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69208/0.69519. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69113/0.69647. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69059/0.69756. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69024/0.69833. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68995/0.69891. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68995/0.69930. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68958/0.69955. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68956/0.69973. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68942/0.69979. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68963/0.69984. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68950/0.69986. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68905/0.69980. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68898/0.69978. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68891/0.69977. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68873/0.69973. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68856/0.69964. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68849/0.69958. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68828/0.69953. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68824/0.69944. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68814/0.69940. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68795/0.69933. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68801/0.69922. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68760/0.69917. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68740/0.69906. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68728/0.69889. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68743/0.69885. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68704/0.69872. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68714/0.69857. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68683/0.69844. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68640/0.69836. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68646/0.69817. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68615/0.69808. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68602/0.69794. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68608/0.69785. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68567/0.69773. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68574/0.69760. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68534/0.69744. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68534/0.69718. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68494/0.69702. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68463/0.69681. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68455/0.69656. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68419/0.69644. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68412/0.69624. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68387/0.69622. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68312/0.69606. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68323/0.69575. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68280/0.69544. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68258/0.69511. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68196/0.69504. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68187/0.69474. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68178/0.69449. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68092/0.69436. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68105/0.69414. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68077/0.69381. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68006/0.69348. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67973/0.69329. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67931/0.69296. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67965/0.69302. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67961/0.69288. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67870/0.69256. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67843/0.69218. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67801/0.69191. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67791/0.69162. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67743/0.69147. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67709/0.69120. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67671/0.69130. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67650/0.69105. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67561/0.69064. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67555/0.69045. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67449/0.69034. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67423/0.69009. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67419/0.69002. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67383/0.69017. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67264/0.68983. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67263/0.68996. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67204/0.68978. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67210/0.68971. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67137/0.68973. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.67103/0.68978. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67000/0.68961. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66964/0.68986. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.66910/0.68967. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.66908/0.68965. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66807/0.68994. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66771/0.68999. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.66675/0.68983. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66594/0.69009. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66570/0.69021. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.66554/0.69048. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66449/0.69083. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66319/0.69104. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66341/0.69136. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66301/0.69150. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.66166/0.69170. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.66096/0.69212. Took 0.08 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69317/0.69285. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69323/0.69238. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69296/0.69190. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69145. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69207/0.69103. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69217/0.69069. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.69042. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69183/0.69022. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69166/0.69009. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69165/0.68998. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69149/0.68987. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69137/0.68979. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69117/0.68976. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69070/0.68976. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69069/0.68977. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69053/0.68978. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69060/0.68983. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69055/0.68987. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69033/0.68990. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69010/0.68999. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68972/0.69002. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68998/0.69005. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68974/0.69017. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68959/0.69025. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68949/0.69030. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68937/0.69038. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68910/0.69044. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68914/0.69054. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68871/0.69064. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68859/0.69068. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68866/0.69076. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68848/0.69085. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68819/0.69085. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68792/0.69084. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68777/0.69084. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68818/0.69085. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68774/0.69080. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68761/0.69077. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68730/0.69078. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68713/0.69073. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68707/0.69070. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68698/0.69075. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68644/0.69069. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68636/0.69062. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68631/0.69062. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68613/0.69056. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68565/0.69040. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68562/0.69034. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68554/0.69036. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68543/0.69040. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68531/0.69029. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68502/0.69017. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68462/0.69005. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68481/0.69004. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.68409/0.69007. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68384/0.69007. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.68355/0.68989. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68317/0.68988. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68278/0.68989. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68208/0.68997. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68253/0.68983. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68223/0.68976. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68166/0.68972. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68145/0.68973. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68089/0.68966. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68052/0.68971. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68063/0.68971. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67966/0.68998. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67928/0.69013. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67918/0.69022. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67847/0.69042. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67804/0.69055. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67781/0.69077. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67696/0.69106. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67705/0.69117. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67629/0.69149. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67598/0.69160. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67529/0.69183. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67454/0.69193. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67505/0.69245. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67379/0.69257. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67308/0.69291. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67327/0.69304. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67288/0.69311. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67183/0.69326. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67172/0.69370. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67083/0.69412. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67128/0.69441. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.67001/0.69457. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66980/0.69495. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66853/0.69524. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66875/0.69520. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66866/0.69552. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66757/0.69553. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66627/0.69562. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66594/0.69594. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66567/0.69604. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66418/0.69635. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66371/0.69623. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66277/0.69670. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69262/0.70064. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69244/0.70065. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.70066. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69203/0.70069. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69204/0.70071. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.70075. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69193/0.70081. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69164/0.70084. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69162/0.70089. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69135/0.70097. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69105/0.70104. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69094/0.70111. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69047/0.70121. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69033/0.70129. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68992/0.70139. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68969/0.70151. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68917/0.70164. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68884/0.70181. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68867/0.70203. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68796/0.70227. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68756/0.70257. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68748/0.70289. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68691/0.70320. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68602/0.70353. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68528/0.70393. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68503/0.70441. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68444/0.70482. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68382/0.70538. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68316/0.70595. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68254/0.70648. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68209/0.70710. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68106/0.70760. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68086/0.70814. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67974/0.70886. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67896/0.70941. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67850/0.71020. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67799/0.71072. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67715/0.71126. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67664/0.71182. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67581/0.71264. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67502/0.71330. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67402/0.71406. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67356/0.71475. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67230/0.71578. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67158/0.71675. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67070/0.71760. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67021/0.71859. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66870/0.71965. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66798/0.72065. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66725/0.72184. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66552/0.72318. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66517/0.72432. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66449/0.72539. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66297/0.72691. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66241/0.72810. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66132/0.72961. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66057/0.73079. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65892/0.73213. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65756/0.73402. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65733/0.73549. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65555/0.73728. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65406/0.73881. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65312/0.74059. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65239/0.74250. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65096/0.74431. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64922/0.74633. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64866/0.74839. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64630/0.75069. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64617/0.75280. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64462/0.75516. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64364/0.75726. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64194/0.75939. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64031/0.76211. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63878/0.76444. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63643/0.76660. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63639/0.76907. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63508/0.77112. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63297/0.77442. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63291/0.77679. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63079/0.77940. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62982/0.78250. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62746/0.78533. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62676/0.78826. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62478/0.79085. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62499/0.79362. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62251/0.79639. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62109/0.79958. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62015/0.80271. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61897/0.80557. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61715/0.80866. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61499/0.81164. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61415/0.81421. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61395/0.81676. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61198/0.81961. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60951/0.82258. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60879/0.82561. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60770/0.82823. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60635/0.83165. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60424/0.83376. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.60322/0.83623. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69303/0.68943. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69319/0.68955. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69317/0.68965. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69290/0.68977. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69299/0.68987. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69264/0.68995. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69222/0.69007. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69247/0.69018. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69233/0.69028. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69161/0.69038. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69170/0.69048. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69166/0.69060. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69123/0.69071. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69114/0.69086. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69101/0.69103. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69079/0.69118. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69058/0.69133. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69021/0.69146. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69006/0.69160. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68980/0.69173. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68960/0.69188. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68927/0.69195. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68882/0.69203. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68869/0.69211. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68834/0.69219. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68800/0.69216. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68788/0.69221. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68719/0.69211. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68694/0.69208. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68617/0.69200. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68572/0.69187. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68525/0.69181. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68491/0.69175. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68380/0.69162. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68347/0.69175. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68301/0.69175. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68192/0.69151. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68127/0.69150. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68082/0.69156. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67990/0.69182. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67909/0.69188. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67821/0.69196. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67686/0.69222. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67611/0.69226. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67553/0.69272. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67463/0.69272. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67335/0.69399. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67231/0.69455. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67144/0.69527. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66999/0.69606. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66908/0.69757. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66832/0.69872. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66679/0.70000. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66613/0.70155. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66506/0.70332. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66369/0.70483. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66306/0.70687. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66181/0.70855. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66012/0.71050. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65909/0.71225. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65805/0.71414. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65691/0.71586. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65596/0.71764. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65487/0.71936. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65349/0.72105. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65286/0.72266. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65223/0.72427. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65096/0.72601. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64906/0.72746. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64798/0.72912. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64605/0.73075. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64596/0.73243. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64385/0.73413. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64323/0.73587. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.64171/0.73767. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64089/0.73966. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63956/0.74151. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63765/0.74330. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63731/0.74506. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63534/0.74700. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63398/0.74893. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63331/0.75110. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63196/0.75328. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62947/0.75566. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62897/0.75774. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62644/0.76036. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62552/0.76280. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62483/0.76510. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62322/0.76733. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.62233/0.76978. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61944/0.77287. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61929/0.77554. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61739/0.77846. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61661/0.78095. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61500/0.78353. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61424/0.78636. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61342/0.78893. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61074/0.79193. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60938/0.79425. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60815/0.79721. Took 0.09 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69507/0.69614. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69458/0.69601. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69465/0.69589. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69423/0.69578. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69397/0.69567. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69395/0.69555. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69345/0.69543. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69324/0.69532. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69338/0.69522. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69324/0.69510. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69277/0.69500. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69278/0.69489. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69229/0.69480. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69196/0.69472. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69178/0.69461. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69141/0.69449. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69108/0.69437. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69048/0.69423. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68976/0.69409. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68944/0.69393. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68865/0.69376. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68804/0.69361. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68747/0.69348. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68649/0.69340. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68558/0.69337. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68487/0.69339. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68417/0.69347. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68353/0.69356. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68292/0.69375. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68252/0.69389. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68191/0.69410. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68103/0.69425. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68088/0.69441. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68037/0.69451. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67987/0.69462. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67966/0.69468. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67926/0.69471. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67876/0.69474. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67826/0.69477. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67785/0.69479. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67758/0.69483. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67731/0.69478. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67684/0.69472. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67650/0.69465. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67610/0.69462. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67543/0.69442. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67563/0.69432. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67519/0.69430. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67487/0.69429. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67408/0.69418. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67373/0.69414. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67340/0.69404. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67324/0.69393. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67244/0.69391. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67195/0.69388. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67186/0.69375. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67116/0.69367. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67090/0.69361. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67020/0.69344. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66975/0.69336. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66948/0.69330. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66877/0.69331. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66785/0.69326. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66755/0.69343. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66718/0.69311. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66683/0.69327. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66573/0.69332. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66559/0.69330. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66473/0.69333. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66411/0.69349. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66299/0.69377. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66230/0.69364. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66178/0.69390. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66110/0.69408. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66087/0.69438. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65945/0.69444. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65871/0.69489. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65804/0.69506. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65710/0.69513. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65659/0.69569. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65590/0.69592. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65517/0.69628. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65349/0.69677. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65328/0.69712. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65222/0.69739. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65181/0.69823. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65036/0.69854. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64930/0.69903. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64885/0.69977. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64795/0.70002. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64658/0.70038. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64646/0.70105. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64457/0.70159. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64375/0.70202. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64305/0.70295. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64273/0.70376. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64088/0.70417. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64011/0.70506. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63872/0.70580. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63764/0.70652. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69366/0.69679. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69631. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69265/0.69601. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69204/0.69583. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69167/0.69577. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69156/0.69580. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69124/0.69590. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.69609. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69077/0.69629. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69059/0.69653. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.69679. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68992/0.69710. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68973/0.69744. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68933/0.69782. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68880/0.69827. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68900/0.69877. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68835/0.69931. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68802/0.69992. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68775/0.70059. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68742/0.70136. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68682/0.70219. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.70306. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68596/0.70402. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68560/0.70505. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68497/0.70614. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68431/0.70729. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68424/0.70847. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68385/0.70968. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68381/0.71084. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68312/0.71199. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68273/0.71313. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68243/0.71425. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68232/0.71529. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68188/0.71625. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68137/0.71722. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68086/0.71819. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68086/0.71906. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68035/0.71989. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68046/0.72067. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68002/0.72133. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67978/0.72197. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67997/0.72255. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67906/0.72319. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67890/0.72375. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67886/0.72429. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67860/0.72483. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67819/0.72530. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67785/0.72574. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67761/0.72615. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67729/0.72658. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67745/0.72695. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67684/0.72733. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67664/0.72766. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67634/0.72798. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67563/0.72831. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67561/0.72870. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67545/0.72909. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67500/0.72939. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67500/0.72968. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67437/0.73003. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67427/0.73023. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67416/0.73044. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67318/0.73061. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67323/0.73092. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67290/0.73115. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67249/0.73138. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67186/0.73160. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67194/0.73176. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67105/0.73202. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67106/0.73218. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67065/0.73228. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67016/0.73244. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67008/0.73263. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66953/0.73274. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66896/0.73292. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66863/0.73306. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66827/0.73329. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66768/0.73345. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66748/0.73360. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66697/0.73374. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66706/0.73380. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66588/0.73387. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66559/0.73404. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66497/0.73423. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66457/0.73430. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66372/0.73448. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66423/0.73471. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66362/0.73469. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66290/0.73482. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66244/0.73499. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66215/0.73503. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66150/0.73516. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66104/0.73522. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66025/0.73550. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65960/0.73549. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65983/0.73561. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65904/0.73568. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65885/0.73583. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65798/0.73617. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65757/0.73632. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70212/0.68948. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69922/0.68863. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69696/0.68812. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69518/0.68800. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69390/0.68819. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69224/0.68847. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69170/0.68873. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69112/0.68890. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69051/0.68902. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69000/0.68910. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68978/0.68916. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68912/0.68917. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68874/0.68916. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68839/0.68916. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68765/0.68912. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68742/0.68909. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68730/0.68905. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68649/0.68901. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68616/0.68897. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68542/0.68892. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68507/0.68888. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68471/0.68884. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68408/0.68882. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68398/0.68876. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68298/0.68872. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68275/0.68870. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68235/0.68865. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68160/0.68861. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68087/0.68855. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68062/0.68849. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68018/0.68842. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67960/0.68836. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67913/0.68825. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67844/0.68817. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67773/0.68807. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67712/0.68798. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67654/0.68786. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67580/0.68772. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67547/0.68759. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67451/0.68748. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67357/0.68735. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67298/0.68721. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67210/0.68706. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67157/0.68692. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67080/0.68675. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66948/0.68664. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66859/0.68648. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66822/0.68637. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66732/0.68626. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66706/0.68620. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66632/0.68610. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66480/0.68599. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66396/0.68595. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66322/0.68586. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66253/0.68578. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66182/0.68574. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66117/0.68575. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66050/0.68572. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65927/0.68575. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65868/0.68567. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65770/0.68572. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65744/0.68581. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65599/0.68583. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65507/0.68582. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65465/0.68599. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65369/0.68607. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65331/0.68618. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65245/0.68637. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65138/0.68657. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65018/0.68672. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64982/0.68694. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64884/0.68717. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64788/0.68743. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64691/0.68765. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64562/0.68800. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64522/0.68833. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64516/0.68848. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64375/0.68903. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64257/0.68926. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64153/0.69000. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64112/0.69040. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63975/0.69081. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63900/0.69131. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63841/0.69183. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63721/0.69204. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63631/0.69286. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63583/0.69343. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63398/0.69396. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63269/0.69465. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63219/0.69520. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63078/0.69608. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62917/0.69701. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62817/0.69735. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62686/0.69866. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62675/0.69955. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62556/0.70050. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62341/0.70145. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62321/0.70283. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62160/0.70380. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61984/0.70468. Took 0.11 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69004/0.70032. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68949/0.70001. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68928/0.69971. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68903/0.69942. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68876/0.69915. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68826/0.69885. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68830/0.69855. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68769/0.69825. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68730/0.69792. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68673/0.69755. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68626/0.69716. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68550/0.69672. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68463/0.69623. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68385/0.69568. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68311/0.69512. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68214/0.69446. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68123/0.69374. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68043/0.69306. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67946/0.69241. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67873/0.69167. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67762/0.69108. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67624/0.69064. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67526/0.68999. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67478/0.68966. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67392/0.68932. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67280/0.68913. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67206/0.68854. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67096/0.68870. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67024/0.68869. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66924/0.68856. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.66844/0.68850. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.66778/0.68854. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66668/0.68888. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66599/0.68915. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66496/0.68921. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66442/0.68977. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66337/0.69010. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66282/0.69085. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66186/0.69078. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66128/0.69150. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66023/0.69178. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65937/0.69211. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65891/0.69265. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65810/0.69300. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.65750/0.69331. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.65730/0.69413. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65541/0.69463. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65519/0.69492. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65368/0.69563. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65346/0.69642. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65328/0.69661. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65211/0.69673. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65196/0.69771. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65017/0.69807. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64953/0.69875. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64964/0.69921. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64864/0.69978. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64710/0.70031. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64684/0.70090. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64548/0.70164. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64516/0.70206. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64338/0.70288. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64322/0.70303. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64258/0.70398. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64213/0.70429. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64071/0.70472. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64031/0.70566. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63856/0.70681. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63831/0.70700. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63748/0.70794. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63623/0.70884. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63508/0.70917. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63440/0.71008. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63291/0.71098. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63192/0.71181. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63046/0.71278. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62993/0.71368. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62908/0.71433. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62814/0.71533. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62745/0.71650. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62524/0.71731. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62538/0.71826. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62391/0.71940. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62317/0.72036. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62182/0.72131. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62058/0.72278. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61937/0.72387. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61916/0.72494. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.61761/0.72612. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61526/0.72708. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61551/0.72841. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61363/0.72958. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61295/0.73071. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61169/0.73153. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61083/0.73263. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60902/0.73377. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60873/0.73476. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60744/0.73571. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60640/0.73663. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60397/0.73819. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69556/0.69503. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69431. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.69416. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69127/0.69437. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69065/0.69469. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69052/0.69503. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68995/0.69535. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69002/0.69566. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68964/0.69596. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68940/0.69628. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68919/0.69664. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68886/0.69701. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68848/0.69739. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68863/0.69781. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68794/0.69831. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68778/0.69882. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.69939. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68678/0.70001. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68665/0.70065. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68614/0.70135. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68581/0.70211. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68568/0.70291. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68470/0.70379. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68459/0.70472. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68439/0.70571. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68382/0.70673. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68353/0.70777. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68319/0.70885. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68262/0.70998. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68261/0.71112. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68194/0.71222. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68171/0.71332. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68121/0.71443. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68107/0.71553. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68028/0.71669. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68034/0.71778. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68025/0.71881. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67979/0.71974. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67986/0.72066. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67903/0.72163. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67884/0.72253. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67863/0.72336. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67823/0.72426. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67786/0.72511. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67784/0.72588. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67707/0.72671. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67710/0.72748. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67683/0.72821. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67607/0.72895. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67569/0.72968. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67614/0.73032. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67539/0.73105. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67516/0.73170. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67504/0.73234. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67462/0.73292. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67409/0.73357. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67402/0.73417. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67344/0.73482. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67272/0.73551. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67260/0.73614. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67252/0.73670. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67204/0.73720. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67206/0.73770. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67131/0.73817. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67115/0.73859. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67009/0.73915. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67006/0.73964. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66953/0.74014. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66966/0.74062. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66925/0.74100. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66833/0.74149. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66813/0.74180. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66763/0.74217. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66731/0.74254. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66680/0.74303. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66634/0.74346. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66571/0.74389. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66547/0.74419. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66464/0.74475. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66445/0.74491. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66365/0.74541. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66328/0.74569. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66327/0.74602. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66222/0.74634. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66237/0.74664. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66076/0.74726. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66157/0.74731. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66084/0.74761. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65936/0.74800. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65947/0.74825. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65907/0.74841. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65871/0.74868. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65783/0.74895. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65767/0.74928. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65682/0.74951. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65646/0.74974. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65563/0.75005. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65496/0.75052. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65490/0.75096. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65385/0.75127. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69180/0.68589. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69160/0.68596. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69122/0.68603. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69095/0.68613. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69079/0.68619. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69105/0.68628. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69084/0.68636. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69050/0.68642. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68983/0.68647. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68970/0.68647. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68958/0.68649. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68907/0.68652. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.68651. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68841/0.68650. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68823/0.68653. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68777/0.68646. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68703/0.68640. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68640/0.68622. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68558/0.68608. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68506/0.68592. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68363/0.68573. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68302/0.68559. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68252/0.68552. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68106/0.68561. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67998/0.68553. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67902/0.68561. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67836/0.68586. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67750/0.68626. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67630/0.68656. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67546/0.68691. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67460/0.68749. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67380/0.68796. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67342/0.68851. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67268/0.68921. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67194/0.68955. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67120/0.69028. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67081/0.69044. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66964/0.69098. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66916/0.69149. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66857/0.69169. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66794/0.69251. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66743/0.69296. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66699/0.69320. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66602/0.69344. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66554/0.69373. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66446/0.69394. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66446/0.69472. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66404/0.69501. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66313/0.69551. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66275/0.69538. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66169/0.69599. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66167/0.69618. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66111/0.69641. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66026/0.69683. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65976/0.69766. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65940/0.69772. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65823/0.69802. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65759/0.69815. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65708/0.69813. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65723/0.69843. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65612/0.69856. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65566/0.69877. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65446/0.69939. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65434/0.70004. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65382/0.69967. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65331/0.70032. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.65251/0.70018. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65137/0.70082. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65058/0.70125. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65098/0.70150. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64997/0.70208. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64944/0.70257. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64850/0.70300. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64792/0.70250. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64723/0.70306. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64599/0.70331. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64579/0.70445. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64522/0.70521. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64456/0.70546. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64352/0.70635. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64325/0.70599. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64201/0.70719. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64166/0.70749. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64118/0.70710. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.64033/0.70817. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64001/0.70979. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63877/0.70988. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63844/0.71123. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63758/0.71146. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63591/0.71225. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63523/0.71225. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63550/0.71317. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63386/0.71444. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63320/0.71552. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63218/0.71550. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63120/0.71702. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63074/0.71714. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62961/0.71825. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62936/0.71996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62902/0.72105. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69000/0.68832. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.68965/0.68841. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68952/0.68852. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68914/0.68862. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68904/0.68873. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68893/0.68886. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68860/0.68900. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68844/0.68914. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68819/0.68929. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68804/0.68947. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68773/0.68966. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68768/0.68985. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68709/0.69006. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68699/0.69027. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68666/0.69050. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68626/0.69075. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68619/0.69102. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68606/0.69130. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68563/0.69161. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68530/0.69191. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68496/0.69223. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68454/0.69254. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68439/0.69285. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68403/0.69318. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68358/0.69350. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68358/0.69378. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68292/0.69407. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68280/0.69437. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68271/0.69461. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68254/0.69481. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68202/0.69500. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68156/0.69516. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.69524. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68100/0.69530. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68099/0.69538. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68018/0.69547. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67967/0.69547. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67963/0.69546. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67928/0.69545. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67860/0.69538. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67838/0.69529. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67834/0.69516. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67751/0.69499. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67728/0.69486. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67694/0.69466. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67654/0.69442. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67561/0.69420. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67560/0.69391. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67461/0.69358. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67444/0.69324. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67370/0.69283. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67300/0.69246. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67262/0.69200. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67196/0.69164. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67193/0.69115. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67083/0.69072. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67028/0.69018. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66934/0.68969. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66870/0.68925. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66851/0.68881. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66779/0.68828. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66694/0.68772. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66586/0.68717. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66551/0.68670. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66475/0.68625. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66390/0.68573. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66274/0.68519. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66209/0.68479. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66150/0.68442. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66099/0.68406. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66029/0.68368. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65963/0.68330. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65841/0.68308. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65784/0.68276. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65728/0.68251. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65680/0.68243. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65554/0.68218. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65491/0.68188. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65428/0.68155. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65359/0.68142. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65237/0.68136. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65225/0.68125. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65157/0.68111. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65038/0.68098. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64954/0.68089. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64966/0.68079. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64827/0.68065. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64748/0.68068. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64755/0.68057. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64626/0.68072. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64566/0.68065. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64510/0.68070. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64435/0.68048. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64298/0.68044. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64290/0.68054. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64196/0.68069. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64097/0.68072. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64092/0.68075. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64022/0.68068. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63925/0.68061. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69264/0.69530. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.69562. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.69592. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.69620. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.69648. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.69676. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69046/0.69707. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69053/0.69740. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69772. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68966/0.69807. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68910/0.69841. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68899/0.69876. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68879/0.69914. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.69955. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68801/0.69997. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.70043. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68750/0.70089. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68701/0.70139. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68660/0.70184. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68616/0.70234. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68568/0.70288. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68513/0.70337. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68474/0.70387. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68487/0.70431. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68411/0.70483. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68395/0.70530. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68330/0.70580. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68281/0.70629. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68227/0.70673. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68187/0.70716. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68111/0.70749. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68062/0.70781. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68038/0.70811. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67960/0.70841. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67916/0.70865. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67831/0.70889. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67820/0.70901. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67720/0.70922. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67696/0.70934. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67630/0.70943. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67595/0.70948. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67496/0.70942. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67478/0.70942. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67424/0.70928. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67353/0.70924. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67294/0.70913. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67240/0.70916. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67205/0.70887. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67164/0.70888. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67092/0.70868. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67026/0.70863. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.66956/0.70853. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66889/0.70813. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66840/0.70809. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.66812/0.70763. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66752/0.70743. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66701/0.70730. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66637/0.70687. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66588/0.70678. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66526/0.70681. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66489/0.70658. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66472/0.70646. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66444/0.70601. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66332/0.70593. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66327/0.70581. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66248/0.70591. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66182/0.70580. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66205/0.70526. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66108/0.70535. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66050/0.70501. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66019/0.70516. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66002/0.70515. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65861/0.70516. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65846/0.70530. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65849/0.70527. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65764/0.70522. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65773/0.70529. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65688/0.70520. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65625/0.70530. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65593/0.70534. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65550/0.70541. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65497/0.70566. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65449/0.70573. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65426/0.70556. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65346/0.70552. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65368/0.70552. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65250/0.70590. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65274/0.70611. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65180/0.70591. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65069/0.70642. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65041/0.70651. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65032/0.70683. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64986/0.70674. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64883/0.70707. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64835/0.70742. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64791/0.70780. Took 0.08 sec\n",
      "Epoch 96, Loss(train/val) 0.64671/0.70796. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64619/0.70846. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64618/0.70878. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.64621/0.70918. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69096/0.68820. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69071/0.68732. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69007/0.68670. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68973/0.68629. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68968/0.68602. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68949/0.68585. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68903/0.68571. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68884/0.68565. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68855/0.68566. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68834/0.68569. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68831/0.68568. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68804/0.68573. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68772/0.68576. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68736/0.68581. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.68584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68686/0.68592. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68683/0.68595. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68622/0.68605. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68596/0.68613. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68585/0.68620. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68531/0.68628. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68494/0.68634. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68500/0.68645. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68449/0.68648. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68414/0.68654. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68398/0.68656. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68312/0.68667. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68346/0.68675. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68294/0.68690. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68283/0.68699. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68220/0.68714. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68166/0.68719. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68169/0.68725. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68100/0.68739. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68070/0.68758. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68040/0.68777. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67992/0.68788. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67934/0.68795. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67944/0.68813. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67845/0.68830. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67820/0.68842. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67762/0.68858. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67696/0.68868. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67674/0.68877. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67603/0.68907. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67546/0.68918. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67493/0.68940. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67440/0.68948. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67367/0.68971. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67340/0.68985. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67235/0.68978. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67225/0.69000. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67184/0.69019. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67080/0.69032. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66992/0.69046. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66921/0.69058. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66881/0.69073. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66773/0.69094. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66743/0.69108. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66724/0.69136. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66649/0.69156. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66490/0.69173. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66469/0.69191. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66399/0.69210. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.66395/0.69233. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66255/0.69245. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66272/0.69273. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66207/0.69287. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66101/0.69317. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66025/0.69336. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65970/0.69367. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66005/0.69381. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65923/0.69424. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65956/0.69446. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65770/0.69476. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65764/0.69511. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65703/0.69566. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65701/0.69611. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65643/0.69659. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65530/0.69669. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65498/0.69729. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65497/0.69773. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65493/0.69827. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65452/0.69862. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65399/0.69903. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65394/0.69946. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65324/0.70012. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65223/0.70061. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65273/0.70102. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65160/0.70135. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65139/0.70188. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65120/0.70233. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65041/0.70295. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65020/0.70355. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64943/0.70410. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64910/0.70469. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64866/0.70532. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64828/0.70580. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64836/0.70626. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64769/0.70672. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69139/0.69186. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69112/0.69171. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69090/0.69157. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69080/0.69143. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69061/0.69129. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69066/0.69116. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.69102. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.69087. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69025/0.69073. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69002/0.69057. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68982/0.69040. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68967/0.69021. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68957/0.69001. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68934/0.68981. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68899/0.68958. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68880/0.68932. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68840/0.68903. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68816/0.68872. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68795/0.68839. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68763/0.68800. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68711/0.68753. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68650/0.68697. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68608/0.68633. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68541/0.68563. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68460/0.68482. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68355/0.68388. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68294/0.68285. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68161/0.68170. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68044/0.68053. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67929/0.67940. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67783/0.67818. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67622/0.67705. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67494/0.67602. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67349/0.67514. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67196/0.67434. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67141/0.67375. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67042/0.67337. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66875/0.67304. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66785/0.67279. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66686/0.67267. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66601/0.67260. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66571/0.67275. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66438/0.67285. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66329/0.67297. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66306/0.67314. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66201/0.67339. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66148/0.67361. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66059/0.67392. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66016/0.67425. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65931/0.67459. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65827/0.67493. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65817/0.67527. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65704/0.67560. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.65692/0.67591. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65606/0.67632. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65497/0.67663. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65424/0.67694. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.65383/0.67730. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65276/0.67778. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65261/0.67825. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65206/0.67861. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65121/0.67908. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65065/0.67970. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65008/0.68020. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64895/0.68075. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64822/0.68116. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64747/0.68167. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64622/0.68222. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64603/0.68267. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64586/0.68318. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64577/0.68384. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64425/0.68444. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64339/0.68504. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64311/0.68556. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64175/0.68606. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64207/0.68680. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64041/0.68736. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64074/0.68832. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63887/0.68875. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63852/0.68916. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63724/0.68983. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63722/0.69051. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63663/0.69111. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63603/0.69191. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63487/0.69260. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63441/0.69319. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63320/0.69380. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.63268/0.69472. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63155/0.69544. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63073/0.69629. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63010/0.69655. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62916/0.69738. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62863/0.69820. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62722/0.69898. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62649/0.69989. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62583/0.70074. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62476/0.70119. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62354/0.70200. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62198/0.70269. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62180/0.70368. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69324/0.69247. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69316/0.69246. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69248. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69253. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69263. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69203/0.69280. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69307. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69349. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68941/0.69402. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68887/0.69456. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68809/0.69501. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68778/0.69534. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68777/0.69559. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68731/0.69578. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68729/0.69588. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68707/0.69598. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68695/0.69601. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68667/0.69599. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68653/0.69596. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68639/0.69590. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68610/0.69581. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68565/0.69574. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68548/0.69567. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68543/0.69557. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68528/0.69544. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68487/0.69529. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68479/0.69523. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68447/0.69510. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68418/0.69501. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68411/0.69491. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68346/0.69478. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68304/0.69464. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68261/0.69457. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68208/0.69433. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68175/0.69420. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68169/0.69403. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68092/0.69388. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68074/0.69373. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68001/0.69351. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67979/0.69349. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67896/0.69330. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67848/0.69300. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67796/0.69298. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67757/0.69297. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67689/0.69272. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67618/0.69252. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67614/0.69260. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67504/0.69259. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67441/0.69246. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67360/0.69232. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67321/0.69247. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67255/0.69243. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67170/0.69248. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67128/0.69262. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67037/0.69258. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66973/0.69278. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66910/0.69290. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66828/0.69300. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66767/0.69301. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66692/0.69334. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66610/0.69351. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66557/0.69367. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66444/0.69412. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66424/0.69413. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66391/0.69431. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66262/0.69479. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66188/0.69467. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66113/0.69530. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66053/0.69553. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66002/0.69609. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65937/0.69615. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65818/0.69639. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65800/0.69709. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65664/0.69698. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65586/0.69771. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65524/0.69785. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65471/0.69896. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65407/0.69897. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65386/0.69983. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65199/0.69991. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65099/0.70046. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65029/0.70133. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65000/0.70215. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64910/0.70284. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64845/0.70344. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64687/0.70411. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64666/0.70471. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64556/0.70576. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64475/0.70624. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64418/0.70693. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64284/0.70726. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64244/0.70840. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64184/0.70861. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64031/0.71005. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63951/0.71053. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63846/0.71184. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63775/0.71232. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63718/0.71285. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63688/0.71402. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63547/0.71566. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69637/0.69267. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69508/0.69139. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69410/0.69022. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69316/0.68917. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69243/0.68820. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69174/0.68736. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69103/0.68664. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69071/0.68604. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69042/0.68554. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68982/0.68516. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68961/0.68489. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68924/0.68468. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.68451. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68863/0.68442. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68855/0.68437. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68812/0.68433. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68801/0.68431. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68781/0.68429. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68734/0.68431. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68698/0.68433. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68676/0.68436. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68659/0.68442. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68592/0.68448. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68594/0.68453. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68536/0.68461. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68497/0.68472. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68441/0.68486. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68375/0.68501. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68318/0.68523. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68250/0.68550. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68208/0.68579. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68111/0.68617. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68026/0.68660. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67936/0.68713. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67851/0.68777. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67749/0.68840. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67644/0.68910. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67514/0.68987. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67439/0.69063. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67323/0.69150. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67208/0.69238. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67068/0.69324. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66977/0.69416. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66892/0.69506. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66778/0.69584. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66726/0.69663. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66570/0.69741. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66525/0.69823. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66388/0.69890. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66311/0.69960. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66257/0.70030. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66073/0.70105. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66020/0.70185. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65919/0.70254. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65861/0.70318. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65756/0.70384. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65710/0.70440. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65605/0.70502. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65565/0.70565. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65469/0.70630. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65405/0.70710. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65359/0.70776. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65217/0.70855. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65120/0.70917. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65058/0.70998. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64886/0.71081. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64872/0.71153. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64845/0.71219. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64692/0.71300. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64697/0.71378. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64612/0.71456. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64532/0.71522. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64413/0.71600. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64381/0.71672. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64262/0.71761. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64272/0.71850. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.64151/0.71922. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64057/0.72022. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63998/0.72088. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63818/0.72176. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63839/0.72251. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63780/0.72359. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63591/0.72448. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63559/0.72549. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63557/0.72649. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63480/0.72730. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63312/0.72822. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63270/0.72924. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63213/0.73020. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63144/0.73124. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63048/0.73212. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62958/0.73281. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62753/0.73421. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62761/0.73542. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62651/0.73595. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62622/0.73641. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62489/0.73789. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62388/0.73896. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62325/0.73998. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62307/0.74073. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69427/0.70100. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69386/0.70091. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69387/0.70082. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69393/0.70072. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69364/0.70062. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69338/0.70049. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69336/0.70036. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69328/0.70018. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69281/0.69997. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69261/0.69971. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69255/0.69949. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69212/0.69923. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69199/0.69896. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69155/0.69874. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69132/0.69863. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69090/0.69850. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69033/0.69844. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69020/0.69848. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68959/0.69876. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68931/0.69896. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68861/0.69930. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68824/0.69968. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68781/0.69980. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68742/0.70016. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68663/0.70065. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68618/0.70115. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68551/0.70162. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68517/0.70207. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68437/0.70263. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68419/0.70295. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68348/0.70348. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68283/0.70404. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68240/0.70452. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68145/0.70519. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68104/0.70582. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68004/0.70637. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67964/0.70704. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67885/0.70780. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67832/0.70822. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67737/0.70902. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67668/0.70980. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67616/0.71042. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67534/0.71097. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67465/0.71179. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67400/0.71240. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67311/0.71291. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67252/0.71378. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67143/0.71422. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67111/0.71480. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67050/0.71563. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66963/0.71618. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66924/0.71676. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66852/0.71745. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66731/0.71777. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66680/0.71836. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66640/0.71910. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66579/0.71976. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66507/0.72028. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66459/0.72097. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66392/0.72124. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66311/0.72203. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66270/0.72252. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66182/0.72324. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66088/0.72422. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66066/0.72471. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65985/0.72511. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65917/0.72583. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.65839/0.72690. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65811/0.72787. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65678/0.72874. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65596/0.72927. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65547/0.73050. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65511/0.73075. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65391/0.73199. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65293/0.73244. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65215/0.73412. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65199/0.73462. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65042/0.73554. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65048/0.73633. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64924/0.73751. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64831/0.73920. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64773/0.74013. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64647/0.74108. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64619/0.74194. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64547/0.74358. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64423/0.74430. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64348/0.74569. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64337/0.74669. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64225/0.74821. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64157/0.74979. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64041/0.75095. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63890/0.75190. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63808/0.75345. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63775/0.75540. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63676/0.75636. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63656/0.75668. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63493/0.75889. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63379/0.76019. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63341/0.76206. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63178/0.76409. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69517/0.70262. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69460/0.70184. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69445/0.70130. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69362/0.70089. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69371/0.70060. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69362/0.70037. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69346/0.70024. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69300/0.70019. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69271/0.70014. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69236/0.70012. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69202/0.70017. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69197/0.70032. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69204/0.70053. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69160/0.70080. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69122/0.70102. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69101/0.70131. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69102/0.70165. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69049/0.70202. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69023/0.70243. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69016/0.70279. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68928/0.70322. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68916/0.70371. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68906/0.70412. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68877/0.70458. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68833/0.70511. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68818/0.70554. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68788/0.70607. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68775/0.70653. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68715/0.70696. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68695/0.70745. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68691/0.70788. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68638/0.70830. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68609/0.70877. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68568/0.70931. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68565/0.70969. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68538/0.71008. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68511/0.71056. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68473/0.71096. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68465/0.71136. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68423/0.71172. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68378/0.71213. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68355/0.71255. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68365/0.71290. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68309/0.71311. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68273/0.71354. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68235/0.71391. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68233/0.71436. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68192/0.71460. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68151/0.71486. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68159/0.71513. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68114/0.71540. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68105/0.71568. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68082/0.71595. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68046/0.71629. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67994/0.71637. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67960/0.71658. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67927/0.71693. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67922/0.71721. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67855/0.71743. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67844/0.71752. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67787/0.71783. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67798/0.71816. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67726/0.71832. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67709/0.71828. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67705/0.71875. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67655/0.71903. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67588/0.71926. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67563/0.71941. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67520/0.71966. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67499/0.71989. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67431/0.72007. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67415/0.72019. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67388/0.72028. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67347/0.72054. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67345/0.72074. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67273/0.72103. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67195/0.72112. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67151/0.72109. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67195/0.72154. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67071/0.72156. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67045/0.72204. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67019/0.72219. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66918/0.72223. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66927/0.72258. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66901/0.72303. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66843/0.72294. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66742/0.72290. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66741/0.72299. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66687/0.72325. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66654/0.72325. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66582/0.72391. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66519/0.72401. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66459/0.72383. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66454/0.72452. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66396/0.72437. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66324/0.72487. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66296/0.72487. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66178/0.72498. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66215/0.72493. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66100/0.72524. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69913/0.69488. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69883/0.69449. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69813/0.69419. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69736/0.69396. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69714/0.69378. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69704/0.69361. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69646/0.69345. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69635/0.69331. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69597/0.69316. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69549/0.69298. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69490/0.69277. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69479/0.69254. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69439/0.69229. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69406/0.69200. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69341/0.69173. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69271/0.69145. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69217/0.69121. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69183/0.69103. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69143/0.69088. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69071/0.69076. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69014/0.69071. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68960/0.69074. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68895/0.69076. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68822/0.69087. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68733/0.69101. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68671/0.69131. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68553/0.69170. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68491/0.69217. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68428/0.69273. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68306/0.69340. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68227/0.69421. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68152/0.69508. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68036/0.69605. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67974/0.69706. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67905/0.69807. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67844/0.69900. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67749/0.69994. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67680/0.70088. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.70175. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67567/0.70252. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67465/0.70328. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67388/0.70404. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67375/0.70474. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67263/0.70539. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67193/0.70613. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67103/0.70674. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67067/0.70733. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66980/0.70784. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66949/0.70820. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66844/0.70874. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66807/0.70923. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66724/0.70966. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66591/0.71006. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66545/0.71035. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66487/0.71048. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66391/0.71081. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66243/0.71104. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66262/0.71133. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66136/0.71134. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66075/0.71173. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65965/0.71210. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65938/0.71210. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65777/0.71218. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65681/0.71235. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65564/0.71264. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65475/0.71296. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65395/0.71305. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65310/0.71321. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65203/0.71336. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65059/0.71374. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65008/0.71383. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64928/0.71458. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64799/0.71500. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64747/0.71547. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64586/0.71552. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64537/0.71611. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64474/0.71623. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64396/0.71635. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64250/0.71682. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64202/0.71753. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64038/0.71829. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63963/0.71839. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63886/0.71908. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63767/0.71975. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63693/0.72012. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63625/0.72102. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63457/0.72146. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63386/0.72236. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63337/0.72270. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63108/0.72371. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63031/0.72437. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63012/0.72546. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62919/0.72606. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62688/0.72701. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62640/0.72759. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62619/0.72807. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62532/0.72915. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62396/0.72995. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62342/0.73087. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62150/0.73179. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69426/0.68554. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.68576. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69351/0.68593. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69298/0.68603. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69256/0.68613. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69269/0.68614. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69234/0.68612. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69172/0.68601. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69142/0.68583. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69137/0.68552. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69057/0.68511. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69026/0.68458. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68980/0.68387. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68913/0.68304. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68858/0.68193. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68780/0.68074. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68706/0.67944. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68676/0.67814. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68544/0.67686. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68486/0.67577. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68424/0.67468. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68355/0.67386. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68264/0.67332. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68207/0.67303. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68115/0.67312. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68072/0.67325. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67967/0.67371. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67912/0.67423. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67868/0.67490. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67761/0.67578. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67714/0.67667. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67669/0.67773. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67623/0.67885. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67546/0.67991. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67480/0.68097. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67396/0.68225. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67384/0.68331. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67307/0.68447. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67264/0.68552. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67217/0.68660. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67139/0.68744. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67148/0.68840. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67087/0.68933. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67041/0.69005. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67000/0.69082. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66930/0.69160. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66925/0.69233. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66862/0.69301. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66800/0.69359. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66765/0.69405. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66705/0.69489. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66675/0.69568. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66659/0.69611. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66616/0.69661. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66500/0.69716. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66493/0.69788. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66470/0.69830. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66458/0.69863. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66385/0.69941. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66293/0.69986. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66296/0.70034. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66214/0.70099. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66172/0.70173. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66162/0.70208. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66046/0.70293. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66046/0.70336. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66003/0.70374. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65954/0.70435. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65837/0.70489. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65796/0.70510. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65749/0.70595. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65681/0.70667. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65659/0.70703. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65576/0.70764. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65560/0.70802. Took 0.12 sec\n",
      "Epoch 75, Loss(train/val) 0.65529/0.70840. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65453/0.70932. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65373/0.71023. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65275/0.71087. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65331/0.71096. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65208/0.71090. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65189/0.71138. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65082/0.71182. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64993/0.71209. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64943/0.71261. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64873/0.71334. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64810/0.71378. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64762/0.71426. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64749/0.71455. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64644/0.71477. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64621/0.71573. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64447/0.71610. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64450/0.71687. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64378/0.71752. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64258/0.71796. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64255/0.71865. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64117/0.71898. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64043/0.71952. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63937/0.72031. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63914/0.72113. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69364/0.69205. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69375/0.69203. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69350/0.69202. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69352/0.69201. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69333/0.69199. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69367/0.69197. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69314/0.69196. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69306/0.69194. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69295/0.69191. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69293/0.69187. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69283/0.69183. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69295/0.69179. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69253/0.69175. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69256/0.69170. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69236/0.69164. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69226/0.69158. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69224/0.69151. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69203/0.69142. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69162/0.69133. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69155/0.69125. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69142/0.69116. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69099/0.69108. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69105/0.69099. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69053/0.69091. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69018/0.69083. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68998/0.69079. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68969/0.69073. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68938/0.69070. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68880/0.69069. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68852/0.69070. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68805/0.69071. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68770/0.69077. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68736/0.69087. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68658/0.69093. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68591/0.69103. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68538/0.69123. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68498/0.69138. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68412/0.69155. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68328/0.69189. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68261/0.69206. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68204/0.69235. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68133/0.69266. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68044/0.69308. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67943/0.69347. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67860/0.69390. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67798/0.69428. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67725/0.69479. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67598/0.69523. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67565/0.69560. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67424/0.69610. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67335/0.69658. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67239/0.69690. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67191/0.69717. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67069/0.69754. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66984/0.69796. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66884/0.69842. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66801/0.69870. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66686/0.69940. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66571/0.69982. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66570/0.70012. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66488/0.70043. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66314/0.70091. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66197/0.70150. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66114/0.70193. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66023/0.70299. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65926/0.70295. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65779/0.70422. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65770/0.70454. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65654/0.70537. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65532/0.70619. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65399/0.70670. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65405/0.70751. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65262/0.70873. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65178/0.70923. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65103/0.70988. Took 0.12 sec\n",
      "Epoch 75, Loss(train/val) 0.64939/0.71087. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64854/0.71181. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64843/0.71265. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64762/0.71382. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64626/0.71469. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64571/0.71524. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64436/0.71655. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64325/0.71758. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64271/0.71848. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64206/0.71936. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64124/0.72054. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63945/0.72156. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63904/0.72202. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63855/0.72300. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63813/0.72369. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63588/0.72470. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63542/0.72571. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63448/0.72661. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63360/0.72750. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63191/0.72843. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63159/0.72899. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63064/0.72980. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63041/0.73068. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62916/0.73191. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62851/0.73240. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69153/0.69569. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69125/0.69592. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.69618. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69104/0.69642. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69061/0.69666. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.69691. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.69718. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69018/0.69747. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69775. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68951/0.69806. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68963/0.69840. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.69875. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68913/0.69913. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68900/0.69951. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68843/0.69992. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68838/0.70031. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68854/0.70070. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68802/0.70112. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68798/0.70156. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68747/0.70199. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68722/0.70248. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68706/0.70294. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68644/0.70342. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68631/0.70393. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68634/0.70439. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68585/0.70487. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68539/0.70534. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68534/0.70587. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68455/0.70636. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68447/0.70685. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68474/0.70732. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68392/0.70776. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68395/0.70821. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68364/0.70864. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68304/0.70909. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68279/0.70954. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68255/0.70998. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68199/0.71040. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68184/0.71087. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68141/0.71130. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68106/0.71171. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68087/0.71208. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68035/0.71257. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68019/0.71302. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67983/0.71335. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67944/0.71383. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67893/0.71421. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67852/0.71452. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67794/0.71491. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67757/0.71535. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67764/0.71568. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67673/0.71606. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67667/0.71644. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67572/0.71687. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67549/0.71724. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67500/0.71765. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67467/0.71803. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67413/0.71842. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67361/0.71884. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67302/0.71922. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67244/0.71970. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67195/0.72019. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67141/0.72065. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67104/0.72113. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67023/0.72148. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67011/0.72196. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66904/0.72257. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66872/0.72296. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66815/0.72358. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66725/0.72417. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66716/0.72463. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66687/0.72520. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66592/0.72587. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66560/0.72639. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66494/0.72702. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66424/0.72753. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66368/0.72812. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66302/0.72880. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66205/0.72938. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66254/0.73006. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66173/0.73067. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66092/0.73123. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66041/0.73188. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66013/0.73275. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66007/0.73349. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65908/0.73413. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65839/0.73487. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65858/0.73543. Took 0.12 sec\n",
      "Epoch 88, Loss(train/val) 0.65762/0.73602. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65668/0.73682. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65700/0.73765. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65652/0.73833. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65586/0.73867. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65600/0.73942. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65487/0.74036. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65471/0.74104. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65444/0.74161. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65342/0.74251. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65331/0.74322. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65317/0.74361. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69561/0.69912. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69498/0.69881. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69498/0.69852. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69460/0.69827. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69452/0.69799. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69412/0.69772. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69387/0.69742. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69360/0.69713. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69336/0.69678. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69318/0.69645. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69287/0.69615. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69271/0.69585. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69246/0.69562. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69233/0.69546. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69180/0.69536. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69180/0.69538. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69115/0.69548. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69084/0.69562. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69073/0.69585. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69003/0.69619. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68962/0.69665. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68971/0.69711. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68913/0.69771. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68864/0.69823. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68811/0.69900. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68770/0.69988. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68695/0.70084. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68670/0.70179. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68594/0.70291. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68550/0.70420. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68466/0.70548. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68409/0.70683. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68311/0.70853. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68262/0.71006. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68163/0.71201. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68089/0.71393. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68038/0.71570. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67939/0.71772. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67888/0.71957. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67805/0.72158. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67747/0.72368. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67608/0.72570. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67530/0.72777. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67474/0.72952. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67406/0.73145. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67301/0.73325. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67238/0.73551. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67150/0.73703. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67118/0.73892. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67050/0.74093. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66944/0.74293. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66843/0.74456. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66785/0.74637. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66708/0.74793. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66646/0.74981. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66587/0.75140. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66466/0.75332. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66389/0.75504. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66314/0.75671. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66231/0.75848. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66178/0.76030. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66101/0.76193. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65959/0.76382. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65901/0.76555. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65784/0.76734. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65747/0.76965. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65608/0.77129. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65503/0.77331. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65440/0.77496. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65301/0.77723. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65231/0.77835. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65177/0.78138. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65135/0.78301. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64965/0.78535. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64913/0.78769. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64701/0.78993. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64645/0.79186. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64627/0.79396. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64482/0.79644. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64414/0.79864. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64332/0.80067. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64178/0.80349. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64110/0.80534. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64006/0.80818. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63850/0.81024. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63762/0.81232. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63703/0.81515. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63620/0.81692. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63483/0.81945. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63378/0.82191. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63388/0.82413. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63217/0.82691. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63116/0.82869. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63030/0.83159. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62892/0.83355. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62806/0.83603. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62629/0.83866. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62532/0.84033. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62412/0.84304. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62431/0.84573. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69682/0.69292. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69543/0.69160. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69420/0.69064. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69336/0.68999. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69281/0.68955. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69226/0.68925. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69148/0.68905. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69131/0.68890. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69138/0.68878. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69063/0.68866. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69059/0.68854. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69049/0.68842. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69002/0.68828. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68981/0.68813. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68929/0.68796. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68904/0.68778. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68881/0.68760. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68850/0.68740. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68832/0.68718. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68778/0.68694. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68749/0.68669. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68675/0.68641. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68657/0.68612. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68617/0.68581. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68563/0.68550. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68519/0.68519. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68445/0.68490. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68415/0.68457. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68345/0.68424. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68324/0.68392. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68238/0.68358. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68225/0.68331. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68180/0.68309. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68122/0.68290. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68022/0.68272. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68002/0.68265. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67933/0.68258. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67868/0.68247. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67846/0.68252. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67778/0.68254. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67727/0.68261. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67669/0.68269. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67611/0.68290. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67523/0.68322. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67477/0.68355. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67417/0.68389. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67325/0.68422. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67293/0.68470. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67238/0.68528. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67137/0.68586. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67074/0.68661. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67025/0.68735. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66982/0.68799. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66873/0.68878. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66844/0.68952. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66777/0.69048. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66710/0.69132. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66623/0.69236. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66524/0.69349. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66510/0.69446. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66428/0.69555. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66387/0.69664. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66275/0.69775. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66244/0.69874. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66198/0.69986. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66105/0.70098. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66028/0.70226. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66013/0.70336. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65958/0.70458. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65884/0.70566. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65830/0.70694. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65767/0.70811. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65754/0.70932. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65688/0.71051. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65571/0.71172. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65568/0.71277. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65534/0.71404. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65473/0.71518. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65436/0.71606. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65339/0.71734. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65308/0.71841. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65279/0.71960. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65264/0.72049. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65217/0.72145. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65092/0.72273. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65104/0.72357. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65024/0.72464. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64964/0.72592. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64968/0.72695. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64881/0.72793. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64845/0.72870. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64809/0.72992. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64694/0.73106. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64689/0.73176. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64642/0.73283. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64611/0.73388. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64540/0.73480. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64543/0.73559. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64519/0.73632. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64412/0.73726. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69831/0.69818. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69690/0.69783. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69667/0.69756. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69618/0.69731. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69553/0.69706. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69523/0.69683. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69497/0.69658. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69513/0.69635. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69453/0.69612. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69429/0.69588. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69396/0.69565. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69392/0.69540. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69352/0.69517. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69322/0.69492. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69296/0.69468. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69277/0.69442. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69228/0.69416. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69231/0.69390. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69197/0.69363. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69176/0.69336. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69176/0.69310. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69136/0.69283. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69059/0.69259. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69074/0.69232. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69045/0.69205. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69008/0.69177. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.69006/0.69148. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68953/0.69118. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68943/0.69089. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68951/0.69060. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68884/0.69031. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68896/0.69001. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68885/0.68973. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68814/0.68946. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68803/0.68916. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68802/0.68892. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68722/0.68866. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68690/0.68841. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68692/0.68820. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68631/0.68799. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68618/0.68782. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68551/0.68768. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68538/0.68758. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68500/0.68749. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68464/0.68742. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.68451/0.68739. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68401/0.68737. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68332/0.68743. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68344/0.68748. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68292/0.68757. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68237/0.68767. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.68243/0.68782. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68151/0.68807. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68112/0.68828. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68083/0.68850. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68029/0.68875. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67985/0.68901. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67957/0.68936. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67888/0.68972. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67833/0.69016. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67824/0.69062. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67796/0.69113. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67689/0.69168. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67658/0.69223. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67583/0.69291. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67574/0.69358. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67458/0.69423. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67371/0.69490. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67347/0.69560. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67299/0.69634. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67192/0.69711. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67133/0.69794. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67075/0.69876. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67067/0.69964. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66968/0.70044. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66943/0.70123. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66822/0.70210. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66745/0.70291. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66658/0.70377. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66614/0.70475. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66518/0.70586. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66538/0.70680. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66433/0.70771. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66312/0.70871. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66296/0.70963. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66218/0.71062. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66163/0.71138. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66079/0.71229. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66006/0.71325. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65943/0.71411. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65893/0.71494. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65812/0.71588. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65825/0.71671. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65689/0.71760. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65665/0.71843. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65509/0.71933. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65474/0.72010. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65412/0.72102. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65299/0.72177. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65320/0.72273. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69493/0.70369. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69463/0.70303. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69435/0.70245. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69414/0.70197. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69383/0.70154. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69376/0.70107. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69349/0.70065. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69319/0.70025. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69300/0.69990. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69249/0.69960. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69229/0.69933. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69238/0.69916. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69173/0.69909. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69148/0.69911. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69121/0.69927. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69083/0.69966. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69044/0.70017. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68996/0.70078. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68970/0.70160. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68933/0.70261. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68821/0.70367. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68777/0.70471. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68734/0.70581. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68661/0.70704. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68614/0.70798. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68573/0.70906. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68513/0.71031. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68444/0.71136. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68366/0.71250. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68321/0.71365. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68233/0.71462. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68180/0.71575. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68147/0.71682. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68065/0.71803. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67971/0.71889. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67959/0.71971. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67869/0.72068. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67838/0.72153. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67768/0.72227. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67751/0.72322. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67681/0.72351. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67584/0.72450. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67547/0.72474. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67499/0.72534. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67412/0.72591. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67377/0.72655. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67311/0.72722. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67210/0.72724. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67150/0.72800. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67090/0.72807. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67054/0.72866. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66944/0.72873. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66920/0.72877. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66794/0.72929. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66735/0.72960. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66670/0.72953. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66597/0.72968. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66527/0.72925. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66437/0.72927. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66355/0.72933. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66316/0.72927. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66210/0.72888. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66122/0.72894. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66084/0.72891. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65966/0.72857. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65881/0.72815. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65778/0.72781. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65654/0.72785. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65575/0.72702. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65470/0.72733. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65392/0.72626. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65280/0.72626. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65228/0.72579. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65071/0.72544. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64981/0.72513. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64880/0.72444. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64801/0.72487. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64701/0.72332. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64602/0.72379. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64442/0.72242. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64390/0.72335. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64218/0.72206. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.64198/0.72256. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64073/0.72278. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63892/0.72225. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63849/0.72212. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63812/0.72259. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63647/0.72258. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63569/0.72226. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63454/0.72245. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63280/0.72315. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63294/0.72286. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63153/0.72351. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62998/0.72408. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62895/0.72407. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62772/0.72429. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62754/0.72470. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62694/0.72543. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62576/0.72567. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62401/0.72620. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69494/0.69593. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69459/0.69571. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69469/0.69543. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69435/0.69514. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69412/0.69480. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69372/0.69440. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69333/0.69398. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69268/0.69352. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69244/0.69306. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69171/0.69262. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69149/0.69223. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69138/0.69191. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69085/0.69164. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69071/0.69141. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69000/0.69121. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69013/0.69105. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68963/0.69090. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68987/0.69076. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68883/0.69066. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68869/0.69055. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68839/0.69045. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68772/0.69035. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68754/0.69026. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68702/0.69019. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68640/0.69012. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68594/0.69008. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68573/0.69003. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68519/0.69000. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68481/0.68996. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68420/0.68992. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68384/0.68990. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68342/0.68988. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68286/0.68986. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68273/0.68989. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68194/0.68992. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68135/0.69000. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68026/0.69011. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68038/0.69025. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67976/0.69042. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67905/0.69060. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67847/0.69081. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67779/0.69103. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67741/0.69133. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67689/0.69165. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67608/0.69203. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67573/0.69242. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67476/0.69283. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67389/0.69326. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67363/0.69371. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67294/0.69423. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67194/0.69478. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67188/0.69536. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67067/0.69584. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66987/0.69637. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66961/0.69683. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66903/0.69736. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66759/0.69788. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66753/0.69847. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66666/0.69913. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66598/0.69970. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66557/0.70035. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66472/0.70086. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66382/0.70146. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66307/0.70207. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66254/0.70273. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66121/0.70342. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66146/0.70420. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66071/0.70483. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65919/0.70554. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65845/0.70642. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65774/0.70741. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65662/0.70842. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65638/0.70930. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65500/0.71015. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65377/0.71133. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65356/0.71246. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65271/0.71356. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65221/0.71480. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65104/0.71598. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64983/0.71758. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64915/0.71882. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64729/0.72038. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64617/0.72170. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64564/0.72328. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64439/0.72507. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64371/0.72660. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64205/0.72821. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64150/0.72994. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64055/0.73136. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63935/0.73300. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63844/0.73478. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63754/0.73637. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63625/0.73781. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63508/0.73929. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63380/0.74122. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63359/0.74320. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63249/0.74500. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63187/0.74634. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63022/0.74862. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62994/0.75006. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69256/0.69003. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69243/0.69012. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69238/0.69021. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69230/0.69030. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69217/0.69036. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69191/0.69044. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69206/0.69050. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69191/0.69055. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69174/0.69059. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.69065. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69136/0.69071. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69102/0.69076. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69107/0.69079. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69108/0.69082. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69048/0.69084. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69046/0.69084. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69028/0.69086. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69008/0.69090. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68975/0.69090. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68935/0.69091. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68902/0.69096. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68864/0.69099. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68825/0.69109. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68785/0.69113. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68714/0.69126. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68703/0.69142. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68629/0.69168. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68585/0.69188. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68549/0.69216. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68477/0.69248. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68431/0.69277. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68334/0.69308. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68299/0.69360. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68269/0.69411. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68140/0.69451. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68072/0.69497. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68026/0.69559. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68005/0.69633. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.67943/0.69700. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67822/0.69764. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67751/0.69843. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67732/0.69902. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67690/0.69994. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67598/0.70061. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67528/0.70141. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67454/0.70229. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67418/0.70316. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67359/0.70400. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67273/0.70479. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67259/0.70562. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67230/0.70642. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67131/0.70735. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67085/0.70830. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66992/0.70918. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67025/0.71006. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66897/0.71080. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66887/0.71182. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66858/0.71267. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66764/0.71344. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66711/0.71434. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66626/0.71523. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66585/0.71597. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66525/0.71680. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66458/0.71766. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66460/0.71850. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66369/0.71927. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66328/0.71996. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66244/0.72060. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66240/0.72133. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66137/0.72225. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66062/0.72290. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66037/0.72378. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66010/0.72461. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65961/0.72523. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65966/0.72601. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65844/0.72671. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65759/0.72758. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65755/0.72829. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65682/0.72907. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65669/0.72955. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65586/0.73041. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65580/0.73109. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65439/0.73196. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65438/0.73288. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65347/0.73348. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65310/0.73429. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65221/0.73509. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65196/0.73585. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65131/0.73673. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65136/0.73754. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65031/0.73816. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65054/0.73898. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64922/0.73970. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64879/0.74042. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64821/0.74115. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64787/0.74224. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64652/0.74353. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64619/0.74442. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64587/0.74526. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64556/0.74598. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69522/0.69042. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.68795. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69195/0.68619. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69109/0.68505. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69090/0.68442. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69067/0.68410. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69077/0.68391. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69055/0.68381. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69051/0.68373. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69049/0.68367. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69026/0.68361. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68997/0.68356. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69013/0.68351. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69017/0.68346. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68987/0.68342. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68959/0.68336. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68978/0.68330. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68962/0.68327. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68931/0.68322. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68957/0.68317. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68916/0.68312. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68916/0.68302. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68907/0.68295. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68922/0.68289. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68894/0.68284. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68899/0.68279. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68857/0.68273. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68875/0.68266. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68838/0.68261. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68806/0.68253. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68810/0.68247. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68808/0.68241. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68776/0.68235. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68754/0.68229. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68771/0.68222. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68762/0.68217. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68750/0.68213. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68749/0.68207. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68716/0.68204. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68720/0.68200. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68659/0.68197. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68653/0.68194. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68663/0.68191. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.68623/0.68185. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68618/0.68179. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68612/0.68175. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68545/0.68171. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68562/0.68169. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68568/0.68170. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68526/0.68170. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68506/0.68165. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68480/0.68164. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68446/0.68160. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68425/0.68157. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68433/0.68156. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68401/0.68154. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68372/0.68155. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68355/0.68158. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68320/0.68161. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.68275/0.68163. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68240/0.68169. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68216/0.68173. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68225/0.68179. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68187/0.68186. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.68202/0.68195. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.68160/0.68204. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68124/0.68207. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.68112/0.68216. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68042/0.68224. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68011/0.68229. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.68001/0.68240. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67964/0.68256. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67916/0.68272. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67898/0.68286. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67847/0.68300. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67810/0.68312. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67792/0.68325. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67769/0.68345. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67728/0.68357. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67678/0.68381. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67699/0.68407. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67567/0.68421. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67590/0.68443. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.67515/0.68469. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67465/0.68499. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67445/0.68520. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67396/0.68550. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67337/0.68575. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67314/0.68612. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67251/0.68637. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.67190/0.68671. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67130/0.68707. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67077/0.68748. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66999/0.68789. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66947/0.68830. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66909/0.68852. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66837/0.68899. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66832/0.68956. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66716/0.69002. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66632/0.69046. Took 0.09 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69115/0.68609. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69064/0.68584. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69068/0.68559. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69024/0.68536. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69023/0.68513. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.68494. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68979/0.68476. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68951/0.68457. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68930/0.68443. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.68430. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68913/0.68416. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68878/0.68408. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68842/0.68402. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.68396. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68794/0.68390. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68776/0.68387. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68727/0.68385. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68725/0.68385. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68664/0.68389. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68673/0.68389. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68616/0.68394. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68591/0.68395. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68542/0.68412. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68500/0.68429. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68499/0.68436. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68471/0.68449. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68454/0.68467. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68428/0.68484. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68399/0.68510. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68354/0.68528. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68341/0.68536. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68316/0.68567. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68289/0.68597. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68252/0.68618. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68229/0.68644. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68194/0.68677. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68164/0.68704. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68085/0.68732. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68088/0.68764. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68060/0.68794. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68045/0.68815. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67996/0.68840. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67953/0.68872. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67911/0.68909. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67875/0.68941. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67871/0.68968. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67792/0.68992. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67790/0.69035. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67740/0.69041. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67691/0.69080. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67671/0.69100. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67623/0.69126. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67564/0.69170. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67509/0.69182. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67448/0.69213. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67407/0.69247. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67368/0.69286. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67312/0.69310. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67266/0.69332. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67236/0.69368. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67159/0.69407. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67166/0.69435. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67065/0.69467. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67053/0.69484. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66950/0.69535. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66920/0.69574. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66901/0.69619. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66807/0.69620. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66764/0.69679. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66701/0.69705. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66616/0.69747. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66588/0.69788. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66569/0.69832. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66483/0.69868. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66416/0.69927. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66277/0.69948. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66304/0.69986. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66223/0.70053. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66108/0.70075. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66060/0.70133. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66002/0.70183. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65976/0.70204. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65895/0.70222. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65844/0.70266. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65729/0.70280. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65706/0.70371. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65561/0.70429. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65545/0.70465. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65451/0.70483. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65334/0.70554. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65269/0.70604. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65210/0.70656. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65082/0.70720. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64997/0.70735. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64940/0.70819. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64874/0.70854. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64733/0.70905. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64700/0.70952. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64704/0.71053. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64485/0.71040. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69610/0.69359. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69429/0.69260. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69193. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.69161. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69081/0.69153. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69022/0.69157. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68979/0.69160. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.69162. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68949/0.69160. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68906/0.69154. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68897/0.69152. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68872/0.69150. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68858/0.69150. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.69146. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68796/0.69144. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68781/0.69143. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68754/0.69140. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68744/0.69139. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68713/0.69139. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68698/0.69140. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68682/0.69141. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.69141. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68640/0.69144. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68595/0.69147. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68551/0.69151. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68543/0.69156. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68522/0.69162. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68478/0.69169. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68434/0.69177. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68410/0.69185. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68421/0.69196. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68376/0.69204. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68319/0.69220. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68313/0.69233. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68264/0.69250. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68225/0.69263. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68224/0.69276. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68124/0.69289. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68138/0.69304. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68120/0.69320. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68050/0.69338. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67984/0.69356. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67977/0.69367. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67987/0.69385. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67902/0.69403. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67880/0.69415. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67879/0.69429. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67797/0.69452. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67747/0.69467. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67715/0.69481. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67692/0.69496. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67622/0.69509. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67623/0.69523. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67578/0.69538. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67508/0.69555. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67478/0.69569. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67440/0.69588. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67413/0.69605. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67401/0.69618. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67323/0.69636. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67255/0.69639. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67174/0.69654. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67183/0.69678. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67092/0.69687. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67066/0.69709. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66972/0.69728. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66918/0.69754. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66876/0.69775. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66794/0.69785. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66790/0.69806. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66689/0.69841. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66569/0.69861. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66556/0.69869. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66510/0.69898. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66509/0.69913. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66396/0.69938. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66375/0.69981. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66278/0.70039. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66142/0.70064. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66149/0.70098. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66082/0.70113. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65919/0.70146. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65875/0.70194. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65817/0.70223. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65809/0.70269. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65724/0.70347. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65646/0.70397. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65576/0.70431. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65463/0.70473. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65431/0.70538. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65429/0.70620. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65286/0.70680. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65168/0.70743. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65143/0.70802. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65137/0.70880. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64956/0.70952. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65028/0.71037. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64857/0.71089. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64803/0.71134. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64777/0.71191. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69582/0.69444. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69510/0.69400. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69448/0.69374. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69345/0.69363. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69290/0.69362. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69241/0.69376. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69178/0.69398. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69147/0.69427. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69466. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69038/0.69508. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69021/0.69547. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68985/0.69587. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68954/0.69625. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68911/0.69664. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68904/0.69696. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68860/0.69729. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68848/0.69760. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68838/0.69790. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68785/0.69821. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68765/0.69854. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68750/0.69891. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68700/0.69924. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68687/0.69957. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68635/0.69988. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68635/0.70018. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68610/0.70048. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68524/0.70079. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68502/0.70090. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68446/0.70104. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68431/0.70117. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68411/0.70127. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68342/0.70130. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68328/0.70133. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68296/0.70129. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68268/0.70107. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68235/0.70075. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68169/0.70040. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68143/0.70000. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68105/0.69947. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68062/0.69883. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68017/0.69810. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67956/0.69756. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67900/0.69692. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67885/0.69624. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67816/0.69537. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67745/0.69463. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67705/0.69397. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67639/0.69314. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67603/0.69223. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67571/0.69129. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67505/0.69038. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67383/0.68951. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67368/0.68859. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67327/0.68775. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.67198/0.68690. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67228/0.68585. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67093/0.68490. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67053/0.68393. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66990/0.68299. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66874/0.68213. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66838/0.68140. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66708/0.68043. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66668/0.67953. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66597/0.67894. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66502/0.67799. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66423/0.67716. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66315/0.67624. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66302/0.67566. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66148/0.67492. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66056/0.67440. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66035/0.67380. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65949/0.67288. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65854/0.67251. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65765/0.67226. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65667/0.67166. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65595/0.67135. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65497/0.67123. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65447/0.67123. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65302/0.67105. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65201/0.67084. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65092/0.67077. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65024/0.67037. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64965/0.67062. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.64828/0.67013. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64770/0.67023. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64711/0.67090. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64606/0.67039. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64441/0.67050. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64370/0.67117. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64268/0.67070. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.64246/0.67105. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64085/0.67138. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64039/0.67165. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63918/0.67241. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63778/0.67233. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63743/0.67237. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63681/0.67312. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63585/0.67357. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63408/0.67454. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63381/0.67426. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69135/0.69638. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69083/0.69626. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69047/0.69617. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69020/0.69610. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68999/0.69605. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68966/0.69600. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68942/0.69597. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68920/0.69594. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68933/0.69592. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68889/0.69590. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68859/0.69589. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68846/0.69590. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68817/0.69591. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68815/0.69593. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68793/0.69596. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68776/0.69600. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68788/0.69606. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68768/0.69610. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68743/0.69616. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68719/0.69622. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68680/0.69629. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68679/0.69637. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68651/0.69648. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68637/0.69658. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68655/0.69667. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68634/0.69676. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68618/0.69686. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68597/0.69693. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68562/0.69701. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68549/0.69709. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68542/0.69718. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68529/0.69726. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68518/0.69736. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68540/0.69745. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68480/0.69753. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68462/0.69763. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68435/0.69772. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68440/0.69778. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68400/0.69785. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68385/0.69793. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68360/0.69802. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68379/0.69809. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68358/0.69815. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68319/0.69822. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68298/0.69829. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68295/0.69837. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68255/0.69845. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68257/0.69850. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68236/0.69857. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68191/0.69863. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68153/0.69869. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68145/0.69877. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68128/0.69884. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.68094/0.69894. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68096/0.69901. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68055/0.69905. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.68013/0.69909. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67961/0.69918. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67942/0.69920. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67895/0.69926. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67845/0.69931. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67827/0.69937. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67830/0.69941. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67736/0.69949. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67732/0.69952. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67680/0.69953. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67617/0.69959. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67585/0.69967. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67524/0.69973. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67489/0.69980. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67428/0.69986. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67354/0.69992. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67341/0.70001. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67259/0.70011. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67224/0.70014. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67171/0.70020. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67111/0.70035. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67049/0.70043. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66990/0.70061. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66949/0.70082. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66850/0.70089. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66785/0.70106. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66756/0.70139. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66683/0.70158. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.66636/0.70188. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66588/0.70207. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66490/0.70242. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66472/0.70284. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66289/0.70314. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66309/0.70352. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.66189/0.70391. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66180/0.70431. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66057/0.70485. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65976/0.70528. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65994/0.70573. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65850/0.70610. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65815/0.70659. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65760/0.70711. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65703/0.70764. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65710/0.70810. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69669/0.69575. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69599/0.69562. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69533/0.69543. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69453/0.69518. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69361/0.69484. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69271/0.69453. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69160/0.69437. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69077/0.69437. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69001/0.69453. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68972/0.69477. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68931/0.69503. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68892/0.69530. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68864/0.69557. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68842/0.69584. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68815/0.69612. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68810/0.69641. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68765/0.69668. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.69698. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68734/0.69731. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68675/0.69766. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68684/0.69801. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.68642/0.69839. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68588/0.69880. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68577/0.69925. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68567/0.69969. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68522/0.70016. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68487/0.70072. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68469/0.70128. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68424/0.70186. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68382/0.70244. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68314/0.70303. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68290/0.70368. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68254/0.70435. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68232/0.70500. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68143/0.70567. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68112/0.70634. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68020/0.70709. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67998/0.70780. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67871/0.70853. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67844/0.70919. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67751/0.70988. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67664/0.71064. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67615/0.71126. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67550/0.71198. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67482/0.71267. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67398/0.71339. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67302/0.71423. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67227/0.71495. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67146/0.71569. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67087/0.71644. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67007/0.71721. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66907/0.71804. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66847/0.71888. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66792/0.71953. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66730/0.72030. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66680/0.72113. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66564/0.72184. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66510/0.72249. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66406/0.72347. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66241/0.72442. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66265/0.72495. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66189/0.72560. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66035/0.72642. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65978/0.72723. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65882/0.72800. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65816/0.72882. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65741/0.72961. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65633/0.73010. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65572/0.73081. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65529/0.73149. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65441/0.73220. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65304/0.73263. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65298/0.73314. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65250/0.73363. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65039/0.73426. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65037/0.73460. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64948/0.73489. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64809/0.73548. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64751/0.73601. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64683/0.73621. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64560/0.73680. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64471/0.73706. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64371/0.73741. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64250/0.73783. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64236/0.73835. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64113/0.73882. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64087/0.73913. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63919/0.73921. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63917/0.73940. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63797/0.73958. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63552/0.74005. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63606/0.74038. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63491/0.74021. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63276/0.74048. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63228/0.74086. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63113/0.74099. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63030/0.74120. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62933/0.74136. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62870/0.74132. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62720/0.74158. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69505/0.69687. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69498/0.69660. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69452/0.69630. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69443/0.69594. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69447/0.69552. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69410/0.69497. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69344/0.69428. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69322/0.69329. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69233/0.69207. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69152/0.69076. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69073/0.68954. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.68862. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68916/0.68802. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68870/0.68768. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68831/0.68752. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68797/0.68749. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68757/0.68748. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68727/0.68751. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68704/0.68758. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68658/0.68774. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68595/0.68788. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68556/0.68806. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68536/0.68831. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68502/0.68855. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68431/0.68882. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68367/0.68912. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68322/0.68942. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68242/0.68991. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68205/0.69030. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68134/0.69066. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68095/0.69110. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67993/0.69176. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67917/0.69224. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67880/0.69272. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67802/0.69320. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67715/0.69374. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67657/0.69410. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67587/0.69441. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67537/0.69500. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67475/0.69532. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67326/0.69587. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67252/0.69602. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67175/0.69646. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67161/0.69696. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67010/0.69750. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66969/0.69770. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66889/0.69820. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66770/0.69834. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66727/0.69877. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66537/0.69945. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66502/0.69984. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66440/0.70034. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66357/0.70090. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66239/0.70146. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66142/0.70198. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66066/0.70281. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65933/0.70331. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65958/0.70361. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65851/0.70443. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65792/0.70552. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65692/0.70588. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65645/0.70657. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65457/0.70790. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65421/0.70848. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65356/0.70936. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65265/0.71040. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65215/0.71167. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65081/0.71233. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64877/0.71337. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64965/0.71461. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.64733/0.71550. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64755/0.71685. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64714/0.71772. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64645/0.71896. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64498/0.71987. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64388/0.72091. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64307/0.72266. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64250/0.72353. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64090/0.72534. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64094/0.72628. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63964/0.72744. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63955/0.72847. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63786/0.73068. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63758/0.73187. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63626/0.73286. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63520/0.73401. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63468/0.73610. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63333/0.73785. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63283/0.73901. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63326/0.74020. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63112/0.74156. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63078/0.74412. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63058/0.74478. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62959/0.74636. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62833/0.74870. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62778/0.75069. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62591/0.75130. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.62584/0.75337. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62424/0.75419. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62317/0.75688. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69579/0.69488. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69395/0.69331. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69249/0.69212. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69147/0.69136. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69043/0.69100. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68999/0.69089. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68933/0.69093. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68915/0.69103. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68885/0.69115. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68863/0.69133. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68815/0.69154. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68788/0.69175. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68769/0.69197. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68712/0.69224. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68689/0.69251. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68658/0.69278. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68613/0.69310. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68604/0.69344. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68568/0.69379. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68541/0.69414. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.69453. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68444/0.69492. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68443/0.69531. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68388/0.69572. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68366/0.69617. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68323/0.69662. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68256/0.69707. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68230/0.69756. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68219/0.69797. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68150/0.69838. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68116/0.69886. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68099/0.69926. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68036/0.69968. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67973/0.70013. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67980/0.70051. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67910/0.70085. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67883/0.70126. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67853/0.70164. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67837/0.70195. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67753/0.70231. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67716/0.70265. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67682/0.70307. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67637/0.70332. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67585/0.70365. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67524/0.70389. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67457/0.70413. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67449/0.70446. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67370/0.70464. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67326/0.70493. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67286/0.70515. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67228/0.70531. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67116/0.70564. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67071/0.70586. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67064/0.70590. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66943/0.70629. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66926/0.70636. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66902/0.70656. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66817/0.70673. Took 0.13 sec\n",
      "Epoch 58, Loss(train/val) 0.66674/0.70685. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66685/0.70705. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66616/0.70697. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66543/0.70713. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66369/0.70733. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66370/0.70749. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66327/0.70744. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.66244/0.70758. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66181/0.70740. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66077/0.70747. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65906/0.70763. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65887/0.70763. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65769/0.70778. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.65676/0.70788. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65627/0.70779. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65475/0.70783. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65377/0.70797. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65267/0.70809. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65198/0.70800. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65077/0.70824. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64932/0.70878. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64804/0.70913. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64841/0.70908. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64775/0.70927. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64592/0.70971. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64490/0.71041. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64269/0.71102. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64294/0.71150. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64204/0.71224. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64048/0.71274. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63917/0.71356. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63887/0.71412. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63733/0.71486. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63691/0.71550. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63542/0.71589. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63511/0.71655. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63318/0.71759. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63336/0.71813. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63239/0.71883. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63110/0.71967. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.62999/0.72058. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62846/0.72137. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69501/0.69351. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69450/0.69334. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69443/0.69320. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69418/0.69306. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69395/0.69294. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69366/0.69283. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69369/0.69270. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69338/0.69259. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69340/0.69245. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69327/0.69233. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69322/0.69221. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69291/0.69209. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69284/0.69197. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69305/0.69182. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69286/0.69168. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69254/0.69155. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69254/0.69140. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69233/0.69124. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69206/0.69107. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69202/0.69089. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69182/0.69069. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69153/0.69047. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69152/0.69022. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69125/0.68998. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69063/0.68970. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69062/0.68939. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68995/0.68906. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68992/0.68869. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68924/0.68830. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68917/0.68788. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68863/0.68742. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68797/0.68694. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68764/0.68645. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68698/0.68596. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68619/0.68544. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68588/0.68490. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68555/0.68441. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68452/0.68389. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68388/0.68339. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68327/0.68293. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68275/0.68248. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68214/0.68204. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68171/0.68167. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68119/0.68133. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68042/0.68104. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67980/0.68078. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67910/0.68053. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67866/0.68033. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67835/0.68018. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67780/0.67995. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67719/0.67982. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67667/0.67962. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67644/0.67951. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67580/0.67936. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67531/0.67924. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67467/0.67907. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67425/0.67898. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67389/0.67882. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67362/0.67871. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67277/0.67862. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67215/0.67851. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67187/0.67840. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67115/0.67826. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67077/0.67828. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67023/0.67813. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66932/0.67806. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66885/0.67801. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66868/0.67775. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66774/0.67779. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66731/0.67775. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66678/0.67779. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66595/0.67761. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66515/0.67746. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66451/0.67746. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66383/0.67727. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66351/0.67727. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66271/0.67722. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66176/0.67707. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66152/0.67692. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66053/0.67684. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65977/0.67670. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65861/0.67667. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65874/0.67645. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65833/0.67644. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65751/0.67612. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65637/0.67603. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65575/0.67601. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.65476/0.67577. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65378/0.67560. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65347/0.67563. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65221/0.67529. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65187/0.67521. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65123/0.67505. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65083/0.67484. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64951/0.67462. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64900/0.67454. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64846/0.67441. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64765/0.67421. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64649/0.67386. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64560/0.67357. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69324/0.69825. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69299/0.69804. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69283/0.69789. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69266/0.69775. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69264/0.69762. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69219/0.69752. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69242/0.69744. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69191/0.69739. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69209/0.69734. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69160/0.69731. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69156/0.69728. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69123/0.69728. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69119/0.69733. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69081/0.69739. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69050/0.69744. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69024/0.69755. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69019/0.69770. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68996/0.69785. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68924/0.69799. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68921/0.69821. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68871/0.69844. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68838/0.69867. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68794/0.69890. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68772/0.69920. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68750/0.69957. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68699/0.69987. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68616/0.70020. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68591/0.70058. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68557/0.70094. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68481/0.70134. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68440/0.70158. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68430/0.70199. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68338/0.70236. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68293/0.70284. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68204/0.70345. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68165/0.70380. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68129/0.70435. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68059/0.70461. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67982/0.70524. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67912/0.70564. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67836/0.70612. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67747/0.70651. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67679/0.70689. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67678/0.70761. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67527/0.70758. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67515/0.70820. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67445/0.70873. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67336/0.70887. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67300/0.70955. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67223/0.70963. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67143/0.70971. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67059/0.71007. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66949/0.71016. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66842/0.71045. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66805/0.71097. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66759/0.71102. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66627/0.71139. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66602/0.71168. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66479/0.71212. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66391/0.71221. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66267/0.71279. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66199/0.71293. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66133/0.71332. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66030/0.71382. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65894/0.71412. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65811/0.71476. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65688/0.71488. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65616/0.71531. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65458/0.71651. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65367/0.71638. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65284/0.71750. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65202/0.71724. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65085/0.71881. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64934/0.71899. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64893/0.72001. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64741/0.72112. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64601/0.72115. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64523/0.72205. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64395/0.72323. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64212/0.72352. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64239/0.72506. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64020/0.72649. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63968/0.72648. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63910/0.72786. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63729/0.72902. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63659/0.72918. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63605/0.72969. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63423/0.73152. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63365/0.73264. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63240/0.73307. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63051/0.73416. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62921/0.73500. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62926/0.73557. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62890/0.73609. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62635/0.73722. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62592/0.73799. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62432/0.73947. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62344/0.73938. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.62230/0.73976. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62147/0.73995. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69817/0.69615. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69641/0.69440. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69554/0.69316. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69462/0.69232. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69380/0.69180. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69350/0.69149. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69342/0.69141. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69300/0.69143. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69289/0.69149. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69210/0.69158. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69219/0.69174. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69197/0.69191. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69174/0.69206. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69134/0.69223. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69110/0.69245. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69094/0.69264. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69034/0.69285. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69055/0.69308. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69032/0.69330. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.69025/0.69350. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69003/0.69374. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68957/0.69398. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68899/0.69427. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68904/0.69456. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68884/0.69485. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68829/0.69519. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68801/0.69551. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68794/0.69587. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68773/0.69624. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68730/0.69666. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68718/0.69703. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68694/0.69742. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68677/0.69782. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68605/0.69824. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68576/0.69869. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68540/0.69915. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68478/0.69960. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68450/0.70016. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68432/0.70075. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68388/0.70127. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68351/0.70177. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68299/0.70241. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68277/0.70300. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68205/0.70371. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68192/0.70440. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68121/0.70511. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68118/0.70585. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68032/0.70663. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67977/0.70746. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67905/0.70831. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67863/0.70922. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67801/0.71007. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67750/0.71084. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67733/0.71165. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67652/0.71259. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67576/0.71351. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67532/0.71443. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67500/0.71539. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67429/0.71632. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67301/0.71742. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67256/0.71843. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67246/0.71923. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67138/0.72021. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67140/0.72105. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66971/0.72231. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66921/0.72326. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66928/0.72408. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66782/0.72500. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66746/0.72584. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66665/0.72677. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66578/0.72760. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66548/0.72851. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66450/0.72942. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66335/0.73031. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66285/0.73117. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66199/0.73228. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66067/0.73334. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66033/0.73410. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66003/0.73519. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65907/0.73574. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65800/0.73680. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65773/0.73783. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65695/0.73872. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65495/0.73958. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65439/0.74016. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65431/0.74139. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65301/0.74204. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65197/0.74282. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65131/0.74399. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64996/0.74479. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64908/0.74620. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64861/0.74718. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64705/0.74821. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.64650/0.74918. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64597/0.74981. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64401/0.75086. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64379/0.75229. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64227/0.75323. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64128/0.75472. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64067/0.75574. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70430/0.69664. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.70086/0.69383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69808/0.69133. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69582/0.68943. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69426/0.68828. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69325/0.68765. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69311/0.68727. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69224/0.68702. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69198/0.68681. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69182/0.68663. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69141/0.68646. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69124/0.68628. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69099/0.68611. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69053/0.68595. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69036/0.68578. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.68560. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69011/0.68543. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68971/0.68526. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68964/0.68511. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68899/0.68496. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68873/0.68480. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68852/0.68465. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68839/0.68453. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68845/0.68441. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68801/0.68429. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68735/0.68416. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68740/0.68404. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68711/0.68392. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68674/0.68381. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68669/0.68372. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68664/0.68362. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68625/0.68355. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68593/0.68348. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68568/0.68337. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68550/0.68328. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68514/0.68320. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68500/0.68312. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68439/0.68304. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68456/0.68294. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68422/0.68287. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68390/0.68278. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68381/0.68269. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68334/0.68262. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68287/0.68250. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68268/0.68236. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68220/0.68225. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68184/0.68215. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68113/0.68203. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68125/0.68194. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68062/0.68180. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68044/0.68167. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67994/0.68154. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67936/0.68139. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67875/0.68127. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67855/0.68113. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67831/0.68101. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67741/0.68082. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67676/0.68069. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67624/0.68054. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67594/0.68036. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67550/0.68020. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67448/0.68005. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67373/0.67993. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67301/0.67982. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67240/0.67972. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67140/0.67965. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67094/0.67959. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66945/0.67955. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66887/0.67945. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66807/0.67947. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66680/0.67955. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66615/0.67959. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66568/0.67974. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66346/0.67990. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66311/0.68010. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66224/0.68036. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.66091/0.68068. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65977/0.68098. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65867/0.68146. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65819/0.68199. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65680/0.68256. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65488/0.68304. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65474/0.68363. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65345/0.68427. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65273/0.68483. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65156/0.68557. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65043/0.68626. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64938/0.68702. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64849/0.68793. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64804/0.68866. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64609/0.68952. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64608/0.69029. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64422/0.69117. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64353/0.69194. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64219/0.69277. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64161/0.69352. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64059/0.69430. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63999/0.69510. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63902/0.69604. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63789/0.69682. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70099/0.68799. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.70094/0.68819. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.70007/0.68844. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69929/0.68874. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69805/0.68918. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69657/0.68982. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69458/0.69073. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69286/0.69181. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69169/0.69287. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69106/0.69372. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69068/0.69434. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69019/0.69474. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69001/0.69500. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68958/0.69520. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68966/0.69532. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68910/0.69536. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68864/0.69538. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68867/0.69541. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68816/0.69540. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68781/0.69542. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68779/0.69539. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68727/0.69540. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68697/0.69545. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68654/0.69553. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68619/0.69564. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68577/0.69574. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68563/0.69586. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68497/0.69600. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68473/0.69615. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68447/0.69637. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68387/0.69659. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68385/0.69673. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68359/0.69698. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68308/0.69717. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68273/0.69737. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68247/0.69751. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68233/0.69775. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68181/0.69789. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68151/0.69808. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68100/0.69817. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68067/0.69833. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68060/0.69854. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68015/0.69863. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67964/0.69887. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67954/0.69895. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67913/0.69907. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67870/0.69921. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67846/0.69922. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67779/0.69938. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67780/0.69951. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67738/0.69968. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67696/0.69980. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67629/0.69998. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67599/0.70009. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67559/0.70022. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67491/0.70039. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67463/0.70041. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67374/0.70068. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67347/0.70091. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67263/0.70109. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67257/0.70132. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67173/0.70149. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.67134/0.70159. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67029/0.70189. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67077/0.70229. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66954/0.70235. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66910/0.70269. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66832/0.70273. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66732/0.70309. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66706/0.70328. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66588/0.70388. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66538/0.70420. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66491/0.70427. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66403/0.70468. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66341/0.70526. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66288/0.70531. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66196/0.70599. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66094/0.70589. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65997/0.70654. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65925/0.70702. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65811/0.70735. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65749/0.70772. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65654/0.70784. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65572/0.70769. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65498/0.70802. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65349/0.70835. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65281/0.70900. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65175/0.70942. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64994/0.70961. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65033/0.71059. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64844/0.71121. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64743/0.71055. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64626/0.71154. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64546/0.71222. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64384/0.71267. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64339/0.71309. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64182/0.71324. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64226/0.71484. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64108/0.71442. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63898/0.71517. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69711/0.69667. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69630/0.69604. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69546/0.69541. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69450/0.69476. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69381/0.69419. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69297/0.69375. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69263/0.69341. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69191/0.69318. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69133/0.69302. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69120/0.69290. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.69281. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69069/0.69273. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69053/0.69267. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69016/0.69262. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69011/0.69259. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68965/0.69259. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68954/0.69259. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68933/0.69259. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68901/0.69259. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68891/0.69258. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68841/0.69254. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68826/0.69255. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68811/0.69256. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68770/0.69260. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68740/0.69264. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68693/0.69268. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68656/0.69276. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68599/0.69283. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68575/0.69290. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68492/0.69298. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68494/0.69307. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68423/0.69317. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68331/0.69328. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68261/0.69339. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68186/0.69359. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68114/0.69375. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68065/0.69394. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68000/0.69407. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67927/0.69424. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67809/0.69445. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67733/0.69461. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67650/0.69485. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67582/0.69514. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67468/0.69546. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67414/0.69576. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67344/0.69611. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67291/0.69632. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67211/0.69677. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67145/0.69710. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67062/0.69751. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66989/0.69785. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66916/0.69826. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66863/0.69857. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66813/0.69907. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66716/0.69930. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66677/0.69975. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66609/0.70037. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66517/0.70076. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66428/0.70124. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66418/0.70180. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66305/0.70236. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66224/0.70283. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66168/0.70356. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66100/0.70415. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66054/0.70468. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65959/0.70530. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65857/0.70588. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65784/0.70668. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65659/0.70743. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65603/0.70789. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65529/0.70860. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65439/0.70946. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65329/0.71033. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65253/0.71113. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65139/0.71219. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65071/0.71298. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64985/0.71390. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64876/0.71464. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64792/0.71576. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64719/0.71660. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64565/0.71764. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64431/0.71873. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.64317/0.71981. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64268/0.72081. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64149/0.72184. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63974/0.72302. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63821/0.72418. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63699/0.72528. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63592/0.72648. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63494/0.72766. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63306/0.72893. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63243/0.73054. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63013/0.73175. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62863/0.73297. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62746/0.73452. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62630/0.73611. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62446/0.73769. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62278/0.73944. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62119/0.74101. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61969/0.74276. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69561/0.70056. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69526/0.70067. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69457/0.70073. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69410/0.70065. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69309/0.70052. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69244/0.70036. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69147/0.70028. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69080/0.70032. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68977/0.70061. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68892/0.70113. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68875/0.70185. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68816/0.70269. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68776/0.70366. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68709/0.70470. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68647/0.70584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68575/0.70707. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68514/0.70842. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68466/0.70971. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68418/0.71104. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68310/0.71246. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68302/0.71385. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68178/0.71531. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68159/0.71687. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68087/0.71835. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68023/0.71977. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67967/0.72121. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67903/0.72265. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67855/0.72404. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67783/0.72538. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67743/0.72665. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67689/0.72795. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67637/0.72911. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67599/0.73024. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67524/0.73147. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67470/0.73251. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67397/0.73376. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67383/0.73481. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67313/0.73586. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67265/0.73681. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67205/0.73765. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67122/0.73876. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67077/0.73980. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67016/0.74083. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66948/0.74175. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66933/0.74271. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66858/0.74363. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66780/0.74453. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66726/0.74549. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66672/0.74626. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66600/0.74719. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66513/0.74825. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66430/0.74919. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66359/0.75009. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66309/0.75100. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66227/0.75172. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66135/0.75272. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66029/0.75357. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66018/0.75461. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65884/0.75539. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65791/0.75642. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65729/0.75718. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65686/0.75823. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65550/0.75897. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65557/0.75983. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65468/0.76056. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65287/0.76161. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65194/0.76239. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65157/0.76327. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65079/0.76403. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64994/0.76489. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64856/0.76572. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64728/0.76683. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64631/0.76776. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64572/0.76837. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64419/0.76962. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64311/0.77020. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64255/0.77126. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64101/0.77214. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64016/0.77319. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63930/0.77419. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63805/0.77515. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63709/0.77618. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63640/0.77723. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63578/0.77854. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63381/0.77958. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63303/0.78052. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63169/0.78149. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63111/0.78251. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63016/0.78412. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62843/0.78518. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62875/0.78590. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62623/0.78675. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62598/0.78810. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62517/0.78933. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62395/0.79026. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62325/0.79169. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62171/0.79292. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62119/0.79408. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62118/0.79519. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61895/0.79645. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69249/0.68878. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.68876. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.68872. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.68867. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69149/0.68867. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69157/0.68865. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.68864. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69106/0.68863. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69045/0.68860. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69054/0.68858. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69050/0.68861. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68992/0.68864. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68965/0.68870. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.68876. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68850/0.68886. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68838/0.68900. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68758/0.68915. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68687/0.68930. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68645/0.68952. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68584/0.68982. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68491/0.69023. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68394/0.69077. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68316/0.69131. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68233/0.69206. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68140/0.69277. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68045/0.69361. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67892/0.69460. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67839/0.69560. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67752/0.69661. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67616/0.69775. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67508/0.69886. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67447/0.70018. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67329/0.70135. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67264/0.70261. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67125/0.70399. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67057/0.70529. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66937/0.70673. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66801/0.70813. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66710/0.70967. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66639/0.71117. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66466/0.71268. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66387/0.71422. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66267/0.71584. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66121/0.71751. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65983/0.71923. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65895/0.72093. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65782/0.72264. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65646/0.72431. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65551/0.72607. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65432/0.72778. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65228/0.72957. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65148/0.73148. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65001/0.73310. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64894/0.73496. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64773/0.73675. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64612/0.73813. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64584/0.73995. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64384/0.74146. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64248/0.74344. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64160/0.74507. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64015/0.74696. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63883/0.74852. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63775/0.75004. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63629/0.75175. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63522/0.75319. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63399/0.75476. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63253/0.75607. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63184/0.75782. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63054/0.75901. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62943/0.76041. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62775/0.76190. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62662/0.76317. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62585/0.76464. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62472/0.76590. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62230/0.76726. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62154/0.76814. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62071/0.77015. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61946/0.77163. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.61756/0.77284. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61687/0.77442. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61554/0.77576. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61444/0.77756. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61330/0.77917. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61153/0.78053. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61054/0.78208. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60966/0.78361. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60815/0.78534. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.60611/0.78669. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60592/0.78798. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60347/0.78993. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60271/0.79149. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60099/0.79309. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59977/0.79536. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59812/0.79713. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59685/0.79880. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59471/0.80075. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59414/0.80279. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59238/0.80507. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59008/0.80685. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.58964/0.80870. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69425/0.69180. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69351/0.69175. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69282/0.69177. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69281/0.69180. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69275/0.69187. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69221/0.69194. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69204/0.69202. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69206/0.69211. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69174/0.69222. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.69232. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69113/0.69242. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69109/0.69252. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69087/0.69260. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69048/0.69272. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69019/0.69285. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69028/0.69299. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68970/0.69314. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68940/0.69330. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68912/0.69349. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68845/0.69370. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68805/0.69394. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68755/0.69426. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68749/0.69460. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68651/0.69497. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68574/0.69540. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68545/0.69587. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68488/0.69641. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68406/0.69696. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68364/0.69753. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68289/0.69812. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68228/0.69872. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68140/0.69931. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.69991. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68075/0.70043. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67981/0.70088. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67940/0.70133. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67863/0.70171. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67845/0.70205. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67774/0.70250. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67714/0.70267. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67634/0.70284. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67586/0.70296. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67502/0.70316. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67451/0.70328. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67425/0.70340. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67347/0.70348. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67305/0.70367. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67249/0.70374. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67243/0.70376. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67099/0.70382. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67014/0.70395. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66941/0.70401. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66964/0.70415. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66913/0.70403. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66847/0.70419. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66737/0.70427. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66708/0.70449. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66688/0.70460. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66609/0.70462. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66493/0.70470. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66456/0.70494. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66420/0.70514. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66356/0.70543. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66263/0.70577. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66183/0.70613. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66141/0.70651. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66020/0.70685. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65994/0.70717. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65958/0.70769. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65841/0.70815. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65769/0.70862. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65723/0.70920. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65562/0.70963. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65513/0.71019. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65452/0.71093. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65337/0.71158. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65334/0.71238. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65251/0.71290. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65138/0.71367. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65067/0.71421. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64984/0.71518. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64898/0.71637. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64779/0.71709. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64656/0.71816. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64638/0.71905. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64471/0.71994. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64385/0.72084. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64255/0.72201. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64239/0.72314. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64093/0.72414. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63978/0.72530. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63864/0.72626. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63838/0.72723. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63792/0.72837. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63691/0.72924. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63546/0.73064. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63396/0.73204. Took 0.08 sec\n",
      "Epoch 97, Loss(train/val) 0.63395/0.73252. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63238/0.73354. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63135/0.73438. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69438. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69383/0.69417. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69349/0.69403. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69350/0.69394. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69308/0.69387. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69300/0.69382. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69278/0.69381. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69277/0.69381. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69272/0.69384. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69243/0.69388. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69212/0.69396. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69161/0.69407. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69176/0.69419. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69162/0.69434. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69119/0.69451. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69061/0.69470. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69029/0.69495. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68995/0.69521. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68969/0.69551. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68895/0.69588. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68878/0.69632. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68809/0.69679. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68756/0.69737. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.69811. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68602/0.69899. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68507/0.70003. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68445/0.70124. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68323/0.70266. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68235/0.70419. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68096/0.70589. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68008/0.70769. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67946/0.70944. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67837/0.71121. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67728/0.71291. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67645/0.71459. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67556/0.71608. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67475/0.71739. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67412/0.71865. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67327/0.71982. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67275/0.72079. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67230/0.72166. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67163/0.72246. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67075/0.72324. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67056/0.72390. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66974/0.72455. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66887/0.72526. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66866/0.72573. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66701/0.72633. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66707/0.72679. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66647/0.72731. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66548/0.72782. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66546/0.72827. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66460/0.72894. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66419/0.72944. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66394/0.72967. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66326/0.73028. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66219/0.73084. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66160/0.73132. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66158/0.73181. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66039/0.73240. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65954/0.73300. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65922/0.73385. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65806/0.73443. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65731/0.73521. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65724/0.73572. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65666/0.73638. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65605/0.73721. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65525/0.73798. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65443/0.73902. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65366/0.73945. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65313/0.74050. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65262/0.74124. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65156/0.74214. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65115/0.74319. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65040/0.74445. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64954/0.74541. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64855/0.74657. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64742/0.74782. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64674/0.74899. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64580/0.75029. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64536/0.75162. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64463/0.75340. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64385/0.75488. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64315/0.75627. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64193/0.75773. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64057/0.75941. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64110/0.76075. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63908/0.76257. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63889/0.76430. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63771/0.76571. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63687/0.76789. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63573/0.76962. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63464/0.77197. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63409/0.77339. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63320/0.77538. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63173/0.77746. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63113/0.77972. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62983/0.78211. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62959/0.78390. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62833/0.78616. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69631/0.68788. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69604/0.68814. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69574/0.68840. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69520/0.68865. Took 0.08 sec\n",
      "Epoch 4, Loss(train/val) 0.69528/0.68888. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69502/0.68913. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69464/0.68934. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69453/0.68956. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69443/0.68977. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69427/0.68998. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69420/0.69020. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69383/0.69043. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69369/0.69066. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69349/0.69089. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69332/0.69112. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69332/0.69137. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69290/0.69161. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69288/0.69186. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69246/0.69215. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69211/0.69242. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69211/0.69275. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.69170/0.69312. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69159/0.69349. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69125/0.69389. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69115/0.69434. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.69067/0.69484. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.69032/0.69539. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69003/0.69597. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68965/0.69666. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68947/0.69739. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68922/0.69821. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68870/0.69908. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68834/0.70006. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68820/0.70106. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68789/0.70209. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68719/0.70322. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68691/0.70439. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68677/0.70558. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68623/0.70678. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68589/0.70804. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68562/0.70923. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68504/0.71048. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68499/0.71171. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68454/0.71289. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68454/0.71400. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68433/0.71508. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68355/0.71621. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68359/0.71725. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68308/0.71832. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68291/0.71935. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68266/0.72035. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68230/0.72136. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68213/0.72221. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68121/0.72310. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68125/0.72402. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68100/0.72488. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68074/0.72570. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68023/0.72653. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68008/0.72737. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67972/0.72813. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67917/0.72888. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67889/0.72958. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67836/0.73028. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67871/0.73089. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67815/0.73158. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67767/0.73229. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67730/0.73298. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67708/0.73362. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67628/0.73418. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67605/0.73476. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67530/0.73537. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67491/0.73612. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67500/0.73665. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67440/0.73720. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67405/0.73781. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67348/0.73845. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67309/0.73903. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67289/0.73954. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67261/0.74004. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67210/0.74054. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67163/0.74114. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67114/0.74161. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66999/0.74221. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66986/0.74275. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66981/0.74325. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66947/0.74373. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66869/0.74432. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66796/0.74488. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66800/0.74551. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66733/0.74601. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66680/0.74655. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66677/0.74717. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66511/0.74788. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66560/0.74843. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66466/0.74875. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66412/0.74939. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66353/0.75005. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66302/0.75063. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66267/0.75104. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66202/0.75163. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69812/0.67791. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69707/0.67952. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69619/0.68154. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69536/0.68388. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69459/0.68633. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69405/0.68864. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69358/0.69061. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69309/0.69217. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69293/0.69334. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69274/0.69409. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69265/0.69466. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69271/0.69511. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69256/0.69541. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69237/0.69566. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69228/0.69579. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69217/0.69592. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69197/0.69598. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69194/0.69609. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69185/0.69616. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69158/0.69622. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69174/0.69626. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.69155/0.69625. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69149/0.69633. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69138/0.69646. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.69118/0.69652. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69121/0.69660. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69106/0.69663. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69084/0.69670. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69085/0.69676. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.69069/0.69678. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.69051/0.69683. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.69046/0.69695. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69018/0.69700. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69019/0.69708. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69007/0.69715. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68998/0.69727. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68954/0.69733. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68951/0.69749. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68943/0.69766. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68924/0.69780. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68905/0.69779. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68871/0.69792. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68846/0.69805. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68860/0.69825. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68838/0.69845. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68824/0.69869. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68771/0.69882. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68770/0.69905. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68738/0.69926. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68723/0.69937. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68699/0.69964. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68675/0.70004. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68644/0.70037. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68604/0.70051. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68563/0.70081. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68553/0.70125. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.68518/0.70162. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.68489/0.70185. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68424/0.70232. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68411/0.70292. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68362/0.70326. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68318/0.70380. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68274/0.70397. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.68223/0.70468. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68188/0.70535. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68125/0.70585. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.68064/0.70647. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68013/0.70713. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.67972/0.70789. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.67924/0.70827. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67850/0.70914. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67792/0.71029. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67725/0.71108. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67642/0.71166. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67574/0.71306. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67551/0.71391. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67483/0.71483. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67397/0.71583. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.67359/0.71711. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67316/0.71825. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67234/0.71912. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67149/0.72038. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67086/0.72156. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.67025/0.72281. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66985/0.72411. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66869/0.72521. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66846/0.72624. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66783/0.72702. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66723/0.72850. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66657/0.72975. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66593/0.73081. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66568/0.73222. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66442/0.73315. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66368/0.73445. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66345/0.73557. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66274/0.73667. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66228/0.73773. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66202/0.73864. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66127/0.73964. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66040/0.74088. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69437/0.69289. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69403/0.69263. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69388/0.69233. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69373/0.69202. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69347/0.69169. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69328/0.69135. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69306/0.69096. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69275/0.69054. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69251/0.69011. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69228/0.68968. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69184/0.68922. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69193/0.68882. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69136/0.68844. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69145/0.68812. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69103/0.68783. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69111/0.68759. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69058/0.68734. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69059/0.68710. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69040/0.68693. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69028/0.68677. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68990/0.68660. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68974/0.68645. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68941/0.68632. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68934/0.68620. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68906/0.68610. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68885/0.68604. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68852/0.68595. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68810/0.68591. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68795/0.68581. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68772/0.68583. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68775/0.68590. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68745/0.68594. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68700/0.68601. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68671/0.68616. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68646/0.68634. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68605/0.68651. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68581/0.68664. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68566/0.68683. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68505/0.68717. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68485/0.68741. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68470/0.68767. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68396/0.68802. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68395/0.68834. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68358/0.68875. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68316/0.68913. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68276/0.68951. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68243/0.68977. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68183/0.69018. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68177/0.69064. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68147/0.69102. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68067/0.69142. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68026/0.69187. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68018/0.69236. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67940/0.69274. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67890/0.69319. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67825/0.69357. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67830/0.69409. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67757/0.69470. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67674/0.69522. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67667/0.69575. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67612/0.69635. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67553/0.69687. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67479/0.69738. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67443/0.69801. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67375/0.69857. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67315/0.69932. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67247/0.69999. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67177/0.70062. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67119/0.70131. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67035/0.70226. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66995/0.70294. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66883/0.70408. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66899/0.70467. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66821/0.70544. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66720/0.70655. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66684/0.70773. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66591/0.70838. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66508/0.70903. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66470/0.71018. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66414/0.71089. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66325/0.71179. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66312/0.71263. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66252/0.71349. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66207/0.71455. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66054/0.71572. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66045/0.71650. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65958/0.71757. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65929/0.71830. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65845/0.71954. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65814/0.72005. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65773/0.72083. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65716/0.72178. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65620/0.72267. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65601/0.72358. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65484/0.72428. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65417/0.72542. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65375/0.72563. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65311/0.72684. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65254/0.72723. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65244/0.72791. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69485/0.69719. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69430/0.69648. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69411/0.69588. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69376/0.69533. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69337/0.69487. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69317/0.69445. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69337/0.69406. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69295/0.69373. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69270/0.69342. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69290/0.69314. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69272/0.69288. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69258/0.69263. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69250/0.69240. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69245/0.69217. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69207/0.69195. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69196/0.69175. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69180/0.69155. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69164/0.69134. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69142/0.69115. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69110/0.69095. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69139/0.69075. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69110/0.69054. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69082/0.69032. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69104/0.69009. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69042/0.68988. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69067/0.68965. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68990/0.68941. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69024/0.68915. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68979/0.68890. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68919/0.68862. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68898/0.68837. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68903/0.68810. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68829/0.68783. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68812/0.68759. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68760/0.68736. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68710/0.68716. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68713/0.68697. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68662/0.68684. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68609/0.68676. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68545/0.68675. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68496/0.68683. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68478/0.68692. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68436/0.68711. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68380/0.68738. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68287/0.68774. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68277/0.68817. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68238/0.68868. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68158/0.68926. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68122/0.68991. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68018/0.69063. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68040/0.69141. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67954/0.69225. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67881/0.69316. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67854/0.69413. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67774/0.69518. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67702/0.69624. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67638/0.69733. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67541/0.69843. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67530/0.69962. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67458/0.70081. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67369/0.70204. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67319/0.70327. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67261/0.70458. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67204/0.70590. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67125/0.70714. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67057/0.70851. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66969/0.70988. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66940/0.71119. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66862/0.71262. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66786/0.71394. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66723/0.71528. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66613/0.71672. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66551/0.71813. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66488/0.71952. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66406/0.72100. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66379/0.72243. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66289/0.72401. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66216/0.72537. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66178/0.72681. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66052/0.72818. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65957/0.72965. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65960/0.73105. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65843/0.73242. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.65767/0.73399. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65714/0.73551. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65629/0.73697. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65525/0.73843. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65524/0.74001. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65403/0.74149. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65361/0.74295. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65260/0.74442. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65198/0.74592. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65095/0.74745. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65088/0.74888. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64976/0.75029. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64951/0.75194. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64863/0.75330. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64793/0.75482. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64694/0.75646. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64653/0.75774. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70794/0.70323. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.70081/0.69834. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69569/0.69550. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69259/0.69455. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69140/0.69454. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69090/0.69480. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69104/0.69508. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69079/0.69533. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69078/0.69555. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69031/0.69576. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69059/0.69597. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69031/0.69617. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69005/0.69636. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69014/0.69656. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69014/0.69674. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69004/0.69694. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68988/0.69713. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68968/0.69733. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68962/0.69755. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68948/0.69776. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68951/0.69797. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68942/0.69819. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68917/0.69840. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68911/0.69864. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68897/0.69888. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68890/0.69914. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68899/0.69941. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68864/0.69968. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68866/0.69997. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68852/0.70026. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68854/0.70055. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68794/0.70087. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68807/0.70120. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68799/0.70155. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68774/0.70191. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68777/0.70227. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68768/0.70264. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68755/0.70307. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68728/0.70348. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68726/0.70392. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68706/0.70436. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68692/0.70482. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68670/0.70531. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68662/0.70580. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68645/0.70633. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68635/0.70685. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68608/0.70740. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68581/0.70798. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.68561/0.70855. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68550/0.70917. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68499/0.70976. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68465/0.71039. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68479/0.71106. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68457/0.71167. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68395/0.71232. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68399/0.71299. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68340/0.71366. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68342/0.71434. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68286/0.71504. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68271/0.71573. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68234/0.71644. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68207/0.71721. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68174/0.71790. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68137/0.71854. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68113/0.71925. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68066/0.71992. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67978/0.72062. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67949/0.72136. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67916/0.72211. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67843/0.72277. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67792/0.72346. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67731/0.72419. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67660/0.72496. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67601/0.72574. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67579/0.72641. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67511/0.72706. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67423/0.72775. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67340/0.72844. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67292/0.72911. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67196/0.72975. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67142/0.73029. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67038/0.73094. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66990/0.73154. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66926/0.73202. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66813/0.73258. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66715/0.73295. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66650/0.73341. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66558/0.73396. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66448/0.73432. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66378/0.73464. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66270/0.73492. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66187/0.73519. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66071/0.73548. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65978/0.73578. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65944/0.73600. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65846/0.73626. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65752/0.73633. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65636/0.73667. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65506/0.73698. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65471/0.73695. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69172/0.68657. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69164/0.68663. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69128/0.68669. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69134/0.68674. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69131/0.68679. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69137/0.68685. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69109/0.68689. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69111/0.68695. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69081/0.68699. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.68702. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69046/0.68703. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69044/0.68702. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69026/0.68699. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69008/0.68695. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68985/0.68692. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68988/0.68688. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68966/0.68684. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68945/0.68681. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68948/0.68679. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68915/0.68681. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68895/0.68682. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68864/0.68690. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68856/0.68702. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68805/0.68713. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68804/0.68724. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68788/0.68742. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68771/0.68763. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68738/0.68791. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68718/0.68818. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68682/0.68842. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68633/0.68867. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68606/0.68897. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68568/0.68928. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68562/0.68969. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68516/0.68994. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68467/0.69026. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68419/0.69053. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68367/0.69073. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68330/0.69121. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68296/0.69156. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68236/0.69212. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68175/0.69225. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68134/0.69262. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68081/0.69291. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68023/0.69313. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67956/0.69327. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67930/0.69352. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67900/0.69369. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67839/0.69407. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67786/0.69454. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67732/0.69477. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67647/0.69493. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67625/0.69510. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67550/0.69520. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67521/0.69513. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67467/0.69541. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67372/0.69583. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67342/0.69538. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67273/0.69558. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67202/0.69569. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67108/0.69568. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67056/0.69566. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66990/0.69567. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66918/0.69565. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66826/0.69524. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66764/0.69562. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66680/0.69549. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66628/0.69536. Took 0.08 sec\n",
      "Epoch 68, Loss(train/val) 0.66515/0.69528. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66370/0.69527. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66309/0.69535. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66198/0.69526. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66170/0.69520. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66045/0.69522. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65938/0.69533. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65906/0.69528. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65696/0.69540. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65604/0.69570. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65493/0.69572. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65353/0.69600. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65246/0.69600. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65065/0.69638. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64981/0.69677. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64810/0.69719. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64687/0.69713. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64579/0.69767. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64428/0.69863. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64322/0.69926. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64149/0.69939. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64010/0.69936. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63814/0.70002. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63702/0.70015. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63551/0.70107. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63339/0.70139. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63307/0.70132. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63135/0.70183. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62984/0.70201. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62810/0.70264. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62593/0.70233. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62483/0.70239. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69119/0.68106. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69073/0.68106. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69047/0.68103. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69079/0.68095. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69032/0.68082. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.68070. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68981/0.68055. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.68041. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68941/0.68028. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68902/0.68011. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68901/0.67995. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.67978. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68839/0.67960. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68849/0.67942. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68814/0.67919. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.67895. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68770/0.67872. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68741/0.67852. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68690/0.67831. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68677/0.67810. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68653/0.67790. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68606/0.67773. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68583/0.67756. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68581/0.67739. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68575/0.67721. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68536/0.67704. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68480/0.67690. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68485/0.67678. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68444/0.67664. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68447/0.67651. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68422/0.67641. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68401/0.67637. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68342/0.67627. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68386/0.67622. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68319/0.67609. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68303/0.67613. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68263/0.67613. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68264/0.67601. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68200/0.67601. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68227/0.67600. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68171/0.67600. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68161/0.67604. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68151/0.67602. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68142/0.67602. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68109/0.67604. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68100/0.67601. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68024/0.67604. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68045/0.67606. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67988/0.67608. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68029/0.67612. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67968/0.67612. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67902/0.67604. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67869/0.67614. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67841/0.67618. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67816/0.67621. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67801/0.67622. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67753/0.67625. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67702/0.67626. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67636/0.67628. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67656/0.67639. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67597/0.67651. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67554/0.67652. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67511/0.67655. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67421/0.67667. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67372/0.67668. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67315/0.67687. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67268/0.67687. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67169/0.67688. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67152/0.67708. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67079/0.67717. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67010/0.67748. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66937/0.67767. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66851/0.67795. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66763/0.67821. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66680/0.67836. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66592/0.67885. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66480/0.67927. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66387/0.67965. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66281/0.68015. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66187/0.68068. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66039/0.68110. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65991/0.68195. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65806/0.68277. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.65757/0.68361. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65592/0.68448. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65513/0.68547. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65331/0.68642. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65198/0.68764. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65101/0.68887. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65004/0.69029. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64857/0.69171. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64701/0.69301. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64576/0.69452. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64490/0.69585. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64313/0.69745. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64259/0.69902. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64055/0.70043. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63887/0.70219. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63873/0.70396. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63693/0.70554. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69521/0.69189. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69499/0.69188. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69453/0.69192. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69409/0.69201. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69371/0.69213. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69291/0.69228. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69203/0.69245. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69073/0.69274. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68997/0.69317. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68891/0.69369. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68821/0.69424. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68779/0.69470. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68734/0.69511. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68717/0.69546. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68683/0.69575. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68613/0.69603. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68594/0.69632. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68556/0.69660. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68521/0.69690. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68466/0.69722. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68421/0.69757. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68386/0.69791. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.69827. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68317/0.69866. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68239/0.69906. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68194/0.69940. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68132/0.69978. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68060/0.70014. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68028/0.70050. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68000/0.70083. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67933/0.70112. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67844/0.70137. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67790/0.70164. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67715/0.70187. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67636/0.70214. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67582/0.70239. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67551/0.70260. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67482/0.70282. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67399/0.70302. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67330/0.70322. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67287/0.70355. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67214/0.70379. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67157/0.70411. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67064/0.70437. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67048/0.70455. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66950/0.70489. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66884/0.70521. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66799/0.70560. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66711/0.70591. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66657/0.70624. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66600/0.70658. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66505/0.70691. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66443/0.70744. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66347/0.70791. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66289/0.70844. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66202/0.70920. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66136/0.70953. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66040/0.71018. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65989/0.71061. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65848/0.71133. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65801/0.71215. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65700/0.71300. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65568/0.71339. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65541/0.71423. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65444/0.71499. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65364/0.71550. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65308/0.71640. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65142/0.71687. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65179/0.71765. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65096/0.71878. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64939/0.71930. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64924/0.72028. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64734/0.72093. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64706/0.72168. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64662/0.72268. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64484/0.72347. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64440/0.72435. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64397/0.72498. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64293/0.72596. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64182/0.72658. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64200/0.72713. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64095/0.72801. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63969/0.72873. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63853/0.72948. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63866/0.73037. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63783/0.73140. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63610/0.73211. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63620/0.73301. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63544/0.73346. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63421/0.73435. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63346/0.73497. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.63278/0.73601. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63252/0.73703. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63093/0.73787. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62950/0.73847. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63007/0.73953. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62913/0.74012. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62780/0.74090. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62780/0.74198. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62695/0.74309. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69209/0.69339. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69118/0.69154. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69060/0.68989. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68990/0.68843. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68950/0.68719. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68906/0.68620. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68876/0.68551. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.68506. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68828/0.68473. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68814/0.68457. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68774/0.68440. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68764/0.68429. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68726/0.68421. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68703/0.68414. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68682/0.68410. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68651/0.68410. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68641/0.68407. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68581/0.68402. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68585/0.68406. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68559/0.68406. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68522/0.68412. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68507/0.68410. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68475/0.68406. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68430/0.68405. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68415/0.68411. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68385/0.68411. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68353/0.68402. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68322/0.68405. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68304/0.68400. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68270/0.68396. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68240/0.68399. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68198/0.68397. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68214/0.68390. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68184/0.68388. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68126/0.68379. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68088/0.68365. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68074/0.68360. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68047/0.68356. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67988/0.68343. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67964/0.68329. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67933/0.68334. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67902/0.68317. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67887/0.68298. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67835/0.68285. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67814/0.68286. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67774/0.68259. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67751/0.68248. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67715/0.68224. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67684/0.68202. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67653/0.68196. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67608/0.68181. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67553/0.68159. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67535/0.68147. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67479/0.68126. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67466/0.68106. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67403/0.68081. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67372/0.68069. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67322/0.68057. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67310/0.68036. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67280/0.68026. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67202/0.68006. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67164/0.68002. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67146/0.67967. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67110/0.67956. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67051/0.67949. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66995/0.67924. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66927/0.67927. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66930/0.67895. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66886/0.67882. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66852/0.67877. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66754/0.67878. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66739/0.67876. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66688/0.67859. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66597/0.67873. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66551/0.67882. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66506/0.67886. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66529/0.67853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66419/0.67854. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66386/0.67859. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66337/0.67866. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66266/0.67868. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66242/0.67851. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66147/0.67890. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66108/0.67902. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66097/0.67917. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66022/0.67897. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65945/0.67943. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65942/0.67929. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65921/0.67943. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65828/0.67923. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65781/0.67934. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65646/0.67947. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65632/0.67955. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65597/0.67986. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65596/0.67990. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65481/0.68018. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65455/0.68051. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65400/0.68063. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65351/0.68092. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65229/0.68161. Took 0.09 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69152/0.69337. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69110/0.69371. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69082/0.69407. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68988/0.69448. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68912/0.69497. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68804/0.69559. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68737/0.69636. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68628/0.69725. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68589/0.69808. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68539/0.69888. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68487/0.69961. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68469/0.70025. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68446/0.70088. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68407/0.70145. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68398/0.70202. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68355/0.70259. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68347/0.70316. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68302/0.70376. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68262/0.70442. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68234/0.70507. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68198/0.70578. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68165/0.70654. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68118/0.70740. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68073/0.70824. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68053/0.70917. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68006/0.71014. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67951/0.71113. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67913/0.71217. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67841/0.71330. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67791/0.71445. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67785/0.71564. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67682/0.71677. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67669/0.71796. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67625/0.71912. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67591/0.72030. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67514/0.72147. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67474/0.72246. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67452/0.72336. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67380/0.72435. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67325/0.72521. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67288/0.72597. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67248/0.72679. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67210/0.72750. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67125/0.72823. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67071/0.72874. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67034/0.72917. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66970/0.72952. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.66971/0.73000. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66876/0.73031. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66828/0.73051. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66764/0.73084. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66724/0.73110. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66748/0.73121. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66640/0.73137. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66609/0.73145. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66562/0.73151. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66510/0.73170. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66463/0.73172. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66394/0.73179. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66361/0.73172. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66327/0.73175. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66221/0.73189. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66170/0.73204. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66145/0.73208. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66099/0.73207. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66058/0.73202. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65996/0.73202. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65854/0.73217. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65866/0.73227. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65834/0.73231. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65752/0.73244. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65702/0.73261. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65644/0.73282. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65635/0.73290. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65540/0.73299. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65491/0.73328. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65415/0.73355. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65404/0.73382. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65314/0.73421. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65269/0.73462. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65224/0.73499. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65157/0.73509. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65015/0.73568. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65050/0.73593. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64987/0.73634. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64949/0.73640. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64865/0.73681. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64728/0.73757. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64704/0.73806. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64667/0.73860. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64609/0.73901. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64585/0.73944. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64476/0.74007. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64428/0.74063. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64375/0.74133. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64258/0.74188. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64229/0.74248. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64189/0.74311. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64101/0.74356. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64020/0.74424. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69288/0.69226. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69289/0.69234. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69255/0.69241. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69245/0.69250. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69226/0.69260. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69222/0.69272. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69216/0.69287. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69176/0.69306. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69136/0.69331. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69094/0.69364. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69046/0.69406. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68999/0.69456. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68984/0.69506. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68929/0.69553. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68871/0.69596. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68855/0.69630. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68823/0.69668. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68783/0.69700. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68719/0.69735. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68709/0.69773. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68667/0.69812. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68605/0.69852. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68592/0.69894. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68557/0.69939. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68502/0.69985. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68508/0.70039. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68440/0.70089. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68394/0.70141. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68390/0.70191. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68326/0.70241. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68335/0.70288. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68281/0.70336. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68189/0.70376. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68178/0.70416. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68169/0.70465. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68136/0.70513. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68097/0.70553. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68002/0.70599. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68028/0.70648. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67916/0.70700. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67917/0.70739. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67899/0.70785. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67873/0.70829. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67792/0.70873. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67781/0.70928. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67736/0.70974. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67717/0.71025. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67633/0.71065. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67654/0.71128. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67609/0.71171. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67556/0.71216. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67503/0.71271. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67484/0.71317. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67430/0.71369. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67421/0.71424. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67339/0.71465. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67261/0.71524. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67244/0.71581. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67205/0.71646. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67128/0.71700. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67113/0.71757. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67067/0.71822. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67016/0.71882. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66943/0.71934. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66902/0.71998. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66812/0.72053. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66871/0.72117. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66727/0.72174. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66758/0.72234. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66676/0.72286. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66567/0.72349. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66601/0.72410. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66492/0.72472. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66447/0.72545. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66313/0.72614. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66331/0.72684. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.66240/0.72750. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66196/0.72815. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66169/0.72886. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.66120/0.72949. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.66058/0.73003. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65966/0.73066. Took 0.08 sec\n",
      "Epoch 82, Loss(train/val) 0.65920/0.73141. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65899/0.73211. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65769/0.73284. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.65743/0.73341. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65634/0.73422. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65546/0.73497. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.65495/0.73588. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.65378/0.73645. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65361/0.73725. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65250/0.73779. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65203/0.73852. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65144/0.73932. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65089/0.74007. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.64966/0.74072. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64900/0.74145. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64857/0.74228. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.64694/0.74319. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64682/0.74407. Took 0.08 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69296/0.68772. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69292/0.68778. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69276/0.68783. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69247/0.68790. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69223/0.68798. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69208/0.68807. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69189/0.68817. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69177/0.68829. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69166/0.68842. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69141/0.68859. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69078/0.68876. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69058/0.68897. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69048/0.68922. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69026/0.68943. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68982/0.68966. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68943/0.68988. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68898/0.69011. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68857/0.69032. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68836/0.69054. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68797/0.69074. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68780/0.69096. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68704/0.69121. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68641/0.69153. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68610/0.69186. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68578/0.69220. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68514/0.69254. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68422/0.69288. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68368/0.69321. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68327/0.69355. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68270/0.69398. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68199/0.69433. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68137/0.69459. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68047/0.69479. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68001/0.69519. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67969/0.69543. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67827/0.69577. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67757/0.69606. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67664/0.69635. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67621/0.69679. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67560/0.69686. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67453/0.69741. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67375/0.69769. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67309/0.69804. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67186/0.69860. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67098/0.69935. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67084/0.70028. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66922/0.70052. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66832/0.70127. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66774/0.70274. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66723/0.70377. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66633/0.70456. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66508/0.70514. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66430/0.70683. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66362/0.70809. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66258/0.70926. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66134/0.71061. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66045/0.71100. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66010/0.71286. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65937/0.71414. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65803/0.71549. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65734/0.71662. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65666/0.71811. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65578/0.71953. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65437/0.72008. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65321/0.72248. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65231/0.72340. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65125/0.72518. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65049/0.72620. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64961/0.72759. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64918/0.72877. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64804/0.72999. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64632/0.73131. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64613/0.73239. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64523/0.73380. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64380/0.73540. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64228/0.73637. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64171/0.73800. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64136/0.73930. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63942/0.74058. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63844/0.74210. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63707/0.74377. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63658/0.74494. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63501/0.74627. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63473/0.74825. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63347/0.74950. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63265/0.75075. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63128/0.75220. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63081/0.75432. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62781/0.75561. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62872/0.75714. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62785/0.75819. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62520/0.76032. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62455/0.76190. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62410/0.76405. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62284/0.76533. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62196/0.76670. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62056/0.76842. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61957/0.77059. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61831/0.77232. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61637/0.77419. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69386/0.69812. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69290/0.69933. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.70067. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69115/0.70212. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69085/0.70362. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69047/0.70519. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68998/0.70681. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68954/0.70835. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68932/0.70992. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68884/0.71146. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68887/0.71286. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68840/0.71413. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68794/0.71536. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68782/0.71649. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68753/0.71746. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68761/0.71832. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68742/0.71914. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68699/0.71980. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68709/0.72038. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68680/0.72085. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68654/0.72133. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68680/0.72176. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68672/0.72209. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68647/0.72243. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68640/0.72263. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68608/0.72285. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68590/0.72300. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68589/0.72318. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68576/0.72339. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68552/0.72357. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68529/0.72369. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68547/0.72380. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68513/0.72402. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68514/0.72418. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68523/0.72424. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68474/0.72433. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68495/0.72438. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68472/0.72445. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68436/0.72451. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68444/0.72467. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68438/0.72477. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68412/0.72485. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68411/0.72488. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68378/0.72489. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68366/0.72495. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68321/0.72501. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68321/0.72508. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68316/0.72512. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68322/0.72506. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68280/0.72516. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68220/0.72533. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68238/0.72538. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68228/0.72554. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68180/0.72570. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68160/0.72587. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68129/0.72595. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68097/0.72600. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68112/0.72605. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68083/0.72607. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68048/0.72603. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68029/0.72603. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67988/0.72610. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67973/0.72620. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67954/0.72619. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67901/0.72641. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67910/0.72643. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67860/0.72637. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67820/0.72650. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67816/0.72656. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67768/0.72655. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67734/0.72655. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67725/0.72664. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67651/0.72673. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67650/0.72684. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67626/0.72677. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67569/0.72688. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67514/0.72704. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67479/0.72705. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67403/0.72727. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67376/0.72715. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67336/0.72741. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67330/0.72738. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67278/0.72745. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67219/0.72775. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67173/0.72805. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67135/0.72827. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67096/0.72835. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67071/0.72844. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.67032/0.72847. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67017/0.72833. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66900/0.72875. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66939/0.72887. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66829/0.72921. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66806/0.72935. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66712/0.72926. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66741/0.72927. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66671/0.72951. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66638/0.72968. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66585/0.72996. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66555/0.73035. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69734/0.69940. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69671/0.69867. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69601/0.69767. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69462/0.69634. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69345/0.69486. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69239/0.69358. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69149/0.69268. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69031/0.69215. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69032/0.69184. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68971/0.69161. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68927/0.69142. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68891/0.69123. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68864/0.69107. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68848/0.69088. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68820/0.69072. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68773/0.69055. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68734/0.69036. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68739/0.69016. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68688/0.69000. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68673/0.68981. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68667/0.68964. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68617/0.68949. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68588/0.68938. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68533/0.68926. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68489/0.68915. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68449/0.68905. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68452/0.68897. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68422/0.68891. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68377/0.68885. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68359/0.68885. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68321/0.68884. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68267/0.68891. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68263/0.68892. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68190/0.68898. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68188/0.68904. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68119/0.68913. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68109/0.68907. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68071/0.68906. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68054/0.68900. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68023/0.68900. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67955/0.68900. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67919/0.68899. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67850/0.68906. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67848/0.68887. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67796/0.68882. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67772/0.68870. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67745/0.68859. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67716/0.68856. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67662/0.68843. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67573/0.68833. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67582/0.68816. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67592/0.68796. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67479/0.68764. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67435/0.68747. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67380/0.68715. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67379/0.68695. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67303/0.68676. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67233/0.68655. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67246/0.68633. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67175/0.68595. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67100/0.68584. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67082/0.68537. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66978/0.68518. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66927/0.68501. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66881/0.68470. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66836/0.68450. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66782/0.68464. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66723/0.68434. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66675/0.68411. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66636/0.68383. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66537/0.68380. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66487/0.68347. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66381/0.68320. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66390/0.68345. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66324/0.68322. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66267/0.68321. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66151/0.68303. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66139/0.68340. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66100/0.68322. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65949/0.68296. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65924/0.68308. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65860/0.68326. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65773/0.68340. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65760/0.68371. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65680/0.68368. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65611/0.68397. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65587/0.68426. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65451/0.68450. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65426/0.68460. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65436/0.68447. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65322/0.68454. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65303/0.68503. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65247/0.68510. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65171/0.68531. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65049/0.68568. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65050/0.68589. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64980/0.68641. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64977/0.68634. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64818/0.68708. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64745/0.68746. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69650/0.70443. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69440/0.70286. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69315/0.70194. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69235/0.70142. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69168/0.70117. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69097/0.70109. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69066/0.70112. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69011/0.70126. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68950/0.70146. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68888/0.70170. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68839/0.70200. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68799/0.70236. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68740/0.70276. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68713/0.70321. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68655/0.70370. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68589/0.70425. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68554/0.70481. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68496/0.70539. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68454/0.70599. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68422/0.70656. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68367/0.70715. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68371/0.70772. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68334/0.70826. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68285/0.70877. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68261/0.70926. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68234/0.70973. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68217/0.71020. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68192/0.71058. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68157/0.71097. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68128/0.71131. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68115/0.71164. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68101/0.71200. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68097/0.71232. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68062/0.71258. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68031/0.71285. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68024/0.71308. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67995/0.71330. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67969/0.71351. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67953/0.71369. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67976/0.71391. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67908/0.71415. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67920/0.71438. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67892/0.71457. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67869/0.71472. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67851/0.71490. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67803/0.71508. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67824/0.71529. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67788/0.71544. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67779/0.71560. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67757/0.71574. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67754/0.71595. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67702/0.71615. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67703/0.71633. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67684/0.71654. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67643/0.71669. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67621/0.71687. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67616/0.71707. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67563/0.71722. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67524/0.71745. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67523/0.71757. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67513/0.71771. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67458/0.71790. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67412/0.71804. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67406/0.71822. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67402/0.71838. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67399/0.71862. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67393/0.71884. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67351/0.71904. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67322/0.71918. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67287/0.71940. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67240/0.71960. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67257/0.71977. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67175/0.72000. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67200/0.72015. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67162/0.72044. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67112/0.72073. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67067/0.72096. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67081/0.72122. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67022/0.72148. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67062/0.72171. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67000/0.72193. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66947/0.72219. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66978/0.72240. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66898/0.72270. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66841/0.72300. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66800/0.72336. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66748/0.72383. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66734/0.72421. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66690/0.72457. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66676/0.72490. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66685/0.72521. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66593/0.72557. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66555/0.72601. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66575/0.72647. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66537/0.72680. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66414/0.72730. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66495/0.72767. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66383/0.72823. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66391/0.72879. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66304/0.72920. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69439/0.69396. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69390/0.69407. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69325/0.69420. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69316/0.69440. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69466. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69498. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69192/0.69538. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69144/0.69580. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69125/0.69626. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69118/0.69675. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69078/0.69722. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69057/0.69770. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69030/0.69816. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69005/0.69862. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68993/0.69908. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68994/0.69954. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68952/0.70004. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68938/0.70055. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68895/0.70106. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68885/0.70158. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68876/0.70210. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68850/0.70267. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68821/0.70327. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68800/0.70386. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68815/0.70445. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68764/0.70507. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68767/0.70569. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68726/0.70636. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68750/0.70701. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68689/0.70759. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68657/0.70821. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68654/0.70884. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68618/0.70944. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68619/0.71003. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68578/0.71054. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68595/0.71108. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68557/0.71156. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68539/0.71205. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68513/0.71247. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68508/0.71287. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68451/0.71324. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68452/0.71362. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68442/0.71394. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68418/0.71428. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68384/0.71462. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68360/0.71486. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68329/0.71512. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68292/0.71544. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68261/0.71573. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68293/0.71595. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68251/0.71617. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68187/0.71639. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68205/0.71653. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68159/0.71674. Took 0.08 sec\n",
      "Epoch 54, Loss(train/val) 0.68137/0.71702. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68101/0.71716. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68058/0.71731. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68017/0.71744. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68018/0.71762. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67943/0.71781. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.67949/0.71790. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67876/0.71807. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67875/0.71831. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67796/0.71849. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67788/0.71873. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67703/0.71884. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67682/0.71903. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67655/0.71920. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67554/0.71934. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67538/0.71962. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67468/0.71972. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67426/0.71984. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67356/0.71999. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67318/0.72028. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67217/0.72049. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67203/0.72083. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67120/0.72120. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67019/0.72149. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66943/0.72177. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66892/0.72188. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66828/0.72229. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66733/0.72263. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66661/0.72296. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66560/0.72336. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66520/0.72379. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66456/0.72438. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66347/0.72493. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66269/0.72515. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66142/0.72555. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66055/0.72595. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65949/0.72651. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65884/0.72693. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65804/0.72766. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65712/0.72791. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65628/0.72865. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65490/0.72893. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65423/0.72980. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65388/0.73051. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65212/0.73046. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65156/0.73103. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69100/0.68480. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69100/0.68513. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69043/0.68540. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69037/0.68567. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68973/0.68589. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69002/0.68614. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68967/0.68636. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68938/0.68654. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68930/0.68677. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68885/0.68700. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68849/0.68726. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68848/0.68752. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68816/0.68778. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68807/0.68809. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68757/0.68843. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68733/0.68879. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68714/0.68917. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68690/0.68956. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68651/0.68999. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68636/0.69045. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68609/0.69088. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68596/0.69131. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68604/0.69168. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68543/0.69212. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68537/0.69260. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68522/0.69298. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68525/0.69342. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68524/0.69378. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68476/0.69421. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68490/0.69462. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68463/0.69496. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68435/0.69529. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68407/0.69564. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68396/0.69600. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68388/0.69637. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68373/0.69668. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68375/0.69699. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68343/0.69730. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68331/0.69759. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68317/0.69789. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68275/0.69824. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68288/0.69855. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68269/0.69887. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68225/0.69913. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68215/0.69947. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68218/0.69984. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68186/0.70017. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68168/0.70051. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68138/0.70070. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68114/0.70098. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68119/0.70136. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68081/0.70169. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68040/0.70195. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68036/0.70225. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68019/0.70261. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67976/0.70307. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67965/0.70329. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67881/0.70359. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67897/0.70399. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67866/0.70434. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67840/0.70467. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67803/0.70509. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67742/0.70543. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67708/0.70585. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67674/0.70628. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67654/0.70689. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67587/0.70723. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67581/0.70763. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67491/0.70816. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67432/0.70878. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67387/0.70931. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67300/0.70987. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67207/0.71042. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67164/0.71119. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67081/0.71178. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67029/0.71278. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66952/0.71346. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66873/0.71455. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66776/0.71535. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66642/0.71645. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66569/0.71751. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66438/0.71933. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66312/0.72034. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66288/0.72194. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66137/0.72320. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66030/0.72483. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65900/0.72663. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65749/0.72838. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65645/0.72934. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65494/0.73203. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65423/0.73373. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65338/0.73555. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65186/0.73715. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65169/0.73906. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64995/0.74137. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64806/0.74288. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64772/0.74444. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64586/0.74631. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64505/0.74773. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64427/0.74933. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69018/0.70323. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69022/0.70292. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69022/0.70271. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68986/0.70254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.70243. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68941/0.70236. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68938/0.70231. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68920/0.70228. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68910/0.70225. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68898/0.70225. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68874/0.70228. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68863/0.70233. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68851/0.70240. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68818/0.70248. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68816/0.70257. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68778/0.70270. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68772/0.70284. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68719/0.70300. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68703/0.70317. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68664/0.70335. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68643/0.70351. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68610/0.70372. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68579/0.70397. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68532/0.70421. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68518/0.70454. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68456/0.70475. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68433/0.70512. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68382/0.70554. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68360/0.70586. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68318/0.70629. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68280/0.70668. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68245/0.70709. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68225/0.70752. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68172/0.70794. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68144/0.70825. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68133/0.70861. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68114/0.70899. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68054/0.70931. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.68031/0.70969. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67992/0.71001. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67959/0.71030. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67941/0.71062. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67897/0.71087. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67860/0.71124. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67806/0.71145. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67768/0.71173. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67745/0.71206. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67695/0.71234. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67667/0.71261. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67610/0.71278. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67576/0.71302. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67562/0.71326. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67516/0.71353. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67457/0.71367. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67425/0.71405. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67389/0.71405. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67328/0.71432. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67253/0.71440. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67224/0.71450. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67178/0.71480. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67112/0.71494. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67059/0.71499. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67014/0.71521. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66981/0.71545. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.66910/0.71553. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66838/0.71570. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66771/0.71583. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66739/0.71595. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66653/0.71620. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66600/0.71626. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66556/0.71648. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66499/0.71676. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66433/0.71689. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66354/0.71716. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66251/0.71724. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66239/0.71763. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66151/0.71758. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66067/0.71794. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66032/0.71835. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65982/0.71844. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65914/0.71878. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65823/0.71871. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65731/0.71926. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65678/0.71936. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65614/0.71966. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65465/0.71989. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65527/0.72041. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65446/0.72051. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65324/0.72103. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65258/0.72106. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65212/0.72159. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65086/0.72175. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65018/0.72197. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64912/0.72233. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64859/0.72311. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64818/0.72313. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64784/0.72371. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64619/0.72397. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64496/0.72465. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64463/0.72525. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69234/0.69481. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69197/0.69474. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69182/0.69466. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69179/0.69459. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69178/0.69450. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69161/0.69442. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69135/0.69432. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69123/0.69421. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69093/0.69410. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69078/0.69397. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69056/0.69385. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69042/0.69373. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69018/0.69362. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68987/0.69354. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68966/0.69345. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68918/0.69333. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68897/0.69317. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68893/0.69299. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68857/0.69279. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68865/0.69261. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68854/0.69238. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68846/0.69219. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.68811/0.69186. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68785/0.69152. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68710/0.69119. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68746/0.69081. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68691/0.69039. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68716/0.69000. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68606/0.68952. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68596/0.68902. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68558/0.68848. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68505/0.68793. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68481/0.68739. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68466/0.68681. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68439/0.68622. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68367/0.68562. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68374/0.68496. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68304/0.68430. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68308/0.68365. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68226/0.68305. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68177/0.68238. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68170/0.68174. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68112/0.68114. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68032/0.68051. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68053/0.67991. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67960/0.67939. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67912/0.67877. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67884/0.67831. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67892/0.67780. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67806/0.67732. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67747/0.67684. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67700/0.67651. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67676/0.67605. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67622/0.67576. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67607/0.67544. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67501/0.67513. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67506/0.67487. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67429/0.67464. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67416/0.67442. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67426/0.67425. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67323/0.67415. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67278/0.67398. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67242/0.67392. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67162/0.67381. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67162/0.67384. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67097/0.67381. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67051/0.67368. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67040/0.67367. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66997/0.67367. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66941/0.67376. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66843/0.67388. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66851/0.67402. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66697/0.67423. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66718/0.67450. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.66675/0.67459. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66626/0.67462. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66489/0.67477. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66483/0.67498. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66502/0.67533. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66439/0.67552. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66253/0.67599. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66291/0.67644. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66156/0.67673. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66070/0.67710. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66019/0.67751. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65945/0.67793. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65846/0.67831. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65835/0.67871. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65734/0.67924. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65510/0.67985. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65483/0.68074. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65456/0.68124. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65382/0.68186. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65301/0.68254. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65172/0.68306. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.65093/0.68376. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64971/0.68438. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64893/0.68517. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64677/0.68601. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64623/0.68685. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69620/0.69287. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69563/0.69272. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69536/0.69261. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69503/0.69254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69491/0.69249. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69452/0.69248. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69464/0.69249. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69418/0.69252. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69377/0.69259. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69326/0.69269. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69246/0.69285. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69161/0.69315. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69064/0.69363. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69011/0.69420. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68972/0.69474. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.69525. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68888/0.69575. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68836/0.69621. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68799/0.69667. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68763/0.69717. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68718/0.69768. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68694/0.69820. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68647/0.69875. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68614/0.69932. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68569/0.69987. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68504/0.70047. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68448/0.70112. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68408/0.70177. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68384/0.70242. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68328/0.70302. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68282/0.70363. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68235/0.70419. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68218/0.70472. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68168/0.70526. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68107/0.70574. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68069/0.70618. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68046/0.70661. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68020/0.70701. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67974/0.70743. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67956/0.70778. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67871/0.70814. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67835/0.70848. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67774/0.70878. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67799/0.70897. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67720/0.70921. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67702/0.70943. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67624/0.70967. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67603/0.70985. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67558/0.71002. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67512/0.71022. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67470/0.71038. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67447/0.71052. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67380/0.71066. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67315/0.71085. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67281/0.71105. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67200/0.71120. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67125/0.71143. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67097/0.71153. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66992/0.71164. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66969/0.71177. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66906/0.71187. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66845/0.71202. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66761/0.71216. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66685/0.71225. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66644/0.71238. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66555/0.71245. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66495/0.71268. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66477/0.71279. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66402/0.71296. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66333/0.71309. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66213/0.71330. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66136/0.71361. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66045/0.71379. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65973/0.71390. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65850/0.71430. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65794/0.71478. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65711/0.71523. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65646/0.71548. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65558/0.71586. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65458/0.71639. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65425/0.71684. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65291/0.71750. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65207/0.71772. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65091/0.71818. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64970/0.71876. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64952/0.71948. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64804/0.72018. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64771/0.72068. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64552/0.72151. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64569/0.72208. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64420/0.72279. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64310/0.72368. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64255/0.72488. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64095/0.72585. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64002/0.72652. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63897/0.72754. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63839/0.72884. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63727/0.72948. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63683/0.73044. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63475/0.73138. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69622/0.69558. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69533/0.69501. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69437/0.69443. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69341/0.69389. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69261/0.69350. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69331. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69078/0.69325. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69067/0.69328. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69026/0.69333. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68981/0.69337. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68962/0.69341. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68965/0.69343. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68916/0.69346. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68853/0.69348. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68867/0.69350. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68850/0.69352. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68851/0.69355. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68787/0.69358. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68747/0.69361. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68727/0.69367. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68726/0.69373. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68723/0.69379. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68677/0.69386. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68645/0.69393. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68614/0.69399. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68558/0.69406. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68568/0.69415. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68512/0.69423. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68453/0.69435. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68457/0.69449. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68433/0.69463. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68398/0.69478. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68323/0.69495. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68296/0.69512. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68290/0.69530. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68221/0.69548. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68210/0.69570. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68135/0.69594. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68098/0.69618. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68046/0.69646. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68025/0.69674. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67974/0.69701. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67943/0.69734. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67858/0.69767. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67802/0.69804. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67749/0.69843. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67711/0.69882. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67649/0.69924. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67642/0.69968. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67572/0.70013. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67456/0.70054. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67447/0.70106. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67407/0.70160. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67347/0.70216. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67282/0.70270. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67159/0.70324. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67115/0.70388. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67130/0.70445. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66995/0.70509. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67012/0.70570. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66948/0.70647. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66840/0.70716. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66777/0.70795. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66713/0.70870. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66617/0.70946. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66567/0.71032. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66506/0.71127. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66483/0.71215. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66348/0.71310. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66308/0.71396. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66236/0.71507. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66102/0.71612. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66008/0.71710. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65925/0.71823. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65815/0.71944. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65717/0.72075. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65634/0.72201. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65572/0.72323. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65416/0.72465. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65372/0.72620. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65255/0.72761. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65163/0.72912. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65098/0.73059. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64984/0.73205. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64912/0.73369. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64766/0.73516. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64590/0.73672. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64543/0.73832. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64427/0.73992. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64330/0.74124. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64240/0.74293. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64106/0.74437. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63958/0.74597. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63825/0.74741. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63833/0.74898. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63678/0.75027. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63531/0.75173. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63400/0.75309. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63287/0.75474. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63123/0.75646. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69843/0.69378. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69796/0.69371. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69717/0.69369. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69666/0.69368. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69623/0.69369. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69595/0.69371. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69546/0.69375. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69521/0.69378. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69504/0.69383. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69477/0.69387. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69463/0.69392. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69438/0.69398. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69407/0.69404. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69416/0.69410. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69364/0.69416. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69403/0.69423. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69340/0.69431. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69331/0.69438. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69337/0.69447. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69301/0.69455. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69316/0.69465. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69275/0.69474. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69253/0.69485. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69249/0.69496. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69218/0.69507. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69228/0.69518. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.69193/0.69530. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69187/0.69543. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.69184/0.69555. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69177/0.69568. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.69129/0.69582. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.69155/0.69596. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69115/0.69610. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69119/0.69626. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69115/0.69641. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.69080/0.69655. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.69060/0.69670. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.69071/0.69686. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.69064/0.69701. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.69029/0.69719. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.69000/0.69736. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.69009/0.69753. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68987/0.69771. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68982/0.69788. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68967/0.69806. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68916/0.69825. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68941/0.69843. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68897/0.69860. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68903/0.69877. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68855/0.69896. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68854/0.69913. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68829/0.69932. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68821/0.69952. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68812/0.69972. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68778/0.69992. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68815/0.70011. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68738/0.70029. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68736/0.70051. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68729/0.70071. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.68673/0.70092. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.68701/0.70112. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68632/0.70134. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68632/0.70158. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.68570/0.70179. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68542/0.70205. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68514/0.70229. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68510/0.70253. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68456/0.70279. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68448/0.70305. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.68392/0.70330. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68380/0.70357. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68313/0.70386. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.68304/0.70417. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68265/0.70445. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68240/0.70473. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.68167/0.70506. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68177/0.70534. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68135/0.70563. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.68050/0.70594. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.68022/0.70626. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67970/0.70656. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.67949/0.70685. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67862/0.70723. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67815/0.70762. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67780/0.70798. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67698/0.70833. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67652/0.70871. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.67578/0.70912. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67510/0.70954. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67458/0.70989. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67402/0.71027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67320/0.71069. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67263/0.71115. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.67220/0.71154. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67118/0.71198. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67039/0.71249. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66947/0.71301. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66921/0.71349. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66808/0.71396. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.66738/0.71447. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69527/0.68880. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69441/0.68975. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69382/0.69071. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69344/0.69170. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69272/0.69269. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69245/0.69360. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69228/0.69446. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69193/0.69525. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69160/0.69596. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69130/0.69648. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69099/0.69695. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69099/0.69739. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69068/0.69772. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69043/0.69806. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69034/0.69823. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69016/0.69840. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68966/0.69869. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68935/0.69889. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68927/0.69904. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68858/0.69926. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68826/0.69936. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68819/0.69946. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68778/0.69949. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68757/0.69962. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68717/0.69987. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68652/0.69992. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68651/0.70002. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68584/0.70022. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68581/0.70044. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68524/0.70068. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68453/0.70093. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68414/0.70128. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68386/0.70152. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68315/0.70203. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68274/0.70250. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68193/0.70306. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68157/0.70375. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68075/0.70424. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68039/0.70499. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67984/0.70571. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67911/0.70671. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67858/0.70759. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67786/0.70837. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67746/0.70955. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67645/0.71014. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67602/0.71137. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67510/0.71267. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67421/0.71388. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67348/0.71519. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67317/0.71610. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67174/0.71753. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67141/0.71885. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67138/0.72005. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66947/0.72111. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66958/0.72257. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66834/0.72365. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66735/0.72489. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66697/0.72603. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66658/0.72738. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66570/0.72874. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66461/0.72935. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66450/0.73059. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66314/0.73146. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66315/0.73289. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66156/0.73409. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66111/0.73483. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66035/0.73596. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65923/0.73723. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65848/0.73769. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65753/0.73890. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65709/0.74036. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65596/0.74094. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65573/0.74116. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65470/0.74231. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65374/0.74320. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65289/0.74460. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.65181/0.74469. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65114/0.74540. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64993/0.74660. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64977/0.74778. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64880/0.74811. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64787/0.74820. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64735/0.74928. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64576/0.75041. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64503/0.75065. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64476/0.75144. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64415/0.75321. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64284/0.75328. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64182/0.75448. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64050/0.75441. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64021/0.75642. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63962/0.75657. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63848/0.75738. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63764/0.75895. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63705/0.75835. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63573/0.75947. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63489/0.76066. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63453/0.76160. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63334/0.76223. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63326/0.76275. Took 0.10 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69327/0.69519. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69290/0.69524. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69294/0.69530. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69289/0.69537. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69290/0.69544. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69249/0.69552. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69251/0.69563. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69245/0.69574. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69216/0.69585. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69246/0.69599. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69200/0.69614. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69182/0.69634. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69184/0.69655. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69147/0.69679. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69146/0.69704. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69074/0.69734. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69080/0.69768. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69023/0.69802. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68983/0.69841. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68956/0.69883. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68907/0.69938. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68823/0.69989. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68781/0.70053. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68720/0.70111. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68653/0.70162. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68567/0.70202. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68456/0.70252. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68404/0.70303. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68308/0.70342. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68209/0.70372. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68141/0.70415. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68002/0.70416. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67920/0.70425. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67816/0.70477. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67705/0.70518. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67573/0.70523. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67480/0.70567. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67355/0.70620. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67241/0.70648. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67088/0.70682. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66948/0.70711. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66866/0.70754. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66689/0.70791. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66573/0.70812. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66391/0.70878. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66291/0.70915. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66114/0.70955. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.65981/0.70975. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65847/0.71020. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65732/0.71048. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65605/0.71122. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65478/0.71159. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65248/0.71215. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65188/0.71250. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64982/0.71293. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64873/0.71351. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64693/0.71419. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64572/0.71482. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64395/0.71555. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64350/0.71606. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64306/0.71690. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64056/0.71774. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63938/0.71838. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63875/0.71906. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.63655/0.72021. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63512/0.72117. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63448/0.72196. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63364/0.72280. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63201/0.72391. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62957/0.72475. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62871/0.72536. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62735/0.72655. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62531/0.72765. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62495/0.72876. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62192/0.73011. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62190/0.73045. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.61991/0.73157. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.61860/0.73220. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.61654/0.73351. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61391/0.73451. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61349/0.73572. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61202/0.73667. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61054/0.73747. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.60733/0.73857. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60637/0.73930. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60359/0.74132. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.60136/0.74278. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60070/0.74448. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60014/0.74594. Took 0.12 sec\n",
      "Epoch 89, Loss(train/val) 0.59628/0.74725. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59665/0.74905. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59402/0.75068. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.59155/0.75345. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58864/0.75579. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.58743/0.75888. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58564/0.76125. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58418/0.76439. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58218/0.76712. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58056/0.77048. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.57878/0.77241. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69145/0.68583. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69140/0.68582. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69130/0.68582. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69124/0.68582. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.68582. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.68582. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69056/0.68582. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69048/0.68583. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69039/0.68585. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69002/0.68588. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69006/0.68592. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68984/0.68595. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68976/0.68598. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.68604. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68913/0.68608. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68894/0.68613. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68871/0.68615. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68827/0.68622. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68784/0.68630. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68743/0.68638. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68711/0.68644. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68628/0.68644. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68569/0.68639. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68501/0.68638. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68439/0.68633. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68341/0.68626. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68265/0.68618. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68185/0.68627. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68053/0.68619. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67966/0.68615. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67863/0.68630. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67784/0.68645. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67644/0.68680. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67551/0.68705. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67400/0.68770. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67336/0.68849. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67162/0.68890. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67080/0.69021. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66956/0.69107. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66838/0.69199. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66708/0.69356. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66647/0.69467. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66499/0.69611. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66341/0.69764. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66191/0.69940. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66064/0.70123. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65946/0.70291. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65843/0.70468. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65649/0.70696. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65528/0.70876. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65413/0.71082. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65219/0.71273. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65073/0.71505. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64964/0.71687. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.64797/0.71939. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64658/0.72156. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64506/0.72343. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64319/0.72522. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64252/0.72774. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64065/0.73012. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63886/0.73260. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.63766/0.73400. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63676/0.73625. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63515/0.73867. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63368/0.74112. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.63219/0.74304. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63094/0.74571. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62822/0.74698. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62732/0.74947. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62538/0.75207. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62353/0.75384. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62296/0.75711. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62073/0.75883. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61905/0.76126. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61850/0.76440. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61613/0.76617. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61520/0.76855. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61317/0.77057. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61219/0.77310. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61055/0.77475. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60949/0.77707. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60786/0.77937. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.60592/0.78129. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60404/0.78341. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60297/0.78605. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60097/0.78793. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59956/0.78994. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59953/0.79137. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.59689/0.79388. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59579/0.79681. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.59436/0.79778. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.59407/0.79970. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59214/0.80227. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59098/0.80338. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58799/0.80354. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58742/0.80617. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58640/0.80846. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58356/0.81145. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58154/0.81268. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58104/0.81627. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69966/0.69384. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69763/0.69161. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69573/0.68961. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69436/0.68792. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69314/0.68667. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69216/0.68584. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69174/0.68533. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69148/0.68505. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69113/0.68490. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69111/0.68478. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69103/0.68472. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69098/0.68468. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69100/0.68465. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69086/0.68462. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69067/0.68460. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69051/0.68458. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69058/0.68456. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69029/0.68453. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69015/0.68450. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69020/0.68448. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68998/0.68446. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69018/0.68444. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69002/0.68440. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68974/0.68437. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68982/0.68434. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68942/0.68431. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68951/0.68426. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68937/0.68422. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68937/0.68417. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68914/0.68413. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68896/0.68408. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68907/0.68402. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68882/0.68397. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68851/0.68391. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68834/0.68384. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68803/0.68377. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68824/0.68370. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68791/0.68364. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68790/0.68356. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68768/0.68350. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68763/0.68343. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68731/0.68335. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68741/0.68330. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68693/0.68321. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68689/0.68313. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68663/0.68306. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68632/0.68297. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68613/0.68290. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68598/0.68284. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68589/0.68278. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68525/0.68272. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68509/0.68265. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68477/0.68261. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68445/0.68258. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68447/0.68255. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68394/0.68255. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68387/0.68255. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68339/0.68256. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68284/0.68259. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68274/0.68265. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68234/0.68273. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68222/0.68282. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68181/0.68295. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68114/0.68309. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68094/0.68327. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67996/0.68345. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67999/0.68363. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67943/0.68383. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67898/0.68404. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67815/0.68429. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67779/0.68457. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67714/0.68487. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67662/0.68515. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67621/0.68542. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67546/0.68567. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67480/0.68590. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.67437/0.68614. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67374/0.68637. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67280/0.68653. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67195/0.68668. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67142/0.68681. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67046/0.68683. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66989/0.68689. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66908/0.68686. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66841/0.68685. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66728/0.68670. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66673/0.68659. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66582/0.68645. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66451/0.68615. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66450/0.68595. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66329/0.68560. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66225/0.68515. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66163/0.68462. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66022/0.68437. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65990/0.68383. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65899/0.68348. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65770/0.68290. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65754/0.68255. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65577/0.68191. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65494/0.68135. Took 0.11 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69802/0.69725. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69777/0.69696. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69710/0.69663. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69618/0.69622. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69568/0.69564. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69443/0.69489. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69389/0.69407. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69297/0.69329. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69218/0.69271. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69119/0.69228. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69065/0.69205. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68985/0.69194. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68935/0.69189. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68954/0.69189. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68865/0.69195. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68901/0.69202. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68850/0.69212. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68807/0.69222. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68802/0.69237. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68749/0.69250. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68715/0.69266. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68696/0.69286. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68657/0.69308. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68581/0.69333. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68554/0.69361. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68557/0.69383. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68458/0.69418. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68393/0.69461. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68307/0.69502. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68247/0.69541. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68134/0.69606. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68093/0.69676. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67968/0.69750. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67900/0.69833. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67777/0.69907. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67668/0.70018. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67645/0.70096. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67468/0.70211. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67363/0.70338. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67277/0.70444. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67155/0.70547. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67105/0.70687. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66995/0.70788. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66870/0.70908. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66825/0.71015. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66695/0.71140. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66560/0.71271. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66507/0.71374. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66476/0.71483. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66351/0.71605. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66281/0.71727. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66179/0.71818. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66132/0.71921. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66039/0.72032. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65971/0.72101. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65886/0.72143. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65782/0.72339. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65665/0.72388. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65523/0.72508. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65523/0.72613. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65440/0.72727. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65303/0.72842. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65224/0.72891. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65220/0.73076. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65077/0.73103. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64970/0.73136. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64861/0.73254. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.64808/0.73300. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64667/0.73458. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64673/0.73513. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64437/0.73614. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64359/0.73746. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64219/0.73869. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64111/0.73971. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63972/0.74067. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63933/0.74159. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63681/0.74276. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.63654/0.74425. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63477/0.74489. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63358/0.74627. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63209/0.74850. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63152/0.74887. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62938/0.75162. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.62781/0.75178. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.62683/0.75384. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62499/0.75522. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62431/0.75723. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62117/0.75872. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62051/0.76020. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61951/0.76132. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61682/0.76499. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61539/0.76672. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61401/0.76856. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61298/0.77070. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60938/0.77364. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.60851/0.77641. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60768/0.77803. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60520/0.78050. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60288/0.78297. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60244/0.78593. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69914/0.69540. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69715/0.69461. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69567/0.69385. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69384/0.69323. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69210/0.69301. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69091/0.69317. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69008/0.69356. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68989/0.69396. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68963/0.69433. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68938/0.69465. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68937/0.69490. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68879/0.69514. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.69538. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68877/0.69562. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68842/0.69584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68862/0.69605. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68811/0.69627. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68807/0.69647. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68781/0.69669. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68794/0.69692. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68745/0.69714. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68749/0.69739. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68703/0.69763. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68654/0.69790. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68683/0.69819. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68609/0.69849. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68628/0.69881. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68591/0.69914. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68571/0.69954. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68516/0.69994. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68523/0.70037. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68464/0.70084. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68420/0.70133. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68405/0.70186. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68389/0.70243. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68365/0.70302. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68312/0.70367. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68270/0.70435. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68209/0.70507. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68202/0.70582. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68157/0.70665. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68094/0.70751. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68085/0.70839. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68015/0.70935. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67995/0.71032. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67932/0.71129. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67877/0.71232. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67836/0.71336. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67801/0.71442. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67739/0.71553. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67683/0.71673. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67610/0.71800. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67590/0.71922. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67532/0.72051. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67459/0.72179. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67362/0.72311. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67340/0.72444. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67285/0.72571. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67209/0.72698. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67135/0.72831. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67082/0.72959. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67006/0.73091. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66924/0.73238. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66867/0.73362. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66794/0.73514. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66740/0.73647. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66669/0.73751. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66618/0.73889. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66534/0.74022. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66448/0.74123. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66319/0.74244. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66261/0.74384. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66256/0.74478. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66137/0.74597. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66076/0.74674. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66012/0.74790. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65889/0.74874. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65878/0.74958. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65810/0.75011. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65672/0.75098. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65632/0.75204. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65522/0.75250. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65434/0.75287. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65488/0.75361. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65414/0.75374. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65285/0.75476. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65208/0.75518. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65138/0.75560. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65132/0.75541. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65036/0.75646. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64933/0.75679. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64814/0.75744. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64796/0.75829. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64695/0.75847. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64697/0.75880. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64545/0.75948. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64528/0.75932. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64404/0.75986. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64354/0.76016. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64254/0.76084. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69897/0.70292. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69613/0.70366. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69468/0.70454. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69364/0.70520. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69310/0.70552. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69249/0.70556. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69182/0.70547. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69171/0.70525. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69127/0.70506. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69096/0.70486. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69028/0.70466. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69043/0.70450. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69009/0.70433. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68969/0.70415. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68933/0.70400. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68940/0.70383. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68896/0.70369. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68864/0.70357. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68830/0.70347. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68802/0.70334. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68782/0.70325. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68766/0.70315. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68730/0.70309. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.70304. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68666/0.70295. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68670/0.70292. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68628/0.70285. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68595/0.70282. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68577/0.70281. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68572/0.70283. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68539/0.70292. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68515/0.70293. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68497/0.70293. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68484/0.70299. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68431/0.70306. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68422/0.70314. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68391/0.70322. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68400/0.70331. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68344/0.70338. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68361/0.70347. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68305/0.70362. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68287/0.70376. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68243/0.70390. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68255/0.70404. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68223/0.70423. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68218/0.70440. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68213/0.70454. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68197/0.70481. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68167/0.70493. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68163/0.70504. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68120/0.70530. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68107/0.70550. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68086/0.70569. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68084/0.70588. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68048/0.70603. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68011/0.70633. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68008/0.70647. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67994/0.70676. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67963/0.70702. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67953/0.70726. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67934/0.70752. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67895/0.70774. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67870/0.70801. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.67856/0.70826. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67809/0.70859. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67813/0.70896. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67776/0.70921. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67781/0.70947. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67733/0.70966. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67710/0.71012. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67674/0.71050. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67638/0.71086. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67582/0.71107. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67615/0.71147. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67578/0.71171. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67528/0.71213. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67484/0.71251. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67484/0.71287. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67449/0.71336. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67426/0.71369. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67393/0.71411. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67353/0.71455. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67298/0.71492. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67269/0.71536. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67262/0.71588. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67189/0.71631. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.67164/0.71672. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.67129/0.71717. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67119/0.71762. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67088/0.71813. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.67006/0.71865. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66992/0.71928. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66956/0.71950. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66917/0.72000. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66861/0.72055. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66825/0.72108. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66770/0.72161. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66741/0.72218. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66700/0.72266. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.66643/0.72300. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69040. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69273/0.69065. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.69092. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69155/0.69124. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69108/0.69165. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.69222. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68973/0.69292. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68908/0.69370. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68876/0.69449. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68846/0.69519. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68826/0.69576. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68808/0.69622. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68794/0.69661. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68773/0.69691. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68758/0.69716. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68731/0.69735. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68730/0.69753. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68705/0.69769. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68682/0.69785. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68643/0.69794. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68661/0.69807. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68609/0.69826. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68629/0.69838. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.69858. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68579/0.69874. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68531/0.69882. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68529/0.69900. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68479/0.69916. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68477/0.69933. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68434/0.69956. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68388/0.69975. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68388/0.69997. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68349/0.70025. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68315/0.70048. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68287/0.70077. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68246/0.70110. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68223/0.70147. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68160/0.70178. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68127/0.70210. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68111/0.70243. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68056/0.70285. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68016/0.70331. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67961/0.70363. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67923/0.70402. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67869/0.70439. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67845/0.70481. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67811/0.70507. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67784/0.70550. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67727/0.70587. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67637/0.70612. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67597/0.70645. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67575/0.70684. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67501/0.70701. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67470/0.70729. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67350/0.70762. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67302/0.70789. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67240/0.70823. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67179/0.70852. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67091/0.70868. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67015/0.70894. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66932/0.70923. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66877/0.70950. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66796/0.70985. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66719/0.71020. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.66635/0.71053. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66517/0.71080. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66434/0.71116. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66377/0.71152. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66279/0.71181. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66165/0.71218. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66120/0.71261. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65926/0.71314. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65858/0.71367. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65756/0.71420. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65660/0.71485. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65515/0.71559. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65431/0.71638. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65298/0.71700. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65195/0.71767. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65156/0.71832. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65012/0.71919. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64891/0.71997. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64814/0.72078. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64739/0.72163. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64635/0.72254. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64490/0.72341. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64378/0.72434. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64301/0.72500. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64266/0.72595. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64115/0.72672. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63976/0.72757. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63854/0.72857. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63812/0.72974. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63613/0.73063. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63513/0.73141. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63498/0.73241. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63314/0.73335. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63217/0.73436. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63194/0.73523. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63032/0.73616. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69562/0.70941. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69561/0.70905. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69541/0.70869. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69484/0.70830. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69480/0.70787. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69473/0.70740. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69425/0.70684. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69367/0.70617. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69343/0.70539. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69323/0.70447. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69278/0.70340. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69253/0.70212. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69138/0.70074. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69065/0.69918. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69004/0.69768. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68939/0.69627. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68931/0.69507. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68819/0.69402. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68763/0.69335. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68718/0.69278. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68602/0.69230. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68512/0.69199. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68492/0.69181. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68384/0.69157. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68338/0.69141. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68246/0.69123. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68194/0.69152. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68131/0.69153. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68055/0.69137. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68013/0.69156. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67914/0.69172. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67871/0.69176. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67800/0.69201. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67778/0.69220. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67678/0.69240. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67640/0.69290. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67568/0.69295. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67513/0.69337. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67414/0.69353. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67401/0.69374. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67333/0.69376. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67206/0.69436. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67142/0.69433. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67091/0.69476. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67033/0.69504. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66992/0.69538. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66900/0.69549. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66861/0.69557. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66737/0.69593. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66688/0.69601. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66590/0.69624. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66547/0.69630. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66472/0.69645. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66400/0.69674. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66301/0.69720. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66216/0.69710. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66108/0.69747. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66057/0.69731. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66014/0.69723. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65970/0.69798. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65851/0.69739. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65819/0.69834. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65681/0.69714. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65603/0.69874. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65474/0.69787. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.65331/0.69832. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65362/0.69754. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65234/0.69854. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65158/0.69809. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65070/0.69819. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64994/0.69815. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64834/0.69801. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64785/0.69773. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64754/0.69759. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64572/0.69705. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64591/0.69662. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64516/0.69690. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64355/0.69603. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64285/0.69817. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64116/0.69595. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64040/0.69520. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63985/0.69585. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63953/0.69658. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63775/0.69635. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63659/0.69619. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63528/0.69463. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63502/0.69581. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63309/0.69633. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63290/0.69571. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63130/0.69596. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63090/0.69597. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62966/0.69646. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62880/0.69662. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62786/0.69626. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62626/0.69789. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62529/0.69625. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62519/0.69668. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62293/0.69692. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62248/0.69729. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62098/0.69855. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70089/0.68833. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69939/0.68895. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69807/0.68982. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69678/0.69096. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69533/0.69243. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69430/0.69407. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69350/0.69564. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69290/0.69694. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69241/0.69793. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69206/0.69864. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69193/0.69922. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69159/0.69964. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69143/0.69991. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69123/0.70008. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69080/0.70021. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69048/0.70035. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69043/0.70044. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69013/0.70051. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68989/0.70059. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68947/0.70065. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68911/0.70078. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68884/0.70097. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68849/0.70123. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68832/0.70140. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68797/0.70153. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68756/0.70162. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68722/0.70186. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68677/0.70213. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68660/0.70244. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68603/0.70280. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68603/0.70312. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68560/0.70350. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68532/0.70383. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68476/0.70425. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68436/0.70468. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68389/0.70530. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68332/0.70587. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68297/0.70635. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68245/0.70697. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68180/0.70778. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68168/0.70840. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68074/0.70928. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68032/0.71011. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67966/0.71100. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67914/0.71191. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67827/0.71310. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67779/0.71431. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67704/0.71540. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67611/0.71653. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67532/0.71789. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67500/0.71906. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67360/0.72024. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67326/0.72160. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67217/0.72286. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67148/0.72415. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67052/0.72543. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67013/0.72662. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66873/0.72781. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66802/0.72903. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66747/0.73027. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66630/0.73150. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66555/0.73257. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66457/0.73350. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66331/0.73446. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66298/0.73545. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66178/0.73625. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66078/0.73719. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65934/0.73820. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65858/0.73918. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65744/0.74021. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65719/0.74102. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65552/0.74181. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65472/0.74275. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65413/0.74360. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65255/0.74447. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65207/0.74567. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65078/0.74651. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64943/0.74751. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64856/0.74868. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64746/0.74943. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64699/0.75031. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64602/0.75125. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.64467/0.75231. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64365/0.75340. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64329/0.75436. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64185/0.75551. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64065/0.75679. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64045/0.75794. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63915/0.75902. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63745/0.76005. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63742/0.76134. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63616/0.76248. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63460/0.76379. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63450/0.76479. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63280/0.76585. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63216/0.76687. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63130/0.76807. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63076/0.76914. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62983/0.77054. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62852/0.77167. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69315/0.69286. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69302/0.69290. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69296/0.69292. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69284/0.69294. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69294. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69258/0.69293. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69235/0.69290. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69237/0.69287. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69231/0.69283. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69204/0.69277. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69196/0.69270. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69186/0.69260. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69173/0.69246. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69154/0.69229. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69123/0.69209. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69124/0.69188. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69107/0.69161. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69079/0.69130. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69068/0.69094. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69050/0.69049. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69013/0.69006. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68983/0.68955. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68951/0.68900. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68933/0.68836. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68895/0.68766. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68859/0.68699. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68818/0.68632. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68759/0.68554. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68718/0.68473. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68688/0.68400. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68647/0.68320. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68552/0.68246. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68516/0.68173. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68428/0.68096. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68325/0.68015. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68310/0.67951. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68229/0.67890. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68150/0.67837. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68065/0.67786. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67967/0.67743. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67885/0.67718. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67776/0.67679. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67692/0.67657. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67596/0.67658. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67510/0.67633. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67398/0.67637. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67322/0.67644. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67233/0.67653. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67112/0.67663. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66990/0.67689. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66878/0.67704. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66845/0.67744. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66679/0.67756. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66578/0.67781. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66519/0.67816. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66407/0.67858. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66317/0.67875. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66256/0.67912. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66142/0.67950. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66115/0.67985. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66012/0.68071. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65880/0.68081. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65799/0.68160. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65754/0.68252. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65680/0.68260. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65630/0.68308. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65511/0.68390. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65391/0.68442. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65390/0.68493. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65331/0.68508. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65243/0.68600. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65165/0.68643. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65074/0.68762. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65024/0.68749. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65019/0.68850. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64918/0.68926. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64799/0.68997. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64788/0.69082. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64709/0.69134. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64653/0.69163. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64641/0.69280. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64543/0.69327. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64446/0.69439. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64404/0.69508. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64328/0.69550. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64251/0.69654. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64155/0.69717. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64137/0.69808. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64040/0.69883. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64028/0.69988. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63917/0.70034. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63966/0.70072. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63844/0.70199. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63800/0.70268. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63737/0.70289. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63647/0.70423. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63623/0.70468. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63518/0.70455. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63450/0.70628. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63450/0.70674. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69406/0.69823. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69395/0.69808. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69385/0.69794. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69360/0.69779. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69345/0.69764. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69338/0.69748. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69316/0.69732. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69337/0.69715. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69305/0.69698. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69301/0.69681. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69268/0.69665. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69242/0.69651. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69244/0.69634. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69256/0.69620. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69228/0.69606. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69226/0.69592. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69219/0.69578. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69183/0.69564. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69164/0.69553. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69138/0.69541. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69109/0.69531. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69112/0.69520. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69082/0.69508. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69051/0.69498. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69030/0.69484. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69001/0.69464. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68966/0.69445. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68956/0.69422. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68908/0.69395. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68867/0.69372. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68802/0.69342. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68740/0.69300. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68710/0.69254. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68636/0.69213. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68568/0.69172. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68530/0.69121. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68449/0.69066. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68365/0.69012. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68300/0.68957. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68247/0.68882. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68141/0.68829. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68076/0.68756. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67970/0.68701. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67887/0.68636. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67762/0.68566. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67714/0.68511. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67620/0.68442. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67516/0.68366. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67389/0.68331. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67282/0.68265. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67227/0.68208. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67072/0.68176. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66967/0.68110. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66862/0.68070. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66786/0.67971. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66600/0.67968. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66538/0.67904. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66359/0.67870. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66272/0.67793. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66184/0.67727. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66001/0.67680. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65880/0.67643. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65750/0.67601. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65635/0.67595. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65512/0.67560. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65333/0.67579. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65266/0.67563. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65162/0.67555. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65007/0.67584. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64865/0.67632. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64834/0.67689. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64726/0.67700. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64501/0.67738. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64542/0.67838. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.64391/0.67817. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64269/0.67953. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64091/0.68042. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64071/0.68049. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63881/0.68086. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63901/0.68225. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63855/0.68222. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63700/0.68333. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63520/0.68397. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63518/0.68485. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63308/0.68541. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63286/0.68515. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.63160/0.68653. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63055/0.68723. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62978/0.68759. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62869/0.68864. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62728/0.68858. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62708/0.69031. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62603/0.68988. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62408/0.69101. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62431/0.69225. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62295/0.69295. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62174/0.69284. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62001/0.69385. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62110/0.69544. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61858/0.69583. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69601/0.69495. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69574/0.69443. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69500/0.69388. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69445/0.69332. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69395/0.69280. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69353/0.69238. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69309/0.69208. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69289/0.69187. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69294/0.69171. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69269/0.69156. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69237/0.69141. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69208/0.69128. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69198/0.69118. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69168/0.69105. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69169/0.69092. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69116/0.69079. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69134/0.69066. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69092/0.69053. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69064/0.69040. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69048/0.69026. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69022/0.69011. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68995/0.68994. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68939/0.68981. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68946/0.68965. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68928/0.68948. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68892/0.68929. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68861/0.68913. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68809/0.68895. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68787/0.68876. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68711/0.68859. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68695/0.68846. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68691/0.68827. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68612/0.68804. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68581/0.68784. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68542/0.68766. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68484/0.68745. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68466/0.68724. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68386/0.68707. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68363/0.68680. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68295/0.68657. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68250/0.68632. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68214/0.68611. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68133/0.68593. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68119/0.68566. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68017/0.68546. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67980/0.68520. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67886/0.68498. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67831/0.68477. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67755/0.68460. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67703/0.68430. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67680/0.68407. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67629/0.68392. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67535/0.68371. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67418/0.68358. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67346/0.68320. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67294/0.68295. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67208/0.68270. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67173/0.68238. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67073/0.68214. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67041/0.68181. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66949/0.68155. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66893/0.68129. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66770/0.68119. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66718/0.68084. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66595/0.68051. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66477/0.68027. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66470/0.67999. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66380/0.67970. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66407/0.67938. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66242/0.67902. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66187/0.67874. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66082/0.67837. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66077/0.67808. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65918/0.67774. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65878/0.67764. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65769/0.67732. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65726/0.67704. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65626/0.67677. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65562/0.67652. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65451/0.67635. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65288/0.67603. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65339/0.67584. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65285/0.67564. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65189/0.67545. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65040/0.67521. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65000/0.67501. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64969/0.67501. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64859/0.67505. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64818/0.67478. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64673/0.67474. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64596/0.67459. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64529/0.67446. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.64578/0.67438. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64457/0.67467. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64255/0.67453. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64315/0.67442. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64109/0.67478. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64066/0.67464. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64031/0.67493. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64021/0.67468. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69399/0.68868. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69422/0.68918. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69368/0.68972. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69338/0.69025. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69313/0.69082. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69323/0.69142. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69278/0.69199. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69241/0.69259. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69212/0.69320. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69210/0.69379. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69163/0.69433. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69144/0.69480. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69135/0.69529. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69120/0.69571. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69079/0.69602. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69090/0.69625. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69052/0.69650. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69062/0.69674. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69010/0.69701. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68974/0.69727. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68952/0.69746. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68928/0.69766. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68907/0.69795. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68898/0.69825. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68873/0.69848. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68848/0.69875. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68831/0.69892. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68804/0.69910. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68749/0.69926. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68700/0.69950. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68731/0.69980. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68703/0.70012. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68682/0.70037. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68625/0.70066. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68613/0.70089. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68557/0.70106. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68532/0.70137. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68503/0.70161. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68440/0.70191. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68435/0.70225. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68425/0.70260. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68339/0.70282. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68319/0.70303. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68270/0.70333. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68199/0.70369. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68187/0.70399. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68136/0.70433. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68099/0.70470. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68021/0.70484. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68011/0.70527. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67977/0.70587. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67901/0.70622. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67814/0.70640. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67757/0.70659. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67732/0.70727. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67649/0.70744. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67551/0.70778. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67500/0.70805. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67514/0.70845. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67400/0.70901. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67378/0.70913. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67352/0.70958. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67251/0.71013. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67166/0.71052. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67092/0.71075. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67048/0.71120. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66993/0.71156. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66936/0.71181. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66807/0.71227. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66802/0.71294. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66737/0.71316. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66717/0.71367. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66610/0.71408. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66542/0.71486. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66427/0.71503. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66372/0.71588. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.66327/0.71613. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66232/0.71628. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66227/0.71722. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66100/0.71741. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66013/0.71775. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65981/0.71849. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65851/0.71877. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65795/0.71941. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65723/0.72012. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65687/0.72062. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65595/0.72114. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65545/0.72163. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65391/0.72236. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65379/0.72239. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65188/0.72353. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65206/0.72408. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65006/0.72462. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65050/0.72557. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64995/0.72643. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64797/0.72699. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64754/0.72737. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64674/0.72900. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64487/0.72955. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64360/0.73098. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69513/0.67703. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69501/0.67727. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69486/0.67756. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69447/0.67785. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69430/0.67819. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69407/0.67859. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69398/0.67901. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69379/0.67946. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69354/0.67993. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69334/0.68038. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69311/0.68084. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69239/0.68130. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69273/0.68166. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69260/0.68194. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69206/0.68214. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69204/0.68227. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69151/0.68234. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69170/0.68239. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69124/0.68242. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69092/0.68249. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69059/0.68243. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69023/0.68228. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.69017/0.68235. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68977/0.68228. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68949/0.68228. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68926/0.68227. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68890/0.68219. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68813/0.68226. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68765/0.68263. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68710/0.68297. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68694/0.68335. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68603/0.68376. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68546/0.68405. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68510/0.68484. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68437/0.68563. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68378/0.68651. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68333/0.68764. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68270/0.68852. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68174/0.68967. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68100/0.69075. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68020/0.69220. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67970/0.69328. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67882/0.69494. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67781/0.69603. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67712/0.69773. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67638/0.69943. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67483/0.70102. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67436/0.70250. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67326/0.70419. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67211/0.70585. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67101/0.70757. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66996/0.70917. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66897/0.71114. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66798/0.71284. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66640/0.71444. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66573/0.71628. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66456/0.71794. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66306/0.71945. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66240/0.72147. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66108/0.72315. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65961/0.72471. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65797/0.72619. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65767/0.72769. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65613/0.72929. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65510/0.73089. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65393/0.73218. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65312/0.73399. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65122/0.73525. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65067/0.73619. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64925/0.73731. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64843/0.73855. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.64648/0.73955. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64588/0.74030. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64388/0.74155. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64385/0.74259. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64174/0.74341. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64077/0.74451. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63946/0.74537. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63788/0.74610. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63702/0.74712. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63604/0.74736. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63497/0.74824. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63355/0.74941. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.63307/0.75038. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.63138/0.75068. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63013/0.75139. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62921/0.75198. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62734/0.75225. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62605/0.75348. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62510/0.75384. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62408/0.75471. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62192/0.75539. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62069/0.75649. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61956/0.75752. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61905/0.75903. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61651/0.75973. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61621/0.76094. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61439/0.76215. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61287/0.76314. Took 0.08 sec\n",
      "Epoch 99, Loss(train/val) 0.61222/0.76393. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69876/0.70110. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69735/0.69866. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69632/0.69639. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69501/0.69413. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69376/0.69191. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69271/0.69013. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69209/0.68881. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69185/0.68794. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69156/0.68738. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69176/0.68706. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69124/0.68687. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69142/0.68678. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69098/0.68674. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69101/0.68669. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69070/0.68670. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69064/0.68672. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69075/0.68669. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69040/0.68672. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69016/0.68679. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68997/0.68684. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68993/0.68691. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68956/0.68697. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68950/0.68704. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68926/0.68713. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68909/0.68720. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68862/0.68724. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68843/0.68729. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68857/0.68741. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68831/0.68753. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68777/0.68773. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68750/0.68791. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68729/0.68808. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68691/0.68822. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68692/0.68846. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68648/0.68868. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68622/0.68901. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68558/0.68924. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68531/0.68947. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68493/0.68971. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68483/0.69010. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68408/0.69045. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68376/0.69090. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68342/0.69132. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68290/0.69168. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68240/0.69221. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68156/0.69244. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68136/0.69293. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68067/0.69333. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68036/0.69393. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67971/0.69401. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67974/0.69451. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67860/0.69469. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67784/0.69516. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67790/0.69558. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67672/0.69598. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67655/0.69623. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67570/0.69653. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67520/0.69693. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67476/0.69719. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67380/0.69735. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67379/0.69776. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67257/0.69806. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67180/0.69835. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67143/0.69855. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67116/0.69878. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67037/0.69913. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66823/0.69933. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66858/0.69973. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66768/0.69961. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66723/0.69999. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66609/0.70042. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66542/0.70071. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66498/0.70125. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66334/0.70156. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66317/0.70169. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66246/0.70183. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66133/0.70210. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66010/0.70255. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65895/0.70279. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65821/0.70336. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65673/0.70376. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65672/0.70422. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65601/0.70436. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65542/0.70495. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65424/0.70547. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65261/0.70585. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65175/0.70681. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65049/0.70750. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65015/0.70812. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64860/0.70876. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64749/0.70950. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64654/0.71034. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64623/0.71105. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64434/0.71193. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64489/0.71272. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64238/0.71348. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64101/0.71465. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64058/0.71574. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63986/0.71680. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63766/0.71760. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69322/0.69610. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69279/0.69626. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69642. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69215/0.69661. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69216/0.69682. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69157/0.69704. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69135/0.69729. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69119/0.69755. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69106/0.69784. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69091/0.69812. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69067/0.69834. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69046/0.69849. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69050/0.69864. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69009/0.69875. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68990/0.69883. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68980/0.69889. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68979/0.69896. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68951/0.69900. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68949/0.69902. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68900/0.69905. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68890/0.69904. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68878/0.69906. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68864/0.69913. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68824/0.69913. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68784/0.69914. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68777/0.69915. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68753/0.69912. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68725/0.69918. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68702/0.69914. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68676/0.69915. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68657/0.69913. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68626/0.69909. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68580/0.69904. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68551/0.69897. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68543/0.69885. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68487/0.69881. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68475/0.69871. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68399/0.69848. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68374/0.69843. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68371/0.69839. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68285/0.69829. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68253/0.69805. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68209/0.69790. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68181/0.69781. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68172/0.69769. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68091/0.69732. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68036/0.69703. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68042/0.69691. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67975/0.69666. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67933/0.69637. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67898/0.69596. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67788/0.69582. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67757/0.69555. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67730/0.69531. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67705/0.69506. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67609/0.69479. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67577/0.69441. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67498/0.69425. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67416/0.69393. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67364/0.69368. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67345/0.69353. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67264/0.69309. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67200/0.69299. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67127/0.69283. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67043/0.69247. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67010/0.69223. Took 0.12 sec\n",
      "Epoch 66, Loss(train/val) 0.66931/0.69238. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66835/0.69170. Took 0.12 sec\n",
      "Epoch 68, Loss(train/val) 0.66717/0.69195. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66725/0.69185. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66599/0.69210. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66538/0.69174. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66484/0.69206. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66416/0.69201. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66289/0.69225. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66194/0.69246. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66186/0.69267. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66097/0.69301. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65954/0.69333. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65920/0.69389. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65804/0.69418. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65711/0.69471. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65619/0.69500. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65619/0.69547. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65564/0.69591. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65390/0.69696. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65359/0.69694. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65249/0.69788. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65189/0.69833. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65075/0.69915. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64996/0.70014. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64928/0.70081. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64842/0.70159. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64717/0.70185. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64673/0.70296. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64563/0.70311. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.64526/0.70485. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64424/0.70421. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64334/0.70624. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64272/0.70615. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70121/0.68543. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69696/0.68701. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69425/0.68955. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69202. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69188/0.69376. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69177/0.69492. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69133/0.69567. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69109/0.69617. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.69654. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69085/0.69690. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69053/0.69722. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69025/0.69756. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69034/0.69781. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69020/0.69806. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68974/0.69830. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68982/0.69853. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68987/0.69881. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68950/0.69900. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68916/0.69922. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68905/0.69945. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68898/0.69971. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68901/0.69993. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68880/0.70007. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68873/0.70026. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68846/0.70047. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68873/0.70072. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68821/0.70093. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68792/0.70115. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68789/0.70131. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68762/0.70154. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68760/0.70169. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68742/0.70195. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68730/0.70223. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68712/0.70244. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68697/0.70261. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68677/0.70284. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68667/0.70313. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68666/0.70344. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68636/0.70370. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68604/0.70395. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68654/0.70416. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68612/0.70439. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68600/0.70470. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68567/0.70494. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68557/0.70514. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68520/0.70541. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68509/0.70569. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68505/0.70594. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68463/0.70623. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.68471/0.70651. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68449/0.70669. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68425/0.70696. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68416/0.70722. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68379/0.70744. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68371/0.70776. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68320/0.70793. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68309/0.70828. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68294/0.70857. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68272/0.70875. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68239/0.70904. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68231/0.70931. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68197/0.70976. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68134/0.71001. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.68133/0.71022. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68091/0.71058. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68032/0.71081. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68034/0.71121. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67979/0.71151. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67990/0.71181. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67952/0.71217. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67812/0.71247. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.67854/0.71291. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67816/0.71314. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67732/0.71344. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67706/0.71385. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67649/0.71419. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67640/0.71463. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67552/0.71494. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67505/0.71565. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67483/0.71603. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.67399/0.71647. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67350/0.71700. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67271/0.71746. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67195/0.71817. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67119/0.71883. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67090/0.71938. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66994/0.71989. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66918/0.72063. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66845/0.72160. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66739/0.72232. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.66681/0.72351. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66610/0.72441. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66523/0.72533. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66413/0.72655. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66339/0.72735. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66265/0.72850. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66217/0.72968. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66085/0.73087. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66068/0.73186. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65916/0.73322. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69277/0.69499. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.69459. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69084/0.69437. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68987/0.69430. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69435. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68863/0.69449. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68801/0.69469. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68788/0.69495. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68735/0.69523. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68723/0.69548. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68690/0.69569. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68659/0.69591. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68622/0.69610. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68615/0.69625. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68610/0.69643. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68573/0.69658. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68545/0.69675. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68531/0.69695. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68490/0.69715. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68463/0.69736. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68449/0.69761. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68418/0.69785. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68370/0.69816. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68336/0.69849. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68300/0.69882. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68252/0.69923. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68250/0.69964. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68177/0.70010. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68127/0.70060. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68070/0.70112. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68042/0.70169. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68020/0.70225. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68004/0.70289. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67918/0.70357. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67839/0.70432. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67830/0.70506. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67730/0.70580. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67680/0.70664. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67611/0.70760. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67546/0.70856. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67441/0.70963. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67375/0.71081. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67357/0.71190. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67263/0.71297. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67184/0.71430. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67077/0.71564. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67001/0.71701. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66881/0.71850. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66826/0.72005. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66711/0.72169. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66594/0.72329. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66533/0.72499. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66466/0.72673. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66279/0.72860. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66254/0.73040. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66182/0.73224. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66080/0.73408. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65987/0.73577. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65860/0.73789. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65780/0.73968. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65666/0.74183. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65550/0.74370. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65501/0.74553. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65378/0.74732. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65262/0.74915. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65172/0.75112. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65127/0.75268. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65003/0.75421. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64909/0.75612. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64871/0.75774. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64738/0.75931. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64602/0.76111. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64498/0.76290. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64386/0.76431. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64432/0.76588. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64275/0.76747. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64044/0.76885. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64019/0.77078. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63955/0.77242. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.63786/0.77396. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63674/0.77501. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63602/0.77704. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63498/0.77870. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63412/0.78015. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63345/0.78129. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63237/0.78276. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63114/0.78370. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63019/0.78586. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62856/0.78698. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62819/0.78936. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62743/0.79040. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62726/0.79157. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62610/0.79297. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62487/0.79455. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62315/0.79556. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62319/0.79671. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62218/0.79756. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62121/0.79891. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61992/0.80079. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61905/0.80187. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69644/0.69296. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69588/0.69289. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69553/0.69286. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69513/0.69283. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69495/0.69276. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69466/0.69265. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69437/0.69248. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69350/0.69225. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69314/0.69195. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69215/0.69161. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69112/0.69138. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68979/0.69154. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68842/0.69222. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68767/0.69311. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68730/0.69387. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68686/0.69448. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68666/0.69493. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68652/0.69527. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68618/0.69556. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68604/0.69587. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68583/0.69620. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68553/0.69656. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68513/0.69696. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68517/0.69738. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68455/0.69782. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68436/0.69831. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68394/0.69889. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68377/0.69946. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68325/0.70009. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68286/0.70078. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68250/0.70156. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68210/0.70232. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68189/0.70316. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68122/0.70401. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68075/0.70489. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68040/0.70579. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68008/0.70674. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67960/0.70761. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67902/0.70852. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67845/0.70940. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67810/0.71032. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67759/0.71121. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67675/0.71216. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67654/0.71315. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67590/0.71411. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67531/0.71502. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67475/0.71591. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67427/0.71682. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67353/0.71772. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67313/0.71874. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67225/0.71970. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67163/0.72070. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67093/0.72166. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67040/0.72274. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66956/0.72374. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66871/0.72481. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 0.66792/0.72588. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66764/0.72694. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66666/0.72785. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66570/0.72898. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66495/0.73001. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66423/0.73102. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66330/0.73222. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66224/0.73332. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66172/0.73429. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66082/0.73538. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65993/0.73657. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65895/0.73753. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65790/0.73874. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65680/0.73974. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65611/0.74098. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65522/0.74207. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65394/0.74335. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65261/0.74457. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65131/0.74593. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65037/0.74717. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64982/0.74837. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64854/0.74950. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64760/0.75076. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64673/0.75226. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64505/0.75338. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64431/0.75485. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64184/0.75594. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.64087/0.75730. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64019/0.75914. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63899/0.76043. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63764/0.76218. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63652/0.76388. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63474/0.76547. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63357/0.76671. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63252/0.76824. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63139/0.76992. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62977/0.77191. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62864/0.77362. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62731/0.77564. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62586/0.77674. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62502/0.77784. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62343/0.77957. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62187/0.78153. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.62097/0.78319. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70041/0.68081. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69545/0.68424. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69282/0.68805. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69160/0.69154. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69424. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69051/0.69605. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69025/0.69728. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68987/0.69816. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68983/0.69890. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.69954. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68926/0.70004. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68897/0.70055. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68889/0.70090. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68862/0.70143. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68864/0.70191. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68814/0.70246. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68816/0.70300. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68767/0.70344. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68755/0.70382. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68773/0.70419. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68735/0.70463. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68694/0.70512. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68713/0.70555. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68672/0.70604. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68625/0.70650. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68644/0.70706. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68602/0.70750. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68578/0.70801. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68541/0.70855. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68517/0.70907. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68516/0.70959. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68473/0.71011. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68468/0.71061. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68415/0.71115. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68397/0.71163. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68373/0.71214. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68389/0.71259. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68355/0.71315. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68313/0.71361. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68292/0.71407. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68244/0.71447. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68226/0.71478. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68200/0.71531. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68167/0.71573. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68142/0.71609. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68090/0.71646. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68089/0.71674. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68061/0.71716. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68030/0.71758. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67987/0.71774. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67965/0.71800. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67896/0.71825. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67882/0.71849. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67855/0.71870. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67787/0.71896. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67746/0.71916. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67700/0.71938. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67681/0.71945. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67601/0.71977. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67599/0.71988. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67526/0.72000. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67462/0.72019. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67436/0.72045. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67378/0.72079. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67357/0.72101. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67291/0.72103. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67175/0.72149. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67186/0.72177. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67041/0.72198. Took 0.08 sec\n",
      "Epoch 69, Loss(train/val) 0.66986/0.72231. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66917/0.72243. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66916/0.72300. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66819/0.72293. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66761/0.72301. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66670/0.72370. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66630/0.72394. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66546/0.72409. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66476/0.72466. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66365/0.72491. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66306/0.72539. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66244/0.72563. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66142/0.72631. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66066/0.72673. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65998/0.72691. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65880/0.72779. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65821/0.72829. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65735/0.72874. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65659/0.72980. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65659/0.72939. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65489/0.73059. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65443/0.73130. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65368/0.73159. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65331/0.73246. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65264/0.73292. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65107/0.73353. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.65059/0.73451. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64987/0.73522. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64920/0.73598. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64862/0.73632. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64775/0.73695. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69404/0.69205. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.69142. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69221/0.69094. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69060. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69036. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69061/0.69022. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69049/0.69016. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69000/0.69012. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68975/0.69008. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68955/0.69003. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68930/0.68999. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68892/0.68994. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68882/0.68990. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68828/0.68985. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68806/0.68981. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68794/0.68977. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68764/0.68976. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68742/0.68975. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68702/0.68976. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68668/0.68980. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68654/0.68987. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68589/0.68999. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68555/0.69016. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68502/0.69035. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68454/0.69061. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68442/0.69091. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68416/0.69128. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68359/0.69167. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68307/0.69214. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68298/0.69266. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68244/0.69327. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68198/0.69388. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68147/0.69452. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68112/0.69520. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68055/0.69594. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68052/0.69671. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67976/0.69747. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67929/0.69826. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67868/0.69905. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67861/0.69980. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67777/0.70061. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67745/0.70149. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67713/0.70232. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67681/0.70317. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67607/0.70399. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67566/0.70482. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67501/0.70565. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67487/0.70643. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67410/0.70716. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67371/0.70788. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67342/0.70861. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67269/0.70938. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67196/0.71012. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67133/0.71086. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67149/0.71160. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67073/0.71233. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66997/0.71307. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66915/0.71379. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66891/0.71455. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66817/0.71529. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66771/0.71599. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66760/0.71667. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66697/0.71742. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66629/0.71815. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66561/0.71896. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66523/0.71979. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66426/0.72058. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66379/0.72136. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66349/0.72203. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66273/0.72285. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66255/0.72358. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66256/0.72425. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66142/0.72506. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66094/0.72593. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66032/0.72664. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65938/0.72729. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65846/0.72810. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65824/0.72895. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65722/0.72984. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65662/0.73066. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65686/0.73147. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65519/0.73244. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65474/0.73338. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65340/0.73441. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65346/0.73541. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65213/0.73633. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65159/0.73742. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64990/0.73849. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64953/0.73955. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64828/0.74077. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64698/0.74191. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64649/0.74346. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64573/0.74479. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64382/0.74604. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64341/0.74719. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64283/0.74852. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64157/0.75001. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64074/0.75132. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63882/0.75278. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63858/0.75422. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69478/0.69230. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69432/0.69246. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69397/0.69266. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69363/0.69287. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69263/0.69313. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69196/0.69348. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.69391. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69102/0.69446. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69514. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68948/0.69597. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68852/0.69686. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68756/0.69767. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68705/0.69845. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68658/0.69905. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68597/0.69953. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68518/0.69981. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68445/0.70010. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68418/0.70034. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68332/0.70046. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68237/0.70054. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68178/0.70057. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68087/0.70067. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67996/0.70083. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67887/0.70101. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67767/0.70117. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67651/0.70106. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67543/0.70143. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67433/0.70142. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67275/0.70201. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67196/0.70204. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67099/0.70219. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66987/0.70262. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66853/0.70292. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66782/0.70313. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66706/0.70368. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66597/0.70388. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.66522/0.70423. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66390/0.70431. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66299/0.70448. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66260/0.70470. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66191/0.70482. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66108/0.70534. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66073/0.70536. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65996/0.70592. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65948/0.70561. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65849/0.70599. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65837/0.70576. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65740/0.70616. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65638/0.70681. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65565/0.70678. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65506/0.70712. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65474/0.70719. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65391/0.70751. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65276/0.70800. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65184/0.70800. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65182/0.70848. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65045/0.70903. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65021/0.70884. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64996/0.70935. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64912/0.70973. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64857/0.71033. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.64786/0.71062. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64692/0.71140. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64626/0.71152. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64557/0.71212. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64458/0.71255. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64382/0.71275. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.64331/0.71354. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64308/0.71358. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64156/0.71376. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64092/0.71465. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64083/0.71520. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63969/0.71554. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63831/0.71637. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63789/0.71665. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63676/0.71773. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.63667/0.71744. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63462/0.71833. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63426/0.71842. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63398/0.71942. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63358/0.71991. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63232/0.72023. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63104/0.72084. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63055/0.72107. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62939/0.72245. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62795/0.72267. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62793/0.72328. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62717/0.72403. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62538/0.72446. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62592/0.72470. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62318/0.72566. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62331/0.72652. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62173/0.72711. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62129/0.72793. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62034/0.72766. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61937/0.72896. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61823/0.72925. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.61697/0.72994. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61582/0.73049. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61456/0.73063. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69387/0.69643. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69274/0.69556. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69463. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69110/0.69374. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69026/0.69298. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68957/0.69237. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68904/0.69189. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68898/0.69157. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68859/0.69131. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68852/0.69112. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68802/0.69097. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68796/0.69084. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68802/0.69071. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.69062. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68732/0.69052. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68738/0.69044. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68706/0.69039. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68689/0.69034. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68686/0.69029. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68619/0.69023. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68621/0.69017. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68645/0.69014. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68562/0.69012. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68559/0.69006. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68485/0.68999. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68493/0.68991. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68503/0.68988. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68416/0.68981. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68427/0.68976. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68382/0.68969. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68333/0.68968. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68284/0.68962. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68207/0.68956. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68186/0.68949. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68124/0.68943. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68102/0.68940. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67990/0.68936. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67920/0.68923. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67851/0.68924. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67786/0.68925. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67726/0.68916. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67604/0.68924. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67558/0.68939. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67472/0.68953. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67331/0.68976. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67263/0.68999. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67174/0.69018. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67075/0.69048. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67029/0.69087. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66852/0.69123. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66794/0.69151. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66755/0.69195. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66582/0.69250. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66523/0.69292. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66443/0.69332. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66361/0.69367. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66237/0.69446. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66160/0.69491. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66050/0.69532. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66052/0.69592. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65895/0.69650. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65910/0.69690. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65784/0.69751. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65718/0.69802. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65597/0.69861. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65538/0.69878. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65545/0.69933. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65384/0.69997. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65289/0.70047. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65249/0.70095. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65242/0.70122. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65053/0.70149. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65112/0.70193. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64913/0.70316. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65003/0.70275. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64849/0.70343. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64771/0.70393. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64805/0.70447. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64678/0.70462. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64585/0.70541. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64495/0.70554. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64457/0.70573. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64482/0.70649. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64370/0.70715. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64305/0.70679. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64205/0.70762. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64138/0.70844. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64009/0.70889. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64061/0.70906. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63948/0.70981. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63893/0.70997. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63856/0.71041. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63712/0.71050. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63700/0.71159. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63599/0.71142. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63624/0.71252. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63341/0.71212. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63402/0.71313. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63335/0.71383. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63253/0.71375. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69354/0.69313. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69260/0.69021. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69167/0.68765. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69120/0.68555. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.68393. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69038/0.68262. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69015/0.68170. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68991/0.68103. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68960/0.68058. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68930/0.68025. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68920/0.68000. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68896/0.67981. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68853/0.67964. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68893/0.67947. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68854/0.67939. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68810/0.67931. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68819/0.67925. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68776/0.67918. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68750/0.67920. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68739/0.67923. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68708/0.67923. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68694/0.67935. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68661/0.67943. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68637/0.67955. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68649/0.67959. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68597/0.67962. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68580/0.67965. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68557/0.67978. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68529/0.67989. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68493/0.68013. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68493/0.68034. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68474/0.68050. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68467/0.68070. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68420/0.68075. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68355/0.68094. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68348/0.68096. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68383/0.68107. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68336/0.68112. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68262/0.68122. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68241/0.68154. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68245/0.68178. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68241/0.68191. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68185/0.68201. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68156/0.68203. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68158/0.68221. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68131/0.68255. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68092/0.68267. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68065/0.68252. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68025/0.68276. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67994/0.68298. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67996/0.68314. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67926/0.68324. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67922/0.68321. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67886/0.68344. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67834/0.68354. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67807/0.68382. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67774/0.68371. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67720/0.68393. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67671/0.68419. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67657/0.68420. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67658/0.68456. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67561/0.68467. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67534/0.68463. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67492/0.68487. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67427/0.68529. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67342/0.68546. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67324/0.68571. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67326/0.68569. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67250/0.68646. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67146/0.68661. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67113/0.68676. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67063/0.68717. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66962/0.68767. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66906/0.68797. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66791/0.68799. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66760/0.68905. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66657/0.68915. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66689/0.68965. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66541/0.69029. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66442/0.69068. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66370/0.69171. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66326/0.69240. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66179/0.69288. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66089/0.69368. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65987/0.69447. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65979/0.69535. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65903/0.69675. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65798/0.69734. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65670/0.69868. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65675/0.69948. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65562/0.70066. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65465/0.70191. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65395/0.70264. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65239/0.70328. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65215/0.70453. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65151/0.70590. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65092/0.70762. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65034/0.70835. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64872/0.70953. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64739/0.70974. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69455/0.69140. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69334/0.69428. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69201/0.69776. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.70139. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69036/0.70474. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68989/0.70733. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68945/0.70926. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68929/0.71052. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68886/0.71131. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68906/0.71171. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68850/0.71207. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68835/0.71219. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68785/0.71220. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.71224. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68785/0.71210. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68743/0.71201. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68727/0.71170. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68734/0.71148. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68679/0.71119. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68700/0.71082. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68629/0.71039. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68599/0.71003. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68604/0.70984. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68558/0.70958. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68530/0.70926. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68528/0.70883. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68501/0.70844. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68482/0.70816. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68424/0.70782. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68403/0.70749. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68383/0.70727. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68323/0.70709. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68299/0.70727. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68305/0.70674. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68270/0.70667. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68251/0.70675. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68179/0.70651. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68181/0.70663. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68139/0.70668. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68115/0.70685. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68099/0.70683. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68067/0.70673. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68016/0.70690. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68016/0.70722. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67955/0.70727. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67905/0.70733. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67887/0.70751. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67862/0.70770. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67824/0.70842. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67841/0.70896. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67792/0.70890. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67740/0.70918. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67699/0.70943. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67666/0.70964. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67594/0.70994. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67615/0.70997. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67584/0.71026. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67559/0.71114. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67464/0.71120. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67444/0.71118. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67401/0.71187. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67387/0.71225. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67318/0.71275. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67288/0.71309. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67184/0.71263. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67151/0.71328. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67114/0.71388. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67030/0.71395. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66997/0.71404. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66948/0.71401. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66837/0.71429. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66833/0.71470. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66789/0.71494. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66705/0.71461. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66605/0.71579. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66554/0.71529. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66442/0.71575. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66421/0.71509. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66348/0.71635. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66303/0.71651. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66244/0.71647. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66154/0.71698. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66096/0.71658. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65962/0.71670. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65872/0.71730. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65789/0.71733. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65639/0.71763. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65642/0.71815. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65498/0.71815. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65404/0.71814. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65281/0.71881. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65339/0.71840. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65132/0.71857. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65118/0.71958. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64915/0.72054. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64807/0.71927. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64711/0.71993. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64720/0.72026. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64533/0.72121. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64524/0.72129. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69255/0.68926. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69226/0.68944. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.68960. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69174/0.68975. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69180/0.68992. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69132/0.69009. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69156/0.69028. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69141/0.69047. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69144/0.69066. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69120/0.69092. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69110/0.69116. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69079/0.69140. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69071/0.69168. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69056/0.69195. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69039/0.69226. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69034/0.69256. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69008/0.69288. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68976/0.69323. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68968/0.69357. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68933/0.69392. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68887/0.69429. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68878/0.69467. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68847/0.69507. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68817/0.69548. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68794/0.69581. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68714/0.69619. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68711/0.69653. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68637/0.69692. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68637/0.69726. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68572/0.69754. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68509/0.69783. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68449/0.69809. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68380/0.69832. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68309/0.69856. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68282/0.69857. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68202/0.69872. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68153/0.69852. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68066/0.69834. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67995/0.69851. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67951/0.69816. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67855/0.69806. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67767/0.69769. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67681/0.69743. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67580/0.69750. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67485/0.69730. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67392/0.69721. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67307/0.69694. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67241/0.69655. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67103/0.69665. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67023/0.69662. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66919/0.69629. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66830/0.69643. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66691/0.69643. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66596/0.69642. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66505/0.69640. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66396/0.69631. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66306/0.69612. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66176/0.69594. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66045/0.69622. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65912/0.69649. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65821/0.69648. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65677/0.69627. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65609/0.69636. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65513/0.69632. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65326/0.69662. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65231/0.69651. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65133/0.69699. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65002/0.69593. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64876/0.69666. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64811/0.69648. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64560/0.69640. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64531/0.69679. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.64388/0.69728. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64226/0.69657. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64108/0.69680. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63961/0.69784. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63811/0.69759. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63735/0.69812. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63639/0.69813. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63453/0.69817. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63165/0.69854. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63147/0.69948. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62933/0.69967. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.62724/0.70032. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62565/0.70149. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62566/0.70227. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62290/0.70203. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62190/0.70251. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62048/0.70315. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61754/0.70412. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.61635/0.70489. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61541/0.70592. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61452/0.70641. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61201/0.70805. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61056/0.71029. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60931/0.71089. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60604/0.71241. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60537/0.71278. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60361/0.71426. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60244/0.71565. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69606/0.69528. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69514/0.69340. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69431/0.69226. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69411/0.69153. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69364/0.69117. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69330/0.69097. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69308/0.69088. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69296/0.69096. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69258/0.69105. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69248/0.69112. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69246/0.69123. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69241/0.69130. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69199/0.69141. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69179/0.69157. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69176/0.69173. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69151/0.69192. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69146/0.69205. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69121/0.69220. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69105/0.69232. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69096/0.69253. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69074/0.69271. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69076/0.69290. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69059/0.69307. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69041/0.69321. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.69002/0.69340. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69016/0.69352. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68997/0.69373. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.69005/0.69388. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68958/0.69407. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68941/0.69424. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68937/0.69432. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68917/0.69457. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68923/0.69470. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68887/0.69493. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68887/0.69511. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68876/0.69522. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68863/0.69535. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68870/0.69552. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68834/0.69565. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68838/0.69585. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68833/0.69590. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68796/0.69590. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68823/0.69594. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68798/0.69612. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68767/0.69634. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68794/0.69640. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68774/0.69642. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68755/0.69661. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68729/0.69671. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68717/0.69680. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68736/0.69683. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.68724/0.69679. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68679/0.69688. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68680/0.69692. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68684/0.69695. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68668/0.69701. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68619/0.69703. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68638/0.69707. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68597/0.69714. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68606/0.69723. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68599/0.69718. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68587/0.69724. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68574/0.69731. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68535/0.69727. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.68535/0.69725. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68524/0.69719. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68473/0.69719. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68501/0.69717. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68438/0.69724. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68418/0.69724. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68405/0.69728. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68385/0.69729. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.68357/0.69734. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.68336/0.69728. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68315/0.69734. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.68327/0.69719. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.68254/0.69723. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68279/0.69749. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.68206/0.69743. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.68195/0.69748. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.68124/0.69736. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.68130/0.69767. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.68100/0.69779. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.68078/0.69777. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.68062/0.69768. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.68024/0.69783. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67953/0.69798. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67892/0.69862. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.67861/0.69868. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67807/0.69890. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67808/0.69906. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67749/0.69932. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67681/0.69966. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67645/0.69999. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67584/0.70030. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67558/0.70103. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.67488/0.70136. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.67511/0.70205. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.67429/0.70249. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.67361/0.70342. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69397/0.69404. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69332/0.69353. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69315/0.69316. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69306/0.69286. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69281/0.69262. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69267/0.69243. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69249/0.69226. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69238/0.69211. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69209/0.69202. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69182/0.69192. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69158/0.69183. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69165/0.69174. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69106/0.69166. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69089/0.69159. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69041/0.69149. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69040/0.69141. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68984/0.69133. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68959/0.69125. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68915/0.69116. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68884/0.69106. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68851/0.69093. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68798/0.69083. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68780/0.69078. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68725/0.69070. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68668/0.69061. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68636/0.69054. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68599/0.69051. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68534/0.69047. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68488/0.69041. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68451/0.69035. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68414/0.69029. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68403/0.69033. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68331/0.69030. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68293/0.69022. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68270/0.69021. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68245/0.69011. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68204/0.69011. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68125/0.69009. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68092/0.69011. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68079/0.69003. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68059/0.69009. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67984/0.69008. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67933/0.69009. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67865/0.69013. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67835/0.69011. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67826/0.69020. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67730/0.69027. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67733/0.69032. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67660/0.69034. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67629/0.69048. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67607/0.69067. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67481/0.69083. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67476/0.69088. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67413/0.69106. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67364/0.69119. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67321/0.69151. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67293/0.69179. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67224/0.69188. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67131/0.69217. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67121/0.69249. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67100/0.69275. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67017/0.69304. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66933/0.69346. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66930/0.69369. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66843/0.69406. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.66830/0.69465. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66734/0.69486. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66671/0.69545. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66584/0.69578. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66591/0.69628. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66518/0.69682. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66426/0.69733. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66355/0.69777. Took 0.08 sec\n",
      "Epoch 73, Loss(train/val) 0.66334/0.69838. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66239/0.69889. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66196/0.69962. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66146/0.70028. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66144/0.70069. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66003/0.70141. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66022/0.70192. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65913/0.70263. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65864/0.70309. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65826/0.70375. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65784/0.70424. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65710/0.70504. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65640/0.70586. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65536/0.70650. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65515/0.70714. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65467/0.70778. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.65405/0.70843. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65367/0.70900. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65315/0.70977. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65251/0.71043. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65219/0.71092. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65107/0.71139. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65061/0.71217. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65056/0.71293. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64907/0.71353. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64865/0.71394. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64763/0.71435. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70787/0.71242. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.70201/0.70360. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69783/0.69708. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69558/0.69311. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69477/0.69124. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69410/0.69047. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69371/0.69021. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69363/0.69022. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69343/0.69038. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69346/0.69058. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69283/0.69072. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69248/0.69095. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69233/0.69121. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69219/0.69153. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69201/0.69184. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69147/0.69216. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69130/0.69256. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69107/0.69300. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69038/0.69349. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69036/0.69403. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68942/0.69461. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68867/0.69540. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68819/0.69615. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68754/0.69692. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68728/0.69797. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68644/0.69897. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68569/0.70003. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68534/0.70109. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68551/0.70217. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68441/0.70310. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68396/0.70398. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68334/0.70487. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68295/0.70566. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68237/0.70628. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68222/0.70697. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68166/0.70752. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68134/0.70800. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68157/0.70855. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68078/0.70888. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68046/0.70928. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67958/0.70953. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67925/0.70979. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67916/0.71021. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67855/0.71038. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67771/0.71063. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67769/0.71090. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67728/0.71112. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67661/0.71132. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67571/0.71154. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67563/0.71179. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67529/0.71187. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67456/0.71189. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67384/0.71194. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67358/0.71206. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67328/0.71213. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67233/0.71219. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67216/0.71213. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67152/0.71210. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67052/0.71206. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67028/0.71216. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67025/0.71197. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66935/0.71197. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66812/0.71208. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66781/0.71216. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66713/0.71208. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66679/0.71210. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66603/0.71218. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66578/0.71224. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66401/0.71216. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66412/0.71240. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66309/0.71232. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66267/0.71248. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66195/0.71229. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66101/0.71229. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65976/0.71248. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65955/0.71272. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65814/0.71279. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65811/0.71260. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65664/0.71266. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65655/0.71326. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65574/0.71356. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65434/0.71352. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65464/0.71375. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65363/0.71375. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65237/0.71386. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65098/0.71416. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65072/0.71435. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64944/0.71449. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64911/0.71489. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64828/0.71522. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64800/0.71574. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64631/0.71563. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64543/0.71607. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64535/0.71626. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64441/0.71642. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64401/0.71670. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64291/0.71698. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64217/0.71723. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64164/0.71729. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64065/0.71788. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69233/0.68899. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69216/0.68872. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69190/0.68846. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.68820. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69175/0.68792. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69132/0.68765. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69143/0.68739. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69098/0.68712. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69088/0.68686. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69083/0.68665. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69028/0.68641. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69028/0.68619. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69010/0.68596. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68987/0.68576. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69009/0.68554. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68929/0.68528. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68885/0.68505. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68867/0.68479. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68862/0.68457. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68815/0.68434. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68795/0.68410. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68712/0.68381. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68713/0.68355. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68644/0.68327. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68611/0.68303. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68548/0.68283. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68530/0.68267. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68466/0.68256. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68409/0.68234. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68332/0.68220. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68315/0.68214. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68253/0.68213. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68178/0.68217. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68107/0.68223. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68032/0.68228. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68013/0.68243. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67933/0.68255. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67870/0.68280. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67767/0.68304. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67678/0.68343. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67592/0.68384. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67538/0.68431. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67384/0.68481. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67298/0.68532. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67169/0.68591. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67007/0.68654. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66920/0.68720. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66787/0.68798. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66738/0.68892. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66551/0.68999. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66461/0.69073. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66293/0.69145. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66111/0.69241. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65982/0.69341. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65907/0.69457. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65753/0.69578. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65633/0.69707. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65467/0.69812. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65293/0.69936. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65145/0.70037. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65083/0.70129. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64888/0.70262. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64832/0.70366. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64672/0.70521. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64500/0.70590. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64433/0.70754. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64352/0.70827. Took 0.13 sec\n",
      "Epoch 67, Loss(train/val) 0.64090/0.70920. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64083/0.71088. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63987/0.71171. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63915/0.71246. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63656/0.71388. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63578/0.71491. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63498/0.71482. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63369/0.71602. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63264/0.71702. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63117/0.71753. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62982/0.71924. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62892/0.72011. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62827/0.72036. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62614/0.72151. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62651/0.72274. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62510/0.72286. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62363/0.72321. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62275/0.72432. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62173/0.72492. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61993/0.72649. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61879/0.72737. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61787/0.72785. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61561/0.72913. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61408/0.72997. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61363/0.73135. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61224/0.73222. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61086/0.73338. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61058/0.73409. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60831/0.73492. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60835/0.73589. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60614/0.73702. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60582/0.73743. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60393/0.73831. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70451/0.70665. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.70191/0.70418. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69980/0.70180. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69768/0.69948. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69535/0.69749. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69384/0.69618. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69283/0.69543. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69244/0.69497. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69195/0.69466. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69173/0.69446. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69128/0.69428. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69071/0.69413. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69070/0.69403. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69061/0.69397. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69015/0.69395. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68994/0.69398. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68937/0.69401. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68898/0.69402. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68885/0.69408. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68845/0.69422. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68802/0.69444. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68744/0.69470. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68717/0.69500. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68678/0.69532. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68590/0.69565. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68545/0.69605. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68484/0.69645. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68410/0.69683. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68365/0.69721. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68303/0.69758. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68252/0.69789. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68259/0.69821. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68158/0.69854. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68136/0.69883. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68009/0.69907. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67927/0.69926. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67931/0.69940. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67886/0.69959. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67807/0.69970. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67756/0.69978. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67642/0.69974. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67641/0.69973. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67566/0.69968. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67496/0.69961. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67415/0.69949. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67301/0.69931. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67256/0.69900. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67202/0.69873. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67117/0.69841. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67033/0.69801. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66919/0.69758. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66875/0.69717. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66813/0.69675. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66721/0.69624. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66656/0.69579. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66521/0.69524. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66493/0.69472. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66387/0.69411. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66257/0.69359. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66175/0.69319. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66087/0.69267. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65936/0.69217. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65841/0.69180. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65773/0.69127. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65707/0.69092. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65509/0.69062. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65484/0.69044. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65350/0.69044. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65204/0.69025. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65110/0.69036. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64950/0.69047. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64889/0.69058. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64739/0.69099. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64649/0.69153. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64540/0.69209. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64364/0.69253. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64190/0.69318. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64052/0.69403. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63949/0.69504. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63827/0.69624. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63670/0.69748. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63496/0.69883. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63412/0.70031. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63203/0.70171. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63043/0.70342. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62919/0.70535. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62834/0.70712. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62722/0.70896. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62506/0.71069. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62440/0.71252. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62289/0.71424. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62140/0.71621. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62060/0.71801. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61910/0.71972. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61831/0.72139. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61701/0.72296. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61569/0.72475. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61414/0.72641. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61351/0.72788. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61249/0.72923. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70111/0.70290. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.70038/0.70249. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69991/0.70208. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69950/0.70157. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69871/0.70082. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69801/0.69966. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69656/0.69784. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69448/0.69538. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69235/0.69290. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69061/0.69110. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68957/0.69019. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68896/0.68982. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68857/0.68971. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68824/0.68971. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68768/0.68980. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68771/0.68992. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68737/0.69008. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68695/0.69026. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68677/0.69048. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68654/0.69072. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68609/0.69099. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68584/0.69132. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68564/0.69165. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68506/0.69204. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68480/0.69247. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68445/0.69299. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68388/0.69356. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68376/0.69421. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68320/0.69490. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68272/0.69566. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68220/0.69637. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68195/0.69722. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68117/0.69817. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68092/0.69914. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68025/0.69997. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67992/0.70094. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67969/0.70198. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67902/0.70294. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67833/0.70411. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67800/0.70514. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67764/0.70626. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67728/0.70722. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67704/0.70837. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67606/0.70933. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67582/0.71049. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67548/0.71146. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67459/0.71253. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67452/0.71345. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67377/0.71459. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67327/0.71535. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67249/0.71666. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67198/0.71788. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67168/0.71873. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67094/0.71971. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67045/0.72060. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66938/0.72167. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66916/0.72271. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66833/0.72385. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66775/0.72496. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66686/0.72577. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66634/0.72669. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66566/0.72743. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66456/0.72840. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66419/0.72933. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66366/0.73057. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66313/0.73129. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66177/0.73213. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66144/0.73285. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66063/0.73319. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66014/0.73423. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65954/0.73476. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65894/0.73519. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65801/0.73599. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65744/0.73662. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65666/0.73689. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65594/0.73785. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65525/0.73838. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65411/0.73867. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65323/0.73930. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65256/0.73949. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65186/0.74042. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65073/0.74060. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65025/0.74111. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64964/0.74116. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64845/0.74187. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64797/0.74189. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64659/0.74303. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64578/0.74308. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64523/0.74327. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64462/0.74375. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64344/0.74382. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64275/0.74409. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64181/0.74465. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64044/0.74499. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63972/0.74478. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63890/0.74509. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63785/0.74555. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63687/0.74527. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63641/0.74516. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63464/0.74519. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69108/0.69022. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69105/0.69022. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69054/0.69021. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69012/0.69020. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69009/0.69021. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.69020. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68964/0.69020. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68950/0.69020. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68904/0.69018. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68896/0.69017. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68868/0.69015. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68845/0.69016. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68793/0.69017. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68799/0.69017. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68749/0.69018. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68725/0.69020. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68693/0.69027. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68658/0.69036. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68602/0.69046. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68535/0.69059. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68530/0.69076. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68491/0.69096. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68386/0.69123. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68370/0.69154. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68330/0.69190. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68241/0.69230. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68211/0.69274. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68165/0.69318. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68085/0.69366. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68033/0.69415. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67985/0.69464. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67925/0.69517. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67866/0.69566. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67835/0.69616. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67770/0.69668. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67742/0.69721. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67652/0.69767. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67609/0.69818. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67582/0.69868. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67527/0.69920. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67441/0.69966. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67399/0.70002. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67337/0.70042. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67333/0.70071. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67285/0.70107. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67184/0.70143. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67178/0.70179. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67131/0.70220. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67053/0.70260. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67016/0.70292. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67008/0.70316. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66921/0.70350. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66828/0.70379. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66725/0.70419. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66741/0.70451. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66662/0.70484. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.66596/0.70511. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66554/0.70556. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66491/0.70591. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66462/0.70624. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66403/0.70666. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66307/0.70707. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66261/0.70756. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66253/0.70804. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66171/0.70848. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65998/0.70909. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66017/0.70957. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66048/0.71003. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65921/0.71059. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65869/0.71128. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65790/0.71184. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65802/0.71252. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65709/0.71324. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65648/0.71398. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65609/0.71479. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65561/0.71560. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65493/0.71637. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65441/0.71729. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65327/0.71814. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65306/0.71911. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65263/0.71998. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65202/0.72094. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65176/0.72175. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65118/0.72271. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65120/0.72366. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65018/0.72467. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64920/0.72578. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64841/0.72693. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64861/0.72808. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64753/0.72914. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64724/0.73021. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64675/0.73112. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64637/0.73216. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64565/0.73323. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64488/0.73438. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64442/0.73539. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64364/0.73641. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64393/0.73762. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64355/0.73890. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64308/0.73978. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69119/0.69231. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69055/0.69271. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68979/0.69326. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68931/0.69404. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68901/0.69499. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68853/0.69597. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68816/0.69685. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68802/0.69754. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68746/0.69804. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68727/0.69828. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68716/0.69839. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68697/0.69846. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68673/0.69845. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68665/0.69842. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68620/0.69841. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68594/0.69833. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68568/0.69823. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68567/0.69816. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68540/0.69812. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68526/0.69797. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68493/0.69787. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68459/0.69793. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68439/0.69783. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68405/0.69770. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68409/0.69778. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68383/0.69779. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68366/0.69779. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68323/0.69790. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68305/0.69788. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68266/0.69775. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68217/0.69785. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68271/0.69820. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68179/0.69839. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68110/0.69858. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68117/0.69852. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68079/0.69875. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68032/0.69896. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68023/0.69939. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68001/0.69972. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67987/0.69998. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67953/0.70025. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67892/0.70043. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67915/0.70098. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67851/0.70121. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67846/0.70167. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67796/0.70196. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67773/0.70243. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67759/0.70277. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67733/0.70305. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67658/0.70348. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67681/0.70393. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67645/0.70403. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67585/0.70433. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67543/0.70459. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67517/0.70482. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67524/0.70517. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67481/0.70562. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67454/0.70604. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67416/0.70642. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67380/0.70664. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67337/0.70682. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67287/0.70695. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67276/0.70735. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67235/0.70770. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67191/0.70767. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67116/0.70767. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67109/0.70824. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67054/0.70845. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66983/0.70865. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66980/0.70879. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66915/0.70878. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66888/0.70897. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66810/0.70912. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66751/0.70956. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66727/0.70939. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66679/0.70980. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66624/0.70982. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66517/0.70997. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66452/0.70974. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66449/0.71041. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66342/0.71041. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66299/0.71063. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66243/0.71069. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66137/0.71062. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66037/0.71087. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66036/0.71108. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65965/0.71132. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65842/0.71133. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65808/0.71200. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65704/0.71176. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65632/0.71207. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65550/0.71241. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65487/0.71291. Took 0.08 sec\n",
      "Epoch 93, Loss(train/val) 0.65324/0.71289. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65267/0.71335. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65121/0.71363. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65057/0.71416. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64943/0.71452. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64894/0.71486. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64776/0.71557. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.70546/0.71958. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.70254/0.71501. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69927/0.71068. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69679/0.70569. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69337/0.69982. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69030/0.69405. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68844/0.69000. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68744/0.68800. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68656/0.68734. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68658/0.68724. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68605/0.68749. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68574/0.68778. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68550/0.68822. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68543/0.68858. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68498/0.68895. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68479/0.68950. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68451/0.69011. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68401/0.69072. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68393/0.69125. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68350/0.69184. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68295/0.69256. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68281/0.69326. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68238/0.69402. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68223/0.69473. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68159/0.69546. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68134/0.69625. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68110/0.69714. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68082/0.69803. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68045/0.69898. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68000/0.69989. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67962/0.70069. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67912/0.70151. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67869/0.70243. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67858/0.70327. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67821/0.70408. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67812/0.70505. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67738/0.70583. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67697/0.70667. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67704/0.70774. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67638/0.70845. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67599/0.70924. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67612/0.70975. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67528/0.71055. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67492/0.71142. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67454/0.71222. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67447/0.71271. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67385/0.71348. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67325/0.71432. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67301/0.71527. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67249/0.71590. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67208/0.71645. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67181/0.71735. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67141/0.71804. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67107/0.71872. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67065/0.71939. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67019/0.72006. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66960/0.72064. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66897/0.72157. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66895/0.72226. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66806/0.72320. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66740/0.72390. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66720/0.72487. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66638/0.72568. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66582/0.72617. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66533/0.72716. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66490/0.72812. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66383/0.72909. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66336/0.72969. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66261/0.73069. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66179/0.73142. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66103/0.73269. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66104/0.73310. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66024/0.73429. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65905/0.73540. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65842/0.73630. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65746/0.73705. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65735/0.73804. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65611/0.73863. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65533/0.73956. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65456/0.74105. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65335/0.74155. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65291/0.74262. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65187/0.74348. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65033/0.74446. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65029/0.74535. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64911/0.74612. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64889/0.74727. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64741/0.74793. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64659/0.74860. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64630/0.74953. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64425/0.75027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64384/0.75137. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64216/0.75227. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64173/0.75313. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64034/0.75389. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63974/0.75499. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63855/0.75541. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63778/0.75681. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63620/0.75752. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63539/0.75851. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69086/0.68870. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69054/0.68860. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.68847. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68979/0.68831. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68965/0.68809. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68905/0.68777. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68848/0.68733. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68779/0.68687. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68706/0.68651. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68677/0.68626. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68630/0.68609. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68595/0.68597. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68554/0.68591. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68521/0.68591. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68496/0.68592. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68479/0.68595. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68426/0.68599. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68364/0.68608. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68332/0.68618. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68278/0.68629. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68299/0.68645. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68189/0.68665. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68080/0.68688. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68064/0.68715. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68023/0.68745. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67996/0.68777. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67944/0.68814. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67878/0.68855. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67823/0.68898. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67773/0.68941. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67717/0.68990. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67727/0.69046. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67674/0.69098. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67632/0.69150. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67608/0.69205. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67544/0.69258. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67514/0.69307. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67488/0.69363. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67418/0.69416. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67364/0.69464. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67348/0.69511. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67270/0.69563. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67256/0.69613. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67211/0.69663. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67216/0.69711. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67124/0.69761. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67142/0.69806. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67090/0.69855. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67038/0.69908. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67005/0.69959. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66970/0.70013. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66908/0.70068. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66847/0.70120. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66829/0.70175. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66831/0.70223. Took 0.08 sec\n",
      "Epoch 55, Loss(train/val) 0.66721/0.70274. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66728/0.70321. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.66655/0.70362. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66605/0.70414. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66551/0.70466. Took 0.08 sec\n",
      "Epoch 60, Loss(train/val) 0.66527/0.70520. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66482/0.70575. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.66405/0.70629. Took 0.08 sec\n",
      "Epoch 63, Loss(train/val) 0.66382/0.70677. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66296/0.70739. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66260/0.70792. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.66196/0.70846. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66236/0.70889. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66092/0.70945. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66090/0.70998. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66057/0.71043. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66080/0.71084. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66005/0.71116. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65908/0.71166. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65822/0.71220. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.65793/0.71282. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65740/0.71327. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65692/0.71389. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65683/0.71430. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65588/0.71489. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65589/0.71554. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65476/0.71601. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65442/0.71666. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65337/0.71699. Took 0.08 sec\n",
      "Epoch 84, Loss(train/val) 0.65310/0.71768. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65288/0.71782. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.65207/0.71876. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.65167/0.71906. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65027/0.71979. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65050/0.72044. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64935/0.72128. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64855/0.72172. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.64803/0.72267. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64755/0.72341. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.64746/0.72443. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64642/0.72486. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64586/0.72567. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64504/0.72611. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64457/0.72725. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64376/0.72775. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70073/0.69888. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69873/0.69797. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69711/0.69682. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69491/0.69548. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69226/0.69422. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68971/0.69336. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68775/0.69300. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68636/0.69295. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68574/0.69295. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68534/0.69301. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68463/0.69306. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68460/0.69310. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68383/0.69311. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68324/0.69312. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68285/0.69315. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68239/0.69321. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68170/0.69326. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68137/0.69331. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68117/0.69338. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68073/0.69348. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68018/0.69363. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67959/0.69377. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67913/0.69393. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67875/0.69417. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67840/0.69444. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67800/0.69476. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67707/0.69508. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67653/0.69546. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67591/0.69590. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67568/0.69639. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67538/0.69693. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67483/0.69752. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67422/0.69809. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67375/0.69868. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67292/0.69933. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67283/0.69998. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67241/0.70071. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67146/0.70136. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67080/0.70209. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67074/0.70280. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66953/0.70352. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66957/0.70426. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66913/0.70495. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66807/0.70561. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66810/0.70626. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66759/0.70688. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66662/0.70753. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66724/0.70814. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66599/0.70881. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66583/0.70940. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66457/0.70999. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66405/0.71061. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66394/0.71111. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66357/0.71164. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66326/0.71219. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66213/0.71265. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66188/0.71318. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66129/0.71375. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66056/0.71438. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66031/0.71492. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65919/0.71551. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65876/0.71619. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65832/0.71671. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65751/0.71727. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65721/0.71789. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65616/0.71851. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65597/0.71910. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65561/0.71975. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65464/0.72039. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65364/0.72113. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65297/0.72191. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65297/0.72260. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65200/0.72323. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65106/0.72386. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65083/0.72445. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64983/0.72528. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64936/0.72602. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64874/0.72689. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64806/0.72765. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64655/0.72857. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64593/0.72939. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64523/0.73041. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64488/0.73128. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64370/0.73218. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64236/0.73312. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64165/0.73416. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64148/0.73485. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64013/0.73576. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63995/0.73674. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63864/0.73767. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63821/0.73859. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63797/0.73960. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63688/0.74044. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63569/0.74142. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63446/0.74269. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63329/0.74383. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63254/0.74503. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63198/0.74639. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63082/0.74758. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63000/0.74883. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69329/0.69216. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69212. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69258/0.69211. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69239/0.69211. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69220/0.69212. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69229/0.69215. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69156/0.69220. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69154/0.69226. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69150/0.69237. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69106/0.69249. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69109/0.69263. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69081/0.69280. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69050/0.69300. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69007/0.69319. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69015/0.69339. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68986/0.69358. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68971/0.69374. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68968/0.69387. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68944/0.69398. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68921/0.69406. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68891/0.69414. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68904/0.69421. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68886/0.69430. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68865/0.69433. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68868/0.69436. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68850/0.69440. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68830/0.69441. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68829/0.69443. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68794/0.69445. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68774/0.69443. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68747/0.69444. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68753/0.69445. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68709/0.69450. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68703/0.69451. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68675/0.69458. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68659/0.69466. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68622/0.69468. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68598/0.69470. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68571/0.69478. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68507/0.69487. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68500/0.69498. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68450/0.69512. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68428/0.69532. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68413/0.69544. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68350/0.69560. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68310/0.69575. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68257/0.69605. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68243/0.69617. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68195/0.69652. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68157/0.69690. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.68114/0.69721. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68044/0.69757. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67986/0.69787. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67935/0.69816. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67872/0.69862. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67848/0.69921. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67771/0.69962. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67719/0.70013. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67633/0.70055. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67631/0.70124. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67485/0.70162. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67468/0.70232. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67358/0.70309. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67300/0.70361. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67281/0.70442. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67175/0.70507. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67090/0.70599. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66996/0.70686. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66921/0.70749. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66823/0.70850. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66751/0.70936. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66627/0.71043. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66550/0.71137. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66467/0.71258. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66346/0.71367. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66244/0.71489. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66122/0.71619. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66008/0.71745. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65850/0.71883. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65820/0.72018. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65643/0.72187. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65558/0.72344. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65380/0.72519. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65256/0.72707. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65115/0.72896. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65002/0.73078. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64981/0.73265. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64740/0.73460. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64608/0.73675. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64552/0.73896. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64417/0.74072. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64201/0.74275. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64220/0.74467. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64042/0.74677. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63949/0.74855. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63814/0.75021. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63672/0.75224. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63553/0.75445. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63507/0.75602. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63296/0.75730. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69389/0.68761. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69360/0.68780. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.68798. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.68817. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69316/0.68838. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69289/0.68859. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69290/0.68881. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69275/0.68905. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69251/0.68928. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69234/0.68952. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69232/0.68976. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69217/0.69001. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69205/0.69025. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69220/0.69051. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69199/0.69077. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69184/0.69103. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69211/0.69130. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69183/0.69159. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69171/0.69186. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.69141/0.69216. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69141/0.69247. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69137/0.69276. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69123/0.69308. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69120/0.69339. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69106/0.69371. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.69099/0.69406. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69091/0.69440. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69083/0.69475. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.69060/0.69510. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69037/0.69545. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69020/0.69579. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69037/0.69615. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69039/0.69653. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69014/0.69691. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.69002/0.69728. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68978/0.69766. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68975/0.69803. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68974/0.69840. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68926/0.69877. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68941/0.69911. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68926/0.69948. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68911/0.69986. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68901/0.70025. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68886/0.70060. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68896/0.70096. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68885/0.70134. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68860/0.70168. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68805/0.70204. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68836/0.70239. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68814/0.70270. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68833/0.70305. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68787/0.70340. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68790/0.70373. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68764/0.70404. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.68763/0.70433. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68739/0.70468. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68703/0.70500. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68686/0.70528. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68681/0.70560. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68649/0.70592. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68670/0.70625. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68624/0.70655. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68632/0.70684. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68600/0.70711. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68534/0.70736. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68565/0.70763. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.68540/0.70787. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68485/0.70816. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68460/0.70841. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68472/0.70864. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68390/0.70894. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68455/0.70914. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.68374/0.70941. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68356/0.70961. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68310/0.70985. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.68318/0.71005. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68274/0.71021. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68199/0.71050. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.68189/0.71063. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.68185/0.71082. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.68154/0.71101. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.68110/0.71120. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.68063/0.71139. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.68018/0.71160. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.67983/0.71170. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67957/0.71192. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67854/0.71193. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67850/0.71210. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67819/0.71217. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67762/0.71230. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.67702/0.71251. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67658/0.71273. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67585/0.71279. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67542/0.71306. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67514/0.71330. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67469/0.71338. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.67406/0.71349. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.67326/0.71372. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.67300/0.71402. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.67218/0.71449. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69875/0.69424. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69858/0.69416. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69825/0.69408. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69815/0.69401. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69778/0.69398. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69747/0.69398. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69676/0.69409. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69584/0.69442. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69457/0.69515. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69295/0.69634. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69182/0.69786. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69119/0.69928. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69079/0.70036. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69045/0.70114. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68998/0.70161. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68990/0.70189. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68967/0.70211. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68939/0.70228. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68918/0.70241. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68916/0.70259. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68878/0.70278. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68867/0.70287. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68812/0.70293. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68818/0.70309. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68771/0.70323. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68753/0.70329. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68749/0.70356. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68691/0.70373. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68683/0.70398. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68667/0.70416. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68619/0.70433. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68575/0.70455. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68536/0.70475. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68520/0.70487. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68497/0.70510. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68474/0.70524. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68406/0.70565. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68387/0.70588. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68339/0.70609. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68343/0.70627. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68277/0.70641. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68271/0.70665. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68194/0.70692. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68191/0.70716. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68146/0.70746. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68085/0.70769. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68062/0.70810. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67987/0.70830. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67992/0.70866. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67983/0.70900. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67925/0.70928. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67879/0.70967. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67861/0.71008. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67847/0.71047. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67771/0.71080. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67751/0.71123. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67751/0.71162. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67676/0.71198. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67658/0.71225. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67623/0.71273. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67556/0.71323. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67533/0.71351. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67499/0.71384. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67491/0.71446. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67436/0.71495. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67388/0.71521. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67342/0.71591. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67297/0.71621. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67247/0.71649. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67226/0.71692. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67169/0.71753. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67175/0.71795. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67084/0.71839. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67042/0.71881. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66970/0.71929. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66971/0.71957. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66950/0.72009. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66851/0.72069. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66815/0.72124. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66793/0.72176. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66688/0.72216. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66695/0.72264. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66571/0.72310. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66546/0.72385. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66485/0.72436. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66472/0.72495. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66422/0.72554. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66354/0.72621. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66300/0.72677. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66222/0.72731. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66160/0.72794. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66162/0.72855. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66062/0.72903. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65996/0.72961. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65889/0.73015. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65924/0.73082. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65831/0.73114. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65783/0.73183. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65678/0.73232. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65558/0.73300. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69058/0.68247. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69039/0.68258. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69034/0.68275. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68989/0.68288. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68982/0.68302. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68980/0.68322. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68971/0.68350. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68919/0.68381. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68873/0.68407. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68864/0.68446. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68837/0.68490. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68810/0.68541. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68776/0.68596. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68745/0.68656. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68670/0.68733. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68622/0.68830. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68587/0.68935. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68497/0.69051. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68470/0.69173. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68386/0.69318. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68329/0.69465. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68260/0.69617. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68200/0.69782. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68180/0.69930. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68059/0.70082. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68047/0.70241. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68017/0.70366. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67911/0.70497. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67894/0.70603. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67779/0.70712. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67746/0.70829. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67708/0.70909. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67672/0.70999. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67612/0.71101. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67585/0.71207. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67490/0.71274. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67450/0.71343. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67450/0.71394. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67345/0.71446. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67311/0.71520. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67227/0.71607. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67208/0.71702. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67136/0.71744. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67072/0.71813. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66991/0.71872. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66974/0.71897. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66875/0.72016. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66857/0.72077. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66740/0.72161. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66723/0.72228. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66694/0.72276. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66591/0.72390. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66489/0.72451. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66402/0.72500. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66348/0.72615. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66267/0.72628. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66217/0.72695. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66146/0.72788. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66044/0.72918. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65941/0.72987. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65922/0.73054. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65801/0.73181. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65692/0.73307. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65661/0.73396. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65512/0.73529. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65468/0.73612. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65331/0.73731. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65276/0.73861. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65142/0.73941. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65143/0.74103. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65059/0.74245. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64889/0.74396. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64874/0.74471. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64683/0.74689. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64665/0.74818. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64451/0.74962. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64393/0.75087. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64304/0.75257. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64258/0.75508. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64125/0.75497. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64030/0.75723. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63916/0.75814. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63851/0.75880. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63757/0.76121. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63605/0.76299. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63650/0.76573. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63466/0.76678. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63392/0.76806. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63322/0.77017. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63225/0.77162. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62986/0.77374. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62964/0.77462. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62776/0.77594. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62759/0.77820. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.62673/0.78004. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62576/0.78187. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62481/0.78277. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62474/0.78400. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62147/0.78536. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62199/0.78712. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.68997/0.68335. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68969/0.68348. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68937/0.68359. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68916/0.68371. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.68385. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68885/0.68398. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68860/0.68412. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68837/0.68431. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.68449. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68780/0.68471. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68769/0.68496. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68693/0.68522. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68655/0.68554. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68636/0.68592. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68555/0.68637. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68493/0.68681. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68419/0.68732. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68346/0.68776. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68245/0.68815. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68168/0.68864. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68078/0.68900. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68003/0.68942. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67911/0.68984. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67829/0.69023. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67786/0.69066. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67711/0.69100. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67629/0.69145. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67584/0.69170. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67533/0.69199. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67481/0.69231. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67440/0.69264. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67407/0.69287. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67365/0.69309. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67320/0.69338. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67272/0.69373. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67229/0.69388. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67194/0.69408. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67175/0.69425. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67109/0.69445. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67072/0.69468. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67043/0.69483. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66991/0.69513. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66934/0.69530. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66922/0.69547. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66905/0.69551. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66883/0.69581. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66788/0.69600. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66771/0.69616. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66783/0.69632. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66713/0.69667. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66651/0.69689. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66618/0.69705. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66557/0.69739. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66512/0.69756. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66531/0.69784. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66458/0.69802. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66425/0.69830. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66376/0.69849. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66335/0.69868. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66326/0.69884. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66236/0.69915. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66232/0.69917. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66177/0.69962. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66122/0.69989. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66111/0.70007. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66025/0.70032. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65997/0.70066. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65913/0.70085. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65936/0.70104. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65836/0.70147. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65775/0.70178. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65767/0.70208. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65648/0.70239. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65662/0.70299. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65613/0.70321. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65513/0.70355. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65434/0.70385. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65421/0.70434. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65324/0.70476. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65310/0.70505. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65246/0.70534. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65203/0.70563. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65114/0.70608. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65112/0.70657. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64995/0.70679. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64963/0.70717. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64870/0.70771. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64805/0.70824. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64792/0.70857. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64740/0.70900. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64599/0.70948. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64579/0.71008. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64551/0.71042. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64478/0.71093. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64371/0.71147. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64313/0.71191. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64258/0.71245. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64167/0.71299. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64129/0.71359. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64064/0.71414. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69035/0.68517. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69002/0.68516. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69046/0.68513. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68966/0.68509. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68952/0.68506. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68975/0.68499. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68957/0.68495. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68955/0.68488. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68910/0.68481. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68913/0.68474. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.68467. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68915/0.68460. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68863/0.68454. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68843/0.68446. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68839/0.68439. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68838/0.68432. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68759/0.68429. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68759/0.68427. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68671/0.68428. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68656/0.68433. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68608/0.68436. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68520/0.68442. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68408/0.68449. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68381/0.68462. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68260/0.68487. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68156/0.68518. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68039/0.68555. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67928/0.68610. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67803/0.68679. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67697/0.68753. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67577/0.68825. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67455/0.68912. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67417/0.69000. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67236/0.69100. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67173/0.69171. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67074/0.69259. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66976/0.69360. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66860/0.69451. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66838/0.69531. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66731/0.69597. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.66687/0.69712. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66591/0.69827. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66534/0.69885. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.66445/0.69947. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66369/0.70036. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66330/0.70144. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66250/0.70194. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66160/0.70274. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66084/0.70340. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.66027/0.70406. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.65957/0.70497. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65899/0.70552. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65820/0.70599. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65761/0.70693. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65611/0.70716. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65635/0.70811. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65525/0.70878. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65389/0.70945. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65354/0.70998. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65335/0.71084. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65213/0.71109. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65158/0.71195. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64998/0.71273. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64956/0.71344. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64863/0.71424. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64747/0.71481. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64688/0.71536. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64691/0.71602. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64470/0.71635. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64394/0.71711. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64283/0.71779. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64238/0.71844. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64196/0.71895. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64065/0.72012. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63897/0.72070. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63831/0.72144. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63662/0.72215. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63570/0.72317. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63533/0.72365. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63402/0.72454. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63275/0.72532. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63260/0.72621. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63078/0.72727. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62930/0.72832. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62765/0.72943. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62697/0.73022. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62600/0.73192. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62331/0.73291. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62333/0.73399. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62225/0.73461. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62087/0.73637. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62018/0.73767. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61789/0.73881. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61666/0.74032. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.61490/0.74170. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61340/0.74344. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61223/0.74431. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61123/0.74634. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60876/0.74743. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60807/0.74979. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69275/0.70867. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69172/0.70689. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69097/0.70548. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69036/0.70434. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68998/0.70339. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68954/0.70256. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68896/0.70184. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.70116. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68834/0.70055. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68791/0.69994. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68753/0.69932. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68692/0.69873. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68643/0.69814. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68613/0.69751. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68564/0.69681. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68513/0.69610. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68460/0.69535. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.69461. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68333/0.69380. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68239/0.69300. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68181/0.69218. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68110/0.69132. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68024/0.69053. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67936/0.68985. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67905/0.68917. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67845/0.68859. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67800/0.68811. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67764/0.68771. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67730/0.68739. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67630/0.68713. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67622/0.68694. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67583/0.68681. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67554/0.68667. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67528/0.68657. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67498/0.68647. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67439/0.68645. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67444/0.68635. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67414/0.68635. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67374/0.68633. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67363/0.68617. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67367/0.68605. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67333/0.68596. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67297/0.68601. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67287/0.68590. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67271/0.68578. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67206/0.68573. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67216/0.68568. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67175/0.68568. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67156/0.68568. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67126/0.68560. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67099/0.68547. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67080/0.68538. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67065/0.68537. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67008/0.68523. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66981/0.68519. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66924/0.68523. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66931/0.68526. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66913/0.68518. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66876/0.68506. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66833/0.68503. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66750/0.68506. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66808/0.68506. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66701/0.68506. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66679/0.68516. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66648/0.68511. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66632/0.68511. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66597/0.68512. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66516/0.68526. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66512/0.68525. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66461/0.68527. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66450/0.68531. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66370/0.68529. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66343/0.68551. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66269/0.68560. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66265/0.68570. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66197/0.68580. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.66170/0.68608. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.66099/0.68621. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66079/0.68639. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65954/0.68660. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65958/0.68671. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65895/0.68684. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65807/0.68695. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65747/0.68728. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65670/0.68750. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65628/0.68789. Took 0.08 sec\n",
      "Epoch 86, Loss(train/val) 0.65551/0.68808. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65489/0.68830. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.65408/0.68869. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.65268/0.68890. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65327/0.68930. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65192/0.68990. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65177/0.69027. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65081/0.69089. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64981/0.69105. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64906/0.69144. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64822/0.69202. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64722/0.69232. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.64645/0.69309. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64616/0.69359. Took 0.08 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69078/0.68835. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69030/0.68850. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69025/0.68865. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68994/0.68881. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.68896. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68941/0.68911. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.68924/0.68927. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68894/0.68944. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68860/0.68961. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68835/0.68978. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68824/0.68998. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68799/0.69020. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68756/0.69045. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68698/0.69073. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68685/0.69105. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68632/0.69141. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.69187. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68520/0.69242. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68454/0.69308. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68361/0.69390. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68287/0.69493. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68174/0.69621. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68046/0.69778. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67896/0.69969. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67788/0.70193. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67637/0.70442. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67517/0.70709. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67423/0.70980. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67296/0.71242. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67214/0.71492. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67120/0.71733. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67050/0.71955. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67008/0.72163. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.66934/0.72354. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66885/0.72528. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66828/0.72686. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66760/0.72838. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66702/0.72984. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66673/0.73120. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66602/0.73262. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66533/0.73385. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66549/0.73497. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66493/0.73607. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66439/0.73724. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66391/0.73837. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66308/0.73948. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66271/0.74073. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66224/0.74186. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66152/0.74296. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66104/0.74412. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66058/0.74514. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66014/0.74630. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65978/0.74728. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65880/0.74847. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65834/0.74968. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65815/0.75088. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65746/0.75193. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.65703/0.75313. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65642/0.75428. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65588/0.75549. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65588/0.75673. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65485/0.75798. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65417/0.75914. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65382/0.76043. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65310/0.76172. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65235/0.76307. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65224/0.76444. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65166/0.76566. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65100/0.76702. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65038/0.76849. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64984/0.76987. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64920/0.77127. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64887/0.77255. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64793/0.77407. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64732/0.77546. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64672/0.77694. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.64602/0.77845. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64547/0.77989. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64485/0.78135. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64423/0.78293. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64365/0.78452. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64317/0.78596. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64262/0.78740. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64158/0.78896. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64150/0.79069. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63991/0.79214. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64006/0.79359. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63920/0.79535. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63845/0.79709. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63793/0.79869. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63732/0.80023. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63660/0.80184. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63600/0.80342. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63561/0.80507. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63458/0.80660. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63385/0.80830. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63331/0.81010. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63361/0.81153. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63178/0.81322. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63084/0.81499. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.68988/0.69711. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68977/0.69732. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68961/0.69755. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68905/0.69779. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68882/0.69805. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68919/0.69840. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68828/0.69876. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68843/0.69913. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68802/0.69952. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68810/0.69991. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68804/0.70027. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68722/0.70064. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68754/0.70094. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68747/0.70117. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68680/0.70138. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68674/0.70153. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68640/0.70168. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68604/0.70173. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68574/0.70174. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68565/0.70174. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68518/0.70166. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68457/0.70162. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68409/0.70151. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68313/0.70132. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68278/0.70103. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68196/0.70087. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68065/0.70070. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67982/0.70042. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.67917/0.70006. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67826/0.69992. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67692/0.69976. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67592/0.69960. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67525/0.69942. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67437/0.69919. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67348/0.69917. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67246/0.69913. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67166/0.69906. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67063/0.69913. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67000/0.69909. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66881/0.69906. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66814/0.69928. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66761/0.69949. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66706/0.69945. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66619/0.69983. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66539/0.70008. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66446/0.70025. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66368/0.70060. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66343/0.70106. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66278/0.70107. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66200/0.70120. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66059/0.70134. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66025/0.70197. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66015/0.70233. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65893/0.70260. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65790/0.70295. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65732/0.70340. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65608/0.70334. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65578/0.70379. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65527/0.70449. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65419/0.70455. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65329/0.70482. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65217/0.70518. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65084/0.70587. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65114/0.70625. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64981/0.70621. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64929/0.70645. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64724/0.70693. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.64693/0.70756. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64554/0.70821. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64509/0.70860. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64428/0.70938. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64322/0.70954. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64169/0.71021. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64028/0.71009. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63996/0.71143. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63816/0.71182. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63719/0.71220. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63581/0.71298. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63487/0.71346. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63364/0.71411. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63268/0.71481. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63167/0.71519. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63040/0.71530. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62806/0.71675. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62759/0.71678. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62640/0.71756. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62423/0.71835. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62297/0.71918. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62172/0.71911. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62115/0.72015. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61891/0.72136. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61892/0.72176. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61699/0.72244. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61582/0.72269. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61408/0.72355. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61334/0.72417. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61117/0.72485. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61074/0.72553. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60934/0.72702. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60719/0.72635. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69415/0.69334. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.69329. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69247/0.69322. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69035/0.69319. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68910/0.69334. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68759/0.69375. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68651/0.69433. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68579/0.69496. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68526/0.69555. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68478/0.69604. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68428/0.69652. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68363/0.69694. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68369/0.69739. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68317/0.69787. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68263/0.69830. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68203/0.69879. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68206/0.69932. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68116/0.69983. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68046/0.70051. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68016/0.70115. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67936/0.70192. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67865/0.70275. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67833/0.70367. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.67758/0.70455. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67654/0.70542. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67604/0.70642. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67554/0.70744. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67442/0.70838. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67384/0.70942. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67329/0.71040. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67238/0.71131. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67191/0.71222. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67096/0.71304. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67012/0.71384. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66954/0.71468. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.66970/0.71550. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66822/0.71616. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.66771/0.71679. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66717/0.71738. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66676/0.71796. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66585/0.71846. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66571/0.71900. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66492/0.71953. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66431/0.72004. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66372/0.72063. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66288/0.72122. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66260/0.72180. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66214/0.72237. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66188/0.72291. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66104/0.72341. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66023/0.72394. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65942/0.72449. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65949/0.72500. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65847/0.72558. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65775/0.72625. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65739/0.72684. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65671/0.72735. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65655/0.72792. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65574/0.72850. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65550/0.72897. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65514/0.72956. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65418/0.73024. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65330/0.73075. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65248/0.73150. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65230/0.73212. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65178/0.73271. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65161/0.73327. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65085/0.73407. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65014/0.73477. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64925/0.73538. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64894/0.73618. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64857/0.73664. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64788/0.73766. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64715/0.73868. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64645/0.73907. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64573/0.74003. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64568/0.74100. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64453/0.74192. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64378/0.74246. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64390/0.74318. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64296/0.74409. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64254/0.74489. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64216/0.74563. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64141/0.74634. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64070/0.74697. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63960/0.74833. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63991/0.74896. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63928/0.74977. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63770/0.75043. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63731/0.75164. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63727/0.75228. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63642/0.75320. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63557/0.75447. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63572/0.75549. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63616/0.75619. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63442/0.75696. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63403/0.75775. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63300/0.75873. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63273/0.75961. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63266/0.76050. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.68947/0.69213. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68902/0.69177. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68855/0.69140. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68799/0.69103. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68758/0.69069. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68701/0.69040. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68641/0.69017. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68627/0.68999. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68581/0.68986. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68584/0.68976. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68539/0.68967. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68528/0.68958. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68523/0.68950. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68505/0.68940. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68482/0.68930. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68457/0.68920. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68452/0.68912. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68440/0.68904. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68409/0.68897. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68370/0.68887. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68376/0.68878. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68351/0.68871. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68311/0.68863. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68298/0.68856. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68303/0.68851. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68255/0.68847. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68250/0.68845. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68231/0.68841. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68181/0.68836. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68157/0.68835. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68142/0.68834. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68114/0.68834. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68088/0.68835. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68085/0.68834. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68039/0.68836. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68002/0.68836. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67989/0.68834. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67949/0.68832. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67924/0.68828. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67902/0.68827. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67892/0.68825. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67853/0.68822. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67819/0.68821. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67802/0.68820. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67776/0.68815. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67753/0.68810. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67753/0.68809. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67712/0.68807. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67682/0.68802. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67677/0.68797. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67662/0.68793. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67588/0.68792. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67574/0.68790. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67541/0.68788. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67512/0.68790. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67465/0.68792. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67474/0.68791. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67443/0.68792. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67396/0.68792. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67392/0.68794. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67311/0.68799. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67294/0.68802. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67289/0.68803. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67253/0.68806. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67223/0.68812. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67194/0.68818. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67156/0.68823. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67129/0.68830. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67092/0.68838. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67042/0.68844. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67033/0.68853. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66979/0.68862. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66922/0.68876. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66912/0.68878. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66856/0.68890. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66823/0.68904. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66790/0.68921. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66743/0.68936. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66678/0.68954. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66648/0.68969. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66593/0.68989. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66548/0.69007. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66560/0.69032. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66491/0.69053. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66439/0.69075. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66444/0.69097. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66397/0.69119. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66345/0.69145. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66239/0.69180. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66224/0.69199. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66174/0.69227. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66145/0.69253. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66117/0.69280. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66107/0.69314. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66014/0.69350. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65995/0.69384. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65948/0.69425. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65874/0.69452. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65874/0.69487. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65819/0.69519. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69963/0.70211. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69451/0.69586. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69065/0.69114. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68807/0.68824. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68738/0.68678. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68703/0.68616. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68673/0.68590. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68628/0.68579. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68641/0.68577. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68586/0.68577. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68579/0.68581. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68557/0.68587. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68523/0.68592. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68537/0.68600. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68496/0.68610. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68480/0.68622. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68485/0.68633. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68442/0.68647. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68462/0.68661. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68443/0.68676. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68425/0.68689. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68401/0.68704. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68385/0.68721. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68346/0.68736. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68361/0.68753. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68326/0.68772. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68327/0.68791. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68313/0.68811. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68260/0.68827. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68264/0.68847. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68251/0.68866. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68224/0.68888. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68209/0.68910. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68214/0.68931. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68178/0.68951. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68169/0.68971. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68141/0.68992. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68152/0.69015. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68119/0.69037. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68082/0.69059. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68068/0.69082. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68042/0.69109. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68055/0.69136. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.68021/0.69157. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68004/0.69182. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67966/0.69206. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67945/0.69234. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67932/0.69263. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67900/0.69291. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67879/0.69317. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67846/0.69342. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67857/0.69372. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67794/0.69396. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67779/0.69423. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67753/0.69453. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67743/0.69482. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67712/0.69511. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67632/0.69546. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67648/0.69577. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67612/0.69607. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67538/0.69637. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67500/0.69675. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67538/0.69705. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67447/0.69735. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67443/0.69772. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67397/0.69811. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67344/0.69843. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67363/0.69877. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67311/0.69913. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67298/0.69940. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67236/0.69978. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67199/0.70015. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67161/0.70055. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67140/0.70086. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67129/0.70111. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67093/0.70147. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67045/0.70189. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67014/0.70219. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67001/0.70251. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66934/0.70285. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66963/0.70330. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66870/0.70356. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66877/0.70399. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66851/0.70436. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66824/0.70471. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66782/0.70499. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66772/0.70519. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66750/0.70552. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66695/0.70585. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66638/0.70624. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66640/0.70653. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66599/0.70694. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66576/0.70727. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66503/0.70767. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66478/0.70808. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66500/0.70837. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66476/0.70866. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66475/0.70893. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66399/0.70929. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66338/0.70958. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69651/0.69481. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69602/0.69468. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69532/0.69453. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69535/0.69430. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69448/0.69388. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69376/0.69313. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69248/0.69174. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.68955. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68779/0.68705. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68560/0.68525. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68453/0.68437. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68390/0.68407. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68368/0.68404. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68351/0.68413. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68322/0.68429. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68318/0.68448. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68315/0.68471. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68253/0.68495. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68249/0.68521. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68247/0.68547. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68219/0.68575. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68190/0.68603. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68189/0.68630. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68183/0.68661. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68151/0.68694. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68132/0.68730. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68084/0.68768. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68100/0.68805. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68077/0.68837. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68054/0.68874. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68032/0.68909. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68008/0.68948. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67990/0.68985. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67960/0.69026. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67976/0.69067. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67936/0.69112. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67918/0.69152. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67907/0.69194. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67891/0.69234. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67870/0.69269. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67838/0.69309. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67838/0.69341. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67800/0.69378. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67785/0.69409. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67760/0.69444. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67737/0.69472. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67699/0.69494. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67691/0.69523. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67695/0.69546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67667/0.69570. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67642/0.69587. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67591/0.69602. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67566/0.69619. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67568/0.69638. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67532/0.69652. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67501/0.69671. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67430/0.69683. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67434/0.69705. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67418/0.69717. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67354/0.69733. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67360/0.69743. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67337/0.69746. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67282/0.69752. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67242/0.69761. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67241/0.69774. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67152/0.69783. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67094/0.69793. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67111/0.69793. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67101/0.69790. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66970/0.69794. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66976/0.69811. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66913/0.69819. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66851/0.69838. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66854/0.69841. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66754/0.69846. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66654/0.69855. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66660/0.69849. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66574/0.69852. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66517/0.69856. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66462/0.69859. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66365/0.69880. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66324/0.69865. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66273/0.69885. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66195/0.69886. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66155/0.69896. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66067/0.69917. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65974/0.69922. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65925/0.69942. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65858/0.69946. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65740/0.69975. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65612/0.69988. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65539/0.70043. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65506/0.70051. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65376/0.70076. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65314/0.70101. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65269/0.70122. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65214/0.70155. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65099/0.70197. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65039/0.70222. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64827/0.70257. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69832/0.69672. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69657. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68837/0.69782. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68604/0.69966. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68489/0.70109. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68414/0.70187. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68367/0.70225. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68355/0.70236. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68302/0.70239. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68281/0.70238. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68241/0.70240. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68199/0.70238. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68187/0.70233. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68144/0.70231. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68115/0.70231. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68103/0.70228. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68088/0.70229. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68035/0.70229. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68070/0.70227. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68009/0.70227. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.67989/0.70224. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67965/0.70221. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.67972/0.70218. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67922/0.70224. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67898/0.70216. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67876/0.70213. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67900/0.70213. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67839/0.70216. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67837/0.70210. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67807/0.70213. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67808/0.70211. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67772/0.70204. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67781/0.70200. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67759/0.70196. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67721/0.70190. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67693/0.70193. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67679/0.70189. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67666/0.70187. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67649/0.70176. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67628/0.70173. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67633/0.70162. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67590/0.70154. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67578/0.70151. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67546/0.70145. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67521/0.70139. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67516/0.70139. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67519/0.70130. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67478/0.70134. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67459/0.70123. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67456/0.70110. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67398/0.70110. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67394/0.70100. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67408/0.70094. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67348/0.70090. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67360/0.70088. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67303/0.70081. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67320/0.70074. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67265/0.70062. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67293/0.70045. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67236/0.70046. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67152/0.70046. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67176/0.70041. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67142/0.70028. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67124/0.70024. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67074/0.70030. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67038/0.70019. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67020/0.70006. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66971/0.70002. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66978/0.70002. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66962/0.70010. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66920/0.69999. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66880/0.69995. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66848/0.69993. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66791/0.69996. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66770/0.70011. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66718/0.70004. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66677/0.70021. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66649/0.70032. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66633/0.70046. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66550/0.70057. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66545/0.70071. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66465/0.70077. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66458/0.70103. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66380/0.70128. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66369/0.70146. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66323/0.70158. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66254/0.70185. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.66206/0.70228. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66216/0.70250. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66129/0.70305. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66061/0.70320. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66009/0.70385. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65962/0.70409. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65888/0.70454. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65936/0.70494. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65834/0.70565. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65783/0.70608. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65709/0.70677. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65663/0.70712. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65608/0.70799. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69350/0.69411. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69180/0.69383. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68919/0.69385. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.68680/0.69458. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68466/0.69604. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.68321/0.69774. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68232/0.69920. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68192/0.70031. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68159/0.70105. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68147/0.70152. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68111/0.70177. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68105/0.70199. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68082/0.70223. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68070/0.70234. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68060/0.70240. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68049/0.70245. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68013/0.70248. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68002/0.70247. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67979/0.70248. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67948/0.70249. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67929/0.70250. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67913/0.70258. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67899/0.70255. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67867/0.70252. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.67857/0.70250. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67834/0.70254. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67791/0.70247. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67757/0.70244. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67751/0.70245. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67729/0.70249. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67699/0.70232. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67699/0.70224. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67634/0.70220. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67595/0.70213. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67592/0.70218. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67539/0.70218. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67494/0.70215. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67467/0.70217. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67449/0.70222. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67402/0.70218. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67368/0.70225. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67330/0.70226. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67310/0.70229. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67258/0.70221. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67259/0.70238. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67204/0.70234. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67179/0.70230. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67166/0.70261. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67093/0.70246. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67095/0.70233. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67073/0.70233. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67034/0.70223. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66976/0.70213. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66942/0.70215. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66911/0.70211. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66893/0.70179. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66827/0.70188. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66829/0.70168. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66789/0.70165. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66763/0.70142. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66746/0.70131. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66673/0.70089. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66647/0.70060. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66599/0.70052. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66553/0.70027. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66524/0.69993. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66514/0.69952. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66471/0.69910. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66402/0.69892. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66386/0.69878. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66352/0.69861. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66286/0.69784. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66243/0.69718. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66223/0.69713. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66140/0.69652. Took 0.08 sec\n",
      "Epoch 75, Loss(train/val) 0.66148/0.69626. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66103/0.69591. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66062/0.69544. Took 0.08 sec\n",
      "Epoch 78, Loss(train/val) 0.65971/0.69486. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65941/0.69427. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65911/0.69414. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65854/0.69343. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65760/0.69313. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65717/0.69233. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65698/0.69199. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65590/0.69164. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65579/0.69123. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65535/0.69071. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65397/0.69045. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65398/0.68979. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65333/0.68977. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65307/0.68899. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65236/0.68868. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65116/0.68836. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65121/0.68796. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64999/0.68771. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64966/0.68751. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64905/0.68733. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64830/0.68707. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64766/0.68677. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70517/0.70012. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.70392/0.69926. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.70151/0.69774. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69767/0.69564. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69275/0.69383. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68828/0.69306. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68508/0.69310. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68320/0.69352. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68211/0.69404. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68129/0.69455. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68086/0.69495. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68065/0.69526. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68039/0.69548. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68022/0.69564. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68007/0.69574. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.67994/0.69583. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.67972/0.69585. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.67969/0.69586. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67938/0.69585. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67916/0.69589. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67921/0.69589. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67902/0.69584. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67874/0.69583. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67865/0.69579. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67849/0.69577. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67810/0.69575. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67812/0.69571. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67788/0.69569. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67760/0.69565. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67740/0.69563. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67715/0.69562. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67679/0.69564. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67676/0.69558. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67647/0.69553. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67619/0.69552. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67598/0.69552. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67530/0.69549. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67510/0.69543. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67497/0.69539. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67480/0.69537. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67483/0.69538. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67417/0.69529. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67395/0.69527. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67353/0.69528. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67343/0.69520. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67320/0.69527. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67292/0.69529. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67274/0.69530. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67263/0.69530. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67199/0.69522. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67171/0.69513. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67173/0.69512. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67147/0.69502. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67090/0.69494. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67079/0.69502. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67046/0.69493. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67022/0.69481. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67015/0.69478. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66981/0.69472. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66949/0.69474. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66937/0.69470. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66955/0.69454. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66886/0.69451. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66888/0.69454. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66831/0.69443. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66851/0.69425. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66792/0.69408. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66733/0.69390. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66717/0.69377. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66704/0.69371. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66681/0.69364. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66664/0.69338. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66618/0.69323. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66573/0.69310. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66559/0.69289. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66565/0.69273. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66544/0.69253. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66511/0.69235. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66455/0.69205. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66441/0.69194. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66369/0.69173. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66370/0.69152. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66378/0.69138. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66244/0.69102. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66229/0.69076. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66228/0.69055. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66177/0.69040. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66161/0.69016. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66135/0.68994. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66100/0.68966. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66084/0.68924. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66044/0.68914. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65958/0.68874. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65970/0.68857. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65948/0.68829. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65873/0.68785. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65831/0.68758. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65773/0.68735. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65744/0.68694. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65663/0.68654. Took 0.11 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69122/0.69215. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68993/0.69017. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68826/0.68792. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68713/0.68541. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68610/0.68300. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68472/0.68096. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68418/0.67951. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68366/0.67854. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68350/0.67786. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.68305/0.67743. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68288/0.67711. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68272/0.67684. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68253/0.67660. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68250/0.67637. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68220/0.67619. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68213/0.67598. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68182/0.67577. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68165/0.67560. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68149/0.67540. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68120/0.67519. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68104/0.67499. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68081/0.67477. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68070/0.67452. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68040/0.67426. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67997/0.67398. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67967/0.67371. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67942/0.67341. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67900/0.67305. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67871/0.67271. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67825/0.67233. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67810/0.67196. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67775/0.67158. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67725/0.67121. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67675/0.67089. Took 0.12 sec\n",
      "Epoch 34, Loss(train/val) 0.67642/0.67055. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67586/0.67021. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67572/0.66996. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67532/0.66966. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67477/0.66951. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67451/0.66929. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67405/0.66904. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67373/0.66881. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67315/0.66859. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67282/0.66851. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67209/0.66839. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67186/0.66815. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67129/0.66811. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.67086/0.66799. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67041/0.66785. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66965/0.66768. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66958/0.66753. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66884/0.66742. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66829/0.66735. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66782/0.66716. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66701/0.66698. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66657/0.66682. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66580/0.66664. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66510/0.66649. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66453/0.66642. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66386/0.66613. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66344/0.66589. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66311/0.66573. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66217/0.66548. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66179/0.66504. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66080/0.66493. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66016/0.66437. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65910/0.66431. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65908/0.66384. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.65854/0.66336. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65798/0.66303. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65697/0.66269. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65648/0.66226. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65574/0.66177. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65524/0.66143. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65491/0.66106. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65380/0.66073. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65357/0.66031. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65340/0.65986. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65269/0.65962. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65185/0.65919. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65137/0.65891. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65057/0.65854. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64993/0.65815. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65042/0.65773. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64946/0.65753. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64892/0.65728. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64812/0.65707. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64788/0.65675. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64721/0.65647. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64689/0.65624. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64646/0.65592. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64593/0.65568. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64541/0.65551. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64517/0.65529. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64418/0.65500. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64380/0.65505. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64326/0.65496. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64261/0.65482. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64207/0.65466. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64171/0.65450. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.68750/0.69394. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.68655/0.69374. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68646/0.69359. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68602/0.69348. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68591/0.69345. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68515/0.69344. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68476/0.69344. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68483/0.69345. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68422/0.69350. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68417/0.69352. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68374/0.69360. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68319/0.69368. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68304/0.69379. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68272/0.69389. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68271/0.69402. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68224/0.69417. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68162/0.69433. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68158/0.69451. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68112/0.69470. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68063/0.69492. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68056/0.69517. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68016/0.69543. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.67969/0.69571. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67932/0.69601. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67862/0.69636. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67820/0.69674. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67834/0.69714. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67814/0.69754. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67738/0.69801. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67689/0.69852. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67663/0.69905. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67615/0.69961. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67587/0.70021. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67512/0.70085. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67458/0.70153. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67415/0.70227. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67401/0.70306. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67332/0.70386. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67265/0.70475. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67208/0.70568. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67122/0.70674. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67093/0.70778. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67030/0.70888. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66943/0.71007. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66925/0.71132. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66830/0.71259. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66787/0.71396. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66623/0.71531. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66599/0.71687. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66499/0.71835. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66448/0.72001. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66323/0.72182. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66283/0.72359. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66220/0.72541. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66102/0.72730. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65984/0.72933. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65979/0.73114. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65851/0.73341. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65715/0.73540. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65689/0.73736. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65511/0.73943. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65451/0.74153. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65323/0.74379. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65310/0.74584. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65179/0.74818. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65091/0.75080. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65029/0.75287. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.64943/0.75530. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64904/0.75715. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64767/0.75949. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64702/0.76202. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64580/0.76418. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64534/0.76693. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64439/0.76860. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64326/0.77136. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64314/0.77367. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64165/0.77581. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64155/0.77801. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64040/0.78003. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63969/0.78193. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63856/0.78397. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63811/0.78584. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63732/0.78835. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63633/0.79075. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63584/0.79283. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63523/0.79460. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63391/0.79728. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63362/0.79932. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.63231/0.80101. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63157/0.80299. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63101/0.80533. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62918/0.80717. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62937/0.80903. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62777/0.81122. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62752/0.81299. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62767/0.81511. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62644/0.81681. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62572/0.81869. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62482/0.82053. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62383/0.82194. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.71358/0.71100. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.70092/0.69995. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69256/0.69351. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68868/0.69070. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68669/0.68940. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68646/0.68855. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68569/0.68783. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68526/0.68720. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68506/0.68662. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68422/0.68607. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68390/0.68556. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68310/0.68506. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68251/0.68457. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68270/0.68412. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68203/0.68367. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68143/0.68324. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68148/0.68279. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68055/0.68237. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68014/0.68195. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67997/0.68154. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67938/0.68116. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67900/0.68077. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67852/0.68038. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67801/0.68002. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67786/0.67965. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67716/0.67931. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67733/0.67898. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67611/0.67865. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67612/0.67836. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67555/0.67809. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67510/0.67784. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67499/0.67762. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67473/0.67742. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67390/0.67726. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67332/0.67709. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67361/0.67698. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67300/0.67689. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67255/0.67683. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67161/0.67678. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67189/0.67675. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67121/0.67675. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67120/0.67675. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67052/0.67675. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66990/0.67681. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66974/0.67689. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66945/0.67699. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66936/0.67707. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66851/0.67719. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66766/0.67732. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66786/0.67745. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66724/0.67759. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66671/0.67781. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66646/0.67803. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66612/0.67829. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66509/0.67856. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66473/0.67884. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66446/0.67915. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66366/0.67945. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66328/0.67977. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66247/0.68016. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66261/0.68052. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66204/0.68090. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66091/0.68130. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66102/0.68171. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66063/0.68217. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65935/0.68264. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65932/0.68315. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65875/0.68364. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65804/0.68417. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65824/0.68470. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65755/0.68527. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65682/0.68581. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65595/0.68634. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65549/0.68697. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65498/0.68750. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65441/0.68811. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65424/0.68877. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65307/0.68944. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65268/0.69006. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65214/0.69066. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65142/0.69130. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65126/0.69191. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65035/0.69249. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65008/0.69310. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64906/0.69376. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64856/0.69430. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64950/0.69483. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64809/0.69540. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64757/0.69593. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64769/0.69643. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64703/0.69695. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64504/0.69747. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64565/0.69800. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.64554/0.69852. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64485/0.69909. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64450/0.69961. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64344/0.70002. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64351/0.70046. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64316/0.70100. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64165/0.70151. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.68911/0.69219. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68822/0.69146. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68715/0.69069. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68637/0.68994. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68549/0.68935. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68481/0.68893. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68426/0.68868. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68411/0.68852. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68342/0.68847. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68337/0.68845. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68323/0.68845. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68301/0.68845. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68269/0.68846. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68249/0.68849. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68188/0.68853. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68184/0.68857. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68147/0.68862. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68138/0.68870. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68091/0.68878. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68067/0.68889. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68019/0.68902. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.67988/0.68920. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67948/0.68938. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67899/0.68960. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67849/0.68983. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67809/0.69009. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67797/0.69037. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67742/0.69067. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67676/0.69103. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67652/0.69138. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67626/0.69174. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67560/0.69213. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67499/0.69254. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67462/0.69296. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67425/0.69338. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67373/0.69386. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67312/0.69432. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67258/0.69479. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67214/0.69528. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67168/0.69578. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67104/0.69624. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67072/0.69674. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66969/0.69723. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66961/0.69767. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66873/0.69816. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66840/0.69867. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66737/0.69916. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66673/0.69970. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66593/0.70026. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66514/0.70083. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66434/0.70129. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66349/0.70196. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.66219/0.70250. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66161/0.70295. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66090/0.70357. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65947/0.70407. Took 0.11 sec\n",
      "Epoch 56, Loss(train/val) 0.65886/0.70456. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65805/0.70499. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65755/0.70544. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65665/0.70573. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65519/0.70611. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65466/0.70648. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65384/0.70671. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65246/0.70679. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65176/0.70675. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65096/0.70667. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65041/0.70674. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64879/0.70686. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64912/0.70679. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.64782/0.70684. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64687/0.70687. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64577/0.70679. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64562/0.70672. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64415/0.70631. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64380/0.70643. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64225/0.70654. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64119/0.70660. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64141/0.70634. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63964/0.70628. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64000/0.70622. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63930/0.70614. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63811/0.70628. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63678/0.70662. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63723/0.70641. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63681/0.70635. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63552/0.70646. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63445/0.70670. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63442/0.70672. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63332/0.70714. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63267/0.70720. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63161/0.70756. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63124/0.70730. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62968/0.70737. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62938/0.70802. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62869/0.70832. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62790/0.70849. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62755/0.70841. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62687/0.70878. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62546/0.70940. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.62566/0.70929. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69679/0.69557. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69584/0.69501. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69497/0.69459. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69369/0.69436. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69437. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69204/0.69452. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69179/0.69473. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69158/0.69495. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69110/0.69515. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.69532. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69102/0.69547. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69098/0.69559. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69026/0.69570. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69028/0.69584. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68997/0.69598. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.69611. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68989/0.69626. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68918/0.69641. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68971/0.69656. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68946/0.69671. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68901/0.69689. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68905/0.69705. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68849/0.69727. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68890/0.69749. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68840/0.69772. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68831/0.69799. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68811/0.69824. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68776/0.69852. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68747/0.69880. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68708/0.69911. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68717/0.69942. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68674/0.69977. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68645/0.70010. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68636/0.70043. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68648/0.70076. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68604/0.70112. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68599/0.70147. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68560/0.70182. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68521/0.70216. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68559/0.70249. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68476/0.70284. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68464/0.70321. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68478/0.70350. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68440/0.70380. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68428/0.70414. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68397/0.70446. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68344/0.70474. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68319/0.70500. Took 0.12 sec\n",
      "Epoch 48, Loss(train/val) 0.68322/0.70526. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.68295/0.70550. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68278/0.70575. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68203/0.70595. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68179/0.70618. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68162/0.70643. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68137/0.70667. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68132/0.70690. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.68031/0.70712. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68050/0.70739. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67982/0.70762. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67977/0.70783. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67928/0.70798. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67883/0.70817. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.67818/0.70841. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67806/0.70858. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67745/0.70878. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.67712/0.70901. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67663/0.70921. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67618/0.70935. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67558/0.70955. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67517/0.70973. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67484/0.70985. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67331/0.71007. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67346/0.71025. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67316/0.71037. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67221/0.71054. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67143/0.71075. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67115/0.71091. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67045/0.71113. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66978/0.71127. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66881/0.71140. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.66831/0.71151. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66730/0.71174. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66714/0.71194. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.66545/0.71228. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66493/0.71235. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66425/0.71251. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66352/0.71261. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66305/0.71294. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66178/0.71312. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66082/0.71326. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66046/0.71354. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65979/0.71383. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65808/0.71415. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65712/0.71451. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65650/0.71490. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65546/0.71522. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65469/0.71547. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65375/0.71583. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65271/0.71588. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65171/0.71622. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69500/0.69348. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69455/0.69351. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69460/0.69354. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69433/0.69361. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69417/0.69372. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69399/0.69387. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69376/0.69404. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69366/0.69425. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69354/0.69451. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69333/0.69481. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69317/0.69513. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69283/0.69549. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69252/0.69585. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69238/0.69618. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69243/0.69647. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69210/0.69674. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69209/0.69698. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69169/0.69720. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69142/0.69738. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69140/0.69749. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69110/0.69755. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69106/0.69756. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69097/0.69765. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69083/0.69772. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69068/0.69777. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.69037/0.69781. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69017/0.69777. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68984/0.69785. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68941/0.69786. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68921/0.69792. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68902/0.69800. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68835/0.69799. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68789/0.69800. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68761/0.69788. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68743/0.69786. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68691/0.69799. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68638/0.69805. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68586/0.69811. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68475/0.69806. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68436/0.69821. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68359/0.69844. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68311/0.69855. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68223/0.69855. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.68150/0.69846. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68097/0.69877. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67955/0.69879. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67909/0.69880. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67780/0.69875. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67675/0.69869. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67584/0.69871. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67508/0.69884. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67394/0.69876. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67379/0.69888. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67279/0.69869. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67149/0.69861. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67059/0.69867. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66938/0.69858. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66854/0.69875. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66677/0.69848. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66576/0.69870. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66467/0.69845. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66317/0.69917. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66229/0.69945. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66091/0.69976. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65972/0.70027. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65818/0.70082. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65684/0.70164. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65468/0.70211. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65357/0.70305. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65257/0.70399. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64991/0.70534. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64899/0.70655. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64622/0.70823. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64442/0.70900. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64315/0.71137. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64180/0.71228. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.63980/0.71413. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63727/0.71625. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63610/0.71798. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63437/0.71957. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63131/0.72148. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63045/0.72328. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62757/0.72559. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62600/0.72745. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62347/0.72950. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62323/0.73135. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.62096/0.73344. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61703/0.73564. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61544/0.73795. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61461/0.74042. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61240/0.74327. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61071/0.74608. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60862/0.74919. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60643/0.75096. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.60426/0.75330. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60301/0.75505. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60176/0.75786. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59925/0.76098. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59826/0.76343. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59550/0.76627. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69544/0.69608. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69439/0.69530. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69372/0.69474. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69367/0.69433. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69327/0.69396. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69290/0.69366. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69274/0.69338. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69281/0.69312. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69261/0.69289. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69227/0.69267. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.69206/0.69244. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69159/0.69223. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69151/0.69201. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69143/0.69179. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69132/0.69158. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69111/0.69136. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69099/0.69114. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69077/0.69092. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69069/0.69071. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69022/0.69048. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69000/0.69022. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69023/0.68996. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68995/0.68969. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68956/0.68942. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68956/0.68913. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68916/0.68884. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68900/0.68853. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68873/0.68819. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68841/0.68784. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68839/0.68747. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68773/0.68710. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68766/0.68672. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68774/0.68634. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68713/0.68594. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68705/0.68554. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68653/0.68513. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68629/0.68473. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68591/0.68435. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68568/0.68397. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68529/0.68362. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68510/0.68331. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68450/0.68299. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68400/0.68266. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.68353/0.68238. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68326/0.68218. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68290/0.68193. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68263/0.68173. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68239/0.68158. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68155/0.68142. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68091/0.68132. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68106/0.68117. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68070/0.68106. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67983/0.68096. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67937/0.68083. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67872/0.68073. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67853/0.68068. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67751/0.68066. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67698/0.68071. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67622/0.68074. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67572/0.68076. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67491/0.68081. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67418/0.68076. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67358/0.68081. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67254/0.68081. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67156/0.68091. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67121/0.68110. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67036/0.68123. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66889/0.68147. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66868/0.68163. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66811/0.68186. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66638/0.68210. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66594/0.68238. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66444/0.68277. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66373/0.68312. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66308/0.68354. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66154/0.68411. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66005/0.68466. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65972/0.68508. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65889/0.68549. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65760/0.68588. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65663/0.68655. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65580/0.68729. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65516/0.68802. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65363/0.68875. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65201/0.68959. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65124/0.69059. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65026/0.69130. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64975/0.69194. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64853/0.69274. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64695/0.69356. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64672/0.69433. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64530/0.69507. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64361/0.69567. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64200/0.69667. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64191/0.69770. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64025/0.69848. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63945/0.69912. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63874/0.69984. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63825/0.70080. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63774/0.70165. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69419/0.69333. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69409/0.69326. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69379/0.69320. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69338/0.69314. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69337/0.69308. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69312/0.69302. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69300/0.69295. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69288/0.69290. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69257/0.69285. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69233/0.69280. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69218/0.69274. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69172/0.69267. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69146/0.69262. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69119/0.69258. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69093/0.69254. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69078/0.69251. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69046/0.69251. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68989/0.69251. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68947/0.69253. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68936/0.69256. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68906/0.69260. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68871/0.69266. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68831/0.69276. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68784/0.69292. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68721/0.69305. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68692/0.69324. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68628/0.69345. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68585/0.69367. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68554/0.69394. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68520/0.69419. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68488/0.69445. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68434/0.69474. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68413/0.69500. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68337/0.69529. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68295/0.69557. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68217/0.69584. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68179/0.69608. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68097/0.69637. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68022/0.69660. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67957/0.69687. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67891/0.69724. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67812/0.69753. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67699/0.69789. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67671/0.69795. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67589/0.69826. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67476/0.69868. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67384/0.69879. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67334/0.69921. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67213/0.69966. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67110/0.70012. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67039/0.70059. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66884/0.70118. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66794/0.70168. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66714/0.70230. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66611/0.70278. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66434/0.70359. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66358/0.70438. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66265/0.70504. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66129/0.70581. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66006/0.70670. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65893/0.70736. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65850/0.70809. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65761/0.70911. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65586/0.70983. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65485/0.71042. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65336/0.71154. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65263/0.71249. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65148/0.71314. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65042/0.71404. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65036/0.71484. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64869/0.71565. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64775/0.71640. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64620/0.71702. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64553/0.71787. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64508/0.71834. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64339/0.71903. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64382/0.71975. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64263/0.72061. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64096/0.72158. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64066/0.72215. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63974/0.72287. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63904/0.72372. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63806/0.72437. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63673/0.72510. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63579/0.72579. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63431/0.72641. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63454/0.72724. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63319/0.72800. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63254/0.72845. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63156/0.72935. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63031/0.73006. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63017/0.73053. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62880/0.73106. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.62742/0.73172. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62661/0.73251. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62627/0.73373. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62487/0.73447. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62370/0.73493. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62336/0.73536. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.62206/0.73653. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69814/0.68829. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69677/0.68974. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69566/0.69113. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69460/0.69240. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69376/0.69346. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69351/0.69435. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69252/0.69524. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69292/0.69594. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69249/0.69662. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69199/0.69721. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69128/0.69788. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69118/0.69853. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69019/0.69922. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69049/0.69996. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69002/0.70061. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69008/0.70125. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68945/0.70198. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68865/0.70272. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68840/0.70349. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68822/0.70422. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68805/0.70489. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68777/0.70551. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68724/0.70624. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68679/0.70685. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68717/0.70745. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68645/0.70806. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68596/0.70871. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68572/0.70924. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68517/0.70982. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68545/0.71032. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68457/0.71088. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68454/0.71132. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68402/0.71186. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68352/0.71236. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68368/0.71273. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68315/0.71311. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68309/0.71363. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68223/0.71410. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68196/0.71437. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68209/0.71474. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68124/0.71516. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68135/0.71567. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68064/0.71579. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68039/0.71608. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67938/0.71643. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67925/0.71684. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67875/0.71711. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67855/0.71743. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67773/0.71770. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67738/0.71793. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67646/0.71810. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67612/0.71823. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67562/0.71841. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67486/0.71869. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67418/0.71868. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67411/0.71877. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67291/0.71898. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67197/0.71903. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67163/0.71916. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67025/0.71919. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66962/0.71927. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66873/0.71925. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66793/0.71922. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66703/0.71927. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66671/0.71904. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66524/0.71900. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66465/0.71899. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66409/0.71934. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66297/0.71921. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66196/0.71907. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66095/0.71905. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66025/0.71890. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65934/0.71900. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65864/0.71887. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65719/0.71894. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65707/0.71880. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65653/0.71890. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65518/0.71892. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65472/0.71921. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.65406/0.71946. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65302/0.71953. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65189/0.71977. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65145/0.72031. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64980/0.72044. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64996/0.72067. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64859/0.72067. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64803/0.72127. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64721/0.72163. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64654/0.72200. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64596/0.72215. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64538/0.72252. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64448/0.72326. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64351/0.72371. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64288/0.72390. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64241/0.72445. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64132/0.72496. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64074/0.72583. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63961/0.72583. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63885/0.72597. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63861/0.72666. Took 0.11 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69471. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69209/0.69472. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.69472. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.69474. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.69474. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69160/0.69476. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69168/0.69477. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69150/0.69477. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69130/0.69478. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69480. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69106/0.69480. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69086/0.69480. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69056/0.69482. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69052/0.69483. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69030/0.69487. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68963/0.69486. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68948/0.69487. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68905/0.69487. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68843/0.69488. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68777/0.69497. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68745/0.69501. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68686/0.69508. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68642/0.69510. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68564/0.69515. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68496/0.69525. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68430/0.69540. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68353/0.69548. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68252/0.69564. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68175/0.69582. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68106/0.69609. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68005/0.69623. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.67917/0.69631. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67809/0.69665. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67708/0.69699. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67581/0.69749. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67496/0.69760. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67402/0.69826. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67276/0.69873. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67161/0.69898. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67027/0.69961. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66948/0.70015. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66809/0.70063. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66688/0.70114. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66572/0.70186. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66454/0.70258. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66328/0.70317. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66151/0.70362. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66074/0.70497. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.65918/0.70457. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65796/0.70501. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65699/0.70572. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65473/0.70690. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65288/0.70540. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65162/0.70703. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.64999/0.70752. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64821/0.70819. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64652/0.70858. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64440/0.70931. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64324/0.70991. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64106/0.70942. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63977/0.71037. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63860/0.71395. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63606/0.71390. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63475/0.71630. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63301/0.71736. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63104/0.71934. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.62809/0.72191. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62738/0.72320. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62577/0.72241. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62511/0.72684. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62304/0.72936. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62200/0.73111. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61947/0.73368. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61834/0.73585. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61485/0.73875. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.61384/0.74035. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61334/0.74153. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61181/0.74303. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.60973/0.74630. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60908/0.74751. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.60767/0.75050. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.60561/0.75150. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60449/0.75429. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60250/0.75685. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.60210/0.75939. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59932/0.76142. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.59874/0.76467. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.59660/0.76736. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59650/0.77119. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59529/0.77590. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.59342/0.77635. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59169/0.77919. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59045/0.77923. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58997/0.78349. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58869/0.78337. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58831/0.78873. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58573/0.78993. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58455/0.79375. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58426/0.79557. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58199/0.79871. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69558/0.69752. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69558/0.69743. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69509/0.69734. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69502/0.69725. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69478/0.69716. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69447/0.69707. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69436/0.69697. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69412/0.69686. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69351/0.69676. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69324/0.69670. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69301/0.69664. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69262/0.69664. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69231/0.69668. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69192/0.69676. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69164/0.69687. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69132/0.69699. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69123/0.69712. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69097/0.69728. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69073/0.69743. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69054/0.69760. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69029/0.69776. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69000/0.69794. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68970/0.69813. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68939/0.69832. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68901/0.69851. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68876/0.69873. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68830/0.69896. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68811/0.69919. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68775/0.69944. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68738/0.69969. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68686/0.69994. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68601/0.70019. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68540/0.70048. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68483/0.70085. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68426/0.70120. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68346/0.70161. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68242/0.70205. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68162/0.70259. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68064/0.70319. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67997/0.70377. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67870/0.70447. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67804/0.70516. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67686/0.70587. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67566/0.70673. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67461/0.70755. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67361/0.70847. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67242/0.70939. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67147/0.71028. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67038/0.71127. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66900/0.71225. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66796/0.71325. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66702/0.71426. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66556/0.71522. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66443/0.71621. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66311/0.71720. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66187/0.71821. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66105/0.71928. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65968/0.72048. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65836/0.72152. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65709/0.72266. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65615/0.72384. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65487/0.72498. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65385/0.72628. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65271/0.72763. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65163/0.72887. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64999/0.73028. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.64869/0.73172. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64749/0.73310. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64678/0.73446. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64537/0.73597. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64433/0.73730. Took 0.12 sec\n",
      "Epoch 71, Loss(train/val) 0.64324/0.73866. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64168/0.74021. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64092/0.74188. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63946/0.74361. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63841/0.74505. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63704/0.74654. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63565/0.74825. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63510/0.74992. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63386/0.75135. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63286/0.75299. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63134/0.75429. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62976/0.75571. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62930/0.75688. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62759/0.75852. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.62639/0.76031. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.62439/0.76177. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62366/0.76355. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62222/0.76495. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62089/0.76665. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61904/0.76830. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61807/0.77000. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61639/0.77148. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61575/0.77334. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61460/0.77469. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61189/0.77685. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61043/0.77789. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60978/0.77904. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60867/0.78062. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60651/0.78248. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69289/0.69777. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69806. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69837. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.69867. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69154/0.69896. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69108/0.69927. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69051/0.69958. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69020/0.69992. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68977/0.70026. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68939/0.70064. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68934/0.70103. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68897/0.70147. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68864/0.70195. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68834/0.70243. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68788/0.70298. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68770/0.70358. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68707/0.70425. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68665/0.70493. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68624/0.70564. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68600/0.70644. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68554/0.70728. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68533/0.70812. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68494/0.70896. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68448/0.70975. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68403/0.71063. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68370/0.71149. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68343/0.71235. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68298/0.71321. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68299/0.71404. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68249/0.71483. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68187/0.71553. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68164/0.71631. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68162/0.71709. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68129/0.71778. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68103/0.71841. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68091/0.71901. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68056/0.71963. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67990/0.72028. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67966/0.72086. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67925/0.72138. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67892/0.72211. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67859/0.72263. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67808/0.72315. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67802/0.72375. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67769/0.72419. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67692/0.72475. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67686/0.72544. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67648/0.72601. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67594/0.72655. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67551/0.72715. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67516/0.72766. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67503/0.72827. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67452/0.72888. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67361/0.72942. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67343/0.72993. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67316/0.73051. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67247/0.73109. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67204/0.73159. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67149/0.73221. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67050/0.73267. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66990/0.73350. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66983/0.73409. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66928/0.73459. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66868/0.73505. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66810/0.73580. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66708/0.73651. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66687/0.73697. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66636/0.73768. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66533/0.73828. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66548/0.73865. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66406/0.73934. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66345/0.73985. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66313/0.74053. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66266/0.74114. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66178/0.74168. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66110/0.74217. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66042/0.74303. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65985/0.74369. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.65891/0.74428. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65916/0.74499. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65791/0.74562. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65723/0.74601. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65709/0.74677. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65595/0.74735. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65527/0.74774. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65506/0.74868. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65503/0.74912. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65419/0.74974. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65351/0.75030. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65266/0.75094. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65251/0.75144. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65183/0.75222. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65065/0.75284. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64970/0.75344. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64984/0.75414. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64921/0.75451. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64926/0.75498. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64800/0.75569. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64758/0.75645. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64727/0.75669. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69434/0.69275. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69390/0.69267. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69365/0.69263. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69332/0.69259. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69307/0.69259. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69275/0.69265. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69220/0.69275. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69177/0.69284. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69152/0.69292. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69157/0.69298. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.69298. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69104/0.69299. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69091/0.69294. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69045/0.69288. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69021/0.69278. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68996/0.69266. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68993/0.69249. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68922/0.69230. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68942/0.69209. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68886/0.69183. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68831/0.69154. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68798/0.69119. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68747/0.69080. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68709/0.69041. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68684/0.68997. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68588/0.68950. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68535/0.68899. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68499/0.68852. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68419/0.68813. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68336/0.68754. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68245/0.68708. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68200/0.68652. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68156/0.68607. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68083/0.68561. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68008/0.68532. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67942/0.68483. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67922/0.68449. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67864/0.68426. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67814/0.68402. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67726/0.68391. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67758/0.68373. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67645/0.68366. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67602/0.68363. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67559/0.68362. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67497/0.68363. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67473/0.68370. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67455/0.68399. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67352/0.68416. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67351/0.68416. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67279/0.68424. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67277/0.68437. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67226/0.68462. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67120/0.68473. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67088/0.68511. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67097/0.68520. Took 0.12 sec\n",
      "Epoch 55, Loss(train/val) 0.67043/0.68569. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66999/0.68585. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66934/0.68624. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66901/0.68633. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66868/0.68655. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66806/0.68714. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66744/0.68753. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66691/0.68789. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66715/0.68864. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66579/0.68905. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66605/0.68943. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66619/0.69006. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66513/0.69066. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66397/0.69141. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66355/0.69183. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66405/0.69233. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66316/0.69272. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66202/0.69340. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66245/0.69406. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66082/0.69480. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66123/0.69545. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66062/0.69583. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65960/0.69656. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65929/0.69754. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65897/0.69844. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65849/0.69963. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65819/0.70013. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65740/0.70069. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65643/0.70157. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65603/0.70265. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65523/0.70353. Took 0.12 sec\n",
      "Epoch 86, Loss(train/val) 0.65470/0.70423. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65472/0.70507. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65428/0.70589. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65320/0.70669. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65207/0.70751. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65235/0.70839. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65128/0.70918. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65048/0.71058. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65014/0.71130. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64916/0.71205. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64874/0.71296. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64831/0.71408. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64928/0.71482. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64687/0.71592. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69814/0.69500. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69703/0.69480. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69622/0.69465. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69490/0.69464. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69382/0.69485. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69255/0.69528. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69202/0.69582. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69162/0.69635. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69089/0.69679. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69071/0.69712. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69041/0.69740. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69041/0.69765. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69009/0.69787. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68990/0.69804. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68964/0.69821. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68932/0.69841. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68932/0.69864. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68897/0.69887. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68876/0.69911. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68843/0.69937. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68799/0.69963. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68790/0.69991. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68781/0.70022. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68752/0.70053. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68751/0.70087. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68691/0.70127. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68691/0.70165. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68649/0.70205. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68641/0.70247. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68600/0.70291. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68567/0.70335. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68540/0.70376. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68520/0.70419. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68506/0.70459. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68497/0.70502. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68440/0.70549. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68424/0.70595. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68380/0.70644. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68377/0.70685. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.68360/0.70725. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68310/0.70771. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68293/0.70811. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68263/0.70851. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68225/0.70892. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68211/0.70937. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68146/0.70978. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68132/0.71016. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68090/0.71061. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68066/0.71104. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68025/0.71149. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68019/0.71188. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67965/0.71223. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67910/0.71260. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67863/0.71300. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67834/0.71338. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67819/0.71373. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67757/0.71409. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67705/0.71442. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67656/0.71476. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67630/0.71519. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67610/0.71558. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67562/0.71601. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67520/0.71634. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67464/0.71673. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67427/0.71710. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67367/0.71745. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67317/0.71780. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67295/0.71819. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67262/0.71853. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67184/0.71882. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67165/0.71914. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67105/0.71951. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67090/0.71988. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67009/0.72019. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66945/0.72053. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66879/0.72083. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66910/0.72112. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66827/0.72140. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66842/0.72173. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66780/0.72192. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66693/0.72225. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66667/0.72240. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66641/0.72258. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66587/0.72288. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66513/0.72317. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66491/0.72328. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66468/0.72355. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66407/0.72386. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66325/0.72413. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66239/0.72443. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66260/0.72455. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66185/0.72488. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66158/0.72502. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66161/0.72525. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66048/0.72537. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65984/0.72561. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65998/0.72579. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65890/0.72588. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65880/0.72615. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65820/0.72645. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69300. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69454/0.69296. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69394/0.69291. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69371/0.69288. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69344/0.69287. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69319/0.69287. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69297/0.69290. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69263/0.69297. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69230/0.69306. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69227/0.69317. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69172/0.69330. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69156/0.69344. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69138/0.69359. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69127/0.69375. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69100/0.69392. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.69075/0.69412. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69073/0.69432. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69048/0.69455. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68999/0.69479. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68966/0.69506. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68938/0.69537. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68896/0.69573. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68844/0.69614. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68833/0.69661. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68783/0.69717. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68734/0.69780. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68678/0.69854. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68626/0.69935. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68575/0.70026. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68510/0.70123. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68446/0.70227. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68400/0.70334. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68357/0.70441. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68283/0.70550. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68259/0.70658. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68173/0.70761. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68104/0.70861. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.68078/0.70955. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68039/0.71053. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67996/0.71134. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67942/0.71211. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67910/0.71286. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67845/0.71356. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67774/0.71427. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67757/0.71486. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67737/0.71541. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67669/0.71596. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67603/0.71652. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67587/0.71703. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67516/0.71750. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67484/0.71796. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67414/0.71842. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67374/0.71891. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67344/0.71935. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67283/0.71959. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67212/0.71999. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67178/0.72031. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67093/0.72066. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67081/0.72103. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67024/0.72118. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66933/0.72145. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66906/0.72173. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66866/0.72193. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66804/0.72199. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66683/0.72211. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66655/0.72225. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66561/0.72233. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66514/0.72243. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66470/0.72254. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66344/0.72251. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66329/0.72232. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66246/0.72242. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66163/0.72242. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66106/0.72237. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66042/0.72231. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65952/0.72191. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65875/0.72191. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65792/0.72175. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65651/0.72145. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65536/0.72117. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65520/0.72147. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65426/0.72132. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65361/0.72099. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65271/0.72071. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65107/0.72065. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65099/0.72033. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64991/0.72009. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64814/0.72006. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64777/0.71976. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64683/0.71965. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64521/0.71935. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64393/0.71971. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64316/0.71941. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64146/0.71863. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64045/0.71935. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63957/0.71861. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63846/0.71833. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63730/0.71853. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63563/0.71915. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63477/0.71837. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69341/0.69366. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69365. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69288/0.69365. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69264/0.69366. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69246/0.69368. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69217/0.69368. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69186/0.69370. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69154/0.69373. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69134/0.69374. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69101/0.69373. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69068/0.69372. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69043/0.69369. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69004/0.69364. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68976/0.69357. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68918/0.69348. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68928/0.69340. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68866/0.69332. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68811/0.69324. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68763/0.69317. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68738/0.69309. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68694/0.69310. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68646/0.69307. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68589/0.69308. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68572/0.69310. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68479/0.69319. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68471/0.69322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68422/0.69330. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68379/0.69340. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68309/0.69351. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68313/0.69366. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68267/0.69385. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68204/0.69391. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68149/0.69406. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68089/0.69412. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68039/0.69431. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68003/0.69452. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67981/0.69469. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67922/0.69475. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67877/0.69495. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67806/0.69520. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67784/0.69546. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67719/0.69578. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67700/0.69591. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67672/0.69614. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67595/0.69638. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67549/0.69649. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67491/0.69667. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67461/0.69699. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67387/0.69733. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67340/0.69736. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67252/0.69787. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67230/0.69810. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67142/0.69852. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67128/0.69878. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67023/0.69902. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66993/0.69950. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66936/0.69984. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66924/0.70024. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66759/0.70054. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66761/0.70101. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66653/0.70145. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66542/0.70190. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66550/0.70250. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66460/0.70292. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66396/0.70364. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66279/0.70424. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66210/0.70504. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66150/0.70549. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66032/0.70618. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65885/0.70702. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65861/0.70753. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65829/0.70844. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65736/0.70921. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65601/0.71017. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65515/0.71083. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65456/0.71179. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65308/0.71270. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65303/0.71370. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65113/0.71469. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65056/0.71564. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64984/0.71661. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64897/0.71766. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64832/0.71868. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64767/0.71985. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64637/0.72093. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64473/0.72199. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64459/0.72321. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64350/0.72415. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64175/0.72527. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64055/0.72664. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63958/0.72792. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63885/0.72918. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63813/0.73057. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63764/0.73159. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63636/0.73319. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63475/0.73460. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63510/0.73586. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63360/0.73699. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63215/0.73870. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63186/0.73995. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69162. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69419/0.69183. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69371/0.69206. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69351/0.69234. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69334/0.69265. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69255/0.69302. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69190/0.69346. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69162/0.69394. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69118/0.69448. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69084/0.69508. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68999/0.69567. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68966/0.69627. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68896/0.69684. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68820/0.69721. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68754/0.69764. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68696/0.69795. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68637/0.69811. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68564/0.69830. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68469/0.69823. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68351/0.69806. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68264/0.69801. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.68165/0.69808. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68080/0.69796. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.67912/0.69807. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67824/0.69816. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67681/0.69798. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67592/0.69815. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67479/0.69807. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67329/0.69774. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67249/0.69759. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67128/0.69756. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66981/0.69747. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66901/0.69723. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66763/0.69698. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66673/0.69680. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66604/0.69672. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66429/0.69684. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66398/0.69685. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66264/0.69702. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66144/0.69695. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66108/0.69718. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65932/0.69734. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.65828/0.69755. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65769/0.69754. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65659/0.69792. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65567/0.69792. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65448/0.69828. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65399/0.69858. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65330/0.69905. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65200/0.69919. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65116/0.69969. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65021/0.70029. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64878/0.70066. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64799/0.70115. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64741/0.70149. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64612/0.70200. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.64470/0.70246. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64428/0.70308. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64359/0.70377. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64197/0.70426. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64119/0.70490. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63967/0.70561. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63884/0.70657. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63803/0.70697. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63653/0.70784. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63559/0.70851. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63463/0.70919. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63323/0.71000. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63228/0.71108. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.63119/0.71167. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62996/0.71248. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62942/0.71311. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62785/0.71436. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62684/0.71538. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62546/0.71618. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62427/0.71695. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62331/0.71799. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62218/0.71916. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62091/0.71996. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61903/0.72086. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.61693/0.72234. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61610/0.72318. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61507/0.72474. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61499/0.72608. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.61181/0.72677. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61049/0.72778. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60902/0.72959. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.60862/0.73100. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60790/0.73204. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60563/0.73354. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60367/0.73488. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60270/0.73635. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60093/0.73783. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.59959/0.73911. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59801/0.74054. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.59652/0.74163. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59388/0.74378. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59192/0.74532. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.59117/0.74676. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58943/0.74877. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69553/0.69355. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69504/0.69302. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69459/0.69250. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69411/0.69198. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69351/0.69146. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69300/0.69095. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69256/0.69050. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69233/0.69015. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69220/0.68988. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69179/0.68969. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69128/0.68955. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69093/0.68944. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69076/0.68939. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69069/0.68935. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69032/0.68935. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68987/0.68938. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68961/0.68943. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68956/0.68949. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68904/0.68956. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68858/0.68967. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68857/0.68980. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.68832/0.68995. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68803/0.69007. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68759/0.69021. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68752/0.69038. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68711/0.69056. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68713/0.69074. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68663/0.69090. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68629/0.69107. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68610/0.69122. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68552/0.69140. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68565/0.69154. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68536/0.69168. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68502/0.69181. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68462/0.69195. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68492/0.69204. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68448/0.69213. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68394/0.69221. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68374/0.69230. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68335/0.69239. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68315/0.69250. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68298/0.69257. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68245/0.69265. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68229/0.69273. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68207/0.69277. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68158/0.69285. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68154/0.69292. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68094/0.69298. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68084/0.69302. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68035/0.69304. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67981/0.69315. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67962/0.69316. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67913/0.69323. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.67885/0.69331. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67844/0.69331. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67815/0.69332. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67754/0.69337. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67726/0.69342. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67683/0.69345. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67615/0.69355. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67573/0.69358. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67519/0.69367. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67474/0.69370. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67414/0.69376. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67353/0.69386. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67334/0.69379. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67243/0.69398. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67169/0.69414. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67121/0.69410. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67020/0.69427. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66982/0.69425. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66927/0.69455. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66866/0.69446. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66756/0.69468. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.66732/0.69471. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66639/0.69497. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66536/0.69514. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66504/0.69522. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66363/0.69557. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66313/0.69563. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66186/0.69606. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66113/0.69613. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66035/0.69642. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65886/0.69693. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65816/0.69718. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65690/0.69777. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65630/0.69817. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65525/0.69870. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65448/0.69953. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65325/0.70003. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65224/0.70079. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65070/0.70144. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.65011/0.70192. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64917/0.70249. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64770/0.70360. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64712/0.70413. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64643/0.70503. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64576/0.70577. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64320/0.70672. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64308/0.70762. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69430/0.69932. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69350/0.69867. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69316/0.69823. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69275/0.69793. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69240/0.69775. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69194/0.69768. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69206/0.69768. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69158/0.69773. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.69781. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.69108/0.69794. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69104/0.69810. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69088/0.69826. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69086/0.69846. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69046/0.69869. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69045/0.69894. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69022/0.69922. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68989/0.69953. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68969/0.69985. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68943/0.70022. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68911/0.70061. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68886/0.70102. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68855/0.70144. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68827/0.70187. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68799/0.70232. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68769/0.70279. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68747/0.70327. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68715/0.70377. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68685/0.70426. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68642/0.70474. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68628/0.70525. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68574/0.70574. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68568/0.70622. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68550/0.70671. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68525/0.70718. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68475/0.70761. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68453/0.70809. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68426/0.70852. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68396/0.70894. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68369/0.70931. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68354/0.70968. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68323/0.71005. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68292/0.71043. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68282/0.71078. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68228/0.71114. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68188/0.71151. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68189/0.71182. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68163/0.71210. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68141/0.71243. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68106/0.71274. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68047/0.71306. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68040/0.71340. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68029/0.71375. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67957/0.71409. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67927/0.71438. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67884/0.71472. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67866/0.71504. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67827/0.71537. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67805/0.71568. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67756/0.71601. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67716/0.71634. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67666/0.71665. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67668/0.71691. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67598/0.71718. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67564/0.71744. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67532/0.71775. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67492/0.71808. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67442/0.71838. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67384/0.71869. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67332/0.71902. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67303/0.71934. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67281/0.71955. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67193/0.71990. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.67164/0.72013. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67068/0.72040. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67047/0.72069. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66995/0.72097. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66933/0.72122. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66854/0.72141. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66811/0.72173. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66712/0.72197. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66691/0.72215. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66615/0.72245. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66570/0.72263. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66444/0.72292. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.66436/0.72310. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66372/0.72321. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66247/0.72343. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66178/0.72357. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66087/0.72393. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66030/0.72434. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65959/0.72467. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65895/0.72501. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65812/0.72527. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65747/0.72561. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65677/0.72596. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65626/0.72638. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65580/0.72653. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65457/0.72682. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65346/0.72739. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65358/0.72756. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69452/0.69716. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69414/0.69701. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69389/0.69679. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69314/0.69651. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69258/0.69619. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69205/0.69587. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69155/0.69558. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69077/0.69538. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69056/0.69533. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69043/0.69541. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69007/0.69561. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68959/0.69592. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68910/0.69630. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68863/0.69676. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68859/0.69732. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68801/0.69796. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68754/0.69868. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68690/0.69946. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68647/0.70036. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68627/0.70128. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68545/0.70235. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68526/0.70355. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68451/0.70475. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68394/0.70600. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68324/0.70734. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68279/0.70873. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68207/0.71005. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68162/0.71137. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68125/0.71253. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68039/0.71379. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68008/0.71494. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67941/0.71602. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67911/0.71711. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67830/0.71824. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67844/0.71901. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67761/0.71978. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67699/0.72054. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67636/0.72141. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67601/0.72219. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67565/0.72290. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67513/0.72361. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67473/0.72409. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67399/0.72478. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67345/0.72536. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67293/0.72605. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67189/0.72676. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67194/0.72728. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67112/0.72795. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67048/0.72850. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66984/0.72911. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66939/0.72958. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66871/0.73020. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66804/0.73098. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66782/0.73155. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66675/0.73212. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66646/0.73270. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66615/0.73334. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66522/0.73386. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66485/0.73446. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66448/0.73499. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66338/0.73573. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66303/0.73634. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66194/0.73677. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66152/0.73731. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66098/0.73802. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66039/0.73868. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65985/0.73931. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65915/0.73990. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65818/0.74047. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65753/0.74106. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65726/0.74167. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65671/0.74262. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65611/0.74305. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65468/0.74404. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65482/0.74462. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65380/0.74525. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65338/0.74574. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65233/0.74632. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65123/0.74728. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65110/0.74768. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65025/0.74877. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64939/0.74922. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64880/0.75047. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64811/0.75085. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64612/0.75223. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64617/0.75299. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64546/0.75344. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.64481/0.75385. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64500/0.75449. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64414/0.75577. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64285/0.75605. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64204/0.75727. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64236/0.75785. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64121/0.75880. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63921/0.75957. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63951/0.76041. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63843/0.76108. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63763/0.76168. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63698/0.76289. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63543/0.76341. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69430/0.68540. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69332/0.68636. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69258/0.68732. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69232/0.68821. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69166/0.68898. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69143/0.68967. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69101/0.69027. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69074/0.69077. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.69120. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69037/0.69163. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68969/0.69204. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68986/0.69241. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68908/0.69274. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68903/0.69311. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68845/0.69352. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68857/0.69399. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68811/0.69434. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68774/0.69475. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68788/0.69518. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68725/0.69563. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68673/0.69611. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68669/0.69656. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68644/0.69700. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68627/0.69744. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68552/0.69791. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68570/0.69840. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68527/0.69891. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68486/0.69939. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68477/0.69980. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68412/0.70018. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68397/0.70057. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68396/0.70096. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68296/0.70144. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68329/0.70185. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68289/0.70220. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68266/0.70256. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68213/0.70285. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68189/0.70318. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68189/0.70351. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68150/0.70382. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.68131/0.70411. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68086/0.70430. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68036/0.70458. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68023/0.70486. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67939/0.70509. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67852/0.70530. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67884/0.70563. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67859/0.70581. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67796/0.70608. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67764/0.70630. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67711/0.70660. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67639/0.70684. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67607/0.70704. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67565/0.70709. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67523/0.70736. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67455/0.70753. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67421/0.70768. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67404/0.70797. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67309/0.70813. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67286/0.70820. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67219/0.70854. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67194/0.70883. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67160/0.70895. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67098/0.70917. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67018/0.70953. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66961/0.70969. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66888/0.70978. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66868/0.71019. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66817/0.71057. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66715/0.71093. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66668/0.71115. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66613/0.71132. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66566/0.71180. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66490/0.71214. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66453/0.71253. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66385/0.71272. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66376/0.71345. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66298/0.71381. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66233/0.71420. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66174/0.71478. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66151/0.71485. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66081/0.71528. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65996/0.71585. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65914/0.71681. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65945/0.71719. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65869/0.71788. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65820/0.71859. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65773/0.71908. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65700/0.71947. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65657/0.71988. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65609/0.72064. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65554/0.72085. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65531/0.72139. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65401/0.72216. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65437/0.72268. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65333/0.72340. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65325/0.72425. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65253/0.72468. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65207/0.72543. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65105/0.72590. Took 0.10 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69461/0.69499. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69413/0.69529. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69366/0.69560. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69372/0.69595. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69310/0.69635. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69275/0.69679. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69254/0.69724. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69253/0.69770. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69205/0.69812. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69181/0.69850. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69188/0.69886. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69179/0.69915. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69138/0.69939. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69104/0.69957. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69094/0.69977. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69099/0.69996. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69061/0.70007. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69057/0.70015. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.69032/0.70024. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69001/0.70032. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68990/0.70045. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68974/0.70049. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68922/0.70062. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68938/0.70072. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68873/0.70074. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68888/0.70080. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68820/0.70087. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68790/0.70094. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68744/0.70102. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68713/0.70113. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68652/0.70122. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68605/0.70123. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68592/0.70137. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68526/0.70146. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68453/0.70159. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68449/0.70166. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68359/0.70183. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68350/0.70215. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68242/0.70211. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68188/0.70243. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68130/0.70266. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68075/0.70302. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67986/0.70309. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67878/0.70339. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67829/0.70362. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67736/0.70379. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67706/0.70410. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67606/0.70445. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67502/0.70479. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67425/0.70519. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67361/0.70541. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67291/0.70599. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67275/0.70673. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67231/0.70683. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67099/0.70734. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67027/0.70779. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67005/0.70841. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66901/0.70895. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66870/0.70973. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66760/0.71024. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66735/0.71057. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66634/0.71110. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66626/0.71193. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66526/0.71279. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66455/0.71341. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66400/0.71443. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66353/0.71494. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66270/0.71581. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66201/0.71664. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66190/0.71749. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66153/0.71778. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66023/0.71912. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65944/0.71998. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65871/0.72075. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65809/0.72167. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65834/0.72229. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65724/0.72307. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65653/0.72387. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65582/0.72507. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65417/0.72565. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65533/0.72692. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65396/0.72770. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65280/0.72817. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65274/0.72917. Took 0.12 sec\n",
      "Epoch 84, Loss(train/val) 0.65103/0.73047. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65084/0.73067. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65114/0.73226. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64978/0.73290. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64958/0.73407. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64782/0.73500. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64684/0.73612. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64676/0.73693. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64698/0.73810. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64525/0.73855. Took 0.13 sec\n",
      "Epoch 94, Loss(train/val) 0.64394/0.73976. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64399/0.74074. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64252/0.74185. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64257/0.74320. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64130/0.74380. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64088/0.74516. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69271/0.69583. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69266/0.69580. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69279/0.69575. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69237/0.69570. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69567. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.69565. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69217/0.69565. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69200/0.69565. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69203/0.69566. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69179/0.69568. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69176/0.69570. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69155/0.69572. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69166/0.69576. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69156/0.69580. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69155/0.69583. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69146/0.69591. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69115/0.69596. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69103/0.69604. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69064/0.69610. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69062/0.69617. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69051/0.69625. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69017/0.69634. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69027/0.69644. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68955/0.69653. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68933/0.69661. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68892/0.69668. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68901/0.69678. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68821/0.69685. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68784/0.69687. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68752/0.69696. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68687/0.69705. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68632/0.69699. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68527/0.69694. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68489/0.69684. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68419/0.69672. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68368/0.69656. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68252/0.69620. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68172/0.69607. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68078/0.69561. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67988/0.69543. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67932/0.69510. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67828/0.69482. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67750/0.69444. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67672/0.69406. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67574/0.69374. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67446/0.69355. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67369/0.69319. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67282/0.69305. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67169/0.69281. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67087/0.69277. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66974/0.69271. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66913/0.69260. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66807/0.69282. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66725/0.69244. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66650/0.69289. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66550/0.69295. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66494/0.69349. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66410/0.69343. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66294/0.69372. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66213/0.69391. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66159/0.69437. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66096/0.69497. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66013/0.69551. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65915/0.69557. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65795/0.69643. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65730/0.69703. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65661/0.69743. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65600/0.69779. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65545/0.69866. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65439/0.69920. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65350/0.69989. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65253/0.70090. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65224/0.70139. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65169/0.70230. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65071/0.70299. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65108/0.70349. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64916/0.70444. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64905/0.70549. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64804/0.70630. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64726/0.70714. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64648/0.70763. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64520/0.70860. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64493/0.70939. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64349/0.71002. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64294/0.71078. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64216/0.71174. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64193/0.71301. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64086/0.71364. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63966/0.71465. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63910/0.71544. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63898/0.71663. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63800/0.71757. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63634/0.71846. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63661/0.71959. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63575/0.72104. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63506/0.72189. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63419/0.72251. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63362/0.72401. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63274/0.72518. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63267/0.72591. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69429/0.69582. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69430/0.69585. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69396/0.69590. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69389/0.69596. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69380/0.69602. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69355/0.69608. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69336/0.69613. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69303/0.69618. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69295/0.69623. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69268/0.69627. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69233/0.69630. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69200/0.69634. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69147/0.69635. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69122/0.69637. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69077/0.69639. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69068/0.69647. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69002/0.69659. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68987/0.69679. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68913/0.69704. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68859/0.69734. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68829/0.69771. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68746/0.69821. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68693/0.69871. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68653/0.69931. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68592/0.69998. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68527/0.70071. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68462/0.70151. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68427/0.70232. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68352/0.70320. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68258/0.70418. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68205/0.70517. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68164/0.70620. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68066/0.70730. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67979/0.70842. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67912/0.70955. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67851/0.71070. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67799/0.71188. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67713/0.71305. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67595/0.71439. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67528/0.71569. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67442/0.71702. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67365/0.71832. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67257/0.71967. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67184/0.72107. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67141/0.72231. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67062/0.72374. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66954/0.72517. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66837/0.72653. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66793/0.72801. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66690/0.72938. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66581/0.73082. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66477/0.73236. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66429/0.73383. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66372/0.73531. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66284/0.73669. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66184/0.73826. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66074/0.73965. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65994/0.74108. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65909/0.74273. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65859/0.74430. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65731/0.74584. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65670/0.74723. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65598/0.74872. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65475/0.75041. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65417/0.75191. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65317/0.75345. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65249/0.75500. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65159/0.75649. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65100/0.75795. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64977/0.75946. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64903/0.76095. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64847/0.76215. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64748/0.76364. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64679/0.76503. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64570/0.76643. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64482/0.76777. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64400/0.76928. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64311/0.77073. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64229/0.77221. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64156/0.77339. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64058/0.77490. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63990/0.77646. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63862/0.77784. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63740/0.77918. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63699/0.78061. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63570/0.78190. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63514/0.78368. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63414/0.78493. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63266/0.78638. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63168/0.78795. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63087/0.78935. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63037/0.79098. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62947/0.79229. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62765/0.79394. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62691/0.79522. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62609/0.79709. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62547/0.79865. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.62438/0.80001. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62287/0.80151. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62159/0.80302. Took 0.11 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69597/0.70083. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69560/0.70053. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69488/0.70028. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69471/0.70004. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69459/0.69976. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69413/0.69950. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69323/0.69919. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69319/0.69881. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69287/0.69829. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69220/0.69764. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69124/0.69676. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69042/0.69563. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68988/0.69438. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68934/0.69307. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68801/0.69198. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68674/0.69100. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68619/0.69048. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68544/0.69015. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68426/0.69002. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68396/0.69020. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68297/0.69064. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68217/0.69101. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68107/0.69130. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68050/0.69200. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68006/0.69251. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.67915/0.69339. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67845/0.69406. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67850/0.69457. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67748/0.69550. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67629/0.69618. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67533/0.69698. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67479/0.69714. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67448/0.69744. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67412/0.69846. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67356/0.69913. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67227/0.69971. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67197/0.70026. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67198/0.70086. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67007/0.70092. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67025/0.70153. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66926/0.70220. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66900/0.70347. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66807/0.70306. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66769/0.70342. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66778/0.70449. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66602/0.70509. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66564/0.70495. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66572/0.70517. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66447/0.70568. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66481/0.70603. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66363/0.70661. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66300/0.70730. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66356/0.70783. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66302/0.70830. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66204/0.70836. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66195/0.70850. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66026/0.70978. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66026/0.70941. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65998/0.70991. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65874/0.71025. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65788/0.71150. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65748/0.71244. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65676/0.71187. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65585/0.71342. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65605/0.71236. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65561/0.71422. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65441/0.71412. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65385/0.71463. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65322/0.71492. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65249/0.71614. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65309/0.71564. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65113/0.71619. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.64919/0.71605. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64884/0.71851. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64896/0.71816. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64844/0.71860. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64830/0.71941. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64715/0.72011. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64600/0.72112. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64548/0.71985. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64396/0.72091. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64320/0.72132. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64279/0.72206. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64216/0.72275. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64141/0.72303. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63973/0.72361. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63953/0.72393. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63828/0.72627. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63758/0.72571. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63670/0.72639. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63603/0.72711. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63555/0.72664. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63396/0.72799. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63352/0.72838. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63157/0.73024. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63227/0.73068. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63119/0.72932. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62953/0.73068. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62836/0.73082. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62821/0.73118. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69312/0.69301. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.69302. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69273/0.69305. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69266/0.69306. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69251/0.69308. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69257/0.69310. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69209/0.69312. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69211/0.69315. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69181/0.69318. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69169/0.69321. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69153/0.69325. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69160/0.69329. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69135/0.69332. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69110/0.69336. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69080/0.69338. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69056/0.69341. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69037/0.69343. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69006/0.69345. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68992/0.69345. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68969/0.69345. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68953/0.69343. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68918/0.69339. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68900/0.69336. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68844/0.69331. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68830/0.69325. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68809/0.69317. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68739/0.69308. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68690/0.69299. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68625/0.69288. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68586/0.69272. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68547/0.69254. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68493/0.69234. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68427/0.69216. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68382/0.69188. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68305/0.69156. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68270/0.69122. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68182/0.69089. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68149/0.69053. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68059/0.69008. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67983/0.68959. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67916/0.68901. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67829/0.68848. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67745/0.68797. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67656/0.68733. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67588/0.68676. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67479/0.68609. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67379/0.68537. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67268/0.68476. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67205/0.68396. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67064/0.68335. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66960/0.68271. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66838/0.68215. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66782/0.68160. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66634/0.68106. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66515/0.68059. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66403/0.68005. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66246/0.67986. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66149/0.67930. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65997/0.67902. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65872/0.67878. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65723/0.67863. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65580/0.67809. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65347/0.67824. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65270/0.67797. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65134/0.67804. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64998/0.67810. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64886/0.67825. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64666/0.67829. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64604/0.67859. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64339/0.67899. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64164/0.67972. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.64055/0.68030. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63917/0.68081. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63718/0.68183. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.63653/0.68217. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63449/0.68314. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63293/0.68382. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63191/0.68489. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63025/0.68574. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62903/0.68673. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62684/0.68763. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62624/0.68924. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62443/0.68993. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.62246/0.69116. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62207/0.69279. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62002/0.69359. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61838/0.69473. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61704/0.69615. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61548/0.69768. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.61417/0.69915. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61294/0.70029. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61231/0.70183. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.61021/0.70319. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60924/0.70416. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60745/0.70600. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60654/0.70731. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60438/0.70865. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60308/0.71000. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.60277/0.71136. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60072/0.71289. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69273/0.68877. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69241/0.68882. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69240/0.68887. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.68891. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69200/0.68894. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.68901. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.68907. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69136/0.68911. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69117/0.68917. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69101/0.68924. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69089/0.68932. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69060/0.68939. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69069/0.68949. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69011/0.68961. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68999/0.68974. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68940/0.68987. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68911/0.69002. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68873/0.69019. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.69036. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68802/0.69053. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68757/0.69074. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68693/0.69096. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68680/0.69120. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68626/0.69141. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68545/0.69167. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68493/0.69181. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68442/0.69199. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68386/0.69212. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68336/0.69212. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68262/0.69215. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68244/0.69231. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68131/0.69227. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68098/0.69218. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68027/0.69223. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67956/0.69220. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67877/0.69215. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67804/0.69212. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67755/0.69196. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67699/0.69184. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67600/0.69183. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67544/0.69153. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67512/0.69143. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67429/0.69121. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67360/0.69109. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67286/0.69097. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67176/0.69109. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67122/0.69083. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67035/0.69065. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66959/0.69048. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66899/0.69063. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66852/0.69063. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66772/0.69038. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66640/0.69061. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66628/0.69069. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66535/0.69064. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66460/0.69086. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66395/0.69096. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66350/0.69122. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66262/0.69097. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66138/0.69157. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66060/0.69183. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66020/0.69215. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65919/0.69262. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65880/0.69331. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65757/0.69422. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65694/0.69429. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65645/0.69483. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65528/0.69595. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65422/0.69664. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65294/0.69789. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65209/0.69887. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65220/0.69885. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65121/0.70015. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64999/0.70138. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64914/0.70258. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64804/0.70346. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64816/0.70415. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64667/0.70603. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64560/0.70722. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64514/0.70875. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64355/0.70928. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64245/0.71089. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64140/0.71292. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64144/0.71355. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63999/0.71468. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.63861/0.71603. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63785/0.71709. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63786/0.71904. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63629/0.72092. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63536/0.72210. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63418/0.72264. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63451/0.72474. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63262/0.72589. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63180/0.72770. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62991/0.72879. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62963/0.73116. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62839/0.73302. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.62828/0.73484. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62694/0.73702. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62443/0.73832. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70228/0.69623. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.70049/0.69509. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69858/0.69363. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69567/0.69206. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69304/0.69104. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69106/0.69080. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68976/0.69098. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68924/0.69122. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68869/0.69134. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68852/0.69134. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68810/0.69130. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68794/0.69119. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68793/0.69110. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68744/0.69097. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68745/0.69085. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68741/0.69074. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68701/0.69066. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68672/0.69057. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68606/0.69045. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68578/0.69030. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68622/0.69021. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68517/0.69009. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68521/0.68999. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68473/0.68991. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68446/0.68984. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68428/0.68982. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68378/0.68979. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68341/0.68972. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68293/0.68967. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68295/0.68964. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68238/0.68966. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68200/0.68959. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68139/0.68953. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68094/0.68959. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68085/0.68947. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68052/0.68934. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68026/0.68932. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67967/0.68931. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67915/0.68928. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67859/0.68923. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67837/0.68900. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67773/0.68881. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67706/0.68863. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67675/0.68840. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67591/0.68821. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67547/0.68785. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67500/0.68753. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67443/0.68712. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67392/0.68674. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67354/0.68639. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67230/0.68583. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67177/0.68531. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67167/0.68484. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67011/0.68413. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66957/0.68349. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66863/0.68287. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66831/0.68219. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66746/0.68136. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66647/0.68068. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66514/0.67975. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66427/0.67887. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66291/0.67812. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66210/0.67695. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66107/0.67601. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65986/0.67491. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65867/0.67389. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65703/0.67295. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65636/0.67173. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65454/0.67113. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65376/0.66983. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65252/0.66917. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64996/0.66853. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64925/0.66744. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64803/0.66706. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64704/0.66621. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64551/0.66564. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.64421/0.66535. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64263/0.66484. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64089/0.66496. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63999/0.66461. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63853/0.66449. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63738/0.66429. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63608/0.66431. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63414/0.66444. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63319/0.66452. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63153/0.66460. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63036/0.66490. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62813/0.66530. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62750/0.66606. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62625/0.66617. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62487/0.66673. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.62317/0.66727. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62217/0.66756. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61981/0.66783. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61907/0.66900. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61744/0.66927. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61549/0.66971. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61377/0.66988. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61258/0.67130. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61121/0.67155. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69252/0.69496. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68980/0.69649. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.68801/0.69851. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68687/0.70040. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68628/0.70169. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68626/0.70243. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68621/0.70281. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68560/0.70302. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68560/0.70310. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68552/0.70321. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68514/0.70331. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68521/0.70341. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68514/0.70348. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68459/0.70359. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68462/0.70367. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68413/0.70379. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68404/0.70391. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68417/0.70398. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68389/0.70407. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68335/0.70424. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68341/0.70437. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68329/0.70446. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68325/0.70454. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68282/0.70469. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68263/0.70495. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68227/0.70509. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68192/0.70524. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68136/0.70538. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68164/0.70559. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68122/0.70581. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68101/0.70597. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68064/0.70617. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68032/0.70633. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68020/0.70665. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67965/0.70679. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67951/0.70703. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67966/0.70725. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67896/0.70733. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67887/0.70754. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67832/0.70790. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67832/0.70815. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67791/0.70830. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67750/0.70846. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67707/0.70880. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67717/0.70896. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67689/0.70901. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67628/0.70923. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67590/0.70951. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67549/0.70978. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67511/0.70991. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67473/0.71019. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67457/0.71026. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67376/0.71047. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67347/0.71077. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67287/0.71097. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67253/0.71109. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67195/0.71142. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67140/0.71170. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67103/0.71195. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67029/0.71210. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66993/0.71233. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66913/0.71271. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66862/0.71306. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66765/0.71363. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66699/0.71401. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66655/0.71434. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66558/0.71488. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66440/0.71541. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66414/0.71591. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66278/0.71661. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66213/0.71720. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66119/0.71808. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66054/0.71904. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65884/0.71966. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65831/0.72089. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65681/0.72168. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65643/0.72260. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65505/0.72367. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65431/0.72505. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65320/0.72612. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65207/0.72728. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65098/0.72867. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64989/0.72987. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64904/0.73142. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64769/0.73264. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64720/0.73389. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64617/0.73520. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64556/0.73630. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64489/0.73776. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64389/0.73898. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64311/0.74003. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64197/0.74103. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64167/0.74219. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64058/0.74337. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63926/0.74458. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63872/0.74615. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63768/0.74708. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63706/0.74831. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63638/0.74927. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63520/0.75011. Took 0.10 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.68952/0.68556. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68907/0.68559. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.68867/0.68571. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68778/0.68588. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68797/0.68611. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68728/0.68637. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68740/0.68662. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68686/0.68692. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68663/0.68724. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68659/0.68753. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68603/0.68789. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68572/0.68826. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68554/0.68867. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68498/0.68908. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68519/0.68952. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68452/0.69001. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68415/0.69049. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68359/0.69102. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68350/0.69155. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68300/0.69211. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68268/0.69272. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68240/0.69331. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68161/0.69387. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68114/0.69447. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68107/0.69506. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68071/0.69559. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68008/0.69617. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67993/0.69668. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.67925/0.69719. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67867/0.69767. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67832/0.69815. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67805/0.69860. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67799/0.69900. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67785/0.69942. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67670/0.69980. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67620/0.70011. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67550/0.70043. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67520/0.70075. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67489/0.70100. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67423/0.70126. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67345/0.70152. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67332/0.70169. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67277/0.70186. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67273/0.70196. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67177/0.70200. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67121/0.70194. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67086/0.70197. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66956/0.70208. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66945/0.70219. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66885/0.70215. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66800/0.70228. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66764/0.70233. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66667/0.70253. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66581/0.70273. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66512/0.70294. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66444/0.70316. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66370/0.70339. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66369/0.70375. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66234/0.70410. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66147/0.70460. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66123/0.70481. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65980/0.70506. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65922/0.70550. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65852/0.70616. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65794/0.70652. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65622/0.70694. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65572/0.70732. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65514/0.70782. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65496/0.70826. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65365/0.70890. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65317/0.70953. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65120/0.71004. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65064/0.71062. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65000/0.71133. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64862/0.71194. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64850/0.71263. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64761/0.71305. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64602/0.71402. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64540/0.71459. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64404/0.71551. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64366/0.71606. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64207/0.71631. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64108/0.71705. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63998/0.71778. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63885/0.71883. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63817/0.71920. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63785/0.71942. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63573/0.72072. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63554/0.72140. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63490/0.72184. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63371/0.72225. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63177/0.72321. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63197/0.72390. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62991/0.72475. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62938/0.72582. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62759/0.72593. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62778/0.72664. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62649/0.72774. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62438/0.72830. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62361/0.72891. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69085/0.69725. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69075/0.69718. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69059/0.69711. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69026/0.69705. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69033/0.69699. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69017/0.69691. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68989/0.69683. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69005/0.69675. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68978/0.69669. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68965/0.69660. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68961/0.69653. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68900/0.69644. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68896/0.69634. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68894/0.69622. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68863/0.69611. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68840/0.69598. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68832/0.69582. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68815/0.69563. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68805/0.69542. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68773/0.69522. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68749/0.69498. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68709/0.69473. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68685/0.69445. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68656/0.69419. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68606/0.69391. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68558/0.69364. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68541/0.69340. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68504/0.69318. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68490/0.69297. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68421/0.69275. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68387/0.69256. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68363/0.69237. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68310/0.69222. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68244/0.69203. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68210/0.69193. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68192/0.69186. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68169/0.69177. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68069/0.69175. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68051/0.69176. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68002/0.69177. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67981/0.69187. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67884/0.69191. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67859/0.69205. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67850/0.69216. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67788/0.69222. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67721/0.69240. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67689/0.69257. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67654/0.69270. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67604/0.69281. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67557/0.69304. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67490/0.69339. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67461/0.69359. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67376/0.69388. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67313/0.69419. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67265/0.69448. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67240/0.69491. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67142/0.69513. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67122/0.69558. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67056/0.69585. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66937/0.69627. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66907/0.69676. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66841/0.69711. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66794/0.69767. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66721/0.69817. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66717/0.69861. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66618/0.69915. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66621/0.69962. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66480/0.70028. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66379/0.70057. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66357/0.70120. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66312/0.70150. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66210/0.70227. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66157/0.70286. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66098/0.70347. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65968/0.70421. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65930/0.70490. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65915/0.70553. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65808/0.70607. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65751/0.70682. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65626/0.70744. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65569/0.70796. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65561/0.70848. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65390/0.70954. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65297/0.71019. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65235/0.71103. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65160/0.71189. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65059/0.71231. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65000/0.71327. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64924/0.71434. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64911/0.71505. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64770/0.71573. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64711/0.71658. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64611/0.71773. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64505/0.71865. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64436/0.71931. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64363/0.72026. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64282/0.72059. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64166/0.72175. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64141/0.72264. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63987/0.72318. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70323/0.69114. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69934/0.69068. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69607/0.69099. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69307/0.69228. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69118/0.69405. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.69550. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68971/0.69631. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68950/0.69681. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68903/0.69703. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68876/0.69705. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68839/0.69704. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68820/0.69698. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68816/0.69693. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68773/0.69683. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68755/0.69678. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68762/0.69672. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68731/0.69666. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.69666. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68661/0.69660. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68646/0.69651. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68633/0.69639. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.69634. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68587/0.69627. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68560/0.69624. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68530/0.69617. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68522/0.69619. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68499/0.69621. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68470/0.69614. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68417/0.69616. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68418/0.69616. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68382/0.69614. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68340/0.69625. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68332/0.69637. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68279/0.69649. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68266/0.69663. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68259/0.69670. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68196/0.69679. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68179/0.69694. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.69712. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68162/0.69743. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68139/0.69756. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68074/0.69769. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68078/0.69788. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68013/0.69814. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68003/0.69829. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67967/0.69852. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67960/0.69873. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67916/0.69895. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67903/0.69918. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67876/0.69933. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67863/0.69953. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67796/0.69993. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67804/0.70014. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67794/0.70033. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67752/0.70062. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67712/0.70084. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67693/0.70093. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67663/0.70128. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67610/0.70151. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67626/0.70165. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67571/0.70201. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67572/0.70216. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67540/0.70243. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67517/0.70252. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67469/0.70273. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67450/0.70314. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67391/0.70325. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67350/0.70364. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67322/0.70388. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67299/0.70395. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67215/0.70423. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67222/0.70452. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67207/0.70496. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67149/0.70516. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67179/0.70531. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67092/0.70574. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66999/0.70591. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67015/0.70629. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66980/0.70676. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66923/0.70695. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66908/0.70760. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66855/0.70798. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66812/0.70818. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66728/0.70842. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66702/0.70905. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66664/0.70922. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66568/0.70963. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66558/0.71049. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66489/0.71095. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66437/0.71165. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66388/0.71197. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66298/0.71252. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66234/0.71335. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66170/0.71384. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66172/0.71450. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66100/0.71508. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66030/0.71572. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65970/0.71612. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65877/0.71681. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65817/0.71773. Took 0.09 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69163/0.68478. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69147/0.68471. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69121/0.68463. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69094/0.68454. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69064/0.68445. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69061/0.68438. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.68431. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68995/0.68424. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.68419. Took 0.11 sec\n",
      "Epoch 9, Loss(train/val) 0.68929/0.68417. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68889/0.68417. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68868/0.68420. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68836/0.68426. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68809/0.68434. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68742/0.68448. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68728/0.68465. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68675/0.68485. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68634/0.68506. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68540/0.68535. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68533/0.68570. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68472/0.68603. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68419/0.68642. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.68683. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68313/0.68719. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68258/0.68754. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68219/0.68790. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68146/0.68827. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68096/0.68854. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68036/0.68880. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67998/0.68907. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67940/0.68929. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67888/0.68956. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67865/0.68984. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67829/0.69002. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67782/0.69018. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67726/0.69037. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67680/0.69049. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67622/0.69077. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67549/0.69096. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67514/0.69111. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67494/0.69115. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67440/0.69138. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67384/0.69165. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67354/0.69177. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67298/0.69195. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67247/0.69216. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67173/0.69243. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67158/0.69260. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67071/0.69276. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67045/0.69289. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66975/0.69308. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66879/0.69335. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66840/0.69354. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66777/0.69374. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66703/0.69403. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66670/0.69426. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66600/0.69437. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66531/0.69455. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66504/0.69479. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66400/0.69497. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66312/0.69531. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66282/0.69549. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66189/0.69575. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66108/0.69592. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66034/0.69615. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65970/0.69628. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65912/0.69629. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65817/0.69654. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65783/0.69666. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65670/0.69683. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65570/0.69703. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65502/0.69733. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65400/0.69774. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65289/0.69801. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65243/0.69825. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65201/0.69827. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65097/0.69853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64970/0.69889. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64885/0.69894. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64772/0.69919. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64677/0.69940. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64647/0.69942. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64464/0.69978. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64393/0.70031. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64286/0.70078. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64143/0.70072. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64063/0.70121. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63964/0.70123. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63791/0.70156. Took 0.08 sec\n",
      "Epoch 89, Loss(train/val) 0.63612/0.70186. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63513/0.70303. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63381/0.70272. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63236/0.70393. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.63090/0.70500. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.62920/0.70593. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62775/0.70693. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62633/0.70752. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62510/0.70895. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62299/0.70970. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62173/0.71047. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69532/0.69560. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69497/0.69550. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69481/0.69539. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69469/0.69523. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69429/0.69499. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69376/0.69466. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69315/0.69424. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69261/0.69374. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69179/0.69326. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69080/0.69291. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69018/0.69273. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68987/0.69269. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68990/0.69269. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68950/0.69272. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68928/0.69276. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68906/0.69280. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68922/0.69283. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68893/0.69284. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68862/0.69283. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68831/0.69282. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68801/0.69280. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68774/0.69277. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68767/0.69271. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68730/0.69261. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68713/0.69254. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68637/0.69242. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68624/0.69232. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68607/0.69215. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68545/0.69198. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68490/0.69177. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68466/0.69155. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68400/0.69129. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68359/0.69102. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68279/0.69071. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68243/0.69041. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68173/0.69015. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68122/0.68997. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68065/0.68964. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68012/0.68937. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67967/0.68918. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67921/0.68897. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67865/0.68886. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67776/0.68860. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67737/0.68831. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67666/0.68805. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67623/0.68793. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67542/0.68779. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67502/0.68764. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67407/0.68746. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67377/0.68728. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67320/0.68725. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67291/0.68705. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67203/0.68694. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67099/0.68701. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67041/0.68706. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66952/0.68715. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66893/0.68720. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66807/0.68738. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66775/0.68746. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66704/0.68750. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66595/0.68753. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66529/0.68769. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66424/0.68775. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66382/0.68799. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66267/0.68817. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66171/0.68832. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66110/0.68849. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66010/0.68860. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65935/0.68896. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65782/0.68908. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65701/0.68940. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65631/0.68968. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65569/0.68969. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65482/0.69007. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65290/0.69002. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65268/0.69063. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65142/0.69033. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65023/0.69086. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64917/0.69093. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64733/0.69137. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64636/0.69213. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64536/0.69269. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64413/0.69267. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64279/0.69341. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64199/0.69378. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64050/0.69413. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63866/0.69474. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63818/0.69615. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63628/0.69686. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63551/0.69708. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63336/0.69831. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63303/0.69875. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63132/0.70023. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63074/0.70091. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62846/0.70207. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62841/0.70269. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62647/0.70423. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62554/0.70525. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62420/0.70640. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62231/0.70720. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69313/0.68792. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69334/0.68795. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69321/0.68799. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69286/0.68807. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69264/0.68816. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69219/0.68829. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69229/0.68846. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69200/0.68869. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69147/0.68894. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69124/0.68924. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69144/0.68958. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69085/0.68990. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69084/0.69025. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69062/0.69055. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69039/0.69081. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69025/0.69110. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69020/0.69130. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68965/0.69156. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68933/0.69174. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68892/0.69190. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68857/0.69209. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68824/0.69234. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68794/0.69262. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68737/0.69277. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68714/0.69301. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68677/0.69322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68636/0.69362. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68537/0.69406. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68503/0.69441. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68457/0.69494. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68404/0.69556. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68380/0.69624. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68299/0.69666. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68225/0.69738. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68185/0.69773. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68130/0.69864. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68141/0.69938. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68020/0.70006. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68006/0.70083. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67978/0.70132. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67903/0.70199. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67836/0.70249. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67815/0.70328. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67761/0.70408. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67708/0.70438. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67639/0.70530. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67600/0.70552. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67592/0.70623. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67547/0.70673. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67472/0.70721. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67398/0.70790. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67304/0.70834. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67286/0.70933. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67249/0.70962. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67179/0.71057. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67108/0.71085. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67059/0.71150. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67014/0.71197. Took 0.08 sec\n",
      "Epoch 58, Loss(train/val) 0.66897/0.71276. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66879/0.71352. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66846/0.71399. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66745/0.71459. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66721/0.71559. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66616/0.71560. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66601/0.71604. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66526/0.71703. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66476/0.71736. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66391/0.71782. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66321/0.71837. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66265/0.71854. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66133/0.71945. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.66106/0.72003. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66038/0.72106. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66002/0.72104. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65917/0.72170. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65823/0.72272. Took 0.08 sec\n",
      "Epoch 76, Loss(train/val) 0.65727/0.72340. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65630/0.72362. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65587/0.72499. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65515/0.72480. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65390/0.72570. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65311/0.72688. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65254/0.72698. Took 0.08 sec\n",
      "Epoch 83, Loss(train/val) 0.65174/0.72855. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65148/0.72912. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65026/0.72958. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64985/0.73065. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64966/0.73117. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64747/0.73184. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64707/0.73285. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64517/0.73327. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64571/0.73467. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64388/0.73623. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64325/0.73675. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.64238/0.73764. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64088/0.73853. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64147/0.74036. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63986/0.74119. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63995/0.74226. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63687/0.74296. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69647/0.68870. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69573/0.68919. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69512/0.68981. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69447/0.69053. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69424/0.69138. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69386/0.69228. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69345/0.69314. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69318/0.69389. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69293/0.69445. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69286/0.69494. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69280/0.69532. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69250/0.69562. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69228/0.69582. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69222/0.69599. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69210/0.69609. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69194/0.69613. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69179/0.69623. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69162/0.69628. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69135/0.69634. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69136/0.69639. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69109/0.69639. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.69112/0.69647. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69065/0.69655. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69043/0.69653. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.69051/0.69652. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69024/0.69655. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68988/0.69661. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68989/0.69664. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68933/0.69660. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68909/0.69650. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68880/0.69652. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68863/0.69652. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68805/0.69645. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68778/0.69634. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68752/0.69618. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68714/0.69616. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68649/0.69596. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68606/0.69587. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68590/0.69563. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68528/0.69548. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68518/0.69541. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68473/0.69503. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68415/0.69479. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68375/0.69461. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68369/0.69427. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68331/0.69414. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68294/0.69386. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68246/0.69360. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68190/0.69335. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68148/0.69316. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.68151/0.69290. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68089/0.69264. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68057/0.69218. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.67970/0.69189. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67908/0.69203. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67899/0.69157. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67843/0.69127. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67811/0.69122. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67680/0.69122. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67691/0.69072. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67623/0.69047. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67543/0.69055. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67519/0.69010. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67432/0.69010. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67384/0.68979. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67266/0.68955. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67248/0.68923. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67158/0.68947. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67169/0.68906. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67138/0.68921. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66962/0.68903. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66937/0.68893. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66907/0.68868. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66796/0.68841. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66752/0.68887. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66649/0.68864. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66619/0.68853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66489/0.68887. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66426/0.68839. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66348/0.68842. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66290/0.68852. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66186/0.68792. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66084/0.68866. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66013/0.68855. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65973/0.68892. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65809/0.68886. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65807/0.68901. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65667/0.68878. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65532/0.68968. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65493/0.68921. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65381/0.68965. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65402/0.68914. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65230/0.69029. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65162/0.68961. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65026/0.68983. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65017/0.68950. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64973/0.69021. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64775/0.69051. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64711/0.68995. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64647/0.69068. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69203/0.69595. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69165/0.69587. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69141/0.69581. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69104/0.69578. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69069/0.69580. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69068/0.69580. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.69583. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68951/0.69585. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68981/0.69588. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68928/0.69591. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68881/0.69592. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68865/0.69595. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68845/0.69595. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68793/0.69595. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68763/0.69596. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68671/0.69598. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68700/0.69598. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68635/0.69599. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68616/0.69598. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68552/0.69600. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.69599. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68468/0.69601. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68398/0.69603. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68378/0.69605. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68307/0.69615. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68205/0.69624. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68179/0.69635. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68081/0.69647. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68071/0.69670. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67963/0.69692. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67917/0.69716. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67870/0.69745. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67826/0.69776. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67735/0.69805. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67655/0.69839. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67611/0.69878. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67557/0.69916. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67513/0.69958. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67411/0.70003. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67312/0.70056. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67263/0.70115. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67173/0.70163. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67098/0.70217. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67030/0.70285. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66994/0.70355. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66834/0.70423. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66819/0.70489. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66704/0.70562. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66638/0.70635. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66540/0.70718. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66437/0.70802. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66315/0.70886. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66261/0.70967. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66190/0.71058. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66049/0.71149. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65912/0.71254. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65841/0.71350. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65727/0.71450. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65608/0.71548. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65558/0.71652. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65355/0.71766. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65364/0.71866. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65191/0.71978. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65120/0.72081. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64948/0.72197. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64920/0.72301. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64854/0.72412. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64724/0.72541. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64514/0.72630. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64413/0.72771. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64372/0.72899. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64208/0.73042. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64193/0.73197. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64026/0.73298. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63792/0.73464. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63798/0.73592. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63591/0.73718. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63616/0.73879. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63522/0.74017. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63441/0.74171. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63333/0.74275. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63222/0.74430. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63065/0.74576. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63018/0.74696. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62927/0.74831. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62703/0.74981. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62789/0.75127. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62413/0.75250. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62552/0.75371. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62419/0.75508. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62277/0.75640. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62224/0.75739. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62200/0.75931. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61934/0.76010. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61968/0.76205. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61759/0.76277. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61656/0.76449. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61598/0.76629. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61469/0.76724. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61416/0.76900. Took 0.11 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69610. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69422/0.69602. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69360/0.69596. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69325/0.69593. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69322/0.69590. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69300/0.69589. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69232/0.69591. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69209/0.69596. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69194/0.69601. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69131/0.69612. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69106/0.69623. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69045/0.69637. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69029/0.69650. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68996/0.69669. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68931/0.69688. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68892/0.69707. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68844/0.69726. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68769/0.69748. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68759/0.69773. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68696/0.69796. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68654/0.69818. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68594/0.69847. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68538/0.69875. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68509/0.69902. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68437/0.69931. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68365/0.69963. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68292/0.69992. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68280/0.70022. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68226/0.70062. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68148/0.70101. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68048/0.70144. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67988/0.70187. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67931/0.70233. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67839/0.70283. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67756/0.70333. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67717/0.70391. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67568/0.70462. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67508/0.70530. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67456/0.70595. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67332/0.70685. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67222/0.70801. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67100/0.70907. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66990/0.71030. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66914/0.71154. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66764/0.71295. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66661/0.71413. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66563/0.71587. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66432/0.71727. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66328/0.71921. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66214/0.72099. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66127/0.72242. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65995/0.72433. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65901/0.72569. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65826/0.72743. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65605/0.72906. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65584/0.73054. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65515/0.73268. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65377/0.73377. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65298/0.73565. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.65164/0.73726. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65060/0.73865. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64945/0.74050. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64855/0.74188. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64734/0.74398. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64662/0.74511. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64635/0.74690. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64488/0.74869. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64341/0.75030. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64179/0.75192. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64191/0.75309. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64060/0.75507. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63911/0.75687. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63934/0.75826. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63721/0.76089. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63635/0.76154. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63519/0.76376. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63423/0.76586. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63354/0.76659. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63154/0.76898. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63182/0.77059. Took 0.08 sec\n",
      "Epoch 80, Loss(train/val) 0.63049/0.77228. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62891/0.77420. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62765/0.77530. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62666/0.77745. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62553/0.77950. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62524/0.78057. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62364/0.78191. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62237/0.78401. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62087/0.78534. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62028/0.78781. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61882/0.78894. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61775/0.79135. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61567/0.79235. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61588/0.79418. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61409/0.79565. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61339/0.79729. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61191/0.79829. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61106/0.80039. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60972/0.80272. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.60866/0.80386. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69373/0.69281. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69326/0.69258. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69298/0.69236. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69272/0.69211. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69270/0.69186. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69240/0.69160. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69181/0.69133. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69161/0.69105. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69154/0.69085. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69121/0.69066. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69079/0.69049. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69080/0.69038. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69031/0.69031. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69044/0.69029. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69009/0.69032. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69004/0.69036. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68961/0.69045. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68938/0.69055. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68882/0.69065. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68830/0.69080. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68812/0.69098. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68791/0.69116. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68734/0.69140. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68714/0.69168. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68670/0.69194. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68629/0.69229. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68572/0.69271. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68554/0.69317. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68457/0.69369. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68373/0.69423. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68323/0.69493. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68265/0.69563. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68195/0.69639. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68120/0.69719. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68067/0.69810. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67935/0.69894. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67881/0.69985. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67837/0.70072. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67676/0.70172. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67609/0.70260. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67488/0.70354. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67403/0.70451. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67342/0.70529. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67259/0.70612. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67174/0.70682. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67080/0.70756. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66941/0.70843. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66883/0.70911. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66805/0.70975. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66637/0.71043. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66539/0.71099. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66473/0.71181. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66292/0.71239. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66221/0.71292. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66085/0.71342. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65908/0.71401. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65822/0.71451. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65684/0.71514. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65582/0.71553. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65402/0.71618. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65370/0.71677. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65172/0.71709. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65088/0.71775. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64996/0.71811. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64804/0.71869. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64717/0.71897. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64540/0.71958. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64404/0.72010. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64311/0.72049. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64111/0.72101. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63927/0.72188. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63968/0.72231. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63818/0.72304. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63638/0.72331. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63604/0.72355. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63385/0.72434. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63319/0.72456. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63264/0.72481. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62956/0.72598. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62913/0.72653. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62772/0.72727. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62602/0.72843. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62710/0.72876. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62456/0.72947. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62299/0.73068. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62197/0.73151. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62241/0.73149. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61880/0.73241. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61865/0.73304. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61737/0.73410. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61731/0.73496. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61499/0.73529. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61313/0.73677. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61227/0.73735. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61258/0.73834. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61027/0.73920. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60844/0.74012. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60714/0.74106. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60510/0.74175. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.60536/0.74287. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69423/0.69293. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69412/0.69263. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69367/0.69231. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69345/0.69198. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69278/0.69166. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69227/0.69139. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69220/0.69118. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69164/0.69103. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69165/0.69097. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69134/0.69094. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69138/0.69096. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69119/0.69098. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69110/0.69103. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69109/0.69110. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69091/0.69117. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.69083/0.69125. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69067/0.69131. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69085/0.69138. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69060/0.69146. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.69025/0.69154. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69056/0.69161. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69041/0.69169. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69011/0.69178. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68992/0.69188. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68992/0.69198. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68975/0.69210. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68974/0.69223. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68968/0.69235. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68955/0.69247. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68921/0.69262. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68914/0.69277. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68897/0.69292. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68879/0.69308. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68883/0.69326. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68854/0.69343. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68815/0.69363. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68818/0.69380. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68808/0.69401. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68805/0.69422. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68780/0.69446. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68707/0.69471. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68696/0.69497. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68713/0.69525. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68652/0.69553. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.68643/0.69580. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68638/0.69611. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68581/0.69642. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68584/0.69674. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68566/0.69708. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68529/0.69743. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68509/0.69778. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68478/0.69812. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68451/0.69847. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68419/0.69886. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68367/0.69924. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68367/0.69963. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68329/0.69999. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68270/0.70038. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68232/0.70078. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68268/0.70120. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68178/0.70163. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68135/0.70209. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68105/0.70257. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68087/0.70304. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.68029/0.70346. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68007/0.70390. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67949/0.70440. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67880/0.70489. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67876/0.70542. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67803/0.70593. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67776/0.70643. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67702/0.70693. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67618/0.70748. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67649/0.70811. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67534/0.70872. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67482/0.70930. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67404/0.70995. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67344/0.71068. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67206/0.71143. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67111/0.71227. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67058/0.71318. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67006/0.71409. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66890/0.71505. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66775/0.71607. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66715/0.71704. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66624/0.71811. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66569/0.71907. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66458/0.72020. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.66320/0.72141. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66161/0.72287. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66010/0.72418. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65990/0.72534. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65812/0.72687. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65711/0.72834. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65634/0.72983. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65498/0.73133. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65315/0.73261. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65176/0.73460. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65163/0.73616. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65020/0.73773. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69339/0.69275. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.69279. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69320/0.69282. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69264/0.69283. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69230/0.69284. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69286. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69201/0.69290. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69174/0.69293. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69140/0.69298. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69083/0.69303. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69072/0.69311. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69046/0.69321. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68995/0.69331. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68988/0.69342. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68966/0.69352. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68972/0.69360. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68909/0.69366. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68874/0.69371. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68809/0.69373. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68810/0.69374. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68771/0.69367. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68732/0.69361. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68681/0.69354. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68649/0.69343. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68612/0.69337. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68563/0.69342. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68503/0.69327. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68426/0.69308. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68368/0.69300. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68299/0.69300. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68218/0.69286. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68141/0.69284. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68043/0.69287. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68014/0.69287. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67892/0.69300. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67780/0.69316. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67730/0.69332. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67593/0.69365. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67456/0.69397. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67368/0.69443. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67274/0.69503. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67096/0.69561. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67024/0.69621. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66852/0.69701. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66697/0.69783. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66570/0.69878. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66502/0.69989. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66305/0.70109. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66139/0.70254. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66046/0.70402. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65843/0.70538. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65722/0.70689. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65556/0.70850. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65404/0.71019. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65271/0.71175. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.65174/0.71374. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64986/0.71534. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64873/0.71715. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64794/0.71882. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64599/0.72060. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64381/0.72244. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64279/0.72456. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64152/0.72608. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63937/0.72780. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63874/0.72988. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63653/0.73148. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63588/0.73377. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63376/0.73544. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63260/0.73740. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63057/0.73940. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62970/0.74098. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62881/0.74312. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62781/0.74463. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62569/0.74679. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62435/0.74829. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62291/0.75046. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62234/0.75223. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61990/0.75446. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61985/0.75676. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61681/0.75883. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61600/0.76080. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61378/0.76352. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61332/0.76576. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61187/0.76840. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60960/0.77061. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.60879/0.77373. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60644/0.77599. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60685/0.77895. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60477/0.78055. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60278/0.78361. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60259/0.78639. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.59997/0.78961. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59955/0.79257. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59756/0.79519. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59642/0.79873. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59362/0.80158. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59271/0.80447. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59169/0.80673. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58973/0.81072. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58811/0.81351. Took 0.10 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69331/0.68154. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69190/0.68365. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69147/0.68553. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69062/0.68722. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69000/0.68861. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.68955. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68985/0.69036. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68991/0.69090. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68962/0.69124. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.69154. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.68927/0.69170. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68904/0.69188. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68859/0.69197. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68840/0.69200. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68807/0.69203. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68784/0.69205. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68723/0.69209. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68753/0.69206. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68702/0.69193. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68671/0.69195. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68619/0.69207. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68571/0.69215. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68544/0.69232. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68501/0.69233. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68472/0.69248. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68419/0.69261. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68389/0.69287. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68337/0.69311. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68276/0.69334. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68199/0.69379. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68180/0.69428. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68136/0.69475. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68065/0.69506. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67972/0.69557. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67930/0.69627. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67894/0.69664. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67809/0.69727. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67753/0.69762. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67708/0.69817. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67639/0.69878. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67582/0.69932. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67503/0.69988. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67482/0.70054. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67427/0.70120. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67364/0.70141. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67273/0.70175. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67268/0.70244. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67153/0.70297. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67113/0.70344. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66996/0.70413. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66924/0.70446. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66841/0.70496. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66799/0.70545. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66699/0.70593. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66564/0.70660. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66538/0.70717. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66431/0.70774. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66305/0.70861. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66250/0.70866. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66187/0.70892. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66012/0.70943. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65939/0.70937. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65893/0.71102. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65818/0.71093. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65734/0.71115. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65559/0.71172. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65479/0.71212. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65356/0.71250. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65323/0.71255. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65217/0.71333. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65123/0.71335. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64945/0.71319. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64932/0.71462. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64818/0.71471. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64710/0.71462. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64594/0.71574. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64527/0.71589. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64535/0.71584. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64439/0.71633. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64318/0.71614. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64203/0.71710. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64099/0.71784. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63975/0.71747. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63988/0.71756. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63900/0.71828. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63781/0.71812. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63674/0.71870. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63687/0.71800. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63530/0.71822. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63498/0.71914. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63390/0.71891. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63261/0.71923. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63172/0.71972. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63121/0.71974. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63000/0.71983. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62922/0.71899. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62858/0.71977. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62856/0.72038. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62782/0.71932. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62627/0.71929. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69832/0.69109. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69698/0.69090. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69567/0.69089. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69455/0.69106. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69378/0.69137. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69308/0.69176. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69284/0.69216. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69236/0.69254. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69236/0.69286. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.69313. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69175/0.69335. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69151/0.69352. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69154/0.69366. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69157/0.69378. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69122/0.69390. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69101/0.69399. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69113/0.69405. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69086/0.69413. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69028/0.69422. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69018/0.69428. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69031/0.69436. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69022/0.69443. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69010/0.69448. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68972/0.69455. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68938/0.69460. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68943/0.69466. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68910/0.69473. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68894/0.69478. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68883/0.69484. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68852/0.69489. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68869/0.69493. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68833/0.69498. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68815/0.69497. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68790/0.69501. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68769/0.69502. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68749/0.69501. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68757/0.69501. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68755/0.69501. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68718/0.69504. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68671/0.69502. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68654/0.69498. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68643/0.69496. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68618/0.69495. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68588/0.69495. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68597/0.69489. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68559/0.69486. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68557/0.69481. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68523/0.69474. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68544/0.69469. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68507/0.69464. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68470/0.69458. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68482/0.69449. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68420/0.69439. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68414/0.69432. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68410/0.69427. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68355/0.69420. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68366/0.69412. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68324/0.69402. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68321/0.69393. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68302/0.69386. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68263/0.69375. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68245/0.69363. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68211/0.69358. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68206/0.69349. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68192/0.69336. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68124/0.69334. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68140/0.69322. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.68096/0.69314. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68039/0.69301. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68040/0.69292. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68015/0.69278. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67944/0.69268. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67963/0.69258. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67900/0.69244. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67893/0.69228. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67827/0.69219. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67799/0.69211. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67771/0.69201. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67723/0.69184. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67714/0.69180. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67607/0.69166. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67614/0.69152. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67605/0.69137. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67549/0.69123. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67473/0.69118. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67461/0.69108. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67371/0.69105. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67327/0.69086. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.67330/0.69072. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67274/0.69058. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67204/0.69056. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.67201/0.69063. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67133/0.69055. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.67088/0.69049. Took 0.12 sec\n",
      "Epoch 94, Loss(train/val) 0.67062/0.69063. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66979/0.69061. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66936/0.69051. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66828/0.69048. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66774/0.69059. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66748/0.69067. Took 0.08 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69562/0.68875. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69530/0.68881. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69466/0.68885. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69446/0.68888. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69346/0.68890. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69309/0.68890. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69253/0.68890. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69189/0.68887. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69115/0.68884. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69077/0.68878. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69028/0.68871. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68960/0.68861. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68905/0.68852. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68819/0.68842. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68773/0.68832. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68731/0.68825. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68670/0.68815. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68579/0.68810. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68516/0.68808. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68458/0.68805. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68326/0.68808. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68294/0.68815. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68228/0.68827. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68146/0.68841. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68083/0.68852. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67997/0.68873. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67939/0.68897. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67872/0.68935. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67770/0.68974. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67728/0.69006. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67624/0.69061. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67610/0.69124. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67546/0.69182. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67484/0.69232. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67395/0.69281. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67363/0.69346. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67305/0.69417. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67274/0.69482. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67219/0.69547. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67114/0.69608. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67044/0.69683. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67030/0.69750. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66973/0.69840. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66923/0.69922. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66829/0.69986. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66768/0.70073. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66745/0.70113. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66725/0.70199. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66570/0.70268. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66551/0.70340. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66466/0.70413. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66489/0.70494. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66338/0.70543. Took 0.08 sec\n",
      "Epoch 53, Loss(train/val) 0.66256/0.70597. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66183/0.70691. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66201/0.70758. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66074/0.70827. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66005/0.70873. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65920/0.70967. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65874/0.71018. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65727/0.71094. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65731/0.71162. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65616/0.71182. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65538/0.71284. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65487/0.71334. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65442/0.71395. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65244/0.71480. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65302/0.71553. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65065/0.71588. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65075/0.71693. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64934/0.71744. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64890/0.71837. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64803/0.71917. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64635/0.71963. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64621/0.72048. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64462/0.72145. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64378/0.72191. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64265/0.72328. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64184/0.72410. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64114/0.72442. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63924/0.72552. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63980/0.72674. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63806/0.72727. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63787/0.72838. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63620/0.72892. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63483/0.73044. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63386/0.73039. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63402/0.73209. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63266/0.73250. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63181/0.73346. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63039/0.73480. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62962/0.73583. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62703/0.73667. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62760/0.73740. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62479/0.73844. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62597/0.73896. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62410/0.74026. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62319/0.74144. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62216/0.74139. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62167/0.74227. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69564/0.69280. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69549/0.69216. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69531/0.69161. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69478/0.69116. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69500/0.69075. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69455/0.69041. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69458/0.69008. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69438/0.68979. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69410/0.68951. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69417/0.68929. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69384/0.68905. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69373/0.68883. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69358/0.68861. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69339/0.68843. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69320/0.68825. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69300/0.68814. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69279/0.68796. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69286/0.68783. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69266/0.68770. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69243/0.68757. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69237/0.68738. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69214/0.68720. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69181/0.68705. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69173/0.68687. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69150/0.68669. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69137/0.68652. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69121/0.68634. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69060/0.68611. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69067/0.68594. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69034/0.68570. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.69001/0.68547. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68958/0.68521. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68946/0.68489. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68924/0.68463. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68889/0.68428. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68832/0.68403. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68813/0.68368. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68756/0.68319. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68739/0.68284. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68709/0.68268. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68658/0.68241. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68620/0.68199. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68587/0.68158. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68537/0.68127. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68493/0.68109. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68416/0.68079. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68407/0.68063. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68382/0.68026. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68308/0.67982. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68254/0.67955. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68200/0.67929. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68153/0.67902. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68062/0.67853. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68056/0.67837. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67991/0.67822. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67937/0.67777. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67887/0.67778. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67810/0.67745. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67790/0.67716. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67734/0.67717. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67686/0.67695. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67581/0.67676. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67536/0.67654. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67482/0.67637. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67440/0.67619. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67356/0.67621. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67372/0.67591. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67274/0.67596. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67214/0.67578. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67128/0.67593. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67088/0.67582. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66982/0.67576. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66978/0.67559. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66883/0.67566. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66812/0.67575. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66764/0.67554. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66701/0.67568. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66601/0.67569. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66580/0.67589. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66541/0.67613. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66499/0.67608. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66432/0.67635. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66300/0.67633. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66298/0.67661. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66205/0.67706. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66105/0.67727. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66095/0.67754. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66079/0.67763. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65957/0.67764. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65896/0.67780. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65848/0.67799. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65783/0.67818. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65753/0.67869. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65646/0.67904. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65560/0.67949. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65516/0.67941. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65435/0.68010. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65411/0.68043. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65217/0.68088. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65229/0.68161. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69357/0.69173. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69348/0.69179. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69289/0.69185. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.69190. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69271/0.69196. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69244/0.69203. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.69210. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69196/0.69217. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69180/0.69226. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69111/0.69236. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.69247. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69065/0.69260. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69001/0.69275. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68965/0.69291. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68932/0.69311. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68888/0.69334. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68824/0.69362. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.69394. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68727/0.69430. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68679/0.69472. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68623/0.69521. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68557/0.69578. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68477/0.69644. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68397/0.69715. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68303/0.69795. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68261/0.69885. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68168/0.69983. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68069/0.70086. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68034/0.70201. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67944/0.70326. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67866/0.70450. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67762/0.70582. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67728/0.70708. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67610/0.70834. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67563/0.70962. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67498/0.71090. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67391/0.71208. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67337/0.71321. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67279/0.71431. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67202/0.71529. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67123/0.71622. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67044/0.71706. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67003/0.71787. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66963/0.71853. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66836/0.71917. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66827/0.71980. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66706/0.72035. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66618/0.72088. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66619/0.72133. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66476/0.72171. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66502/0.72207. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66393/0.72244. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66299/0.72280. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66275/0.72313. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66162/0.72354. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66083/0.72389. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65941/0.72410. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65834/0.72447. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65852/0.72480. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65755/0.72518. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65659/0.72563. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65545/0.72600. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65422/0.72643. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65377/0.72679. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65277/0.72731. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65204/0.72767. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65051/0.72817. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65070/0.72858. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64874/0.72919. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64776/0.72990. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64693/0.73034. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64560/0.73125. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64499/0.73215. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64308/0.73280. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64238/0.73379. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64081/0.73478. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63980/0.73579. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63857/0.73625. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63796/0.73760. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63601/0.73844. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63484/0.73953. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.63309/0.74037. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63252/0.74148. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63000/0.74273. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62854/0.74363. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62783/0.74557. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62645/0.74696. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62442/0.74796. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62325/0.74889. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62270/0.75106. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62087/0.75271. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61959/0.75375. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61812/0.75465. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61690/0.75693. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61556/0.75824. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61376/0.75922. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61312/0.76066. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61149/0.76204. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60936/0.76353. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60946/0.76443. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69798/0.70144. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69700/0.70052. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69617/0.69946. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69508/0.69802. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69381/0.69608. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69239/0.69404. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69125/0.69242. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69029/0.69127. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68971/0.69054. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68934/0.69012. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68964/0.68986. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68873/0.68973. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68856/0.68972. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68861/0.68973. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68807/0.68974. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68805/0.68975. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68773/0.68973. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68764/0.68978. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68753/0.68986. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68690/0.68991. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68666/0.68999. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68655/0.69007. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68631/0.69013. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.69018. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68576/0.69029. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68553/0.69039. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68526/0.69052. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68481/0.69068. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68433/0.69085. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68421/0.69106. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68375/0.69125. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68357/0.69142. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68297/0.69162. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68293/0.69184. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68204/0.69212. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68171/0.69236. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68137/0.69263. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68100/0.69294. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68014/0.69329. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67986/0.69354. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67948/0.69390. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67933/0.69428. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67890/0.69465. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67769/0.69511. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67780/0.69550. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67735/0.69585. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67663/0.69629. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67618/0.69675. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67584/0.69711. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67516/0.69765. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67454/0.69821. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67379/0.69855. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67336/0.69899. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67267/0.69931. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67174/0.69979. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67152/0.70016. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67043/0.70053. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66985/0.70097. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66943/0.70136. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66877/0.70192. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66759/0.70236. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66654/0.70280. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66651/0.70319. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66519/0.70352. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66456/0.70398. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66425/0.70437. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66264/0.70472. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66207/0.70524. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66104/0.70574. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66004/0.70633. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65901/0.70674. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65804/0.70721. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65679/0.70770. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65682/0.70824. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65570/0.70854. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65475/0.70905. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65390/0.70963. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65275/0.71007. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65145/0.71059. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64984/0.71108. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64855/0.71154. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64892/0.71200. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64742/0.71250. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64586/0.71295. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64428/0.71358. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64436/0.71388. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64215/0.71434. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64096/0.71466. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.63977/0.71519. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63822/0.71567. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63731/0.71632. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63633/0.71692. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63554/0.71723. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63391/0.71777. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63165/0.71814. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63087/0.71842. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62974/0.71891. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62894/0.71933. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62716/0.71952. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62596/0.72001. Took 0.10 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69280/0.69245. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69229/0.69157. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.69069. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69159/0.68978. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69123/0.68883. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69117/0.68784. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.68676. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69027/0.68563. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68952/0.68440. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68908/0.68310. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68835/0.68183. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68820/0.68049. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68781/0.67912. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68704/0.67772. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68696/0.67636. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68643/0.67504. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68582/0.67391. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68552/0.67285. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68471/0.67194. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68471/0.67100. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68441/0.67016. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68361/0.66947. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68323/0.66870. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68283/0.66823. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68231/0.66780. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68180/0.66731. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68154/0.66686. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68126/0.66650. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68024/0.66623. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67994/0.66611. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67922/0.66594. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67881/0.66583. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67818/0.66573. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67757/0.66547. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67671/0.66555. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67604/0.66553. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67570/0.66551. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67487/0.66525. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67404/0.66526. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67343/0.66544. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67267/0.66513. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67217/0.66496. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67127/0.66486. Took 0.15 sec\n",
      "Epoch 43, Loss(train/val) 0.67039/0.66472. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66978/0.66466. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66930/0.66480. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66817/0.66447. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66726/0.66414. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66611/0.66382. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66571/0.66358. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66487/0.66365. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66391/0.66296. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66301/0.66255. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66242/0.66236. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66135/0.66166. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66012/0.66175. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65938/0.66179. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65839/0.66153. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65732/0.66121. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65627/0.66133. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65480/0.66105. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65455/0.66169. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65336/0.66131. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65150/0.66150. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65096/0.66203. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64959/0.66197. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64840/0.66286. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64706/0.66322. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64620/0.66414. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64467/0.66476. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64377/0.66554. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64257/0.66688. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64026/0.66661. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63949/0.66777. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63764/0.66859. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63581/0.67014. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63486/0.67079. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63268/0.67217. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63191/0.67327. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63028/0.67492. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62839/0.67587. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62637/0.67784. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62495/0.67907. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62374/0.68074. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62306/0.68236. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62049/0.68283. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61953/0.68460. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61679/0.68621. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61592/0.68778. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61461/0.68928. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61297/0.69132. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.61090/0.69108. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61049/0.69202. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60823/0.69280. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60657/0.69471. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60556/0.69566. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60308/0.69833. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60199/0.69670. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60072/0.69734. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59832/0.69994. Took 0.09 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.68959/0.69222. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68922/0.69214. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68895/0.69202. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68855/0.69189. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68840/0.69175. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68836/0.69158. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68820/0.69141. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68773/0.69119. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68726/0.69096. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68688/0.69073. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68611/0.69048. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68589/0.69024. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68517/0.69002. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68450/0.68985. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68367/0.68971. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68286/0.68968. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68187/0.68982. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68078/0.69014. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.67973/0.69064. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.67863/0.69142. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67722/0.69230. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.67633/0.69349. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67497/0.69479. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67409/0.69620. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67271/0.69776. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67158/0.69931. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67059/0.70092. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66950/0.70254. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66809/0.70420. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66722/0.70583. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66673/0.70750. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66525/0.70906. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66428/0.71066. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66362/0.71227. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.66227/0.71388. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66164/0.71543. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66085/0.71703. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66027/0.71848. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65898/0.72008. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65810/0.72157. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65700/0.72311. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65624/0.72465. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65519/0.72602. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65387/0.72761. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65333/0.72888. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65206/0.73045. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65136/0.73187. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64976/0.73318. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.64968/0.73440. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64813/0.73567. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64748/0.73687. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64663/0.73824. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64528/0.73920. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64488/0.74040. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64319/0.74170. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64318/0.74259. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64089/0.74373. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63980/0.74463. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63959/0.74572. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63805/0.74669. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63674/0.74770. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63562/0.74865. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63456/0.74959. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63342/0.75078. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63336/0.75160. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63121/0.75255. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62995/0.75322. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.62968/0.75401. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62803/0.75477. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62602/0.75561. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62573/0.75682. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62407/0.75789. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62273/0.75826. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62235/0.75905. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62129/0.76041. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61914/0.76133. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.61844/0.76205. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61727/0.76340. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61546/0.76452. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61504/0.76611. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61314/0.76732. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61191/0.76855. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61003/0.76976. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60903/0.77095. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60859/0.77290. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60729/0.77488. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60467/0.77645. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60397/0.77790. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60264/0.77927. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60161/0.78109. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59940/0.78285. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59922/0.78409. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59641/0.78564. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59584/0.78755. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.59538/0.78920. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59199/0.79096. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59114/0.79256. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58993/0.79407. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58948/0.79605. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58751/0.79852. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69474/0.69985. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69367/0.69851. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69282/0.69688. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69165/0.69498. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69065/0.69298. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68926/0.69119. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68862/0.68983. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68831/0.68891. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68799/0.68833. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68757/0.68795. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68744/0.68777. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68718/0.68770. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68745/0.68767. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68701/0.68768. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68721/0.68776. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68670/0.68786. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68657/0.68796. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68641/0.68809. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68633/0.68823. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68624/0.68836. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68612/0.68856. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68582/0.68874. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68573/0.68892. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68533/0.68917. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68519/0.68944. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68512/0.68968. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68460/0.68995. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68458/0.69021. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68416/0.69051. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68400/0.69083. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68364/0.69110. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68317/0.69146. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68306/0.69184. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68287/0.69205. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68220/0.69249. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68174/0.69295. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68148/0.69340. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68112/0.69378. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68053/0.69420. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68023/0.69465. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67957/0.69511. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67874/0.69560. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67889/0.69606. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67821/0.69645. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67773/0.69697. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67679/0.69746. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67680/0.69784. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67578/0.69818. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67514/0.69868. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67479/0.69906. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67371/0.69958. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67380/0.69987. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67278/0.70003. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67244/0.70060. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67158/0.70070. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67106/0.70088. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66991/0.70151. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66977/0.70194. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66898/0.70207. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66864/0.70231. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66746/0.70240. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66641/0.70265. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66600/0.70272. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66505/0.70319. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66456/0.70320. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66396/0.70368. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66309/0.70372. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66228/0.70411. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66118/0.70427. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66066/0.70441. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65991/0.70445. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65889/0.70479. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65860/0.70508. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65707/0.70545. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65689/0.70552. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65588/0.70560. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65495/0.70593. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65423/0.70552. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65349/0.70579. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65207/0.70639. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65218/0.70656. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65059/0.70649. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65023/0.70656. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64896/0.70719. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64768/0.70749. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64724/0.70719. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64667/0.70752. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64542/0.70725. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64468/0.70740. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64360/0.70800. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64321/0.70776. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64161/0.70801. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64143/0.70798. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64023/0.70831. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64017/0.70807. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63883/0.70856. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63847/0.70794. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63663/0.70815. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63612/0.70850. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63566/0.70927. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69137/0.68095. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69081/0.68050. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69039/0.68020. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68982/0.68003. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68960/0.67991. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68925/0.67985. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68893/0.67978. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68874/0.67973. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68836/0.67971. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68814/0.67969. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.67971. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68745/0.67973. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68712/0.67976. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68690/0.67977. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68664/0.67980. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68602/0.67987. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68603/0.67993. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68567/0.68001. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68517/0.68007. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68493/0.68019. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.68034. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68438/0.68050. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68410/0.68068. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68415/0.68083. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68368/0.68101. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68320/0.68122. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68317/0.68144. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68317/0.68165. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68226/0.68188. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68207/0.68209. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68204/0.68233. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68185/0.68256. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68159/0.68278. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68151/0.68298. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68131/0.68323. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68085/0.68339. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68046/0.68358. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68033/0.68383. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68035/0.68402. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67980/0.68424. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67961/0.68443. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67953/0.68460. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67920/0.68479. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67862/0.68495. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67860/0.68517. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67851/0.68537. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67805/0.68555. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67754/0.68571. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67735/0.68597. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67724/0.68610. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67692/0.68627. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67650/0.68646. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67648/0.68663. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67568/0.68683. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67587/0.68702. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67529/0.68723. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67467/0.68742. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67426/0.68764. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67381/0.68784. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67379/0.68805. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67305/0.68825. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67255/0.68855. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67196/0.68875. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67132/0.68903. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67120/0.68922. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67092/0.68954. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67023/0.68979. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66953/0.68999. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66912/0.69026. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66867/0.69055. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66793/0.69082. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66745/0.69104. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66628/0.69144. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66599/0.69171. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66565/0.69199. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66489/0.69234. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66462/0.69269. Took 0.08 sec\n",
      "Epoch 77, Loss(train/val) 0.66325/0.69302. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66301/0.69346. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66159/0.69376. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66144/0.69416. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66064/0.69455. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66018/0.69490. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65940/0.69537. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65864/0.69579. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65813/0.69619. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65698/0.69666. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65681/0.69709. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65627/0.69759. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65512/0.69809. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.65420/0.69859. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65386/0.69909. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.65297/0.69963. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65251/0.70016. Took 0.08 sec\n",
      "Epoch 94, Loss(train/val) 0.65177/0.70064. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.65080/0.70114. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65043/0.70161. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64957/0.70205. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64864/0.70263. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64829/0.70315. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69064/0.68872. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68952/0.68770. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.68877/0.68660. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68818/0.68545. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68719/0.68437. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68667/0.68347. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68636/0.68290. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68578/0.68252. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68571/0.68232. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68544/0.68220. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68548/0.68216. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68522/0.68212. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68503/0.68211. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68500/0.68214. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68499/0.68215. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68466/0.68217. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68454/0.68220. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68425/0.68224. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68403/0.68227. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68378/0.68231. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68360/0.68234. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68363/0.68240. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68336/0.68248. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68315/0.68254. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68296/0.68261. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68253/0.68268. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68224/0.68279. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68180/0.68290. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68178/0.68304. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68139/0.68316. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68124/0.68331. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68094/0.68349. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68027/0.68371. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67967/0.68395. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67945/0.68426. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67906/0.68453. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67795/0.68486. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67757/0.68519. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67704/0.68558. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67623/0.68602. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67566/0.68641. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67515/0.68688. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67493/0.68731. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67385/0.68789. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67313/0.68847. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67258/0.68914. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67181/0.68983. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67088/0.69053. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67036/0.69124. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66941/0.69205. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66924/0.69273. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66810/0.69357. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66715/0.69445. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66632/0.69546. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66596/0.69636. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66513/0.69737. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66459/0.69846. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66346/0.69944. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66217/0.70053. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66187/0.70163. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66079/0.70281. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66000/0.70405. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65911/0.70529. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65816/0.70652. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65719/0.70776. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65698/0.70902. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65506/0.71024. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65430/0.71164. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65411/0.71291. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65352/0.71419. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65186/0.71563. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65093/0.71692. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65089/0.71807. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64965/0.71947. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64867/0.72070. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64772/0.72204. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64642/0.72332. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64577/0.72455. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64454/0.72592. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64377/0.72719. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64304/0.72844. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64232/0.72965. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64086/0.73075. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63948/0.73227. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63927/0.73364. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63780/0.73460. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63687/0.73592. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63589/0.73725. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63508/0.73871. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63330/0.74012. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63274/0.74166. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63205/0.74270. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63102/0.74438. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62935/0.74532. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62844/0.74696. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62723/0.74840. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62595/0.74966. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62497/0.75124. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62307/0.75276. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62224/0.75410. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.68976/0.69532. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68879/0.69676. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68857/0.69825. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68744/0.69997. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68703/0.70191. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68667/0.70413. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68487/0.70645. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68488/0.70861. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68418/0.71065. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68414/0.71222. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68347/0.71331. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68293/0.71416. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68316/0.71484. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68315/0.71533. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68293/0.71566. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68278/0.71588. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68297/0.71604. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68232/0.71629. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68238/0.71637. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68211/0.71651. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68180/0.71662. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68126/0.71667. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68134/0.71665. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68126/0.71680. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68082/0.71683. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68055/0.71696. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68022/0.71710. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67996/0.71728. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67954/0.71725. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67947/0.71748. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67875/0.71736. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67814/0.71762. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67821/0.71774. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67715/0.71792. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67674/0.71813. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67622/0.71839. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67602/0.71888. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67511/0.71927. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67415/0.71935. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67339/0.72019. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67290/0.72073. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67196/0.72091. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67159/0.72170. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67056/0.72240. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66952/0.72345. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66847/0.72457. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66702/0.72566. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66671/0.72671. Took 0.08 sec\n",
      "Epoch 48, Loss(train/val) 0.66581/0.72750. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66511/0.72890. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66406/0.72996. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66276/0.73094. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66208/0.73277. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66058/0.73428. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65965/0.73552. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65975/0.73746. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65789/0.73884. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65722/0.73980. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65685/0.74101. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65579/0.74189. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65501/0.74301. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65413/0.74448. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65359/0.74500. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65283/0.74738. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65133/0.74799. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65107/0.74891. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65040/0.74961. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64977/0.75198. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64829/0.75160. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64843/0.75347. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64686/0.75486. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64636/0.75464. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64626/0.75600. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64556/0.75662. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64469/0.75707. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64452/0.75783. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64234/0.75921. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64212/0.75975. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64149/0.76117. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64125/0.76098. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64036/0.76159. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63972/0.76226. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.63902/0.76411. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63867/0.76299. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63726/0.76512. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63696/0.76454. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63636/0.76560. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63512/0.76526. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63438/0.76643. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63376/0.76694. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63321/0.76810. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63268/0.76707. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63178/0.76899. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63132/0.76887. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63079/0.76893. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62925/0.77070. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63001/0.76953. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62842/0.76980. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62791/0.77145. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62781/0.77130. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69710/0.70107. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69410/0.70136. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.70231. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69033/0.70364. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68889/0.70500. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68873/0.70606. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68839/0.70692. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68836/0.70753. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68807/0.70799. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68764/0.70837. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68747/0.70868. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68752/0.70893. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68750/0.70915. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68714/0.70941. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68743/0.70964. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68667/0.70990. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68687/0.71016. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68665/0.71040. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68630/0.71064. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68627/0.71089. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68624/0.71113. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68622/0.71135. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68596/0.71156. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68568/0.71176. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68574/0.71199. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68513/0.71226. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68544/0.71250. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68480/0.71272. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68491/0.71294. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68477/0.71318. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68462/0.71340. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68435/0.71365. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68418/0.71391. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68418/0.71420. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68389/0.71444. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68410/0.71467. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68334/0.71494. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68383/0.71517. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68327/0.71542. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68321/0.71568. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68291/0.71595. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68230/0.71624. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68226/0.71656. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68225/0.71689. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68144/0.71722. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68138/0.71753. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68152/0.71787. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.68101/0.71817. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68074/0.71853. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68042/0.71895. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67976/0.71940. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67959/0.71988. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67917/0.72028. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67896/0.72079. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67849/0.72128. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67822/0.72186. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67766/0.72228. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67780/0.72288. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67677/0.72364. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67581/0.72435. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67555/0.72502. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67553/0.72566. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67477/0.72641. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67448/0.72714. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67375/0.72798. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67332/0.72884. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67249/0.72971. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67203/0.73050. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67198/0.73140. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67115/0.73237. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67010/0.73312. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66967/0.73431. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66838/0.73560. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66814/0.73646. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66719/0.73768. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66711/0.73887. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66567/0.74007. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66548/0.74145. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66423/0.74271. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66378/0.74403. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66235/0.74533. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66196/0.74658. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66089/0.74803. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66000/0.74960. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65936/0.75086. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65841/0.75235. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65761/0.75372. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65695/0.75528. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65617/0.75692. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65505/0.75839. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65455/0.76012. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65342/0.76149. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65227/0.76334. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65164/0.76486. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65033/0.76634. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64962/0.76795. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64929/0.76928. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64838/0.77105. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64738/0.77246. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64689/0.77439. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69605/0.69581. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69497/0.69528. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69424/0.69481. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69357/0.69449. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69292/0.69429. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69198/0.69422. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69191/0.69420. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69169/0.69420. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69123/0.69421. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69089/0.69424. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69075/0.69427. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69019/0.69432. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68985/0.69437. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68950/0.69443. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68924/0.69452. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68881/0.69463. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68839/0.69478. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68803/0.69495. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68761/0.69514. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68737/0.69537. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68666/0.69563. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68633/0.69595. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68553/0.69628. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68531/0.69662. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68484/0.69699. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68418/0.69738. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68409/0.69777. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68366/0.69815. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68294/0.69853. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68290/0.69891. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68250/0.69928. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68186/0.69966. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68147/0.69998. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68113/0.70032. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68099/0.70062. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68029/0.70088. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68003/0.70118. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67948/0.70145. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67980/0.70173. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67950/0.70197. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67898/0.70219. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67857/0.70240. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67827/0.70259. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67779/0.70287. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67793/0.70309. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67777/0.70323. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67734/0.70339. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67674/0.70358. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67673/0.70383. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67610/0.70412. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67602/0.70434. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67575/0.70456. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67510/0.70475. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67489/0.70499. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67435/0.70528. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67467/0.70551. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67434/0.70571. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67381/0.70602. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67338/0.70631. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67321/0.70660. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67286/0.70687. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67276/0.70718. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.67253/0.70749. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67172/0.70778. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67163/0.70807. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67086/0.70849. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67067/0.70886. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67055/0.70918. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67041/0.70952. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66959/0.70997. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66903/0.71030. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66923/0.71063. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66891/0.71093. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66828/0.71133. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66818/0.71169. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66722/0.71213. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66677/0.71250. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66689/0.71291. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66655/0.71330. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66655/0.71381. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66550/0.71424. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66514/0.71473. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66516/0.71512. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66382/0.71559. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66377/0.71606. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66329/0.71659. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66291/0.71696. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66244/0.71750. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66167/0.71799. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66152/0.71865. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66104/0.71899. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66034/0.71949. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.66020/0.71998. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65961/0.72059. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65830/0.72124. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65852/0.72171. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65768/0.72229. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65693/0.72279. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65628/0.72348. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65598/0.72411. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69573/0.69537. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69532/0.69502. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69448/0.69464. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69384/0.69426. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69328/0.69392. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69264/0.69366. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69182/0.69350. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69150/0.69346. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69120/0.69350. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69090/0.69359. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69046/0.69373. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69044/0.69388. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68984/0.69409. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68975/0.69429. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68952/0.69453. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68909/0.69479. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68853/0.69507. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68837/0.69540. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68797/0.69576. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68763/0.69613. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68730/0.69654. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68679/0.69696. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68647/0.69736. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68623/0.69780. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68580/0.69826. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68548/0.69870. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68513/0.69918. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68497/0.69962. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68438/0.70007. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68399/0.70049. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68361/0.70091. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68348/0.70131. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68323/0.70171. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68259/0.70207. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68223/0.70246. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68214/0.70286. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68166/0.70321. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68150/0.70356. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68100/0.70385. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68081/0.70415. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68056/0.70447. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67986/0.70484. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67941/0.70516. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67938/0.70544. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67865/0.70570. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67875/0.70603. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67820/0.70628. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67772/0.70650. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67732/0.70676. Took 0.08 sec\n",
      "Epoch 49, Loss(train/val) 0.67744/0.70702. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67683/0.70731. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67648/0.70757. Took 0.08 sec\n",
      "Epoch 52, Loss(train/val) 0.67616/0.70783. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67567/0.70816. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67498/0.70832. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67489/0.70864. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67409/0.70896. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67381/0.70928. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67371/0.70948. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67326/0.70977. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67324/0.71003. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67230/0.71033. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67202/0.71057. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67176/0.71084. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67125/0.71112. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67086/0.71133. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67033/0.71164. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67009/0.71194. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66929/0.71235. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66881/0.71267. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66816/0.71304. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66786/0.71338. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66711/0.71371. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66668/0.71414. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66651/0.71438. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66577/0.71474. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66490/0.71505. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66429/0.71554. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66396/0.71588. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66345/0.71633. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66298/0.71655. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66194/0.71700. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66128/0.71723. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66080/0.71767. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66040/0.71815. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65936/0.71862. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65919/0.71879. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65860/0.71914. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65782/0.71950. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65668/0.71994. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65642/0.72027. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65527/0.72056. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65500/0.72119. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65376/0.72180. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65300/0.72220. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65274/0.72257. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65137/0.72318. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65074/0.72363. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65126/0.72404. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64969/0.72452. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69783/0.69516. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69637/0.69421. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69525/0.69370. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69449/0.69345. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69422/0.69336. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69383/0.69334. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69376/0.69336. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69335/0.69338. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69326/0.69341. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69307/0.69345. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69286/0.69350. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69290/0.69355. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69244/0.69358. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69208/0.69363. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69205/0.69368. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69196/0.69374. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.69158/0.69379. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69164/0.69386. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69131/0.69392. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69129/0.69400. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69100/0.69406. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69061/0.69414. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69052/0.69421. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69029/0.69428. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69021/0.69437. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69009/0.69446. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68989/0.69455. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68970/0.69463. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68930/0.69469. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68886/0.69476. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68889/0.69482. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68860/0.69488. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68843/0.69492. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68814/0.69496. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68787/0.69500. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68771/0.69505. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68768/0.69507. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68722/0.69510. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68705/0.69512. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68660/0.69514. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68667/0.69514. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68624/0.69512. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68594/0.69511. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68545/0.69511. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68528/0.69513. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68523/0.69511. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68478/0.69511. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68446/0.69510. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68382/0.69507. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68384/0.69505. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68353/0.69502. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68323/0.69500. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68237/0.69499. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68221/0.69497. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68194/0.69498. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68172/0.69497. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68107/0.69499. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68082/0.69497. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68013/0.69500. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67988/0.69504. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67927/0.69512. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67891/0.69517. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67852/0.69522. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67796/0.69526. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67713/0.69534. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67649/0.69548. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67609/0.69563. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67579/0.69579. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67522/0.69592. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67456/0.69610. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67385/0.69630. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67340/0.69649. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67238/0.69678. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67178/0.69706. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67154/0.69730. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67059/0.69762. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66981/0.69795. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66919/0.69834. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66850/0.69865. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66826/0.69899. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66707/0.69928. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66672/0.69956. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66588/0.69996. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66500/0.70021. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66479/0.70053. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66393/0.70088. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66343/0.70124. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66239/0.70155. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66197/0.70190. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66071/0.70222. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66086/0.70252. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65969/0.70296. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65942/0.70327. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65840/0.70367. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65825/0.70388. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65737/0.70410. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65687/0.70451. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65596/0.70461. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65596/0.70483. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65453/0.70519. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69334/0.68493. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69325/0.68510. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69290/0.68524. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69268/0.68538. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69262/0.68551. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69254/0.68566. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69206/0.68581. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69213/0.68596. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69171/0.68616. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69142/0.68641. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69097/0.68669. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69056/0.68706. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69045/0.68754. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68989/0.68811. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68938/0.68880. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68882/0.68955. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68849/0.69041. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68749/0.69135. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68704/0.69231. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68615/0.69328. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68550/0.69437. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68473/0.69564. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68383/0.69691. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68279/0.69839. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68172/0.69987. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68056/0.70135. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67969/0.70295. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67831/0.70447. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67728/0.70620. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67646/0.70777. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67499/0.70955. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67447/0.71100. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67341/0.71234. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67185/0.71390. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67058/0.71542. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.66968/0.71611. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.66854/0.71755. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66734/0.71874. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66615/0.72005. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.66469/0.72078. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66329/0.72135. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66178/0.72244. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66103/0.72310. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65983/0.72350. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65871/0.72405. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65680/0.72448. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65578/0.72524. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65421/0.72609. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.65296/0.72685. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65249/0.72692. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65110/0.72770. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65009/0.72851. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64888/0.72927. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64743/0.72992. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64579/0.73097. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64466/0.73212. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64382/0.73317. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64321/0.73378. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64186/0.73502. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.64099/0.73608. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.63994/0.73782. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63851/0.73935. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63695/0.74095. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.63663/0.74259. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63537/0.74307. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63504/0.74570. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63342/0.74635. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63224/0.74801. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63100/0.75025. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63091/0.75191. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62916/0.75410. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62851/0.75489. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.62723/0.75785. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62572/0.76022. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.62527/0.76098. Took 0.12 sec\n",
      "Epoch 75, Loss(train/val) 0.62475/0.76431. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62288/0.76575. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62258/0.76825. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.62111/0.76876. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61998/0.77207. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61995/0.77419. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61725/0.77557. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61587/0.77806. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.61509/0.78013. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61469/0.78222. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61338/0.78464. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61288/0.78684. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61101/0.78956. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61028/0.79129. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60805/0.79401. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.60799/0.79608. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60701/0.79872. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60586/0.80127. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.60416/0.80183. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60356/0.80509. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60187/0.80769. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.60182/0.80920. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59945/0.81164. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.59750/0.81343. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.59744/0.81528. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69548/0.69453. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69462/0.69342. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69361/0.69249. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69270/0.69175. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69237/0.69121. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.69083. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69062. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69048. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69061/0.69045. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69036/0.69046. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69004/0.69053. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68970/0.69065. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68924/0.69079. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68880/0.69099. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68826/0.69123. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68776/0.69150. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68740/0.69184. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68695/0.69220. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68648/0.69261. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68603/0.69307. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68523/0.69359. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68480/0.69415. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68428/0.69472. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68384/0.69531. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68314/0.69596. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68282/0.69654. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68228/0.69718. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68172/0.69775. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68142/0.69838. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68067/0.69893. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68001/0.69948. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67947/0.70002. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67902/0.70057. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67838/0.70107. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67784/0.70149. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67739/0.70196. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67733/0.70237. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67653/0.70263. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67606/0.70294. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67568/0.70335. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67509/0.70361. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67502/0.70386. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67418/0.70416. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67342/0.70436. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67268/0.70463. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67231/0.70490. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67153/0.70509. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67129/0.70527. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67052/0.70554. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67004/0.70571. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66931/0.70586. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66895/0.70602. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66760/0.70628. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66722/0.70645. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66691/0.70666. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66620/0.70686. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66546/0.70708. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66502/0.70716. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66402/0.70736. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66316/0.70749. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66275/0.70772. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66235/0.70787. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66132/0.70802. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66061/0.70821. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66014/0.70866. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65939/0.70864. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65840/0.70887. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65800/0.70881. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65750/0.70920. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65659/0.70936. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65577/0.70967. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65531/0.70985. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65487/0.70993. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65361/0.71037. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65314/0.71063. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65279/0.71115. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65145/0.71146. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65120/0.71170. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65077/0.71237. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64941/0.71276. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64944/0.71296. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64825/0.71340. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64787/0.71397. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64738/0.71425. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64687/0.71448. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64636/0.71496. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64558/0.71521. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64550/0.71563. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64453/0.71631. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64323/0.71643. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64280/0.71703. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64265/0.71716. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64150/0.71771. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64190/0.71837. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64023/0.71875. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63925/0.71905. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63941/0.71937. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63886/0.71988. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63829/0.72034. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63745/0.72082. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69591/0.69346. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69450/0.69222. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69321/0.69106. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69222/0.69028. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69138/0.68992. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69091/0.68996. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69014/0.69022. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69001/0.69063. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68951/0.69108. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68930/0.69159. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68904/0.69216. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68881/0.69276. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68858/0.69342. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68825/0.69408. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68794/0.69479. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68740/0.69553. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.69633. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68689/0.69717. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68645/0.69806. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68626/0.69896. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68584/0.69986. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68565/0.70072. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68496/0.70155. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68503/0.70237. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68450/0.70316. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68400/0.70395. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68380/0.70467. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68364/0.70537. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68307/0.70597. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68305/0.70655. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68275/0.70709. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68226/0.70762. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68195/0.70813. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68169/0.70861. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68148/0.70901. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68136/0.70934. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68070/0.70966. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68022/0.71004. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68017/0.71039. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67944/0.71076. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67938/0.71106. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67914/0.71133. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67906/0.71155. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67846/0.71168. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67816/0.71188. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67770/0.71215. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67745/0.71237. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67729/0.71252. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67667/0.71277. Took 0.12 sec\n",
      "Epoch 49, Loss(train/val) 0.67640/0.71307. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67622/0.71325. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67593/0.71351. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67527/0.71376. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67469/0.71398. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67420/0.71426. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67357/0.71460. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67363/0.71485. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67288/0.71513. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67246/0.71531. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67231/0.71555. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67165/0.71592. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67081/0.71627. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67047/0.71650. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67003/0.71680. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66925/0.71704. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66864/0.71752. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66831/0.71789. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66747/0.71826. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66703/0.71844. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66648/0.71863. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66552/0.71918. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66508/0.71959. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66445/0.72004. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66384/0.72029. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66241/0.72067. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66207/0.72123. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66145/0.72173. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66124/0.72192. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66005/0.72239. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65910/0.72282. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65897/0.72318. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65760/0.72360. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65649/0.72414. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65589/0.72465. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65514/0.72497. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65486/0.72542. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65396/0.72575. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65303/0.72606. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65222/0.72627. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65165/0.72678. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65023/0.72718. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64973/0.72748. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64850/0.72766. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64801/0.72804. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64728/0.72811. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64637/0.72834. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64535/0.72824. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64459/0.72851. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64410/0.72864. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64313/0.72894. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69225/0.69125. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69211/0.69130. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69187/0.69134. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69196/0.69142. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69175/0.69151. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69128/0.69163. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69120/0.69179. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69092/0.69199. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69068/0.69225. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69026/0.69259. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68976/0.69300. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68952/0.69351. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68902/0.69411. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68822/0.69480. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68802/0.69556. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68714/0.69646. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68644/0.69745. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68591/0.69854. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68522/0.69977. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68404/0.70111. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68326/0.70252. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68256/0.70406. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68169/0.70563. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68079/0.70721. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68021/0.70874. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67967/0.71022. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67865/0.71167. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67788/0.71300. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67719/0.71429. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67708/0.71546. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67629/0.71653. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67581/0.71748. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67533/0.71834. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67515/0.71906. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67445/0.71981. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67404/0.72041. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67320/0.72099. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67312/0.72148. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67261/0.72193. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67201/0.72236. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67156/0.72277. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67098/0.72305. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67045/0.72334. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66990/0.72359. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66960/0.72391. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66921/0.72413. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66877/0.72430. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66815/0.72449. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66793/0.72462. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66726/0.72484. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66691/0.72503. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66619/0.72515. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66609/0.72527. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66506/0.72550. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66474/0.72565. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66444/0.72583. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66333/0.72600. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66323/0.72619. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66256/0.72635. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66169/0.72656. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66101/0.72678. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66075/0.72700. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65970/0.72727. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65962/0.72748. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65921/0.72772. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65829/0.72798. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65722/0.72820. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65663/0.72859. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65613/0.72898. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65521/0.72932. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65441/0.72967. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65317/0.73011. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65242/0.73047. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65182/0.73073. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65097/0.73093. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64946/0.73131. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64900/0.73154. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64801/0.73174. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64678/0.73199. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64570/0.73220. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64433/0.73235. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64372/0.73243. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64273/0.73249. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64149/0.73282. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63936/0.73298. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63858/0.73315. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63689/0.73312. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63669/0.73309. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63517/0.73309. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63343/0.73299. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63200/0.73294. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63138/0.73307. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62957/0.73308. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62806/0.73324. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62782/0.73321. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62570/0.73327. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62434/0.73373. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62244/0.73435. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.62083/0.73489. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62063/0.73538. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69117/0.69664. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69101/0.69679. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69084/0.69694. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69054/0.69710. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69033/0.69728. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69747. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68953/0.69769. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68941/0.69793. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68894/0.69821. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68877/0.69854. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68851/0.69890. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68776/0.69931. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68727/0.69980. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68659/0.70038. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68592/0.70102. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68508/0.70181. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68424/0.70273. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68347/0.70376. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68244/0.70492. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68151/0.70620. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68093/0.70763. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68005/0.70918. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67930/0.71073. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67858/0.71233. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67757/0.71392. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67702/0.71552. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67633/0.71710. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67546/0.71875. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67477/0.72037. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67417/0.72197. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67400/0.72352. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67343/0.72494. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67254/0.72634. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67250/0.72763. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.67145/0.72894. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67144/0.73014. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67092/0.73143. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67037/0.73248. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66971/0.73366. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66980/0.73477. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66859/0.73599. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66851/0.73704. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66756/0.73812. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66714/0.73934. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66685/0.74040. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66604/0.74143. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66590/0.74237. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66553/0.74341. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66464/0.74442. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66416/0.74565. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66383/0.74649. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66331/0.74752. Took 0.12 sec\n",
      "Epoch 52, Loss(train/val) 0.66216/0.74857. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66148/0.74962. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66156/0.75066. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66070/0.75168. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65969/0.75292. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65918/0.75404. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65940/0.75513. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65846/0.75639. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65755/0.75726. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65697/0.75852. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65703/0.75961. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65632/0.76113. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65544/0.76214. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65391/0.76340. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65479/0.76477. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.65347/0.76582. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65273/0.76715. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65218/0.76823. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65112/0.76974. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64998/0.77123. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64977/0.77265. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64932/0.77382. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64846/0.77531. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64637/0.77680. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64737/0.77858. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64566/0.77990. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64529/0.78178. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64408/0.78266. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64426/0.78424. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64276/0.78619. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.64239/0.78736. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64116/0.78921. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64035/0.79061. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64054/0.79212. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63945/0.79330. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63848/0.79532. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63667/0.79678. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63623/0.79864. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63545/0.80058. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63413/0.80241. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63396/0.80431. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63296/0.80589. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63284/0.80738. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63088/0.80917. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63090/0.81081. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63001/0.81292. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62884/0.81341. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62812/0.81573. Took 0.11 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69428/0.69295. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69276/0.69394. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.69498. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69134/0.69595. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69035/0.69682. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69001/0.69746. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68972/0.69783. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68934/0.69797. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68882/0.69795. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68823/0.69786. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.69755. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68737/0.69725. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68686/0.69699. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68624/0.69663. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68592/0.69617. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68555/0.69567. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68469/0.69521. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68415/0.69477. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68344/0.69429. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68268/0.69401. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68209/0.69367. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68148/0.69342. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68089/0.69319. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68023/0.69312. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67986/0.69317. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67905/0.69322. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67838/0.69327. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67804/0.69356. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67746/0.69385. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.67714/0.69413. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67688/0.69452. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67624/0.69505. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67570/0.69528. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67542/0.69580. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67485/0.69635. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67459/0.69654. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67392/0.69715. Took 0.08 sec\n",
      "Epoch 37, Loss(train/val) 0.67404/0.69747. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67324/0.69783. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67316/0.69850. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67248/0.69893. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67207/0.69931. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67158/0.69986. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67104/0.70069. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67058/0.70114. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67023/0.70175. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66992/0.70241. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66908/0.70290. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66851/0.70341. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66785/0.70418. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66750/0.70484. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66705/0.70558. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66637/0.70629. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66592/0.70685. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66493/0.70773. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66420/0.70814. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66350/0.70934. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66375/0.71050. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66237/0.71084. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66187/0.71206. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66108/0.71271. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66059/0.71361. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65989/0.71490. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65884/0.71561. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65840/0.71705. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65804/0.71764. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65699/0.71932. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65618/0.71958. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65584/0.72088. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65496/0.72218. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65385/0.72313. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65324/0.72440. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65299/0.72594. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65189/0.72726. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65143/0.72816. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65046/0.72949. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65001/0.73055. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64846/0.73124. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64876/0.73286. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64710/0.73394. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64643/0.73495. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64546/0.73607. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64530/0.73723. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.64414/0.73873. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64408/0.73896. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64326/0.74032. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64233/0.74099. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64151/0.74245. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64078/0.74321. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63971/0.74481. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64025/0.74610. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63910/0.74617. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63804/0.74716. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63774/0.74854. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63648/0.74889. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63618/0.75045. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63514/0.75171. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63416/0.75140. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63364/0.75304. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63303/0.75409. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.72146/0.71494. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.70902/0.70156. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.70105/0.69318. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69682/0.68980. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69559/0.68869. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69480/0.68825. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69398/0.68799. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69324/0.68786. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69232/0.68777. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69182/0.68769. Took 0.11 sec\n",
      "Epoch 10, Loss(train/val) 0.69125/0.68772. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69078/0.68773. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68991/0.68777. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68947/0.68788. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68919/0.68802. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68867/0.68820. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68813/0.68838. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68803/0.68864. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68746/0.68889. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68693/0.68918. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68658/0.68951. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68615/0.68983. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68579/0.69014. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68535/0.69049. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68486/0.69086. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68460/0.69121. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68447/0.69160. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68391/0.69196. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68365/0.69227. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68333/0.69257. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68331/0.69291. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68275/0.69322. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68255/0.69353. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68198/0.69376. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68205/0.69402. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68192/0.69421. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68152/0.69442. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68146/0.69465. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68115/0.69480. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68076/0.69497. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68036/0.69509. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68031/0.69522. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68013/0.69528. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67968/0.69543. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67961/0.69551. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67924/0.69552. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67862/0.69556. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67831/0.69561. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67868/0.69563. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67826/0.69564. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67758/0.69561. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67740/0.69566. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67758/0.69563. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67689/0.69563. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67637/0.69560. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67628/0.69559. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67564/0.69554. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.67597/0.69553. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67538/0.69539. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67500/0.69535. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67513/0.69528. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67446/0.69525. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67409/0.69518. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67376/0.69511. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67375/0.69500. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67300/0.69489. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67279/0.69476. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67283/0.69464. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67182/0.69456. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67166/0.69444. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67149/0.69435. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67079/0.69416. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.67063/0.69417. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67048/0.69408. Took 0.08 sec\n",
      "Epoch 74, Loss(train/val) 0.66964/0.69386. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66916/0.69376. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66921/0.69364. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66824/0.69355. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66791/0.69336. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66740/0.69325. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66697/0.69316. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66683/0.69299. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66604/0.69294. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66577/0.69275. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.66565/0.69268. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66505/0.69264. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66430/0.69243. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66391/0.69240. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66332/0.69226. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66314/0.69211. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66257/0.69203. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66163/0.69199. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66128/0.69198. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66107/0.69181. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66064/0.69167. Took 0.08 sec\n",
      "Epoch 95, Loss(train/val) 0.65950/0.69160. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65977/0.69149. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65861/0.69144. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65804/0.69144. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65738/0.69135. Took 0.08 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69769/0.69097. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69713/0.69071. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69659/0.69045. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69602/0.69023. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69539/0.69010. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69464/0.69011. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69425/0.69027. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69375/0.69054. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69353/0.69080. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69331/0.69104. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69322/0.69123. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69321/0.69141. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69287/0.69157. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69291/0.69169. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69251/0.69174. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69268/0.69182. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69248/0.69189. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69221/0.69196. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69233/0.69198. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69232/0.69203. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69219/0.69208. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69194/0.69209. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69180/0.69212. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69183/0.69216. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69122/0.69217. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.69149/0.69214. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69148/0.69213. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.69108/0.69210. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69128/0.69209. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69108/0.69209. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69119/0.69210. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69090/0.69207. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69056/0.69209. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69008/0.69209. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.68999/0.69204. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68988/0.69204. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68982/0.69198. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68965/0.69194. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68932/0.69190. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68920/0.69187. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68895/0.69181. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68881/0.69174. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68852/0.69163. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68824/0.69151. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68792/0.69137. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68777/0.69119. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68764/0.69100. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68736/0.69080. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68696/0.69062. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68668/0.69038. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68647/0.69014. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68608/0.68984. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68594/0.68954. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68566/0.68927. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68523/0.68892. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68472/0.68856. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68477/0.68829. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68389/0.68792. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68362/0.68746. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68344/0.68707. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68301/0.68663. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.68273/0.68615. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.68204/0.68562. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68179/0.68532. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.68103/0.68494. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68077/0.68446. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.68019/0.68397. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.67957/0.68356. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67915/0.68328. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67877/0.68307. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67816/0.68268. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67788/0.68226. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67734/0.68224. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67673/0.68201. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67599/0.68199. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67592/0.68177. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67505/0.68170. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67433/0.68164. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67385/0.68180. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67351/0.68197. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67279/0.68204. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67250/0.68189. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67209/0.68189. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67098/0.68222. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67100/0.68238. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67026/0.68264. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66950/0.68265. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66894/0.68286. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.66806/0.68322. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66766/0.68348. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66681/0.68388. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66618/0.68411. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66591/0.68445. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66483/0.68481. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66418/0.68509. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66380/0.68539. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66336/0.68587. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66272/0.68636. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66176/0.68652. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66144/0.68684. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69544/0.69555. Took 0.28 sec\n",
      "Epoch 1, Loss(train/val) 0.69450/0.69426. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69430/0.69331. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69404/0.69264. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69387/0.69214. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69333/0.69172. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69345/0.69142. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69347/0.69118. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69344/0.69101. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69293/0.69086. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69242/0.69076. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69257/0.69065. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69240/0.69051. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69217/0.69041. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69244/0.69029. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69235/0.69027. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69172/0.69016. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69229/0.69006. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69164/0.69002. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69180/0.68995. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69195/0.68983. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69178/0.68974. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69138/0.68965. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69117/0.68953. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69073/0.68944. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69076/0.68932. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69066/0.68922. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69060/0.68918. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69026/0.68907. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.69042/0.68896. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69050/0.68888. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.69003/0.68880. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69014/0.68875. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68980/0.68864. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.68932/0.68853. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68996/0.68848. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68919/0.68834. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68933/0.68826. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68901/0.68817. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68874/0.68808. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.68914/0.68796. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68842/0.68776. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68826/0.68760. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68839/0.68753. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68829/0.68742. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68791/0.68729. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68799/0.68717. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68779/0.68701. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68718/0.68680. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68696/0.68658. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68696/0.68634. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68696/0.68623. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68650/0.68607. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68593/0.68585. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68639/0.68566. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68591/0.68541. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.68533/0.68527. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68556/0.68505. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68523/0.68485. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68510/0.68460. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.68459/0.68440. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 0.68379/0.68407. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.68367/0.68389. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68376/0.68357. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.68304/0.68325. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.68307/0.68299. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.68281/0.68279. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.68203/0.68248. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68185/0.68224. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68147/0.68187. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.68084/0.68163. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.68115/0.68145. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.68036/0.68107. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.68033/0.68070. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67980/0.68045. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67903/0.68025. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67842/0.67991. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.67830/0.67944. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67735/0.67921. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67733/0.67878. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.67696/0.67856. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67606/0.67828. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67551/0.67793. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.67517/0.67765. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67474/0.67716. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67451/0.67698. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.67388/0.67671. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.67302/0.67641. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67299/0.67622. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.67184/0.67600. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67150/0.67561. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67033/0.67555. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.67023/0.67541. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66989/0.67530. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66844/0.67504. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.66815/0.67472. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66799/0.67469. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66707/0.67447. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.66625/0.67441. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66529/0.67440. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69331/0.69050. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.69058. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69332/0.69065. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69310/0.69071. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69311/0.69077. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69268/0.69084. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69268/0.69090. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69246/0.69096. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69241/0.69102. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69236/0.69108. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69215/0.69114. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69195/0.69122. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69191/0.69130. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69182/0.69136. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69126/0.69142. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69146/0.69147. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69131/0.69154. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69089/0.69159. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69080/0.69163. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69074/0.69169. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69073/0.69175. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69017/0.69180. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68994/0.69184. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68967/0.69186. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68950/0.69191. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68885/0.69192. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68882/0.69192. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68870/0.69191. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68832/0.69185. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68785/0.69178. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68764/0.69171. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68709/0.69161. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68670/0.69145. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68635/0.69127. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68648/0.69112. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68596/0.69089. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68569/0.69065. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68512/0.69037. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68452/0.69007. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.68452/0.68974. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68400/0.68938. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68324/0.68902. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68345/0.68865. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68292/0.68828. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.68222/0.68792. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68182/0.68755. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68197/0.68720. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68139/0.68683. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68068/0.68645. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68011/0.68602. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.67988/0.68562. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67922/0.68531. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67895/0.68487. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67835/0.68455. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67785/0.68419. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67727/0.68392. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67702/0.68357. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67619/0.68332. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67632/0.68306. Took 0.12 sec\n",
      "Epoch 59, Loss(train/val) 0.67558/0.68279. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67537/0.68258. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67464/0.68235. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67431/0.68206. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67367/0.68173. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67334/0.68157. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.67278/0.68139. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67198/0.68126. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67156/0.68107. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67043/0.68088. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67105/0.68073. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67000/0.68076. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66890/0.68081. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66821/0.68079. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66849/0.68063. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66724/0.68049. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66715/0.68041. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66643/0.68035. Took 0.12 sec\n",
      "Epoch 77, Loss(train/val) 0.66587/0.68040. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66561/0.68048. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66520/0.68038. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.66439/0.68060. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66355/0.68065. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66216/0.68066. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66265/0.68088. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66164/0.68112. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66091/0.68127. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66016/0.68147. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65959/0.68169. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65911/0.68168. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65875/0.68198. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65815/0.68220. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65767/0.68239. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65606/0.68254. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65584/0.68266. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65484/0.68309. Took 0.12 sec\n",
      "Epoch 95, Loss(train/val) 0.65463/0.68313. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65428/0.68368. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65291/0.68376. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65256/0.68403. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65161/0.68427. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69480/0.69776. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69453/0.69752. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69458/0.69729. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69419/0.69703. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69380/0.69677. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69356/0.69652. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69338/0.69625. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69294/0.69592. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69244/0.69558. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69216/0.69524. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69179/0.69484. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69166/0.69443. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69068/0.69394. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69036/0.69339. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68963/0.69284. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68938/0.69227. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68850/0.69164. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68836/0.69096. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68785/0.69026. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68685/0.68957. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68612/0.68892. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68559/0.68825. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68424/0.68758. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68372/0.68688. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68288/0.68614. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68176/0.68539. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68101/0.68460. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67958/0.68380. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67932/0.68302. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67779/0.68220. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67653/0.68140. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67591/0.68073. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67500/0.68011. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67377/0.67935. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67265/0.67888. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67141/0.67861. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67011/0.67826. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66846/0.67798. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66703/0.67789. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66589/0.67825. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66471/0.67807. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66248/0.67831. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66127/0.67851. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66014/0.67880. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65841/0.67923. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65708/0.67947. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65550/0.67967. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65392/0.68046. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65280/0.68047. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65056/0.68182. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64941/0.68152. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64783/0.68243. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64578/0.68273. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64444/0.68296. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64312/0.68421. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.64157/0.68465. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64034/0.68589. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63908/0.68664. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.63635/0.68702. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63497/0.68876. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63370/0.69038. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63190/0.69100. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63002/0.69337. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62901/0.69555. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.62734/0.69647. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62576/0.69845. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62441/0.70037. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62252/0.70187. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62145/0.70429. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.61925/0.70650. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.61736/0.70901. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61696/0.71045. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61491/0.71261. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61312/0.71487. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.61125/0.71731. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61014/0.72014. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.60791/0.72194. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60729/0.72540. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60543/0.72776. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.60275/0.73092. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60261/0.73216. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60107/0.73520. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.59851/0.73885. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.59812/0.74181. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59713/0.74371. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.59506/0.74598. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.59371/0.74872. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59260/0.75206. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59244/0.75377. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.58947/0.75666. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.58825/0.75975. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.58650/0.76373. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.58586/0.76599. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58479/0.76743. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.58236/0.77129. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.58160/0.77596. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58088/0.77855. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.57935/0.78002. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.57790/0.78512. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57750/0.78909. Took 0.11 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69275/0.69478. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69243/0.69474. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69195/0.69472. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.69471. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69173/0.69473. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69129/0.69478. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69112/0.69485. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69092/0.69496. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69071/0.69509. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69023/0.69522. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69032/0.69538. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68988/0.69558. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68988/0.69577. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68969/0.69597. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68970/0.69617. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.69637. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68907/0.69656. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68890/0.69678. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68872/0.69700. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68855/0.69724. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68825/0.69749. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68809/0.69773. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68801/0.69798. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68787/0.69825. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68753/0.69852. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68723/0.69880. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68708/0.69910. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68680/0.69940. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68652/0.69973. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68639/0.70007. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68602/0.70043. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68563/0.70080. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68542/0.70119. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68511/0.70158. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68474/0.70200. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68421/0.70243. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68419/0.70291. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68361/0.70338. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68299/0.70388. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68303/0.70438. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68248/0.70493. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68189/0.70551. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68156/0.70606. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68114/0.70659. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68045/0.70715. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68020/0.70773. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67982/0.70838. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67949/0.70900. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67895/0.70960. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67840/0.71030. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67748/0.71101. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67710/0.71169. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67715/0.71245. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67587/0.71318. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67542/0.71400. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67520/0.71480. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67453/0.71560. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67414/0.71643. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67304/0.71729. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67276/0.71820. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67225/0.71910. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67114/0.72000. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67054/0.72100. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66984/0.72204. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66975/0.72305. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66864/0.72393. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66779/0.72502. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66724/0.72589. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66658/0.72689. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66564/0.72783. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66526/0.72875. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66426/0.72987. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66374/0.73100. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.66277/0.73212. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66208/0.73327. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66160/0.73424. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.66043/0.73522. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65946/0.73630. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65932/0.73729. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65844/0.73810. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65711/0.73917. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65680/0.74049. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.65611/0.74144. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65464/0.74254. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65407/0.74357. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65370/0.74430. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65242/0.74544. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65206/0.74644. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65081/0.74782. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65088/0.74892. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64966/0.74987. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64871/0.75102. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64803/0.75201. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64726/0.75307. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64691/0.75424. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64557/0.75530. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64498/0.75653. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64425/0.75769. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64265/0.75878. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64262/0.75984. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69291/0.68012. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.68034. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69225/0.68053. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69201/0.68066. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69188/0.68077. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.68084. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69162/0.68091. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69145/0.68094. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69131/0.68093. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69081/0.68094. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69101/0.68089. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69054/0.68083. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69031/0.68075. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69052/0.68066. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69039/0.68056. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68995/0.68041. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68997/0.68027. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68981/0.68011. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68955/0.67994. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68908/0.67975. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68914/0.67955. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68883/0.67938. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68870/0.67917. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68853/0.67899. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68826/0.67881. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68803/0.67862. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68784/0.67843. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68779/0.67826. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68735/0.67808. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68707/0.67791. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68684/0.67773. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68688/0.67758. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68633/0.67742. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68650/0.67728. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68591/0.67714. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68556/0.67697. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68549/0.67684. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68523/0.67668. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68491/0.67661. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68459/0.67649. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68446/0.67635. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68429/0.67625. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68398/0.67614. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68365/0.67602. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68323/0.67578. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68321/0.67573. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68289/0.67573. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68246/0.67564. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68215/0.67546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68176/0.67539. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68135/0.67529. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68105/0.67517. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68079/0.67507. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68055/0.67501. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68035/0.67496. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67990/0.67493. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67957/0.67488. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67900/0.67493. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67866/0.67488. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67813/0.67482. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67784/0.67480. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67800/0.67478. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67752/0.67484. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67690/0.67490. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67626/0.67495. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67581/0.67492. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67586/0.67499. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67533/0.67509. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67496/0.67516. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67443/0.67527. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67409/0.67536. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.67388/0.67548. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.67320/0.67566. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67262/0.67569. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.67232/0.67594. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67174/0.67609. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67112/0.67632. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.67079/0.67656. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67025/0.67671. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66967/0.67699. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66935/0.67723. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66845/0.67760. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66809/0.67782. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66761/0.67820. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66685/0.67853. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66581/0.67884. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66572/0.67926. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.66526/0.67971. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66499/0.68005. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66428/0.68057. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66388/0.68087. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66328/0.68137. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.66225/0.68181. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66244/0.68226. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66125/0.68282. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.66079/0.68335. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66021/0.68400. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65929/0.68445. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65883/0.68506. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65819/0.68555. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69586/0.69266. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69482/0.69242. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69432/0.69229. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69353/0.69226. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69310/0.69231. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69251/0.69242. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69214/0.69260. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69113/0.69284. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69084/0.69316. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.69355. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69004/0.69400. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68973/0.69452. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68944/0.69508. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68859/0.69572. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68838/0.69641. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68790/0.69716. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68742/0.69797. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68685/0.69890. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68613/0.69985. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68531/0.70087. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68471/0.70195. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68465/0.70302. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68367/0.70422. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68335/0.70534. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68299/0.70643. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68219/0.70757. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68193/0.70868. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68126/0.70980. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68146/0.71083. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68074/0.71179. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68061/0.71275. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68022/0.71366. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67992/0.71455. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67937/0.71544. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67940/0.71630. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67864/0.71709. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67861/0.71786. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67860/0.71860. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67794/0.71936. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67793/0.72006. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67735/0.72074. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67734/0.72145. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67712/0.72212. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67685/0.72265. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67643/0.72328. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67618/0.72389. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67578/0.72455. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67571/0.72508. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67526/0.72561. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67466/0.72611. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67456/0.72667. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67427/0.72726. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67435/0.72779. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67363/0.72828. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.67346/0.72879. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67269/0.72931. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67317/0.72977. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67198/0.73032. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67177/0.73079. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67157/0.73135. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67084/0.73190. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67091/0.73239. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67080/0.73297. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66976/0.73349. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66917/0.73405. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66857/0.73464. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66870/0.73512. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66797/0.73577. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66735/0.73646. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66686/0.73693. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66622/0.73759. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66524/0.73817. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66502/0.73887. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66494/0.73940. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66400/0.73997. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66389/0.74057. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66263/0.74115. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66222/0.74183. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66167/0.74243. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66151/0.74302. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66052/0.74355. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.65967/0.74435. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65868/0.74488. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65802/0.74545. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65752/0.74634. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65760/0.74673. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65573/0.74730. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65528/0.74829. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65469/0.74902. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65428/0.74970. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65308/0.75051. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65277/0.75133. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65208/0.75205. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65160/0.75281. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65127/0.75373. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64992/0.75457. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.64963/0.75555. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64841/0.75655. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64723/0.75758. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64723/0.75867. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69027/0.71080. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68949/0.70988. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68894/0.70935. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68844/0.70907. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68821/0.70887. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68749/0.70880. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68699/0.70879. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68647/0.70888. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68616/0.70901. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68613/0.70919. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68548/0.70939. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68506/0.70964. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68461/0.70991. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68387/0.71028. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68368/0.71062. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68323/0.71099. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68305/0.71138. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68260/0.71178. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68238/0.71220. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68188/0.71264. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68182/0.71300. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68170/0.71342. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68123/0.71377. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68066/0.71423. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68091/0.71461. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68020/0.71500. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68031/0.71529. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68003/0.71564. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67993/0.71599. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67973/0.71627. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67965/0.71647. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67922/0.71674. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67881/0.71695. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.67907/0.71730. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67858/0.71746. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67816/0.71763. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67802/0.71785. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67800/0.71798. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67775/0.71819. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67713/0.71829. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67692/0.71854. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67683/0.71861. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67672/0.71867. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67613/0.71877. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67561/0.71893. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67555/0.71898. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67556/0.71904. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67498/0.71911. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67488/0.71912. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67449/0.71914. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67394/0.71917. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67388/0.71902. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67360/0.71917. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67294/0.71911. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67273/0.71904. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67223/0.71903. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67195/0.71887. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67177/0.71883. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67108/0.71883. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67060/0.71873. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67016/0.71864. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66936/0.71857. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66912/0.71839. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66903/0.71817. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66847/0.71815. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66748/0.71806. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66740/0.71797. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66676/0.71792. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66644/0.71788. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66535/0.71783. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66552/0.71737. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66506/0.71737. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66396/0.71728. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66359/0.71725. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66318/0.71735. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66221/0.71728. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66196/0.71725. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.66148/0.71726. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66112/0.71715. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66021/0.71726. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66003/0.71739. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65907/0.71727. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65907/0.71741. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65802/0.71779. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65774/0.71752. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65705/0.71767. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65692/0.71751. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65554/0.71799. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65532/0.71769. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65447/0.71797. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65408/0.71826. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65323/0.71798. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65286/0.71781. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.65233/0.71866. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65207/0.71872. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65071/0.71909. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.64978/0.71952. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64992/0.71934. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64961/0.71987. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64858/0.72015. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69230/0.69162. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69211/0.69163. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69210/0.69163. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69176/0.69163. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.69164. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69165. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69158/0.69166. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69154/0.69167. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69136/0.69168. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69116/0.69167. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69108/0.69168. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69093/0.69169. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69076/0.69170. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69049/0.69173. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69033/0.69177. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69002/0.69182. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68969/0.69191. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68942/0.69203. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68904/0.69219. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68867/0.69242. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68827/0.69273. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68770/0.69309. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68694/0.69352. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68634/0.69406. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68565/0.69461. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68503/0.69517. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68397/0.69584. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68313/0.69652. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68214/0.69722. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68153/0.69790. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68078/0.69845. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67954/0.69904. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67897/0.69960. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.67770/0.70017. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67719/0.70062. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67616/0.70101. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67523/0.70139. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67449/0.70167. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67336/0.70208. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67244/0.70231. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67181/0.70247. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67086/0.70266. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67017/0.70278. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66930/0.70301. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66869/0.70305. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66748/0.70314. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66700/0.70328. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66638/0.70339. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66505/0.70364. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66469/0.70385. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66387/0.70404. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66278/0.70435. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66214/0.70460. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66143/0.70490. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66074/0.70527. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65977/0.70561. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65911/0.70602. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65864/0.70650. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65829/0.70704. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65756/0.70739. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65658/0.70789. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65615/0.70826. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65562/0.70882. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65502/0.70928. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65379/0.70984. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65282/0.71041. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65279/0.71096. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65171/0.71150. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65091/0.71188. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65024/0.71235. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64954/0.71298. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64840/0.71363. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64818/0.71438. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64718/0.71498. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64709/0.71576. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64564/0.71638. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64420/0.71740. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64374/0.71805. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64322/0.71876. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64245/0.71957. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64167/0.72026. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64068/0.72098. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63982/0.72201. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63908/0.72291. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63770/0.72374. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63732/0.72480. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63694/0.72575. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63621/0.72662. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63468/0.72758. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63488/0.72851. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63336/0.72950. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63255/0.73057. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63078/0.73184. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63048/0.73276. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62951/0.73382. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62901/0.73509. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.62771/0.73642. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62682/0.73744. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.62554/0.73842. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62578/0.73961. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69641/0.69848. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69575/0.69834. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69543/0.69828. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69506/0.69826. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69451/0.69827. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69436/0.69828. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69383/0.69827. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69317/0.69817. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69221/0.69793. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69159/0.69757. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69066/0.69718. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68993/0.69685. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68903/0.69665. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68838/0.69663. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68761/0.69672. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68721/0.69684. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68655/0.69701. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68558/0.69727. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68520/0.69762. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68435/0.69803. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68348/0.69848. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68246/0.69901. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68143/0.69964. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68044/0.70036. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67957/0.70122. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67833/0.70219. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67665/0.70324. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67574/0.70442. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67440/0.70574. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67339/0.70710. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67237/0.70854. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67097/0.70998. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66995/0.71154. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66887/0.71314. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66805/0.71465. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66722/0.71614. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66556/0.71784. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.66508/0.71944. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66385/0.72104. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66342/0.72262. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66216/0.72436. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66152/0.72570. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66022/0.72750. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.65960/0.72903. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65866/0.73059. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65783/0.73206. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65697/0.73369. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65648/0.73521. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65542/0.73704. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65470/0.73859. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65375/0.74013. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65281/0.74196. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65219/0.74368. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65186/0.74514. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65047/0.74652. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.64952/0.74820. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64937/0.74950. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64812/0.75113. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64741/0.75257. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64677/0.75443. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64592/0.75566. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64490/0.75743. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64423/0.75889. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64362/0.75987. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64239/0.76192. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64153/0.76345. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.64092/0.76484. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63997/0.76636. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63941/0.76774. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63847/0.76943. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63800/0.77055. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63698/0.77170. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63573/0.77333. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63482/0.77467. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63419/0.77595. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63299/0.77781. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63231/0.77890. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63132/0.78049. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63118/0.78189. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.62960/0.78357. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62846/0.78501. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62766/0.78633. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.62733/0.78759. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.62704/0.78874. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62539/0.79056. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62401/0.79186. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62312/0.79272. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62281/0.79448. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62181/0.79601. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62009/0.79798. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61957/0.79930. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61876/0.79971. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61792/0.80134. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61672/0.80309. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.61558/0.80414. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.61434/0.80584. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61387/0.80682. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61304/0.80797. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61107/0.80999. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61035/0.81070. Took 0.11 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69472/0.68901. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69432/0.68919. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69355/0.68939. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69283/0.68963. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69232/0.68992. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69194/0.69029. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69153/0.69075. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69073/0.69130. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69021/0.69192. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68970/0.69249. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.69295. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.69338. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68858/0.69368. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68806/0.69402. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68781/0.69420. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68751/0.69437. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68722/0.69454. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68674/0.69467. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68640/0.69481. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68590/0.69488. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68556/0.69503. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68496/0.69519. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68425/0.69528. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68406/0.69529. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68378/0.69541. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68303/0.69558. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68283/0.69573. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68182/0.69574. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68154/0.69592. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68057/0.69623. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67993/0.69650. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67941/0.69676. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67834/0.69724. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67754/0.69753. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67672/0.69812. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67590/0.69853. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67500/0.69907. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67423/0.69961. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67342/0.70035. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67226/0.70089. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67109/0.70157. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67060/0.70223. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66931/0.70310. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66831/0.70399. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66711/0.70479. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66629/0.70562. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66522/0.70670. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66438/0.70750. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66345/0.70835. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66221/0.70923. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66074/0.71016. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65905/0.71127. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65829/0.71230. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65766/0.71334. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65561/0.71419. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65492/0.71505. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65325/0.71618. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.65215/0.71736. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65010/0.71859. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64981/0.71973. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64889/0.72085. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64728/0.72176. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64556/0.72300. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64385/0.72432. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64400/0.72540. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64161/0.72676. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64094/0.72827. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63976/0.72899. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63804/0.73014. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63713/0.73120. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63620/0.73222. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63471/0.73314. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63448/0.73458. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63171/0.73560. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63144/0.73665. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62920/0.73791. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62795/0.73945. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62698/0.74081. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.62472/0.74168. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62417/0.74316. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62236/0.74427. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62132/0.74599. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62115/0.74690. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61793/0.74819. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.61749/0.74989. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61765/0.75025. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61509/0.75147. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61410/0.75298. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61224/0.75422. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61115/0.75565. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.60948/0.75693. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60808/0.75803. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60755/0.75949. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.60556/0.76057. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60538/0.76158. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60409/0.76270. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60287/0.76390. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60163/0.76488. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60039/0.76606. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.59887/0.76656. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69409/0.70033. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69387/0.69986. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69376/0.69948. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69346/0.69916. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69311/0.69888. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69303/0.69864. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69289/0.69843. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69270/0.69825. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69279/0.69811. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69267/0.69796. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69243/0.69783. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69234/0.69772. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69223/0.69761. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69202/0.69753. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69190/0.69744. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69179/0.69737. Took 0.13 sec\n",
      "Epoch 16, Loss(train/val) 0.69170/0.69729. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69126/0.69720. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69117/0.69710. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69097/0.69705. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.69092/0.69698. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69054/0.69688. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.69024/0.69675. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.69000/0.69663. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68976/0.69655. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68962/0.69648. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68921/0.69644. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68902/0.69643. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68878/0.69638. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68843/0.69635. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68788/0.69629. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68761/0.69627. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68746/0.69626. Took 0.13 sec\n",
      "Epoch 33, Loss(train/val) 0.68693/0.69629. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68642/0.69632. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68594/0.69637. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68582/0.69655. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68524/0.69666. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68465/0.69678. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68407/0.69694. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68380/0.69725. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68324/0.69742. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68278/0.69766. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68243/0.69809. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68165/0.69832. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68121/0.69878. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68052/0.69935. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68012/0.69978. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67952/0.70014. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67874/0.70036. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67847/0.70106. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67732/0.70144. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67671/0.70193. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67633/0.70252. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67530/0.70303. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67471/0.70336. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67420/0.70414. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67375/0.70478. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67257/0.70523. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67231/0.70556. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67162/0.70598. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67077/0.70652. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67001/0.70696. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66907/0.70782. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66816/0.70824. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66800/0.70941. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66698/0.70930. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66534/0.71017. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66460/0.71043. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66355/0.71148. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66298/0.71173. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66149/0.71272. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66100/0.71332. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65934/0.71437. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65867/0.71529. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65761/0.71577. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65584/0.71701. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65531/0.71756. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65367/0.71847. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65283/0.71960. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65169/0.72101. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64994/0.72184. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64910/0.72201. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64770/0.72356. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64626/0.72426. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64522/0.72576. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64324/0.72623. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64167/0.72835. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64064/0.72971. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63931/0.73063. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63841/0.73225. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63657/0.73249. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63478/0.73423. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63389/0.73593. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63256/0.73676. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63090/0.73894. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62899/0.73929. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62826/0.74161. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62656/0.74361. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62482/0.74491. Took 0.10 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69884/0.69222. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69666/0.69149. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69524/0.69115. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69398/0.69111. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69313/0.69130. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69311/0.69156. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69258/0.69180. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69235/0.69201. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69203/0.69218. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69214/0.69228. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69163/0.69236. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69171/0.69244. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69171/0.69250. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69164/0.69258. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69127/0.69265. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69118/0.69272. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69110/0.69279. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69126/0.69283. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69077/0.69290. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69079/0.69296. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69061/0.69301. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69033/0.69307. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69039/0.69314. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.69007/0.69320. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69007/0.69327. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68972/0.69335. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68963/0.69346. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68941/0.69356. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68936/0.69363. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68911/0.69371. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68883/0.69382. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68871/0.69394. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68856/0.69408. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68818/0.69419. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68796/0.69434. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68780/0.69449. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68781/0.69462. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68753/0.69479. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68742/0.69496. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68691/0.69514. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.68658/0.69534. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68643/0.69551. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68611/0.69568. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68611/0.69588. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68560/0.69605. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68535/0.69623. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68511/0.69647. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68480/0.69667. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68472/0.69690. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68453/0.69713. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68384/0.69739. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68389/0.69764. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68317/0.69784. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68289/0.69812. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68275/0.69841. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68181/0.69862. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68156/0.69887. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68111/0.69915. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.68121/0.69944. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68068/0.69973. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68025/0.70012. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67980/0.70041. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67925/0.70084. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67850/0.70119. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67838/0.70146. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67749/0.70192. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67701/0.70229. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67631/0.70267. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67575/0.70302. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67520/0.70337. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67436/0.70378. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67414/0.70412. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67282/0.70449. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67253/0.70507. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67105/0.70542. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67059/0.70578. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.67016/0.70612. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66918/0.70649. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66785/0.70688. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66706/0.70734. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66577/0.70766. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66490/0.70796. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66416/0.70841. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66284/0.70889. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66217/0.70936. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66104/0.70971. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66051/0.71000. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65912/0.71056. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65823/0.71080. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65680/0.71124. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65580/0.71147. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65421/0.71184. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65492/0.71224. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65294/0.71243. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.65142/0.71263. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65039/0.71288. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64836/0.71309. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64851/0.71351. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64700/0.71381. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64639/0.71396. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69304/0.69579. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69250/0.69627. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69262/0.69665. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69689. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69708. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69216/0.69719. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69723. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69164/0.69723. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69178/0.69721. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69139/0.69717. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69118/0.69712. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69174/0.69713. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69123/0.69708. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69072/0.69705. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.69130/0.69702. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69082/0.69698. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69057/0.69699. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69081/0.69698. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69075/0.69697. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.69050/0.69690. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69060/0.69685. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69034/0.69685. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69030/0.69687. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69009/0.69690. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69006/0.69692. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69013/0.69689. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68961/0.69697. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68931/0.69699. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68958/0.69698. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68964/0.69703. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68923/0.69717. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68902/0.69719. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68857/0.69728. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68845/0.69740. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68868/0.69756. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68840/0.69765. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68817/0.69775. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68770/0.69788. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68776/0.69805. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68775/0.69825. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68735/0.69844. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68707/0.69853. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68663/0.69872. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68650/0.69895. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68598/0.69923. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68604/0.69953. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68580/0.69988. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68497/0.70015. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68507/0.70043. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68487/0.70076. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68446/0.70100. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68421/0.70128. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68383/0.70153. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68358/0.70198. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68317/0.70233. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.68293/0.70256. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68230/0.70287. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68234/0.70331. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68188/0.70375. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68122/0.70408. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68100/0.70443. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.68038/0.70483. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68010/0.70498. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67980/0.70544. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67930/0.70580. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67873/0.70623. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67843/0.70664. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67808/0.70694. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67754/0.70725. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67684/0.70763. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67635/0.70784. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67566/0.70829. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67548/0.70857. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67461/0.70892. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67394/0.70927. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67340/0.70954. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67267/0.71012. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67211/0.71023. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67138/0.71055. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67075/0.71118. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66992/0.71132. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66936/0.71173. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66881/0.71243. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66796/0.71242. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66757/0.71293. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66681/0.71327. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66596/0.71362. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66533/0.71378. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.66487/0.71416. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66372/0.71450. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66303/0.71474. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.66183/0.71566. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66120/0.71540. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66024/0.71583. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66022/0.71639. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65871/0.71677. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65816/0.71729. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65795/0.71739. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65671/0.71788. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65651/0.71794. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69234/0.69899. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69901. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.69904. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69216/0.69909. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.69914. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69158/0.69922. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69145/0.69931. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69176/0.69940. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69152/0.69953. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69115/0.69968. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69092/0.69984. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69099/0.70005. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69057/0.70024. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69031/0.70047. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68989/0.70070. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68966/0.70099. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68925/0.70133. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68908/0.70168. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68881/0.70209. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68849/0.70251. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68744/0.70301. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68726/0.70354. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68676/0.70411. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68599/0.70472. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68572/0.70546. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68493/0.70620. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68415/0.70699. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68354/0.70785. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68228/0.70873. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68166/0.70970. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68065/0.71074. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67962/0.71196. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67895/0.71300. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67777/0.71448. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67670/0.71571. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67595/0.71705. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67428/0.71840. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67349/0.71998. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67222/0.72158. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67123/0.72303. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67003/0.72460. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66859/0.72618. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66704/0.72791. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66583/0.72963. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66485/0.73129. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66296/0.73306. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66175/0.73471. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65972/0.73643. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.65797/0.73838. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65678/0.74010. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65486/0.74197. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65280/0.74417. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65180/0.74628. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65006/0.74822. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64780/0.75047. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64657/0.75265. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64394/0.75500. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.64189/0.75744. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64141/0.75974. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63950/0.76209. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.63729/0.76478. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63471/0.76748. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63271/0.76992. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63158/0.77222. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.62869/0.77519. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.62737/0.77818. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.62519/0.78088. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62342/0.78339. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.62155/0.78602. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61971/0.78917. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.61776/0.79129. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.61610/0.79427. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.61378/0.79671. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.61337/0.79992. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.60950/0.80142. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.60944/0.80458. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.60650/0.80759. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.60567/0.80989. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60166/0.81318. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60048/0.81539. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.59795/0.81794. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.59586/0.82078. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.59425/0.82355. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.59215/0.82633. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.59129/0.82786. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.58735/0.83145. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.58588/0.83457. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.58514/0.83760. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.58249/0.83938. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.57907/0.84071. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.57849/0.84369. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.57551/0.84631. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.57349/0.84886. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.57231/0.85103. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.56838/0.85412. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.56868/0.85539. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.56478/0.85844. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.56183/0.86069. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.56212/0.86312. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.55773/0.86564. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69935/0.70278. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69863/0.70175. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69773/0.70075. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69734/0.69951. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69635/0.69790. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69530/0.69580. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69423/0.69376. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69331/0.69216. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69263/0.69111. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69237/0.69047. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69198/0.69008. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69182/0.68982. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69154/0.68965. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69151/0.68954. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69156/0.68943. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69138/0.68933. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.69120/0.68925. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69091/0.68919. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69090/0.68909. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69074/0.68898. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69055/0.68894. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69051/0.68887. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.69009/0.68877. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68985/0.68871. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68971/0.68860. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68955/0.68849. Took 0.11 sec\n",
      "Epoch 26, Loss(train/val) 0.68912/0.68837. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68897/0.68826. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68881/0.68816. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68871/0.68805. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68842/0.68791. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68799/0.68781. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68800/0.68771. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68764/0.68757. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68720/0.68743. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68710/0.68729. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68636/0.68716. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68642/0.68701. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.68617/0.68690. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68545/0.68680. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68522/0.68672. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68496/0.68656. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68464/0.68646. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68416/0.68638. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68353/0.68631. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68347/0.68620. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68319/0.68608. Took 0.13 sec\n",
      "Epoch 47, Loss(train/val) 0.68236/0.68603. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68198/0.68600. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68148/0.68597. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68063/0.68592. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68056/0.68591. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67998/0.68590. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67923/0.68593. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67882/0.68588. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67799/0.68589. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67770/0.68600. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67659/0.68606. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67616/0.68611. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67598/0.68621. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67484/0.68623. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67466/0.68638. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.67341/0.68652. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67267/0.68670. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67258/0.68682. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67128/0.68699. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67080/0.68722. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67018/0.68748. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66934/0.68765. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66879/0.68779. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66725/0.68811. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66661/0.68847. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66609/0.68878. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66513/0.68919. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66475/0.68961. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66360/0.69019. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66285/0.69055. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66218/0.69096. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66132/0.69143. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66064/0.69204. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65966/0.69249. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65894/0.69295. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65764/0.69342. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65680/0.69416. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65581/0.69477. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65522/0.69538. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65448/0.69610. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65277/0.69673. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.65292/0.69731. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65175/0.69797. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.65090/0.69887. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64992/0.69955. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.64895/0.70022. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64815/0.70115. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64726/0.70200. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64596/0.70281. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64506/0.70353. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64504/0.70460. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64405/0.70529. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64310/0.70624. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69171/0.70130. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69128/0.70112. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69117/0.70092. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69087/0.70073. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69091/0.70052. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.70029. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69076/0.70005. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69074/0.69979. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69037/0.69950. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69015/0.69922. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68958/0.69892. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68949/0.69860. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68902/0.69825. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68889/0.69790. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68866/0.69748. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68813/0.69704. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68795/0.69660. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68747/0.69610. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68720/0.69558. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68660/0.69506. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68619/0.69459. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68544/0.69413. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68463/0.69361. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68429/0.69313. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68395/0.69276. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68291/0.69255. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68261/0.69219. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68174/0.69193. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68104/0.69173. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68038/0.69150. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67944/0.69118. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67867/0.69103. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67812/0.69090. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67743/0.69072. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67672/0.69059. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67545/0.69048. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67441/0.69050. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67306/0.69041. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67235/0.69050. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67113/0.69037. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66990/0.69035. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66899/0.69033. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66736/0.69051. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66670/0.69065. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66520/0.69073. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66365/0.69075. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66287/0.69094. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66041/0.69095. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65928/0.69116. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.65756/0.69125. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65584/0.69125. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65471/0.69141. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65317/0.69144. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65134/0.69175. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65013/0.69175. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.64914/0.69194. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64768/0.69178. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.64590/0.69183. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64409/0.69198. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64284/0.69225. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64110/0.69223. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.63992/0.69212. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63861/0.69247. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63625/0.69269. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63644/0.69259. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63444/0.69282. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63144/0.69268. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63123/0.69247. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62991/0.69263. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.62851/0.69348. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62721/0.69292. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62529/0.69326. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.62373/0.69357. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62232/0.69404. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62138/0.69385. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61890/0.69446. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61805/0.69463. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61620/0.69487. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61504/0.69635. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61289/0.69628. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61073/0.69850. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61033/0.69890. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.60803/0.69994. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.60689/0.70103. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.60501/0.70221. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60222/0.70235. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60090/0.70407. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.59958/0.70614. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59860/0.70634. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.59561/0.70877. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.59324/0.70989. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59145/0.71352. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59050/0.71626. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.58783/0.71829. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58718/0.71902. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58493/0.72107. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.58336/0.72366. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58183/0.72672. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58010/0.72732. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57722/0.72884. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69255/0.69067. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69223/0.69070. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69072. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69182/0.69076. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69174/0.69080. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69152/0.69084. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69158/0.69087. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69138/0.69091. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69121/0.69097. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69117/0.69101. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69096/0.69106. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69082/0.69112. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69085/0.69119. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69051/0.69124. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69028/0.69130. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69013/0.69137. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68981/0.69144. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68950/0.69155. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68929/0.69167. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68879/0.69181. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68867/0.69194. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68826/0.69210. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.69231. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68739/0.69256. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68687/0.69286. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68633/0.69318. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68559/0.69359. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68486/0.69403. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68406/0.69472. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68323/0.69551. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68241/0.69615. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68120/0.69703. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68027/0.69810. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67936/0.69913. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67842/0.70017. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67694/0.70139. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67629/0.70255. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67522/0.70379. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67405/0.70501. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67285/0.70606. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67177/0.70706. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67093/0.70809. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66940/0.70919. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.66851/0.71020. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66732/0.71130. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66614/0.71227. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66536/0.71305. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66372/0.71417. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66272/0.71507. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66091/0.71605. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65968/0.71698. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65802/0.71807. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65662/0.71919. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.65539/0.72013. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65362/0.72119. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65200/0.72196. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65044/0.72275. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64908/0.72362. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.64695/0.72429. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64536/0.72507. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64320/0.72566. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64171/0.72630. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64001/0.72673. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63852/0.72732. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63675/0.72780. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63491/0.72816. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63357/0.72857. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63121/0.72893. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62999/0.72938. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.62916/0.72980. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.62684/0.72991. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62605/0.73014. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62440/0.73063. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.62306/0.73070. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62122/0.73082. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62007/0.73069. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61906/0.73067. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.61726/0.73061. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.61489/0.73094. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.61376/0.73146. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61255/0.73166. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61092/0.73172. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.60957/0.73184. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60835/0.73252. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60601/0.73217. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.60575/0.73248. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60344/0.73239. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60178/0.73305. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.60036/0.73332. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59898/0.73391. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.59776/0.73396. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59646/0.73439. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.59545/0.73487. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59419/0.73585. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.59304/0.73640. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59153/0.73700. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.58988/0.73720. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.58791/0.73824. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.58775/0.73890. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58552/0.73968. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69140/0.70031. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69085/0.69999. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69083/0.69968. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.69939. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69038/0.69917. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69016/0.69898. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68978/0.69881. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68963/0.69867. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68929/0.69860. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68914/0.69856. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68883/0.69857. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68846/0.69858. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68808/0.69861. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68769/0.69869. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68721/0.69878. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68667/0.69893. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68626/0.69908. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68578/0.69922. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68496/0.69935. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68468/0.69942. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68424/0.69952. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68355/0.69953. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68288/0.69962. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68254/0.69957. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68183/0.69945. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68150/0.69931. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68083/0.69913. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68039/0.69908. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67995/0.69874. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67928/0.69845. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67902/0.69829. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67835/0.69805. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67788/0.69781. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67753/0.69758. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67679/0.69725. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67639/0.69700. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67589/0.69677. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67513/0.69658. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67447/0.69619. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67457/0.69602. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67372/0.69582. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67292/0.69566. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67236/0.69542. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67149/0.69524. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67089/0.69510. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67062/0.69499. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66967/0.69476. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66877/0.69462. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66820/0.69453. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66798/0.69461. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66679/0.69436. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66582/0.69443. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66497/0.69444. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66452/0.69433. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66374/0.69450. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66301/0.69430. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66225/0.69431. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66167/0.69438. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66052/0.69433. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65951/0.69463. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65933/0.69459. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65766/0.69473. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65678/0.69475. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65574/0.69495. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65452/0.69554. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65433/0.69552. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65290/0.69594. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65206/0.69645. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65070/0.69690. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64954/0.69714. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64884/0.69765. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64709/0.69830. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.64665/0.69890. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64609/0.69969. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64484/0.69995. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64274/0.70055. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64250/0.70165. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64155/0.70231. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63959/0.70297. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63922/0.70371. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63840/0.70414. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63653/0.70485. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63640/0.70552. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63479/0.70627. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.63382/0.70739. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63329/0.70826. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63153/0.70815. Took 0.12 sec\n",
      "Epoch 87, Loss(train/val) 0.63141/0.70911. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63005/0.71010. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62973/0.71091. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62807/0.71112. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62757/0.71200. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62686/0.71293. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62603/0.71321. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62434/0.71429. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62313/0.71490. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.62263/0.71555. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62103/0.71607. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62027/0.71662. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61918/0.71774. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69537/0.69263. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69491/0.69294. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69457/0.69323. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69401/0.69356. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69369/0.69397. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69338/0.69446. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69263/0.69512. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69225/0.69588. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69174/0.69663. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69129/0.69731. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69087/0.69784. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69076/0.69827. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69039/0.69861. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69064/0.69891. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68994/0.69920. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68960/0.69946. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68898/0.69972. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68877/0.70000. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68875/0.70033. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68789/0.70066. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68796/0.70102. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68748/0.70132. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68696/0.70167. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68631/0.70211. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68593/0.70250. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68559/0.70299. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68532/0.70344. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68453/0.70397. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68384/0.70461. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68343/0.70528. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68245/0.70594. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68270/0.70668. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68221/0.70739. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68164/0.70805. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68106/0.70884. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68053/0.70943. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68060/0.71043. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67990/0.71111. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67944/0.71181. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67940/0.71246. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67813/0.71319. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67790/0.71390. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67746/0.71473. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67732/0.71550. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67693/0.71624. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67580/0.71698. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67537/0.71763. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67493/0.71847. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67417/0.71905. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67329/0.71990. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67298/0.72069. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67223/0.72112. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67137/0.72211. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67145/0.72280. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66996/0.72334. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66933/0.72404. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66860/0.72476. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66803/0.72537. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66741/0.72602. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66623/0.72688. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66529/0.72746. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66476/0.72890. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66310/0.72933. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66327/0.73036. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66176/0.73080. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66079/0.73190. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66017/0.73286. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65959/0.73402. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65893/0.73496. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65759/0.73579. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65727/0.73693. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65618/0.73838. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.65450/0.73951. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65489/0.74062. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65410/0.74167. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65312/0.74317. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65236/0.74427. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65146/0.74527. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65141/0.74690. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64961/0.74770. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64923/0.74903. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64848/0.75068. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64769/0.75208. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.64662/0.75312. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64672/0.75417. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64583/0.75572. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64426/0.75661. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64380/0.75835. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64327/0.75929. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64191/0.76070. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64099/0.76200. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64109/0.76316. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63987/0.76443. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63881/0.76605. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63822/0.76760. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63839/0.76905. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.63647/0.77010. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63588/0.77180. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63500/0.77345. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63365/0.77448. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69290/0.69200. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69266/0.69208. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69262/0.69215. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69257/0.69222. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69244/0.69229. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69235/0.69237. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69225/0.69243. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69219/0.69249. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69211/0.69256. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69197/0.69262. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69199/0.69269. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69181/0.69276. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69173/0.69283. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69176/0.69291. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69134/0.69298. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.69142/0.69306. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69122/0.69313. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69083/0.69323. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69076/0.69329. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69048/0.69337. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69027/0.69343. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69039/0.69353. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69013/0.69362. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68956/0.69374. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68938/0.69393. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68923/0.69411. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68878/0.69435. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68827/0.69463. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68807/0.69495. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68768/0.69534. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68722/0.69577. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68678/0.69631. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68639/0.69689. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68604/0.69753. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68540/0.69819. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68475/0.69902. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68446/0.69986. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68379/0.70076. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68345/0.70171. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68269/0.70274. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68245/0.70380. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68172/0.70490. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68083/0.70618. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68041/0.70743. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67988/0.70855. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67898/0.70982. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67874/0.71108. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67796/0.71241. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67708/0.71366. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67638/0.71507. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67592/0.71634. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67542/0.71765. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67431/0.71893. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67339/0.72023. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67287/0.72162. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67235/0.72277. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67175/0.72431. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67103/0.72565. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66998/0.72702. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66981/0.72812. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66864/0.72967. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66809/0.73097. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66746/0.73231. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66656/0.73365. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66573/0.73504. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66459/0.73657. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66432/0.73771. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66307/0.73917. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66307/0.74028. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66182/0.74164. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66094/0.74276. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66021/0.74406. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65955/0.74509. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65938/0.74660. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65860/0.74759. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65733/0.74896. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65661/0.74987. Took 0.12 sec\n",
      "Epoch 77, Loss(train/val) 0.65515/0.75118. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65528/0.75217. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65480/0.75335. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65401/0.75423. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65233/0.75502. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65221/0.75592. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65163/0.75667. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65022/0.75756. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64963/0.75890. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64923/0.75986. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64822/0.76045. Took 0.08 sec\n",
      "Epoch 88, Loss(train/val) 0.64709/0.76146. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.64732/0.76185. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64565/0.76273. Took 0.08 sec\n",
      "Epoch 91, Loss(train/val) 0.64437/0.76347. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64456/0.76434. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64389/0.76441. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64210/0.76641. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64086/0.76643. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64075/0.76758. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63920/0.76797. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63834/0.76948. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63708/0.76966. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69575/0.70046. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69591/0.70042. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69563/0.70038. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69560/0.70033. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69542/0.70030. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69523/0.70028. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69502/0.70025. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69494/0.70023. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69480/0.70021. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69437/0.70019. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69416/0.70015. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69414/0.70005. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69399/0.69990. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69355/0.69963. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69302/0.69913. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69272/0.69827. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69203/0.69711. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69140/0.69558. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69067/0.69420. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69011/0.69318. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68970/0.69237. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68907/0.69204. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68873/0.69196. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68831/0.69197. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68785/0.69196. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68718/0.69212. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68669/0.69222. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68633/0.69233. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68591/0.69236. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68516/0.69252. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68488/0.69250. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68416/0.69236. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68381/0.69233. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68333/0.69235. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68273/0.69221. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68220/0.69222. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68191/0.69241. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68123/0.69225. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68067/0.69167. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68042/0.69154. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67958/0.69160. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67905/0.69096. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67874/0.69080. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67828/0.69077. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67787/0.69071. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67740/0.69038. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67711/0.69025. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67637/0.68974. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67604/0.68957. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67570/0.68923. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67521/0.68940. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67476/0.68938. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67407/0.68907. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67376/0.68850. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67346/0.68832. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67290/0.68846. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67263/0.68778. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67217/0.68796. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67151/0.68782. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67139/0.68753. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67087/0.68764. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67016/0.68729. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67002/0.68647. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66941/0.68711. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66898/0.68668. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66872/0.68637. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66806/0.68694. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66821/0.68665. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66743/0.68635. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66667/0.68631. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66636/0.68654. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66585/0.68641. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66548/0.68604. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66486/0.68601. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66433/0.68622. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66396/0.68613. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66341/0.68593. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66283/0.68639. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66235/0.68608. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66205/0.68602. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66078/0.68612. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66060/0.68569. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66046/0.68611. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65916/0.68625. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65886/0.68661. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65859/0.68613. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65766/0.68669. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65723/0.68635. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.65624/0.68687. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65606/0.68681. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65539/0.68679. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65448/0.68734. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65407/0.68747. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65330/0.68766. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65273/0.68757. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65144/0.68855. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65077/0.68820. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65036/0.68805. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64963/0.68910. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64919/0.68903. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69326/0.69202. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69322/0.69218. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69308/0.69236. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69284/0.69277. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69262/0.69301. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69240/0.69330. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69195/0.69361. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69177/0.69395. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69157/0.69433. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69158/0.69473. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69105/0.69515. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69092/0.69557. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69054/0.69599. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69054/0.69642. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69031/0.69683. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68983/0.69722. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68959/0.69760. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68956/0.69801. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68916/0.69840. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68868/0.69879. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68814/0.69919. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68776/0.69958. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68734/0.69990. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68682/0.70030. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68625/0.70062. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68566/0.70099. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68541/0.70130. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68477/0.70158. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68389/0.70186. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68363/0.70209. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68310/0.70238. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68259/0.70260. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68215/0.70280. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68138/0.70307. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68136/0.70310. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68030/0.70332. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67989/0.70362. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67932/0.70365. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67942/0.70400. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67881/0.70432. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67865/0.70449. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67776/0.70473. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67705/0.70502. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67705/0.70519. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67655/0.70537. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67640/0.70561. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67519/0.70604. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67512/0.70612. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67459/0.70646. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67389/0.70669. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67379/0.70734. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67325/0.70765. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67287/0.70772. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67196/0.70842. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67177/0.70864. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67108/0.70877. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67091/0.70930. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67043/0.70972. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66976/0.70998. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66873/0.71064. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66878/0.71118. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66808/0.71127. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66802/0.71207. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66698/0.71233. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66716/0.71279. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66649/0.71312. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66537/0.71335. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66533/0.71389. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66465/0.71399. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66453/0.71461. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66393/0.71511. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66306/0.71585. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66264/0.71637. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66108/0.71689. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66125/0.71750. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66034/0.71814. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66029/0.71879. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65918/0.71933. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65915/0.71998. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65819/0.72056. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65789/0.72103. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65703/0.72180. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65617/0.72229. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65471/0.72347. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65515/0.72402. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65547/0.72442. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65441/0.72519. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65320/0.72620. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65308/0.72685. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65185/0.72794. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65220/0.72851. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65052/0.72925. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65036/0.73031. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64899/0.73142. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64805/0.73265. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64821/0.73342. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64683/0.73453. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64642/0.73524. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64589/0.73628. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69358/0.68594. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.68575. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69303/0.68559. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69274/0.68546. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69248/0.68526. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69214/0.68505. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.68484. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69142/0.68467. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69115/0.68448. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69095/0.68427. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.68402. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69043/0.68377. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68980/0.68348. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68970/0.68317. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68946/0.68283. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68886/0.68244. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68871/0.68209. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68798/0.68176. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68746/0.68139. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68692/0.68113. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68629/0.68078. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68585/0.68056. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68510/0.68027. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68458/0.68007. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68412/0.68009. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68333/0.68014. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68304/0.68035. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68218/0.68065. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68151/0.68105. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68116/0.68150. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68095/0.68206. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68012/0.68250. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67987/0.68293. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67921/0.68365. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67874/0.68435. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67822/0.68498. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67751/0.68586. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67690/0.68669. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67693/0.68740. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67606/0.68801. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67532/0.68884. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67485/0.68937. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67433/0.69015. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67343/0.69088. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67275/0.69140. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67269/0.69236. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67195/0.69283. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67095/0.69352. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67022/0.69379. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66986/0.69452. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66944/0.69511. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66834/0.69542. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66776/0.69605. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66672/0.69621. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66563/0.69660. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66525/0.69709. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66429/0.69739. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66342/0.69794. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66262/0.69821. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66191/0.69851. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66011/0.69880. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.65993/0.69900. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65840/0.69923. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65780/0.69962. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65728/0.69982. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65566/0.69977. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65484/0.69960. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65319/0.69970. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65325/0.70006. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65179/0.70048. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65099/0.70062. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64970/0.70007. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64938/0.70042. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64793/0.70067. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64702/0.70088. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64521/0.70053. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64469/0.70059. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64361/0.70053. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64172/0.70057. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64085/0.70051. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64069/0.70068. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63914/0.70086. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63846/0.70079. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63615/0.70043. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63539/0.70016. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63546/0.70024. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63457/0.70015. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63248/0.70044. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63153/0.70074. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63024/0.70063. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62859/0.70066. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62782/0.70067. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62672/0.70094. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62633/0.70013. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62521/0.70078. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62305/0.70039. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62279/0.70043. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62115/0.70089. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61946/0.70063. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61935/0.70102. Took 0.10 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69506/0.69371. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69428/0.69375. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69364/0.69383. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69309/0.69400. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69258/0.69431. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69176/0.69478. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69538. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.69036/0.69605. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69006/0.69667. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68933/0.69723. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.69767. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68890/0.69801. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68884/0.69824. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68880/0.69842. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68849/0.69855. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68841/0.69862. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68816/0.69864. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68830/0.69867. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68794/0.69865. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68775/0.69860. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69857. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68727/0.69853. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68729/0.69848. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68702/0.69840. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68664/0.69828. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68640/0.69820. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68624/0.69811. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68571/0.69800. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68552/0.69789. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68564/0.69774. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68491/0.69759. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68469/0.69742. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68436/0.69730. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68432/0.69710. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68419/0.69696. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68384/0.69677. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68335/0.69655. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68304/0.69641. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68262/0.69620. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68244/0.69595. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68209/0.69577. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68179/0.69553. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68151/0.69530. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68081/0.69504. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68059/0.69474. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68065/0.69441. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68002/0.69409. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67979/0.69393. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67963/0.69367. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67895/0.69337. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67849/0.69302. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67796/0.69274. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67739/0.69236. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67741/0.69198. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67683/0.69172. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67649/0.69144. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67607/0.69114. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67548/0.69093. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67497/0.69074. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67422/0.69031. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67423/0.69012. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67359/0.68983. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67270/0.68964. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67212/0.68958. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67159/0.68937. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67126/0.68926. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67073/0.68909. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66975/0.68901. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66919/0.68885. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66826/0.68901. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66772/0.68886. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66717/0.68883. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66666/0.68896. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66545/0.68901. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66482/0.68916. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66398/0.68923. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66301/0.68935. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66239/0.68948. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66129/0.68965. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66086/0.68983. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66012/0.69007. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65906/0.69034. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65802/0.69039. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65740/0.69074. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65647/0.69111. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65576/0.69112. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65569/0.69187. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65422/0.69192. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65288/0.69230. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65353/0.69253. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65196/0.69281. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65144/0.69296. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64996/0.69349. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64957/0.69394. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64898/0.69433. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64803/0.69459. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64730/0.69484. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64607/0.69490. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64542/0.69540. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64512/0.69579. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69376/0.69184. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69346/0.69182. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69256/0.69184. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69224/0.69194. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69187/0.69213. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69138/0.69239. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69088/0.69273. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69059/0.69314. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69033/0.69360. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69021/0.69405. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68925/0.69435. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68934/0.69459. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68959/0.69478. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68882/0.69492. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68888/0.69498. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68845/0.69501. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68857/0.69493. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69488. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.69481. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68841/0.69467. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68793/0.69451. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68824/0.69441. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68781/0.69432. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68751/0.69413. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68717/0.69391. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68707/0.69376. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68684/0.69366. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68668/0.69355. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68670/0.69335. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68655/0.69320. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68595/0.69308. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68567/0.69293. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68588/0.69272. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68583/0.69247. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68524/0.69229. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68492/0.69208. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68470/0.69196. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68473/0.69178. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68458/0.69162. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68386/0.69151. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68360/0.69137. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68349/0.69133. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68244/0.69118. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68211/0.69114. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68249/0.69098. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68182/0.69101. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68132/0.69094. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68080/0.69080. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68038/0.69071. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68011/0.69086. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67973/0.69107. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67881/0.69086. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67857/0.69088. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67818/0.69075. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67725/0.69071. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67786/0.69088. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67664/0.69063. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67672/0.69065. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67635/0.69087. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.67553/0.69076. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67496/0.69069. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67388/0.69061. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67386/0.69031. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67322/0.69048. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67288/0.69040. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67218/0.69039. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67183/0.69034. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67035/0.69012. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.67061/0.69000. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67049/0.68988. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66915/0.68964. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66866/0.68931. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66827/0.68931. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66717/0.68902. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66632/0.68891. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66638/0.68921. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66587/0.68857. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66468/0.68866. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66390/0.68869. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66278/0.68842. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.66208/0.68858. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66188/0.68849. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66161/0.68841. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66089/0.68828. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65972/0.68871. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65972/0.68841. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65840/0.68831. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65782/0.68911. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65718/0.68868. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65654/0.68952. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65590/0.68911. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65441/0.69002. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65412/0.69008. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65369/0.69055. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65172/0.69047. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.65151/0.69154. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65127/0.69218. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65054/0.69316. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64933/0.69246. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64819/0.69408. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69231/0.69245. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69245/0.69246. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.69244. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69163/0.69244. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69151/0.69246. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69124/0.69248. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69099/0.69250. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69084/0.69259. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69031/0.69271. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69020/0.69281. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.69300. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68981/0.69322. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68926/0.69349. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68927/0.69381. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68840/0.69418. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68820/0.69462. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68811/0.69508. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68742/0.69560. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68717/0.69619. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68683/0.69683. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68616/0.69759. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68557/0.69834. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68461/0.69922. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68402/0.70005. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68369/0.70094. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68294/0.70179. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68199/0.70272. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68139/0.70354. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68058/0.70435. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68015/0.70522. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67931/0.70612. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67891/0.70696. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67773/0.70771. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67733/0.70819. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67623/0.70882. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67510/0.70945. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67434/0.71002. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67390/0.71056. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67275/0.71091. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67189/0.71138. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67083/0.71180. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66986/0.71222. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.66883/0.71253. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66790/0.71299. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66682/0.71348. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66577/0.71378. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66446/0.71404. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66366/0.71476. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66192/0.71495. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66157/0.71528. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66040/0.71600. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65908/0.71670. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65803/0.71704. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65723/0.71754. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65585/0.71806. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65453/0.71838. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65386/0.71870. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65271/0.71925. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65095/0.71973. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65046/0.72036. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64921/0.72088. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64764/0.72161. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64693/0.72191. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64645/0.72202. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64460/0.72234. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64302/0.72300. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64226/0.72353. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64136/0.72349. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63994/0.72411. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63912/0.72470. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63757/0.72458. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63717/0.72513. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63685/0.72547. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63476/0.72503. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63387/0.72630. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63343/0.72599. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63218/0.72623. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63061/0.72629. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63006/0.72629. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62897/0.72655. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62765/0.72624. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62675/0.72602. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62606/0.72686. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62494/0.72687. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62378/0.72698. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62268/0.72673. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62134/0.72687. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62131/0.72766. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62035/0.72757. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61887/0.72755. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61821/0.72757. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61706/0.72787. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61670/0.72848. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61473/0.72789. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61366/0.72847. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61388/0.72854. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61288/0.72793. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61145/0.72763. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61123/0.72802. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60869/0.72747. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69220/0.69164. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.69171. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69177/0.69177. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69146/0.69183. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69109/0.69189. Took 0.08 sec\n",
      "Epoch 5, Loss(train/val) 0.69158/0.69194. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69092/0.69200. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69067/0.69206. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69053/0.69214. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69018/0.69221. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68993/0.69229. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68976/0.69238. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68931/0.69248. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68909/0.69260. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68885/0.69273. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68873/0.69289. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68830/0.69305. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68763/0.69325. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68708/0.69345. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68715/0.69370. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68649/0.69399. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68603/0.69430. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68570/0.69464. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68501/0.69503. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68447/0.69548. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68384/0.69598. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68296/0.69651. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68243/0.69706. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68173/0.69774. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68087/0.69840. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68012/0.69918. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67923/0.69994. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67851/0.70064. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67777/0.70138. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67689/0.70207. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67587/0.70286. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67467/0.70357. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67461/0.70419. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67351/0.70475. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67277/0.70554. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67155/0.70623. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67061/0.70686. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66974/0.70747. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66890/0.70802. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66810/0.70860. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66685/0.70938. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66530/0.71003. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66439/0.71063. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66325/0.71143. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66263/0.71218. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66140/0.71291. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65985/0.71372. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65886/0.71448. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65777/0.71519. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.65644/0.71606. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65556/0.71693. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65440/0.71757. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65355/0.71853. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65195/0.71934. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65085/0.72024. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64917/0.72107. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64867/0.72186. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64758/0.72267. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64598/0.72334. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64477/0.72422. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64360/0.72503. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64205/0.72591. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64125/0.72656. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63965/0.72738. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63877/0.72812. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63804/0.72871. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63716/0.72914. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63504/0.73002. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63457/0.73057. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63282/0.73115. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63199/0.73202. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63089/0.73247. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62946/0.73315. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62799/0.73377. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62661/0.73432. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62525/0.73462. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62442/0.73562. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62298/0.73625. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62182/0.73687. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.62136/0.73751. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61973/0.73800. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61777/0.73837. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61685/0.73949. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61554/0.74031. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61465/0.74110. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61321/0.74172. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61225/0.74255. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61104/0.74291. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60974/0.74339. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60820/0.74477. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60811/0.74536. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.60615/0.74649. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60483/0.74689. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60297/0.74780. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60180/0.74943. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69270/0.69731. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69246/0.69716. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69257/0.69701. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69249/0.69686. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69238/0.69670. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69216/0.69654. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69179/0.69636. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69199/0.69619. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69175/0.69601. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69176/0.69583. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69191/0.69563. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69127/0.69543. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69142/0.69521. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69134/0.69497. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69111/0.69472. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69075/0.69445. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69056/0.69414. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69034/0.69379. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69051/0.69342. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68981/0.69301. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68969/0.69255. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68943/0.69200. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68933/0.69142. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68871/0.69078. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68855/0.69010. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68790/0.68928. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68754/0.68846. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68701/0.68753. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68608/0.68659. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68538/0.68547. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68503/0.68467. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68381/0.68374. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68300/0.68272. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68228/0.68216. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68155/0.68106. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68025/0.68046. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67917/0.67976. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67801/0.67897. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67661/0.67818. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67555/0.67759. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67414/0.67687. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67339/0.67589. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67169/0.67532. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67043/0.67442. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66933/0.67364. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66814/0.67290. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66663/0.67215. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66603/0.67146. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66512/0.67092. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66379/0.67016. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66283/0.66940. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66165/0.66917. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66062/0.66804. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65990/0.66782. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65903/0.66746. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65814/0.66681. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65677/0.66613. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.65532/0.66582. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65466/0.66569. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65343/0.66515. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65276/0.66466. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65175/0.66469. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.65057/0.66426. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64941/0.66441. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64844/0.66384. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64668/0.66329. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64636/0.66419. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64570/0.66359. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64463/0.66391. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64304/0.66354. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64217/0.66319. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64020/0.66272. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64000/0.66316. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63927/0.66328. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63792/0.66358. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63621/0.66345. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63498/0.66329. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63470/0.66430. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63206/0.66393. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63094/0.66437. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63049/0.66452. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62991/0.66523. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62714/0.66690. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62655/0.66636. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62500/0.66627. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62416/0.66776. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62284/0.66825. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62143/0.66969. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61898/0.67061. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61843/0.67201. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61729/0.67249. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61495/0.67300. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.61432/0.67510. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61225/0.67620. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61123/0.67656. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60964/0.67796. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60924/0.67954. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60761/0.68133. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60670/0.68238. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60349/0.68243. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69424/0.69370. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69397/0.69363. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69382/0.69354. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69337/0.69347. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69323/0.69339. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69297/0.69332. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69261/0.69325. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69239/0.69320. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69217/0.69315. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69171/0.69311. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69146/0.69309. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69106/0.69308. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69060/0.69312. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69022/0.69316. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69017/0.69319. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68974/0.69324. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68936/0.69331. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68910/0.69339. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68843/0.69349. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68788/0.69361. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68753/0.69371. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68719/0.69383. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68697/0.69395. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68598/0.69410. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68505/0.69429. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68445/0.69448. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68346/0.69470. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68265/0.69500. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68151/0.69535. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68076/0.69573. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67956/0.69619. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67883/0.69667. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67744/0.69726. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67652/0.69791. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67562/0.69857. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67458/0.69928. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67376/0.70003. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67227/0.70077. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67175/0.70155. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67022/0.70233. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66921/0.70312. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66826/0.70389. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66715/0.70450. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66625/0.70520. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66551/0.70596. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66425/0.70663. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66362/0.70710. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66322/0.70775. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66259/0.70822. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66114/0.70872. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66005/0.70921. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65945/0.70943. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65881/0.70998. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65707/0.71043. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65662/0.71080. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65643/0.71113. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65554/0.71153. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65454/0.71171. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65363/0.71208. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65265/0.71254. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65175/0.71272. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65091/0.71304. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65002/0.71353. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64955/0.71396. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64853/0.71428. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64852/0.71453. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64689/0.71485. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64575/0.71503. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64572/0.71566. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64521/0.71594. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64389/0.71635. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64332/0.71661. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64199/0.71709. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64119/0.71748. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64040/0.71790. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63948/0.71834. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63899/0.71853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63826/0.71879. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63689/0.71919. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63652/0.71936. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63489/0.72003. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63497/0.72032. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63443/0.72074. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63270/0.72124. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63156/0.72164. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63145/0.72226. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63034/0.72252. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62928/0.72291. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62837/0.72338. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62779/0.72328. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62573/0.72366. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62502/0.72371. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62463/0.72455. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62386/0.72481. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62195/0.72548. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62130/0.72595. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62120/0.72620. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62037/0.72631. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61888/0.72671. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61787/0.72686. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69804/0.70229. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69663/0.70037. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69544/0.69864. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69447/0.69716. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69386/0.69596. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69299/0.69512. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69284/0.69453. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69231/0.69412. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69228/0.69386. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69204/0.69365. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69170/0.69348. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69177/0.69331. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69152/0.69319. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69128/0.69311. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69122/0.69302. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69097/0.69297. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.69074/0.69290. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69050/0.69282. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69041/0.69277. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69019/0.69270. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68997/0.69263. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68985/0.69251. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68968/0.69243. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68906/0.69230. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68903/0.69221. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68862/0.69211. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68857/0.69200. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68822/0.69189. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68762/0.69177. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68766/0.69168. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68707/0.69156. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68657/0.69148. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68628/0.69141. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68578/0.69130. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68531/0.69119. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68491/0.69117. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68473/0.69109. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68393/0.69115. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68337/0.69112. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68291/0.69105. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68244/0.69097. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68187/0.69103. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68151/0.69101. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68083/0.69098. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68042/0.69100. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67951/0.69110. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67909/0.69115. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67828/0.69106. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67826/0.69101. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67781/0.69105. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67677/0.69105. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67607/0.69103. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67583/0.69100. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67502/0.69093. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67391/0.69103. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67366/0.69098. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67299/0.69103. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67236/0.69108. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67186/0.69101. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67113/0.69095. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67023/0.69104. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67003/0.69097. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66947/0.69105. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66810/0.69102. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66774/0.69099. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66696/0.69110. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66597/0.69107. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66538/0.69124. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66447/0.69137. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66372/0.69140. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66308/0.69140. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66194/0.69161. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66153/0.69155. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66015/0.69175. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65924/0.69199. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65889/0.69207. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65787/0.69196. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65680/0.69203. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65594/0.69216. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65505/0.69214. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65421/0.69248. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65338/0.69263. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65209/0.69323. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65184/0.69310. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65025/0.69316. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64940/0.69320. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64797/0.69317. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64676/0.69351. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64642/0.69363. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64540/0.69358. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64307/0.69368. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64313/0.69443. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64202/0.69443. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64032/0.69468. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63916/0.69515. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63800/0.69524. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63656/0.69553. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63473/0.69624. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63421/0.69623. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63270/0.69624. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69342/0.69183. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69287/0.69196. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69274/0.69209. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69275/0.69223. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69272/0.69237. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69233/0.69253. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69208/0.69268. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69196/0.69284. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69181/0.69300. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.69315. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69143/0.69333. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69145/0.69351. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69125/0.69369. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69115/0.69389. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69119/0.69409. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69073/0.69430. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69065/0.69452. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69029/0.69474. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69014/0.69495. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69005/0.69518. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68976/0.69543. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68973/0.69568. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68916/0.69594. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68901/0.69620. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68881/0.69649. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68873/0.69678. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68847/0.69707. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68844/0.69736. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68815/0.69766. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68797/0.69796. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68756/0.69824. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68739/0.69854. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68731/0.69883. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68686/0.69910. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68678/0.69937. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68681/0.69963. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68624/0.69991. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68604/0.70016. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68599/0.70042. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68559/0.70066. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68517/0.70089. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68512/0.70111. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68482/0.70131. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68455/0.70153. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68434/0.70171. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68389/0.70188. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68374/0.70205. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68354/0.70220. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68340/0.70237. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68311/0.70249. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68285/0.70264. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68228/0.70276. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68177/0.70288. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68143/0.70300. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68134/0.70308. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68116/0.70319. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68086/0.70325. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68048/0.70329. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67997/0.70333. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67958/0.70340. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67913/0.70345. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67889/0.70350. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67839/0.70355. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67808/0.70356. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67766/0.70356. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67719/0.70358. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67678/0.70357. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67637/0.70355. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67544/0.70354. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67525/0.70352. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67494/0.70350. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67439/0.70347. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67395/0.70342. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67312/0.70338. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67304/0.70335. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67244/0.70331. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67199/0.70326. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67150/0.70323. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67050/0.70320. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67039/0.70320. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67004/0.70320. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66924/0.70320. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66870/0.70326. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66834/0.70329. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66767/0.70327. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66730/0.70326. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66714/0.70328. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66655/0.70331. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66588/0.70338. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66516/0.70341. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66508/0.70346. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66444/0.70356. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66406/0.70367. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66386/0.70382. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66321/0.70387. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66268/0.70391. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66198/0.70401. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66198/0.70410. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66130/0.70422. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66089/0.70427. Took 0.10 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69452/0.68931. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69447/0.68946. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69415/0.68961. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69420/0.68977. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69372/0.68994. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69368/0.69015. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69323/0.69038. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69285/0.69064. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69258/0.69092. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69253/0.69124. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69215/0.69158. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69178/0.69191. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69137/0.69222. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69129/0.69252. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69110/0.69278. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69081/0.69302. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69044/0.69330. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69033/0.69353. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68989/0.69375. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68968/0.69402. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68941/0.69428. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68917/0.69456. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68839/0.69490. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68820/0.69523. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68764/0.69560. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68733/0.69593. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68647/0.69630. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68590/0.69677. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68570/0.69721. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68492/0.69763. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68427/0.69804. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68356/0.69840. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68286/0.69871. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68209/0.69902. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68199/0.69926. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68134/0.69965. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68082/0.69989. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68016/0.70014. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67964/0.70034. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67886/0.70054. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67855/0.70072. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67796/0.70101. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67742/0.70125. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67680/0.70145. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67625/0.70166. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67558/0.70184. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67529/0.70221. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67465/0.70249. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67437/0.70287. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67338/0.70314. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67310/0.70366. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67233/0.70394. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67196/0.70437. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67103/0.70473. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67023/0.70540. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66942/0.70581. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66923/0.70624. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66877/0.70685. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66820/0.70719. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66734/0.70758. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66705/0.70819. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66595/0.70855. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66568/0.70899. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66437/0.70991. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66368/0.71020. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66363/0.71071. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66261/0.71136. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66197/0.71189. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66073/0.71206. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66034/0.71236. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65938/0.71329. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65895/0.71359. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65806/0.71413. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65740/0.71444. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65640/0.71513. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65534/0.71541. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65488/0.71589. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65424/0.71566. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65343/0.71626. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65228/0.71647. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65174/0.71708. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65088/0.71734. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64962/0.71721. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64923/0.71696. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64804/0.71711. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64731/0.71760. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64609/0.71734. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64548/0.71722. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64485/0.71735. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64358/0.71714. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64284/0.71700. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64072/0.71747. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64160/0.71697. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63956/0.71697. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63888/0.71696. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63788/0.71713. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63728/0.71638. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63612/0.71715. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63491/0.71638. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63445/0.71651. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69592/0.69397. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69549/0.69392. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69522/0.69385. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69446/0.69378. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69427/0.69367. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69397/0.69357. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69332/0.69349. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69278/0.69344. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69254/0.69348. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69174/0.69361. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69148/0.69378. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69104/0.69401. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69070/0.69427. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69084/0.69455. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69018/0.69486. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68968/0.69520. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68929/0.69556. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68914/0.69593. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68882/0.69635. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68843/0.69676. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68805/0.69718. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68783/0.69762. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68746/0.69807. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68733/0.69852. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68642/0.69894. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68655/0.69941. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68596/0.69982. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68568/0.70017. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68576/0.70053. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68466/0.70089. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68502/0.70122. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68425/0.70151. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68415/0.70182. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68356/0.70211. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68355/0.70237. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68343/0.70256. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68318/0.70277. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68293/0.70295. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68236/0.70306. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68220/0.70319. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68184/0.70325. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68157/0.70332. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68155/0.70339. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68127/0.70343. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68125/0.70349. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68026/0.70350. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68039/0.70354. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67997/0.70353. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67986/0.70350. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67926/0.70354. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67923/0.70353. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67876/0.70352. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67847/0.70351. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67787/0.70349. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67762/0.70345. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67758/0.70337. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67720/0.70338. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67731/0.70335. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67684/0.70339. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67673/0.70346. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67540/0.70344. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67531/0.70337. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67507/0.70343. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67458/0.70342. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67436/0.70344. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67394/0.70350. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67322/0.70353. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67254/0.70357. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67247/0.70370. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67216/0.70384. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67177/0.70400. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67156/0.70419. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67037/0.70433. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67041/0.70458. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67022/0.70489. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66953/0.70520. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66884/0.70551. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66806/0.70575. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66815/0.70614. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66755/0.70654. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66667/0.70691. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66655/0.70732. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66598/0.70773. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66553/0.70822. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66440/0.70866. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66457/0.70922. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66327/0.70972. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66310/0.71020. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66300/0.71084. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66183/0.71143. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.66070/0.71201. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66079/0.71253. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66002/0.71305. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65897/0.71358. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65937/0.71434. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65783/0.71503. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65811/0.71562. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65726/0.71645. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65686/0.71702. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65591/0.71765. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69383/0.68862. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69372/0.68864. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69342/0.68867. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69368/0.68871. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69346/0.68873. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69344/0.68876. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69305/0.68878. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69312/0.68880. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69298/0.68883. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69327/0.68886. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69284/0.68889. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69283/0.68891. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69251/0.68893. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69249/0.68894. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69211/0.68895. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69219/0.68896. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69181/0.68896. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69176/0.68896. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69149/0.68895. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69133/0.68896. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69132/0.68895. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69078/0.68893. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69054/0.68892. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69004/0.68891. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68952/0.68889. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68929/0.68890. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68904/0.68891. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68822/0.68891. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68768/0.68895. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68686/0.68897. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68614/0.68897. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68506/0.68901. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68430/0.68913. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68287/0.68930. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68198/0.68950. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68088/0.68979. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67972/0.69021. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67813/0.69069. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67711/0.69124. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67556/0.69199. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67463/0.69259. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67287/0.69334. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67148/0.69427. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67013/0.69515. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66842/0.69615. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66713/0.69686. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66582/0.69769. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66397/0.69840. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66265/0.69929. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66140/0.70020. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65906/0.70097. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65788/0.70193. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65672/0.70289. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65469/0.70382. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65355/0.70480. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65106/0.70601. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64994/0.70674. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64885/0.70783. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.64691/0.70883. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64582/0.70982. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64452/0.71111. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64347/0.71209. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64214/0.71325. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64031/0.71428. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.63919/0.71533. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63823/0.71666. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63707/0.71819. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63572/0.71925. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63375/0.72031. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63285/0.72117. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63221/0.72234. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63113/0.72315. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62962/0.72371. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62820/0.72517. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62760/0.72630. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62589/0.72761. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62605/0.72876. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62387/0.72939. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62325/0.73025. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62300/0.73150. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62119/0.73227. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62070/0.73323. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61885/0.73385. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61870/0.73518. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61612/0.73581. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61709/0.73631. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61536/0.73703. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61338/0.73803. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61312/0.73880. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61065/0.73972. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61005/0.74098. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.60946/0.74196. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60893/0.74273. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60642/0.74357. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60765/0.74400. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60579/0.74520. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60402/0.74577. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60428/0.74694. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60329/0.74746. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.60202/0.74848. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69606/0.69188. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69475/0.69017. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69368/0.68876. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69308/0.68772. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69225/0.68704. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69208/0.68659. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69194/0.68628. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69156/0.68606. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.68586. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69120/0.68569. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69100/0.68555. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69076/0.68541. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69055/0.68525. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69018/0.68511. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69011/0.68495. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68978/0.68478. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68952/0.68462. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68933/0.68445. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68913/0.68429. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68862/0.68415. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68855/0.68401. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68832/0.68382. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68785/0.68371. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68724/0.68362. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68680/0.68354. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68652/0.68347. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68595/0.68341. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68531/0.68338. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68480/0.68339. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68442/0.68342. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68404/0.68343. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68343/0.68349. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68294/0.68360. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68265/0.68367. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68182/0.68380. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68156/0.68392. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68073/0.68406. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68029/0.68419. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67977/0.68430. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67954/0.68442. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67912/0.68456. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67867/0.68468. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67810/0.68484. Took 0.08 sec\n",
      "Epoch 43, Loss(train/val) 0.67763/0.68491. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67706/0.68498. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67631/0.68510. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67601/0.68523. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67546/0.68524. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67482/0.68532. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67462/0.68539. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67390/0.68561. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67331/0.68561. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67267/0.68563. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67210/0.68581. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67154/0.68590. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67103/0.68598. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67042/0.68612. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67023/0.68617. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66949/0.68632. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66881/0.68647. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66793/0.68670. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66718/0.68684. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66655/0.68702. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66619/0.68731. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66524/0.68753. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66544/0.68797. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66444/0.68808. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66389/0.68828. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66311/0.68860. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66269/0.68891. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66228/0.68920. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66186/0.68941. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66075/0.68962. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66056/0.69008. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65931/0.69052. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65871/0.69101. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65841/0.69136. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65831/0.69174. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65777/0.69218. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65697/0.69239. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65591/0.69282. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65576/0.69323. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65557/0.69361. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65476/0.69415. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65398/0.69468. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65339/0.69492. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65269/0.69556. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65301/0.69609. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65164/0.69706. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65110/0.69768. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65121/0.69840. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64975/0.69925. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65015/0.69990. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64932/0.70029. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64867/0.70084. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64831/0.70155. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64727/0.70189. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64694/0.70254. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64599/0.70290. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64518/0.70427. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69534/0.68557. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69484/0.68622. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69470/0.68672. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69442/0.68712. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69446/0.68750. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69403/0.68781. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69366/0.68814. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69311/0.68842. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69328/0.68870. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69260/0.68894. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69252/0.68921. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69237/0.68946. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69199/0.68972. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69186/0.68992. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69189/0.69017. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69136/0.69043. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69132/0.69061. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69088/0.69085. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69080/0.69106. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69035/0.69131. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69000/0.69151. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68984/0.69173. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68939/0.69198. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68899/0.69220. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68863/0.69245. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68814/0.69274. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68796/0.69302. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68749/0.69323. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68712/0.69351. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68657/0.69370. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68598/0.69395. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68560/0.69431. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68549/0.69457. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68486/0.69486. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68407/0.69519. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68383/0.69545. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68364/0.69580. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68282/0.69628. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68247/0.69660. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68224/0.69697. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68182/0.69738. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68124/0.69770. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68054/0.69817. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68031/0.69851. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67931/0.69906. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67908/0.69955. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67849/0.70006. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67783/0.70054. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67733/0.70127. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67664/0.70164. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67616/0.70206. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67524/0.70263. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67486/0.70363. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67393/0.70425. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67291/0.70506. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67234/0.70580. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67198/0.70668. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67076/0.70794. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66975/0.70873. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66886/0.70992. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66795/0.71151. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66770/0.71241. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66643/0.71396. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66553/0.71557. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66437/0.71708. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66313/0.71864. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66174/0.72083. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66136/0.72265. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66050/0.72437. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65948/0.72673. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65805/0.72834. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65672/0.73071. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65576/0.73289. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65475/0.73492. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65445/0.73739. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65343/0.73970. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65301/0.74193. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65129/0.74451. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65028/0.74698. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64856/0.74921. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64834/0.75196. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64680/0.75445. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64524/0.75754. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64550/0.75986. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64357/0.76205. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64392/0.76479. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64232/0.76805. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64158/0.76933. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64110/0.77174. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64030/0.77449. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63845/0.77684. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63886/0.77917. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63690/0.78197. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63581/0.78396. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63489/0.78703. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63485/0.78926. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63362/0.79130. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63242/0.79349. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63111/0.79597. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63137/0.79829. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69375/0.69082. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69347/0.69077. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69310/0.69070. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69290/0.69064. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69275/0.69056. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69244/0.69045. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69199/0.69033. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69156/0.69019. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69097/0.69002. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69087/0.68979. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69027/0.68955. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68967/0.68925. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68906/0.68890. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68804/0.68850. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68727/0.68804. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68598/0.68750. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68475/0.68692. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68324/0.68626. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68177/0.68553. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68012/0.68472. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67826/0.68388. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67656/0.68293. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67515/0.68217. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67355/0.68146. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67228/0.68082. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67061/0.68031. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.66955/0.67988. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.66829/0.67956. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.66667/0.67931. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.66533/0.67918. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66438/0.67919. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66339/0.67924. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66218/0.67945. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66105/0.67967. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.65998/0.67994. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.65831/0.68026. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.65746/0.68061. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.65645/0.68110. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65466/0.68178. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65383/0.68229. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65309/0.68295. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65099/0.68369. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65012/0.68422. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.64861/0.68525. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.64741/0.68577. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.64600/0.68686. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.64510/0.68783. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64384/0.68835. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64225/0.68927. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.64095/0.69071. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64021/0.69149. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.63928/0.69224. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.63766/0.69321. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.63710/0.69378. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.63536/0.69550. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.63384/0.69634. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.63308/0.69678. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.63148/0.69858. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.63130/0.69918. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.62945/0.70143. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.62820/0.70141. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.62796/0.70240. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.62639/0.70180. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.62338/0.70537. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.62393/0.70683. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.62222/0.70722. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.62100/0.70816. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.62021/0.70980. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.61881/0.71085. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.61829/0.71257. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.61678/0.71462. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.61646/0.71418. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.61471/0.71651. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.61463/0.71733. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.61205/0.71931. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.61157/0.72178. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.61006/0.72200. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.60950/0.72422. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.60835/0.72627. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.60676/0.72890. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.60552/0.73121. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.60518/0.73349. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.60349/0.73572. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60212/0.73931. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.60143/0.74156. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60152/0.74258. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.59926/0.74783. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.59744/0.74941. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.59849/0.75338. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59729/0.75526. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.59572/0.75736. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59524/0.76214. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59529/0.76410. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59414/0.76606. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.59235/0.76998. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59281/0.77120. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59372/0.77588. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59160/0.77645. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59259/0.77679. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59136/0.77899. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69424/0.70594. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69396/0.70557. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69352/0.70518. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69321/0.70467. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69278/0.70407. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69238/0.70345. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69234/0.70275. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69191/0.70205. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69129/0.70129. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69078/0.70058. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.69996. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.68972/0.69953. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68937/0.69918. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68892/0.69899. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68834/0.69887. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68789/0.69897. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68715/0.69907. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68638/0.69937. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68562/0.69971. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68500/0.70005. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68419/0.70047. Took 0.08 sec\n",
      "Epoch 21, Loss(train/val) 0.68340/0.70101. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68249/0.70149. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68138/0.70219. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68072/0.70293. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67954/0.70376. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.67855/0.70444. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67808/0.70479. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67655/0.70587. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67598/0.70679. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67513/0.70778. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67414/0.70882. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67332/0.70992. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67278/0.71110. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67205/0.71195. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67067/0.71328. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67011/0.71458. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.66936/0.71587. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66802/0.71721. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.66741/0.71841. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66652/0.72013. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66568/0.72165. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66489/0.72264. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66376/0.72372. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66341/0.72511. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66218/0.72688. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66168/0.72794. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66053/0.72928. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66000/0.72981. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65899/0.73191. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65791/0.73304. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65672/0.73512. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65667/0.73595. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65538/0.73727. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65408/0.73877. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65410/0.74066. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65288/0.74130. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65098/0.74368. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65098/0.74403. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65011/0.74599. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64919/0.74673. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64834/0.74831. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64829/0.74981. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64695/0.75016. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64659/0.75220. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64512/0.75375. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64410/0.75492. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64380/0.75527. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64248/0.75802. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64193/0.75879. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64112/0.76025. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64025/0.76103. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63989/0.76189. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63820/0.76348. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63731/0.76554. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63642/0.76625. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63581/0.76723. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63383/0.77072. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63404/0.77043. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63315/0.77278. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63175/0.77383. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63173/0.77343. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63089/0.77607. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63002/0.77790. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62870/0.77912. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62720/0.78140. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62638/0.78162. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62509/0.78516. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62321/0.78542. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62307/0.78764. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62217/0.78989. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62130/0.79223. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61961/0.79214. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61807/0.79567. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61698/0.79628. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61625/0.79850. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61528/0.80065. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.61391/0.80151. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61201/0.80434. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61084/0.80621. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69514/0.69508. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69480/0.69493. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69473/0.69477. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69402/0.69461. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69405/0.69444. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69357/0.69430. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69339/0.69418. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69324/0.69406. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69316/0.69394. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69281/0.69379. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69248/0.69367. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69238/0.69353. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69225/0.69340. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69163/0.69324. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69140/0.69311. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69114/0.69295. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69107/0.69278. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69081/0.69260. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69000/0.69237. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68953/0.69212. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68922/0.69189. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68835/0.69169. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68822/0.69137. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68782/0.69094. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68667/0.69052. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68649/0.69005. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68568/0.68969. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68437/0.68920. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68373/0.68875. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68336/0.68837. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68247/0.68790. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68130/0.68735. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68080/0.68676. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67970/0.68597. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67900/0.68586. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67807/0.68540. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67747/0.68494. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67636/0.68479. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67544/0.68439. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67451/0.68402. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67354/0.68411. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67300/0.68377. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67196/0.68367. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67156/0.68357. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67048/0.68333. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67035/0.68388. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66952/0.68328. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66856/0.68396. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66816/0.68430. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66772/0.68444. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66577/0.68452. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66634/0.68469. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66503/0.68441. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66454/0.68484. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66349/0.68526. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66275/0.68587. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66248/0.68617. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66164/0.68664. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66144/0.68672. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66030/0.68718. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65949/0.68728. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.65913/0.68775. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65886/0.68760. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65783/0.68871. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65703/0.68813. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65752/0.68946. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65664/0.68912. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65534/0.68939. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65483/0.68954. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65379/0.69005. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65390/0.69043. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65221/0.69093. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65180/0.69106. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65213/0.69238. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65093/0.69209. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65078/0.69249. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65005/0.69339. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64969/0.69332. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64840/0.69423. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64770/0.69403. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64671/0.69435. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64630/0.69506. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64616/0.69533. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64483/0.69540. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64342/0.69660. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64407/0.69687. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64264/0.69730. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64202/0.69790. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64122/0.69850. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64020/0.69920. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63973/0.69964. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63842/0.69976. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63763/0.70033. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63678/0.70106. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63689/0.70266. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63524/0.70268. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63519/0.70369. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63456/0.70409. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63426/0.70548. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63167/0.70619. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69162/0.69431. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69171/0.69410. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69139/0.69389. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69134/0.69369. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.69350. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69072/0.69330. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69064/0.69312. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69012/0.69295. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69004/0.69281. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68981/0.69268. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.69257. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68951/0.69249. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68949/0.69243. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68923/0.69240. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68916/0.69239. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68890/0.69240. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68862/0.69244. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68856/0.69247. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68835/0.69252. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68822/0.69258. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68789/0.69266. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68771/0.69274. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68750/0.69286. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68748/0.69297. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68719/0.69307. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68700/0.69320. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68656/0.69336. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68637/0.69349. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68613/0.69363. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68606/0.69379. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68599/0.69398. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68538/0.69418. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68521/0.69438. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68476/0.69458. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68469/0.69479. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68448/0.69503. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68398/0.69524. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68366/0.69548. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68353/0.69573. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68328/0.69601. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68320/0.69627. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68254/0.69653. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68241/0.69683. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68195/0.69708. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68193/0.69736. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68151/0.69767. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68121/0.69793. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68050/0.69828. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68064/0.69854. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.68016/0.69886. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67955/0.69913. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67948/0.69940. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67942/0.69968. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67890/0.69994. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67854/0.70017. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67856/0.70044. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67804/0.70071. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67769/0.70097. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67692/0.70121. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67700/0.70134. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67662/0.70159. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67660/0.70182. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67649/0.70197. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67541/0.70218. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67550/0.70233. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67468/0.70262. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67407/0.70277. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67428/0.70298. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67371/0.70318. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67326/0.70333. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67310/0.70340. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67243/0.70358. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67236/0.70369. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67182/0.70392. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67125/0.70404. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67084/0.70420. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67054/0.70433. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67012/0.70427. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66971/0.70450. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66883/0.70463. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66871/0.70449. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66830/0.70465. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66786/0.70471. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66747/0.70492. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66635/0.70500. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66590/0.70524. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66554/0.70524. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66536/0.70540. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66445/0.70531. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66365/0.70531. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66377/0.70523. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66244/0.70543. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66170/0.70551. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66176/0.70553. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66132/0.70547. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65989/0.70573. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65966/0.70576. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65900/0.70588. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65918/0.70584. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65787/0.70572. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69273/0.69268. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.69256. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.69247. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69215/0.69239. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69193/0.69232. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69226. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69153/0.69219. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69161/0.69213. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69097/0.69209. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69078/0.69204. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69087/0.69200. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69016/0.69196. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69031/0.69193. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68992/0.69191. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69002/0.69193. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68973/0.69194. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68914/0.69196. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68922/0.69200. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68886/0.69207. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68858/0.69214. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68811/0.69223. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68796/0.69233. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68764/0.69242. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68736/0.69252. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68687/0.69264. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68661/0.69273. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68615/0.69285. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68512/0.69294. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68511/0.69305. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68508/0.69316. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68438/0.69325. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68390/0.69333. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68344/0.69335. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68284/0.69337. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68214/0.69346. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68175/0.69355. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68130/0.69358. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68088/0.69359. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67988/0.69366. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67966/0.69375. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67924/0.69380. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67829/0.69392. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67762/0.69399. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67696/0.69405. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67622/0.69416. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67545/0.69427. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67416/0.69432. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67375/0.69440. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67328/0.69457. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67218/0.69470. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67159/0.69488. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67063/0.69511. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66984/0.69530. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66914/0.69548. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66855/0.69567. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66716/0.69589. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66611/0.69616. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66535/0.69640. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66403/0.69665. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66305/0.69685. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66193/0.69717. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66075/0.69745. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65989/0.69778. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65836/0.69811. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65730/0.69846. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65653/0.69885. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65529/0.69924. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65430/0.69979. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65255/0.70014. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65122/0.70071. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64987/0.70114. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64838/0.70165. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64732/0.70232. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64644/0.70304. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64489/0.70363. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64362/0.70457. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64222/0.70531. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64093/0.70614. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63911/0.70687. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63887/0.70793. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63725/0.70900. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63532/0.70999. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63396/0.71126. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63298/0.71229. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63204/0.71343. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63063/0.71439. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62926/0.71548. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62915/0.71671. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62679/0.71800. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62608/0.71910. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62445/0.72012. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62303/0.72195. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62190/0.72303. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62001/0.72415. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61886/0.72564. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61795/0.72667. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61765/0.72843. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61495/0.72935. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61391/0.73089. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61348/0.73268. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69155/0.69063. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69113/0.69041. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69083/0.69022. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69062/0.69005. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69053/0.68990. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69024/0.68975. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69000/0.68963. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68964/0.68953. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68955/0.68945. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68927/0.68937. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68874/0.68931. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68861/0.68928. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68831/0.68924. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68823/0.68922. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68766/0.68920. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68767/0.68920. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68753/0.68919. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68692/0.68920. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68669/0.68924. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68611/0.68928. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68596/0.68932. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68555/0.68938. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68524/0.68944. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68494/0.68951. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68434/0.68958. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68415/0.68969. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68376/0.68983. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68343/0.68999. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68295/0.69012. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68248/0.69026. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68172/0.69042. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68172/0.69062. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68090/0.69083. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68096/0.69109. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68016/0.69137. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67969/0.69168. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67895/0.69201. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67888/0.69234. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67790/0.69272. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67744/0.69313. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67732/0.69352. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67611/0.69404. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67641/0.69448. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67524/0.69497. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67488/0.69553. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67415/0.69606. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67415/0.69673. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67281/0.69733. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67247/0.69794. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67175/0.69858. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67085/0.69931. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67033/0.70013. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66916/0.70095. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66908/0.70165. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66810/0.70250. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66841/0.70323. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66721/0.70410. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66637/0.70479. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66608/0.70580. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66513/0.70658. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66439/0.70733. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66312/0.70822. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66281/0.70919. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66238/0.70994. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66161/0.71071. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66163/0.71157. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66072/0.71247. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65961/0.71326. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65858/0.71405. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65898/0.71506. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65798/0.71583. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65670/0.71653. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65665/0.71722. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65579/0.71820. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65525/0.71901. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65493/0.71973. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65381/0.72056. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65282/0.72143. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65313/0.72201. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65195/0.72273. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65206/0.72347. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65158/0.72428. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65025/0.72512. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65004/0.72596. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64805/0.72661. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64883/0.72731. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64787/0.72792. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64701/0.72862. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64660/0.72961. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64585/0.73025. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64529/0.73100. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64433/0.73177. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64419/0.73239. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64368/0.73324. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64276/0.73424. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64281/0.73508. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64218/0.73559. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64094/0.73607. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64096/0.73682. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63999/0.73774. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69558/0.69177. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69459/0.69075. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69325/0.68979. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69255/0.68894. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69146/0.68826. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69101/0.68776. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69048/0.68737. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.68712. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69000/0.68694. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68971/0.68677. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.68663. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.68649. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68896/0.68635. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68885/0.68623. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68847/0.68610. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68846/0.68597. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68827/0.68586. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68801/0.68575. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68771/0.68563. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68747/0.68549. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68739/0.68538. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68698/0.68528. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68672/0.68517. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68640/0.68504. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68641/0.68495. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68602/0.68487. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68578/0.68479. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68545/0.68472. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68531/0.68468. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68478/0.68466. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68436/0.68464. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68383/0.68461. Took 0.11 sec\n",
      "Epoch 32, Loss(train/val) 0.68399/0.68462. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68342/0.68462. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68324/0.68465. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68261/0.68474. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68239/0.68480. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68209/0.68488. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68146/0.68500. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68094/0.68509. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.68069/0.68524. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68032/0.68536. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67997/0.68554. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67904/0.68575. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67925/0.68592. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67792/0.68621. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67778/0.68653. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67722/0.68677. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67683/0.68696. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67644/0.68718. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67520/0.68740. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67502/0.68766. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67464/0.68791. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67352/0.68804. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67321/0.68831. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67253/0.68854. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67191/0.68861. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67124/0.68890. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67083/0.68916. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67001/0.68941. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66938/0.68954. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66882/0.68998. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66761/0.69015. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66708/0.69053. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66629/0.69069. Took 0.08 sec\n",
      "Epoch 65, Loss(train/val) 0.66542/0.69101. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66507/0.69121. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66392/0.69140. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66317/0.69170. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66247/0.69195. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66129/0.69236. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66074/0.69265. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66037/0.69286. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65921/0.69322. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65785/0.69378. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65716/0.69430. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65694/0.69449. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65603/0.69462. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65545/0.69524. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65398/0.69565. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65298/0.69613. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65216/0.69657. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65109/0.69706. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65057/0.69747. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64964/0.69793. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64848/0.69867. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64816/0.69921. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64657/0.69987. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64562/0.70041. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64520/0.70088. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64435/0.70152. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64380/0.70201. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64251/0.70266. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64218/0.70308. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64065/0.70348. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64004/0.70419. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.63892/0.70478. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63757/0.70527. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63726/0.70587. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63591/0.70647. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69470/0.69392. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69388/0.69316. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69301/0.69233. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69217/0.69147. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69124/0.69075. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69040/0.69028. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68999/0.69004. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68970/0.68992. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68954/0.68987. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68942/0.68984. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68938/0.68981. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68921/0.68978. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68900/0.68974. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68890/0.68971. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68888/0.68967. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68866/0.68964. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68847/0.68962. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68828/0.68958. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.68955. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68801/0.68951. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68794/0.68947. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68779/0.68944. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68759/0.68939. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68743/0.68935. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68727/0.68932. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68702/0.68928. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68673/0.68925. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68643/0.68922. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68640/0.68919. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68598/0.68918. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68574/0.68919. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68551/0.68920. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68504/0.68921. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68469/0.68923. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68423/0.68928. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68390/0.68936. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68354/0.68944. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68308/0.68954. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68256/0.68968. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68218/0.68983. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68185/0.69001. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68086/0.69024. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68030/0.69054. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67965/0.69085. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67909/0.69122. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67822/0.69164. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67765/0.69203. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67690/0.69249. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67593/0.69305. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67517/0.69365. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67406/0.69429. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67337/0.69499. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67203/0.69575. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67131/0.69658. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67019/0.69751. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66921/0.69849. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66807/0.69939. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66683/0.70055. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66580/0.70178. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66441/0.70310. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66358/0.70464. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66224/0.70618. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66135/0.70766. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66040/0.70916. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65927/0.71078. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65843/0.71230. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65737/0.71388. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65693/0.71560. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65538/0.71711. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65480/0.71879. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65392/0.72040. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65359/0.72204. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65253/0.72361. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65198/0.72504. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65120/0.72661. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65038/0.72791. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65004/0.72898. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64926/0.73048. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64831/0.73177. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64830/0.73321. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64805/0.73440. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64691/0.73546. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64668/0.73645. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64600/0.73790. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64528/0.73908. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64475/0.74021. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64360/0.74152. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64386/0.74258. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64324/0.74348. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64181/0.74446. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64140/0.74562. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64085/0.74661. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64085/0.74781. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64028/0.74867. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63906/0.74966. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63901/0.75064. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63837/0.75142. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63752/0.75246. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63719/0.75347. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63646/0.75452. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69221/0.69374. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69090/0.69103. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68966/0.68840. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68858/0.68614. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68779/0.68444. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68765/0.68325. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68695/0.68242. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68691/0.68178. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68676/0.68126. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68658/0.68085. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68656/0.68041. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68617/0.68007. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68604/0.67968. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68577/0.67931. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68527/0.67887. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68524/0.67849. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68497/0.67807. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68469/0.67762. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68450/0.67715. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68364/0.67660. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68346/0.67608. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68341/0.67549. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68333/0.67488. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68283/0.67425. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68239/0.67362. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.67296. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68164/0.67226. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68111/0.67147. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68042/0.67068. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67993/0.66985. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67940/0.66901. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67868/0.66822. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67831/0.66751. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67763/0.66688. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67715/0.66615. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67680/0.66544. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67611/0.66504. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67539/0.66466. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67447/0.66436. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67383/0.66414. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67326/0.66404. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67210/0.66393. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67182/0.66416. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67078/0.66417. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67027/0.66444. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66953/0.66479. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66859/0.66526. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66783/0.66575. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66731/0.66645. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66623/0.66724. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66531/0.66801. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66489/0.66864. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66383/0.66944. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66312/0.67041. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66243/0.67142. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66163/0.67238. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66125/0.67341. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66040/0.67434. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65949/0.67543. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65889/0.67671. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65803/0.67806. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65701/0.67923. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65696/0.68042. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65551/0.68166. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65535/0.68269. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65453/0.68402. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65349/0.68562. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65336/0.68704. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65228/0.68807. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65147/0.68930. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65055/0.69077. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65010/0.69225. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64981/0.69390. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64891/0.69528. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64901/0.69652. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64755/0.69796. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64696/0.69915. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64660/0.70067. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64588/0.70192. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64549/0.70339. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64486/0.70455. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64466/0.70557. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64442/0.70710. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64289/0.70852. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64323/0.70964. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64164/0.71088. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64148/0.71237. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64046/0.71406. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64093/0.71509. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63901/0.71657. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63833/0.71754. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63863/0.71885. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63786/0.72036. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63756/0.72119. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63698/0.72231. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63625/0.72344. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.63526/0.72489. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63537/0.72594. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63428/0.72764. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63385/0.72880. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.70349/0.69327. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69989/0.69272. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69667/0.69285. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69345/0.69389. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69122/0.69562. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68980/0.69738. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68923/0.69883. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68849/0.69987. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68812/0.70059. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68789/0.70110. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68750/0.70149. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68721/0.70181. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68698/0.70211. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68699/0.70237. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68660/0.70259. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68630/0.70288. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68588/0.70316. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68587/0.70342. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68557/0.70365. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68534/0.70392. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68509/0.70417. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68461/0.70441. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68445/0.70470. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68415/0.70496. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68402/0.70530. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68388/0.70565. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68306/0.70595. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68309/0.70622. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68262/0.70653. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68240/0.70679. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68202/0.70706. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68180/0.70741. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68113/0.70776. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68119/0.70825. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68093/0.70846. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68024/0.70879. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67975/0.70927. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67933/0.70954. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67922/0.70999. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67843/0.71023. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67789/0.71075. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67780/0.71116. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67753/0.71142. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67717/0.71166. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67639/0.71196. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67631/0.71220. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67603/0.71256. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67548/0.71285. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67422/0.71307. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67421/0.71329. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67395/0.71382. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67310/0.71422. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67266/0.71444. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67288/0.71475. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67184/0.71500. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67119/0.71523. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67076/0.71569. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67058/0.71597. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66957/0.71618. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66907/0.71680. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66866/0.71709. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66772/0.71726. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66723/0.71760. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66673/0.71784. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66589/0.71851. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66552/0.71883. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66484/0.71933. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66396/0.71963. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66316/0.72027. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66237/0.72071. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66133/0.72142. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66054/0.72184. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66034/0.72223. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65955/0.72302. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65877/0.72343. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65753/0.72381. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65674/0.72464. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65660/0.72508. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65529/0.72580. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65505/0.72625. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65408/0.72637. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65342/0.72722. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65206/0.72768. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65185/0.72859. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65138/0.72910. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65029/0.72957. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64931/0.73001. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64904/0.73068. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64852/0.73101. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64690/0.73133. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64632/0.73217. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64516/0.73273. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64522/0.73317. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64433/0.73326. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64336/0.73399. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64140/0.73447. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64182/0.73503. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64091/0.73515. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64052/0.73571. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63840/0.73592. Took 0.09 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.68855/0.69913. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68817/0.69905. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68802/0.69898. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68778/0.69892. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68761/0.69885. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68722/0.69878. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68706/0.69869. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68675/0.69861. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68650/0.69852. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68636/0.69843. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68604/0.69832. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68572/0.69824. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68517/0.69817. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68483/0.69805. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68411/0.69793. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68423/0.69779. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68347/0.69771. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68304/0.69760. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68224/0.69747. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68187/0.69747. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68114/0.69750. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68023/0.69750. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67997/0.69756. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67922/0.69772. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67888/0.69771. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67820/0.69780. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67756/0.69799. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67701/0.69795. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67676/0.69798. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67616/0.69797. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67533/0.69802. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67460/0.69799. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67459/0.69795. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.67414/0.69790. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67327/0.69778. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67261/0.69762. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67217/0.69741. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67185/0.69719. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67105/0.69719. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67066/0.69704. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66989/0.69692. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66942/0.69688. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66848/0.69678. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66821/0.69627. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66741/0.69629. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66721/0.69604. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66673/0.69597. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66539/0.69572. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66534/0.69569. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66473/0.69539. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66377/0.69520. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66277/0.69486. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66270/0.69480. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66203/0.69482. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66103/0.69470. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66020/0.69451. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65951/0.69466. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65895/0.69456. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65835/0.69472. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65718/0.69477. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65643/0.69494. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65603/0.69512. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65477/0.69533. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65414/0.69552. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65339/0.69552. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65218/0.69601. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65072/0.69667. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65092/0.69707. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64958/0.69733. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64859/0.69766. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64806/0.69839. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64616/0.69884. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64588/0.69958. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64477/0.70020. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64415/0.70116. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64309/0.70175. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64197/0.70217. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64103/0.70302. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64102/0.70402. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63985/0.70473. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63884/0.70593. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63870/0.70658. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63701/0.70741. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63655/0.70821. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63568/0.70924. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63387/0.71046. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63311/0.71167. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63314/0.71272. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63223/0.71348. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63122/0.71456. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62893/0.71622. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62884/0.71701. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62752/0.71868. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62610/0.71993. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62629/0.72158. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62568/0.72323. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62300/0.72449. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62303/0.72564. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62302/0.72653. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62038/0.72807. Took 0.10 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.70130/0.69775. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69959/0.69749. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69779/0.69713. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69565/0.69698. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69310/0.69752. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69120/0.69870. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68953/0.69995. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68903/0.70091. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68892/0.70151. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68839/0.70190. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.70218. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68752/0.70238. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68739/0.70257. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68702/0.70271. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68670/0.70284. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68668/0.70298. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68634/0.70307. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68600/0.70327. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68587/0.70339. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68544/0.70354. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68549/0.70362. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68503/0.70378. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68463/0.70396. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68435/0.70407. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68406/0.70428. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68399/0.70440. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68385/0.70460. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68339/0.70476. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68330/0.70489. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68292/0.70510. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68290/0.70521. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68260/0.70542. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68250/0.70559. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68251/0.70583. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68201/0.70597. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68186/0.70615. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68165/0.70632. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68144/0.70649. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68111/0.70661. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68082/0.70671. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68117/0.70687. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68057/0.70695. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68026/0.70700. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67969/0.70723. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67959/0.70736. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67933/0.70745. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67933/0.70755. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67870/0.70772. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67868/0.70787. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67870/0.70809. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67832/0.70819. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67790/0.70841. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67748/0.70857. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67696/0.70869. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67705/0.70886. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67624/0.70903. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.67629/0.70917. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67570/0.70927. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67531/0.70940. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67506/0.70959. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67506/0.70979. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67446/0.70987. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67401/0.71010. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67388/0.71026. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67325/0.71049. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67327/0.71074. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67240/0.71077. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67237/0.71100. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67169/0.71135. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67114/0.71153. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67075/0.71167. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67016/0.71196. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66995/0.71213. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66944/0.71240. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66897/0.71250. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66839/0.71281. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66826/0.71290. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66763/0.71304. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66677/0.71344. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66675/0.71357. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66558/0.71370. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66535/0.71409. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66431/0.71414. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66428/0.71444. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66382/0.71474. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66282/0.71493. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66233/0.71525. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66149/0.71549. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66073/0.71560. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66098/0.71584. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65915/0.71604. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65934/0.71637. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65835/0.71678. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65799/0.71711. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65701/0.71734. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65597/0.71774. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65527/0.71816. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65506/0.71861. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65399/0.71920. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65306/0.71971. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69262/0.68547. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69235/0.68557. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69257/0.68565. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69250/0.68575. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69240/0.68587. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69229/0.68600. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69196/0.68614. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69183/0.68629. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69162/0.68645. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.68664. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69124/0.68684. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69117/0.68705. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69121/0.68728. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69090/0.68752. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69103/0.68778. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69044/0.68805. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69062/0.68833. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69027/0.68859. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.69011/0.68888. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68986/0.68917. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68967/0.68950. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68935/0.68982. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68907/0.69012. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68867/0.69045. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68866/0.69076. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68798/0.69108. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68777/0.69141. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68725/0.69174. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68715/0.69205. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68661/0.69239. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68642/0.69268. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68592/0.69300. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68532/0.69329. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68508/0.69360. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68463/0.69391. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68418/0.69427. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68358/0.69465. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68294/0.69501. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68262/0.69538. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68191/0.69576. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68144/0.69615. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68101/0.69652. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68006/0.69694. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67952/0.69743. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67889/0.69784. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67840/0.69833. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67772/0.69886. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67726/0.69940. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67645/0.69995. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67563/0.70049. Took 0.11 sec\n",
      "Epoch 50, Loss(train/val) 0.67480/0.70107. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67427/0.70168. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67318/0.70227. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67234/0.70293. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67193/0.70354. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67117/0.70415. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67026/0.70478. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66978/0.70550. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66906/0.70617. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.66796/0.70685. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66759/0.70763. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66661/0.70822. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66573/0.70880. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66514/0.70951. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66472/0.71014. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66360/0.71087. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66263/0.71158. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66236/0.71229. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66164/0.71301. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66120/0.71357. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66068/0.71431. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65935/0.71511. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65925/0.71582. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65800/0.71637. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65748/0.71690. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65623/0.71757. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65534/0.71834. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65480/0.71905. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65478/0.71942. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65415/0.71993. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65283/0.72026. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65161/0.72110. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65186/0.72145. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65093/0.72224. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64965/0.72286. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64872/0.72344. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64817/0.72385. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64696/0.72476. Took 0.12 sec\n",
      "Epoch 88, Loss(train/val) 0.64759/0.72511. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64681/0.72547. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64546/0.72628. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64417/0.72692. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64439/0.72724. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64374/0.72780. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64197/0.72800. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64167/0.72910. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64072/0.72985. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63921/0.73043. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63949/0.73129. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63792/0.73220. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69547/0.69664. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69430/0.69632. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69344/0.69609. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69306/0.69589. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69276/0.69578. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69199/0.69570. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69562. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69155/0.69554. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69096/0.69543. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69080/0.69530. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.69514. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69033/0.69497. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69034/0.69481. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68969/0.69459. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68960/0.69440. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68937/0.69416. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68868/0.69394. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68859/0.69370. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.69345. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68784/0.69320. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68751/0.69295. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68685/0.69269. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68676/0.69249. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68590/0.69226. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68581/0.69204. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68520/0.69186. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68488/0.69174. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68425/0.69164. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68390/0.69157. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68305/0.69151. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68264/0.69152. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68218/0.69156. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68147/0.69167. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68123/0.69179. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68089/0.69201. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67999/0.69224. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67976/0.69248. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67941/0.69276. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67848/0.69307. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67834/0.69339. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67788/0.69372. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67734/0.69409. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67660/0.69453. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67647/0.69495. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67587/0.69538. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67518/0.69586. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67535/0.69627. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67475/0.69677. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67428/0.69721. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67369/0.69769. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67366/0.69817. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67267/0.69866. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67201/0.69913. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67135/0.69965. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67127/0.70010. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67064/0.70068. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67033/0.70115. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66958/0.70170. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66972/0.70223. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.66886/0.70273. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66792/0.70328. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66738/0.70386. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66700/0.70448. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66646/0.70526. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66532/0.70574. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66458/0.70652. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66388/0.70709. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66291/0.70769. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66299/0.70836. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66256/0.70910. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66158/0.70997. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66076/0.71079. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66014/0.71149. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.65927/0.71247. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65913/0.71327. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65714/0.71421. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65747/0.71509. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65549/0.71579. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65558/0.71654. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65467/0.71758. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65361/0.71867. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65346/0.71956. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65233/0.72074. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65134/0.72186. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65029/0.72291. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64893/0.72407. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64903/0.72479. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64796/0.72598. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64640/0.72712. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64550/0.72819. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64469/0.72934. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64425/0.73051. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64306/0.73168. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64176/0.73281. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64100/0.73387. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63985/0.73515. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63934/0.73641. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63834/0.73744. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63733/0.73833. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63576/0.73925. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69459/0.69496. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69405/0.69451. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69374/0.69406. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69342/0.69365. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69320/0.69329. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69269/0.69299. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69238/0.69273. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69192/0.69256. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69143/0.69246. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69106/0.69240. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.69240. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69046/0.69242. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68993/0.69248. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69004/0.69256. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68959/0.69266. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68936/0.69278. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68893/0.69294. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68858/0.69313. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.69333. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68757/0.69354. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68749/0.69379. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68675/0.69408. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68633/0.69441. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68600/0.69477. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68531/0.69513. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68487/0.69554. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68431/0.69599. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68413/0.69647. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68322/0.69701. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68291/0.69755. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68220/0.69812. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68174/0.69870. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68098/0.69935. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68055/0.69997. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67968/0.70063. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67955/0.70121. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67896/0.70187. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67811/0.70256. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67761/0.70321. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67683/0.70388. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67634/0.70456. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67546/0.70522. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67498/0.70586. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67448/0.70652. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67385/0.70720. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67360/0.70790. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67257/0.70857. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67189/0.70929. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67152/0.70997. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67033/0.71078. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67009/0.71153. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66950/0.71224. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66852/0.71296. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66745/0.71374. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66698/0.71450. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66613/0.71528. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66519/0.71602. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66431/0.71681. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66349/0.71762. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66291/0.71846. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66210/0.71933. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66143/0.72016. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66108/0.72092. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65936/0.72177. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65872/0.72261. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65766/0.72352. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65660/0.72447. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65611/0.72548. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65488/0.72634. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65381/0.72723. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65258/0.72820. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65200/0.72918. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65044/0.73006. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64996/0.73095. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64946/0.73178. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64824/0.73273. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.64749/0.73376. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64630/0.73472. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64530/0.73559. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64433/0.73658. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64307/0.73738. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64265/0.73825. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64150/0.73893. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64046/0.73980. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63916/0.74070. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.63787/0.74158. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63714/0.74248. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63601/0.74344. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63514/0.74418. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63431/0.74513. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63285/0.74610. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63234/0.74681. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63116/0.74768. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62980/0.74864. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62849/0.74937. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62830/0.75015. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62675/0.75084. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62510/0.75170. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62457/0.75255. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62301/0.75335. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69520/0.68380. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69497/0.68400. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69486/0.68417. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69445/0.68434. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69430/0.68451. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69405/0.68467. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69409/0.68484. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69387/0.68501. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69363/0.68516. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69332/0.68531. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69336/0.68542. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69304/0.68550. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69278/0.68556. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69233/0.68559. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69205/0.68565. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69164/0.68568. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69122/0.68565. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69074/0.68566. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69048/0.68568. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68996/0.68566. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68920/0.68569. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68880/0.68579. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68817/0.68598. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68739/0.68630. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68687/0.68666. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68585/0.68703. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68539/0.68745. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68475/0.68789. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68383/0.68836. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68337/0.68891. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68228/0.68944. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68162/0.68993. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68081/0.69039. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68013/0.69091. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67955/0.69153. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67866/0.69222. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67780/0.69290. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67656/0.69363. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67538/0.69441. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67470/0.69522. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67341/0.69627. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67213/0.69721. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67105/0.69838. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66974/0.69952. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66869/0.70097. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.66756/0.70226. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66619/0.70366. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66486/0.70520. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66363/0.70663. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66233/0.70826. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.66136/0.71007. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66000/0.71185. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65853/0.71388. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65770/0.71569. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65598/0.71790. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65497/0.72012. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65385/0.72261. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65255/0.72475. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65145/0.72698. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.64982/0.72899. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64851/0.73133. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64747/0.73379. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64676/0.73613. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64443/0.73862. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64347/0.74124. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64237/0.74397. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64040/0.74713. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63957/0.74985. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63856/0.75259. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63696/0.75599. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63584/0.75880. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63473/0.76154. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63294/0.76449. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.63110/0.76705. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62968/0.76981. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62891/0.77255. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62758/0.77531. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62592/0.77803. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62472/0.78090. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.62294/0.78433. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62135/0.78733. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.61967/0.79051. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61865/0.79360. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61749/0.79578. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61605/0.79774. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61447/0.80128. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61199/0.80500. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61248/0.80772. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61118/0.81066. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60854/0.81303. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60708/0.81534. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60619/0.81803. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60384/0.82132. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60230/0.82404. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60233/0.82595. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60027/0.82901. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59927/0.83146. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59836/0.83402. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.59405/0.83652. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59462/0.83987. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69443/0.68853. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69427/0.68853. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69392/0.68855. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69378/0.68857. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69355/0.68861. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69341/0.68864. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69300/0.68867. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69256/0.68869. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69248/0.68871. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69193/0.68870. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69205/0.68871. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69134/0.68871. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69121/0.68872. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69080/0.68874. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68998/0.68879. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68950/0.68883. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68938/0.68888. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68879/0.68893. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68794/0.68894. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68783/0.68887. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68726/0.68872. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68677/0.68860. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68624/0.68846. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68635/0.68827. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68536/0.68806. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68522/0.68781. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68456/0.68764. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68443/0.68735. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68341/0.68705. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68338/0.68670. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68283/0.68643. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68231/0.68609. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68139/0.68575. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68078/0.68539. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68054/0.68502. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68013/0.68461. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67933/0.68425. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67897/0.68388. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67832/0.68352. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67800/0.68305. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67699/0.68254. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67673/0.68213. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67558/0.68171. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67520/0.68115. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67439/0.68072. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67312/0.68029. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67296/0.67978. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67198/0.67937. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67126/0.67887. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67036/0.67846. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67004/0.67792. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66881/0.67737. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66851/0.67682. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66708/0.67630. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66650/0.67577. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66573/0.67534. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66451/0.67491. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66307/0.67444. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66200/0.67401. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66109/0.67365. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66043/0.67339. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65988/0.67307. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65868/0.67277. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65817/0.67249. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65636/0.67233. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65574/0.67215. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65416/0.67197. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65388/0.67184. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65177/0.67177. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65106/0.67186. Took 0.12 sec\n",
      "Epoch 70, Loss(train/val) 0.65108/0.67187. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64944/0.67201. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64797/0.67222. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64682/0.67268. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64563/0.67317. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64415/0.67372. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64333/0.67425. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64282/0.67477. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64196/0.67557. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63961/0.67661. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63874/0.67766. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63748/0.67860. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63560/0.67958. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63438/0.68075. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63242/0.68201. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.63163/0.68324. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63033/0.68457. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62921/0.68582. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62827/0.68699. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62656/0.68846. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62491/0.68996. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62375/0.69140. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62305/0.69313. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62007/0.69444. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62001/0.69605. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61993/0.69737. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61707/0.69844. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61638/0.69978. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61397/0.70117. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61364/0.70246. Took 0.11 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69654/0.70009. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69479/0.69766. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69403/0.69603. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69361/0.69496. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69313/0.69431. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69312/0.69393. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69248/0.69369. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69242/0.69355. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69246/0.69346. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69223/0.69340. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69180/0.69335. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69203/0.69333. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69170/0.69330. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69183/0.69331. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69131/0.69329. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69128/0.69330. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.69095/0.69330. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69102/0.69325. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69068/0.69322. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.69011/0.69321. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68996/0.69320. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69000/0.69318. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69021/0.69315. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68977/0.69310. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68953/0.69308. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68932/0.69310. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68914/0.69308. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68899/0.69302. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68849/0.69301. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68826/0.69302. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68819/0.69297. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68791/0.69289. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68796/0.69290. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68759/0.69288. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68710/0.69283. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68711/0.69283. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68636/0.69282. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68657/0.69279. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68626/0.69273. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68595/0.69274. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68548/0.69271. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68585/0.69268. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68500/0.69270. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68483/0.69268. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68468/0.69267. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68469/0.69266. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68417/0.69271. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68411/0.69266. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68354/0.69273. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68302/0.69278. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68295/0.69273. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68252/0.69276. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68216/0.69284. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68169/0.69287. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68132/0.69292. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68105/0.69300. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68069/0.69310. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68003/0.69316. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67952/0.69327. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67955/0.69333. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67902/0.69346. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67863/0.69354. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67789/0.69359. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67807/0.69364. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67730/0.69365. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67686/0.69376. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67639/0.69389. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.67620/0.69395. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67564/0.69407. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67533/0.69407. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67460/0.69412. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67413/0.69417. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67371/0.69432. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67288/0.69429. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67247/0.69441. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67203/0.69456. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67128/0.69454. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67061/0.69458. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67054/0.69478. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66949/0.69478. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66881/0.69495. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66842/0.69506. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.66802/0.69522. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66704/0.69534. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66712/0.69546. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66650/0.69561. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66582/0.69577. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66505/0.69588. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66460/0.69610. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66408/0.69618. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66360/0.69643. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66296/0.69663. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66253/0.69702. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66191/0.69724. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66123/0.69755. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66060/0.69770. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66070/0.69821. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65992/0.69847. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65884/0.69876. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65883/0.69908. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69524/0.69917. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69429/0.69865. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69423/0.69801. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69355/0.69729. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69367/0.69658. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69290/0.69592. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69245/0.69532. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69253/0.69473. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69203/0.69416. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69149/0.69361. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69165/0.69318. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69151/0.69273. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69085/0.69234. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69108/0.69199. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69064/0.69159. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69057/0.69131. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69018/0.69098. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69022/0.69067. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68965/0.69042. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68968/0.69016. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68952/0.68991. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68941/0.68973. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68865/0.68958. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68840/0.68944. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68860/0.68942. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68794/0.68933. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68832/0.68928. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68750/0.68918. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68751/0.68916. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68729/0.68924. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68739/0.68928. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68661/0.68938. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68635/0.68942. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68584/0.68955. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68560/0.68976. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68610/0.69000. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68537/0.69025. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68500/0.69053. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68443/0.69084. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68424/0.69118. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68369/0.69154. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68346/0.69197. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68284/0.69258. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68273/0.69306. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68210/0.69354. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68155/0.69421. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68155/0.69492. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68051/0.69572. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68027/0.69654. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67952/0.69747. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67892/0.69849. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67846/0.69951. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67826/0.70073. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67723/0.70189. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67646/0.70292. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67591/0.70423. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67489/0.70532. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67452/0.70693. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67393/0.70820. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67367/0.70965. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67292/0.71116. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67217/0.71227. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67137/0.71363. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67066/0.71509. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67018/0.71648. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66992/0.71748. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66952/0.71878. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66826/0.71986. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66829/0.72096. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66769/0.72230. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66708/0.72352. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66667/0.72445. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66633/0.72557. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66567/0.72645. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66554/0.72729. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66484/0.72828. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66480/0.72885. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66419/0.72966. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66387/0.73038. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66305/0.73092. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66282/0.73126. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66249/0.73182. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66126/0.73225. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66138/0.73301. Took 0.13 sec\n",
      "Epoch 84, Loss(train/val) 0.66107/0.73379. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66066/0.73432. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66041/0.73425. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65999/0.73521. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65945/0.73583. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65908/0.73599. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65831/0.73606. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65901/0.73671. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65781/0.73710. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65741/0.73733. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65670/0.73746. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65639/0.73739. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65693/0.73732. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65596/0.73803. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65581/0.73889. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65548/0.73892. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69704/0.69274. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69670/0.69267. Took 0.17 sec\n",
      "Epoch 2, Loss(train/val) 0.69626/0.69259. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69560/0.69252. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69521/0.69248. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69464/0.69247. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69400/0.69252. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69341/0.69262. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69314/0.69275. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69270/0.69288. Took 0.08 sec\n",
      "Epoch 10, Loss(train/val) 0.69286/0.69299. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69245/0.69308. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69209/0.69313. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69166/0.69316. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69162/0.69320. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69115/0.69320. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69106/0.69323. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69039/0.69322. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69013/0.69316. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68986/0.69313. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68958/0.69311. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68869/0.69311. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68839/0.69310. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68825/0.69312. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68745/0.69317. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68700/0.69325. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68660/0.69337. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68604/0.69351. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68577/0.69369. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68518/0.69387. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68493/0.69409. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68417/0.69432. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68371/0.69459. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68338/0.69484. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68308/0.69511. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68243/0.69541. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68199/0.69573. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68160/0.69607. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68093/0.69642. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68047/0.69676. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68009/0.69717. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67937/0.69756. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67891/0.69794. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67870/0.69833. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67791/0.69874. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67688/0.69912. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67642/0.69950. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67572/0.69993. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67500/0.70032. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67469/0.70066. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67379/0.70102. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67302/0.70144. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67204/0.70180. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67163/0.70213. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67118/0.70246. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67005/0.70280. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66921/0.70314. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66812/0.70336. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66783/0.70370. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66671/0.70399. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66585/0.70426. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66461/0.70473. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66363/0.70494. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66262/0.70528. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66168/0.70566. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66100/0.70599. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65954/0.70645. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65831/0.70683. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65781/0.70741. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65650/0.70790. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65558/0.70849. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65443/0.70920. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65234/0.70987. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65143/0.71050. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65073/0.71166. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64904/0.71263. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64823/0.71327. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64746/0.71419. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64568/0.71483. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64458/0.71585. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64414/0.71680. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.64297/0.71781. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64213/0.71912. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64063/0.71998. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63973/0.72066. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63881/0.72201. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63730/0.72307. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63633/0.72407. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63600/0.72490. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63462/0.72616. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63338/0.72697. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63266/0.72841. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63181/0.72937. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63099/0.73067. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62999/0.73196. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62936/0.73287. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.62799/0.73385. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62756/0.73517. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62653/0.73609. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62452/0.73720. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69630/0.69685. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69538/0.69657. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69447/0.69641. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69440/0.69631. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69409/0.69625. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69386/0.69629. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69314/0.69637. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69289/0.69652. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69251/0.69672. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69208/0.69701. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69173/0.69731. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69140/0.69765. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69125/0.69803. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69098/0.69846. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68992/0.69894. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.69947. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68942/0.70006. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68857/0.70072. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68829/0.70149. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68780/0.70227. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68703/0.70306. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68649/0.70389. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68575/0.70473. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.70558. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68418/0.70646. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68359/0.70724. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68296/0.70793. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68196/0.70858. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68141/0.70916. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68113/0.70966. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68024/0.70993. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67976/0.71022. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67894/0.71051. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67874/0.71080. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67790/0.71100. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67761/0.71113. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67696/0.71123. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67655/0.71131. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.71134. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67563/0.71148. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67515/0.71155. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67415/0.71155. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67431/0.71160. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67414/0.71179. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67293/0.71179. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67266/0.71177. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67223/0.71180. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67178/0.71183. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67066/0.71189. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67077/0.71194. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67035/0.71209. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66918/0.71221. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66883/0.71241. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66862/0.71256. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66787/0.71269. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66667/0.71272. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66619/0.71272. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66607/0.71285. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.66528/0.71316. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66467/0.71341. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66378/0.71362. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66373/0.71391. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66236/0.71399. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66243/0.71427. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66130/0.71459. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66074/0.71487. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65986/0.71540. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66003/0.71547. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65867/0.71568. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65787/0.71583. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65682/0.71647. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65671/0.71713. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65623/0.71760. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65555/0.71794. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65399/0.71815. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65311/0.71879. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65238/0.71924. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65205/0.71985. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65079/0.72038. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65056/0.72096. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64987/0.72164. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64853/0.72230. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64822/0.72283. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64738/0.72322. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64663/0.72414. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64541/0.72488. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64488/0.72573. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64390/0.72674. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64324/0.72735. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64276/0.72830. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64150/0.72915. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64106/0.72993. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63994/0.73052. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63977/0.73151. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63834/0.73202. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63833/0.73355. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63664/0.73450. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63615/0.73515. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63499/0.73640. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63513/0.73725. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69282/0.68964. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69249/0.68969. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.68978. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69221/0.68989. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.68999. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69160/0.69010. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69108/0.69024. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69088/0.69036. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.69053. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68948/0.69064. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68894/0.69078. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.69091. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68716/0.69109. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68627/0.69128. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68516/0.69157. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68422/0.69202. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68332/0.69240. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68226/0.69289. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68169/0.69343. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68044/0.69395. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.67950/0.69445. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67891/0.69476. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67803/0.69517. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67766/0.69539. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67717/0.69565. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67690/0.69585. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67592/0.69608. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67583/0.69631. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67534/0.69640. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67476/0.69652. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67426/0.69661. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67399/0.69665. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67320/0.69682. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67278/0.69688. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67252/0.69693. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67193/0.69713. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67208/0.69711. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67116/0.69717. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67093/0.69739. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67010/0.69752. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67016/0.69763. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66956/0.69771. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66912/0.69770. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66860/0.69802. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66779/0.69817. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66709/0.69817. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66709/0.69831. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66603/0.69850. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66548/0.69858. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66540/0.69882. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66465/0.69892. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66425/0.69921. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66371/0.69931. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66305/0.69962. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66269/0.69967. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66194/0.69973. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66118/0.69986. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66077/0.70004. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.66009/0.70005. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65952/0.70032. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65851/0.70028. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65776/0.70064. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65718/0.70074. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65577/0.70091. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65568/0.70108. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65476/0.70100. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65405/0.70116. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.65289/0.70135. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65243/0.70187. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65175/0.70174. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64987/0.70197. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64902/0.70218. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64784/0.70217. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64712/0.70276. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64632/0.70319. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64553/0.70366. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64401/0.70450. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64260/0.70462. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64181/0.70521. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64079/0.70551. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63989/0.70643. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63804/0.70730. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.63681/0.70843. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63584/0.70913. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63378/0.70984. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63311/0.71145. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63208/0.71310. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63131/0.71386. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62962/0.71567. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62814/0.71680. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62652/0.71894. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62558/0.72049. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62392/0.72188. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62246/0.72361. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62092/0.72608. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.61975/0.72738. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61845/0.73000. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.61771/0.73186. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61656/0.73369. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61422/0.73491. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69629/0.69727. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69511/0.69604. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69460/0.69484. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69363/0.69366. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69255/0.69255. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69166. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69165/0.69095. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69087/0.69041. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69056/0.69003. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68957/0.68971. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68957/0.68948. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68916/0.68929. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68865/0.68913. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68797/0.68902. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68725/0.68893. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68660/0.68891. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68581/0.68894. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68519/0.68903. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68413/0.68920. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68367/0.68944. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68288/0.68971. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68216/0.69003. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68169/0.69041. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68065/0.69080. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.67990/0.69122. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67964/0.69163. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67869/0.69200. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67823/0.69232. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67795/0.69263. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67758/0.69290. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67660/0.69308. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67654/0.69330. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67649/0.69351. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67569/0.69363. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67519/0.69375. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67488/0.69382. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67473/0.69394. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67402/0.69402. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67382/0.69409. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67353/0.69421. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67325/0.69421. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67254/0.69433. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67246/0.69424. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67201/0.69428. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67101/0.69424. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67094/0.69420. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67064/0.69426. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67021/0.69423. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67005/0.69421. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66936/0.69417. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66875/0.69411. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66860/0.69400. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66812/0.69402. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66785/0.69387. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66689/0.69388. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66652/0.69375. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66599/0.69384. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66597/0.69359. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66488/0.69363. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66447/0.69360. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66374/0.69345. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66279/0.69349. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66261/0.69339. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66226/0.69331. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66128/0.69343. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66109/0.69342. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65975/0.69338. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65939/0.69324. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65888/0.69335. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65804/0.69327. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65733/0.69322. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65671/0.69343. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65614/0.69367. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65534/0.69381. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65479/0.69369. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65421/0.69386. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65301/0.69442. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65314/0.69448. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65163/0.69480. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65082/0.69501. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64975/0.69552. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64928/0.69568. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64926/0.69643. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64807/0.69662. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64749/0.69742. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64641/0.69768. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64557/0.69864. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64520/0.69923. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64436/0.69963. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64339/0.70055. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64293/0.70126. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64204/0.70190. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64113/0.70271. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64057/0.70336. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63991/0.70444. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63878/0.70528. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63799/0.70613. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63705/0.70747. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63607/0.70838. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63518/0.70942. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69187/0.69633. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69154/0.69633. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69107/0.69631. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69093/0.69631. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69080/0.69632. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69069/0.69629. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.69631. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68999/0.69632. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68958/0.69630. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68976/0.69629. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.69628. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68929/0.69631. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68866/0.69632. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68817/0.69635. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68785/0.69635. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68765/0.69635. Took 0.08 sec\n",
      "Epoch 16, Loss(train/val) 0.68695/0.69631. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68679/0.69632. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68637/0.69633. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68607/0.69634. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68532/0.69632. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68504/0.69631. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68453/0.69629. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68361/0.69626. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68354/0.69620. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68264/0.69616. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68200/0.69614. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68143/0.69606. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68092/0.69593. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67986/0.69584. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67918/0.69572. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67827/0.69555. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67818/0.69531. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67664/0.69510. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67628/0.69483. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67522/0.69456. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67449/0.69435. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67349/0.69421. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67312/0.69394. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67215/0.69378. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67135/0.69363. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67063/0.69346. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66993/0.69336. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66898/0.69325. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66863/0.69328. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66800/0.69331. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66693/0.69346. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66636/0.69362. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66534/0.69386. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66472/0.69402. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66384/0.69431. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66292/0.69460. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66206/0.69495. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66166/0.69543. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66006/0.69595. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65990/0.69649. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65875/0.69699. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.65825/0.69770. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65736/0.69834. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65693/0.69899. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65613/0.69968. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65513/0.70044. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65483/0.70116. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65355/0.70200. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65325/0.70295. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65175/0.70390. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65136/0.70480. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65028/0.70569. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64977/0.70673. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64887/0.70764. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64743/0.70877. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64730/0.70965. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64641/0.71058. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64552/0.71165. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64458/0.71281. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64286/0.71404. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64187/0.71528. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64090/0.71684. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64028/0.71828. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63942/0.71956. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63885/0.72067. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.63758/0.72217. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63698/0.72343. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63492/0.72504. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63388/0.72652. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63274/0.72779. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63281/0.72950. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63086/0.73100. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62970/0.73245. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62901/0.73387. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.62747/0.73541. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62683/0.73707. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62580/0.73880. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.62319/0.74067. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62317/0.74180. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62110/0.74334. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62014/0.74499. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61931/0.74663. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.61852/0.74819. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61737/0.74927. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69435/0.70038. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69976. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69342/0.69923. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.69877. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69241/0.69838. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69217/0.69803. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.69773. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69166/0.69744. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69116/0.69717. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.69691. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69089/0.69668. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69049/0.69646. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69042/0.69624. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69024/0.69605. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68969/0.69584. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68952/0.69566. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68916/0.69547. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68880/0.69528. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68816/0.69511. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68787/0.69495. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68751/0.69478. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68728/0.69464. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68644/0.69449. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68583/0.69438. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68566/0.69428. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68454/0.69422. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68412/0.69414. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68371/0.69409. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68303/0.69397. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68249/0.69388. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68185/0.69379. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68147/0.69371. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68016/0.69360. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67974/0.69342. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67946/0.69319. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67885/0.69297. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67801/0.69277. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67743/0.69252. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67740/0.69224. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67658/0.69200. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67621/0.69170. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67577/0.69130. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67501/0.69091. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67431/0.69048. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67400/0.69013. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67346/0.68972. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67258/0.68925. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67263/0.68881. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67226/0.68845. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67153/0.68803. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67166/0.68758. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67056/0.68723. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66942/0.68687. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66929/0.68654. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66819/0.68612. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66821/0.68588. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66762/0.68553. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66728/0.68528. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66718/0.68501. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66664/0.68471. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66505/0.68447. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66546/0.68426. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66498/0.68408. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66462/0.68392. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66419/0.68371. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66327/0.68364. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66273/0.68355. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66257/0.68354. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66121/0.68342. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66082/0.68342. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66034/0.68343. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65843/0.68338. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65917/0.68346. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65872/0.68346. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65759/0.68338. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65777/0.68348. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65654/0.68361. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65662/0.68375. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65539/0.68382. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65422/0.68413. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65387/0.68439. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65304/0.68447. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65216/0.68454. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65142/0.68476. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65046/0.68507. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65052/0.68545. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64917/0.68575. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64866/0.68635. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64809/0.68672. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64694/0.68714. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64583/0.68739. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64487/0.68781. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64427/0.68865. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64321/0.68898. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64160/0.68966. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64091/0.69006. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64032/0.69060. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.63885/0.69119. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63838/0.69179. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63731/0.69264. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69668/0.69716. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69646/0.69692. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69620/0.69667. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69611/0.69634. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69530/0.69584. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69457/0.69510. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69363/0.69416. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69269/0.69320. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69166/0.69244. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69088/0.69197. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69070/0.69172. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69042/0.69157. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68989/0.69152. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68960/0.69152. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68906/0.69159. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68900/0.69171. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68871/0.69188. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68817/0.69207. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68799/0.69230. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68725/0.69260. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68704/0.69293. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68664/0.69331. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68649/0.69368. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68607/0.69409. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68588/0.69447. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68553/0.69487. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68552/0.69523. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68517/0.69560. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68489/0.69602. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68444/0.69636. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68428/0.69671. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68423/0.69718. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68359/0.69757. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68312/0.69793. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68297/0.69824. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68268/0.69862. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68217/0.69900. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68193/0.69933. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68178/0.69971. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68114/0.70016. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68071/0.70046. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68038/0.70071. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67985/0.70110. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67961/0.70134. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67919/0.70159. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67869/0.70174. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67789/0.70211. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67776/0.70214. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67745/0.70235. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67607/0.70248. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67610/0.70268. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67566/0.70264. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67462/0.70280. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67402/0.70276. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67330/0.70285. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67256/0.70292. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67215/0.70283. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67119/0.70286. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67109/0.70288. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67012/0.70254. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66922/0.70249. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66904/0.70229. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66803/0.70214. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66719/0.70184. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66580/0.70184. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66495/0.70122. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66459/0.70120. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66317/0.70095. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66282/0.70056. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66162/0.70029. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66079/0.69990. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65987/0.69938. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65843/0.69911. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65758/0.69868. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65673/0.69817. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65666/0.69790. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65453/0.69743. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65372/0.69745. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65264/0.69693. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65086/0.69667. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65005/0.69634. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64922/0.69627. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64800/0.69595. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.64684/0.69580. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64585/0.69536. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64435/0.69535. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.64293/0.69520. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64266/0.69534. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64110/0.69535. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64076/0.69509. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63897/0.69573. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63765/0.69574. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63661/0.69636. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63517/0.69688. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63440/0.69671. Took 0.12 sec\n",
      "Epoch 95, Loss(train/val) 0.63317/0.69716. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63214/0.69746. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63044/0.69771. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63057/0.69805. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62737/0.69814. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69738/0.69283. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69621/0.69262. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69527/0.69250. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69406/0.69249. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69323/0.69266. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69199/0.69308. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69133/0.69372. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69057/0.69449. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68963/0.69525. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68951/0.69580. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68925/0.69621. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68914/0.69648. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.69665. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68861/0.69670. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68897/0.69664. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68847/0.69660. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68796/0.69656. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68789/0.69649. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68754/0.69634. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68728/0.69622. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68753/0.69611. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68692/0.69595. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68684/0.69581. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68633/0.69569. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68617/0.69547. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68592/0.69544. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68571/0.69527. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68563/0.69510. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68544/0.69498. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68509/0.69483. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68472/0.69470. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68416/0.69458. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68379/0.69445. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68375/0.69423. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68293/0.69419. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68283/0.69410. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68219/0.69397. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68193/0.69388. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68145/0.69376. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68182/0.69366. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68048/0.69354. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68023/0.69345. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67987/0.69333. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67988/0.69329. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67902/0.69309. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67897/0.69306. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67809/0.69303. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67756/0.69282. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67757/0.69289. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67674/0.69290. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67695/0.69297. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67639/0.69284. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67565/0.69288. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67500/0.69296. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67482/0.69276. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67412/0.69298. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67374/0.69292. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67344/0.69314. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67294/0.69309. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67231/0.69349. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67236/0.69363. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67164/0.69388. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67083/0.69400. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67074/0.69417. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66976/0.69460. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66935/0.69479. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66878/0.69503. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66825/0.69519. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66820/0.69545. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66726/0.69592. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66640/0.69637. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66657/0.69681. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66539/0.69689. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66576/0.69740. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66431/0.69766. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66415/0.69796. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66314/0.69853. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66310/0.69897. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66197/0.69961. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66163/0.69995. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66084/0.70030. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66042/0.70067. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66031/0.70118. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65957/0.70164. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65830/0.70207. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65793/0.70257. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65684/0.70294. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65692/0.70305. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65568/0.70396. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65482/0.70466. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.65429/0.70463. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65356/0.70481. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65302/0.70530. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65224/0.70589. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65182/0.70632. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65112/0.70672. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65021/0.70763. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64905/0.70805. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64859/0.70851. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64690/0.70852. Took 0.09 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69274/0.69108. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69255/0.69133. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69212/0.69154. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.69176. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.69200. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69124/0.69226. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.69253. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69063/0.69286. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69027/0.69321. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68985/0.69362. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68948/0.69409. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68903/0.69460. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68841/0.69517. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68774/0.69581. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.69648. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68667/0.69722. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.69804. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68519/0.69889. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68416/0.69985. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68338/0.70085. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68268/0.70186. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68195/0.70294. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68098/0.70409. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68018/0.70520. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67917/0.70639. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67849/0.70758. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67786/0.70892. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67696/0.71027. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.67610/0.71150. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67524/0.71277. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67438/0.71420. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67360/0.71551. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67240/0.71691. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67226/0.71846. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67135/0.71981. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67079/0.72117. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66971/0.72254. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66933/0.72391. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66869/0.72499. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66799/0.72630. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.66739/0.72757. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66667/0.72877. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.66586/0.72993. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66503/0.73116. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66431/0.73237. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66348/0.73361. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66306/0.73476. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66205/0.73569. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66135/0.73694. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66032/0.73807. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65973/0.73903. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65889/0.74018. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65824/0.74116. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65757/0.74202. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65607/0.74305. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65566/0.74389. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65455/0.74500. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65385/0.74591. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65342/0.74697. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65188/0.74794. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65101/0.74882. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64954/0.74998. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64856/0.75063. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64824/0.75130. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64632/0.75256. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64628/0.75337. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64445/0.75422. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64367/0.75516. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64285/0.75611. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64186/0.75688. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.64059/0.75782. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.63907/0.75883. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63833/0.75998. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.63669/0.76100. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.63493/0.76214. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63427/0.76323. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.63266/0.76460. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63309/0.76607. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63027/0.76742. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62863/0.76904. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62835/0.77016. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62656/0.77137. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.62506/0.77292. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62388/0.77468. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62281/0.77616. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.62163/0.77778. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62117/0.77942. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61890/0.78051. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.61815/0.78210. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61608/0.78388. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61467/0.78562. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61403/0.78793. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61199/0.79004. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61037/0.79184. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60972/0.79374. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60823/0.79507. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.60742/0.79703. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.60482/0.79966. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60349/0.80140. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.60271/0.80364. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69306/0.70037. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69229/0.70037. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69222/0.70036. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69220/0.70040. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69208/0.70046. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69149/0.70053. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69133/0.70062. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69141/0.70074. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69091/0.70084. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69083/0.70100. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69047/0.70115. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69030/0.70131. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69021/0.70148. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68988/0.70169. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68971/0.70190. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68929/0.70213. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68913/0.70241. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.70263. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.70291. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68787/0.70318. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68767/0.70344. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68752/0.70370. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68708/0.70394. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68672/0.70413. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68656/0.70437. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68610/0.70459. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68589/0.70469. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68528/0.70487. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68536/0.70499. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68502/0.70514. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68424/0.70523. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68412/0.70528. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68369/0.70533. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68347/0.70532. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68309/0.70524. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68273/0.70518. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68241/0.70508. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68205/0.70500. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68143/0.70493. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68115/0.70482. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68068/0.70472. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68058/0.70460. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68030/0.70449. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67984/0.70435. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67945/0.70421. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67926/0.70407. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67908/0.70397. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67837/0.70382. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67801/0.70370. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67771/0.70352. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67739/0.70335. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67723/0.70317. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67664/0.70306. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67617/0.70291. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67604/0.70276. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67579/0.70264. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67521/0.70250. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67473/0.70233. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.67426/0.70224. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67419/0.70217. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67367/0.70218. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67304/0.70209. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67291/0.70201. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67259/0.70198. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67194/0.70195. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67209/0.70196. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67158/0.70194. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67067/0.70190. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67033/0.70193. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67003/0.70192. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66950/0.70201. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66865/0.70212. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66848/0.70229. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66812/0.70246. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66763/0.70260. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66706/0.70276. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66632/0.70283. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66658/0.70284. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66588/0.70307. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66553/0.70328. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66490/0.70348. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66447/0.70378. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66359/0.70403. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66336/0.70421. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66294/0.70456. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66194/0.70499. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66151/0.70528. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66114/0.70559. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66008/0.70599. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65963/0.70628. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65882/0.70661. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65876/0.70718. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65834/0.70762. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65736/0.70809. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.65712/0.70874. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65590/0.70915. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65566/0.70961. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65477/0.71013. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65461/0.71087. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65384/0.71141. Took 0.11 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69287/0.69658. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69663. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69186/0.69672. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69166/0.69681. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69144/0.69691. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69094/0.69702. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69050/0.69715. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69044/0.69727. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69016/0.69741. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69008/0.69753. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68948/0.69766. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68968/0.69779. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68952/0.69792. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68898/0.69804. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68879/0.69819. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68844/0.69833. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68815/0.69851. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68773/0.69868. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68714/0.69886. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68714/0.69904. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68716/0.69923. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68630/0.69947. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68579/0.69974. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68511/0.70000. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68526/0.70027. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68418/0.70054. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68407/0.70083. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68337/0.70113. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68309/0.70144. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68246/0.70173. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68192/0.70206. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68136/0.70238. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68109/0.70269. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68081/0.70297. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67979/0.70333. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67949/0.70365. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67912/0.70389. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67782/0.70425. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67792/0.70454. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67724/0.70491. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67657/0.70527. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67592/0.70561. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67528/0.70598. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67490/0.70630. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67480/0.70663. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67357/0.70693. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67356/0.70718. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67289/0.70740. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67299/0.70768. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67156/0.70793. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67093/0.70816. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67060/0.70841. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67027/0.70863. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66962/0.70878. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66902/0.70905. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66842/0.70924. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66819/0.70955. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66791/0.70972. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66668/0.71000. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66656/0.71022. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66535/0.71041. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66478/0.71063. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66447/0.71082. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66319/0.71107. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66302/0.71121. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66254/0.71125. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66170/0.71141. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66090/0.71144. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66059/0.71161. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65972/0.71178. Took 0.14 sec\n",
      "Epoch 70, Loss(train/val) 0.65862/0.71210. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65757/0.71252. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65753/0.71278. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65611/0.71297. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65516/0.71332. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65483/0.71371. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65380/0.71408. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65321/0.71435. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65135/0.71482. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65147/0.71505. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65041/0.71555. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64898/0.71607. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64837/0.71645. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64713/0.71701. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64678/0.71747. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64487/0.71768. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64444/0.71860. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64353/0.71953. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64244/0.71974. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64138/0.72046. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64062/0.72134. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64005/0.72273. Took 0.08 sec\n",
      "Epoch 92, Loss(train/val) 0.63888/0.72353. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63773/0.72426. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63727/0.72475. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63625/0.72571. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63446/0.72665. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63427/0.72755. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63222/0.72874. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63176/0.72966. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69722/0.69992. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69647/0.69839. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69517/0.69644. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69397/0.69424. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69308/0.69211. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69227/0.69056. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69178/0.68951. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69167/0.68885. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69141/0.68856. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69088/0.68845. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69069/0.68847. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69084/0.68856. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69025/0.68868. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69027/0.68885. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68980/0.68903. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68959/0.68925. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68940/0.68945. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68950/0.68969. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68952/0.68992. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68897/0.69023. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68853/0.69046. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68868/0.69073. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68797/0.69106. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68806/0.69136. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68781/0.69171. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68769/0.69210. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68754/0.69260. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68694/0.69306. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68607/0.69353. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68623/0.69410. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68585/0.69462. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68557/0.69516. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68486/0.69578. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68475/0.69650. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68437/0.69708. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68407/0.69795. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68317/0.69893. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68274/0.69986. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68201/0.70077. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68188/0.70160. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68136/0.70259. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68076/0.70374. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67975/0.70483. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67929/0.70596. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67875/0.70713. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67824/0.70839. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67741/0.70952. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67695/0.71097. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67574/0.71212. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67505/0.71312. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67455/0.71458. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67368/0.71589. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67291/0.71723. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67177/0.71860. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67114/0.71994. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67027/0.72122. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66939/0.72262. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66887/0.72374. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66759/0.72473. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66704/0.72656. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66615/0.72748. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66472/0.72914. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66428/0.72991. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66319/0.73138. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66240/0.73249. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66121/0.73446. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66107/0.73499. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65969/0.73668. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65875/0.73737. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65824/0.73861. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65690/0.74040. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65585/0.74180. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65599/0.74229. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65476/0.74342. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65435/0.74415. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65278/0.74616. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65244/0.74697. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65141/0.74817. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65104/0.74903. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65018/0.75017. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64893/0.75157. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64684/0.75283. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64637/0.75418. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64635/0.75481. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.64578/0.75632. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64519/0.75757. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64450/0.75868. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64426/0.75989. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64264/0.76061. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64223/0.76182. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64054/0.76286. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64114/0.76363. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63955/0.76510. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.63918/0.76499. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63823/0.76664. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63862/0.76692. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63589/0.76864. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63658/0.76926. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63615/0.76975. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63565/0.77159. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69725/0.70490. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69699/0.70486. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69618/0.70475. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69611/0.70456. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69520/0.70424. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69450/0.70372. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69370/0.70302. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69269/0.70222. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69193/0.70147. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69090/0.70087. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69022/0.70048. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68976/0.70027. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68948/0.70021. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68911/0.70029. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68886/0.70042. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68872/0.70061. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68813/0.70087. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68806/0.70111. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68785/0.70136. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68723/0.70165. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68698/0.70199. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68650/0.70239. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68624/0.70282. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.70319. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68546/0.70358. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68522/0.70401. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68448/0.70448. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68428/0.70497. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68411/0.70543. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68349/0.70599. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68255/0.70652. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68219/0.70708. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68147/0.70759. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68109/0.70801. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68076/0.70841. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67993/0.70888. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67939/0.70933. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67815/0.70977. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67837/0.71014. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67766/0.71057. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67676/0.71089. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67618/0.71123. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67534/0.71150. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67527/0.71167. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67421/0.71193. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67316/0.71209. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.67258/0.71234. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67208/0.71265. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67154/0.71285. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67032/0.71306. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66937/0.71323. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66864/0.71344. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66774/0.71377. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66704/0.71405. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66577/0.71457. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66500/0.71486. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66389/0.71541. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66295/0.71575. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66170/0.71603. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66010/0.71642. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65921/0.71709. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65870/0.71764. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.65728/0.71840. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65609/0.71925. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65464/0.71997. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65337/0.72107. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65302/0.72185. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65085/0.72280. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.65075/0.72395. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64930/0.72501. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64742/0.72617. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.64608/0.72751. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64431/0.72869. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64285/0.73016. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.64163/0.73176. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64147/0.73300. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63867/0.73476. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.63863/0.73616. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63724/0.73784. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.63621/0.73925. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63443/0.74073. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63271/0.74232. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63139/0.74413. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63160/0.74562. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62945/0.74740. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.62860/0.74907. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62716/0.75085. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62586/0.75233. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62429/0.75441. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.62332/0.75583. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62271/0.75717. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62048/0.75902. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.61983/0.76084. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61861/0.76233. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61722/0.76421. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61602/0.76596. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61368/0.76782. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61332/0.76954. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.61293/0.77116. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61116/0.77292. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69504/0.68789. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69417/0.68880. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69366/0.68944. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69349/0.68983. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69334/0.69011. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69268/0.69034. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69241/0.69050. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69239/0.69066. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69205/0.69074. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69170/0.69080. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69162/0.69084. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69136/0.69095. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69097/0.69100. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69081/0.69104. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69045/0.69110. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.69026/0.69120. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69009/0.69123. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68929/0.69144. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68924/0.69167. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68898/0.69176. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68841/0.69199. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68799/0.69232. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68741/0.69265. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68698/0.69301. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68636/0.69345. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68576/0.69389. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68484/0.69449. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68446/0.69513. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68372/0.69576. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68284/0.69651. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68226/0.69732. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68195/0.69813. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68099/0.69898. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68014/0.69971. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67967/0.70057. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67941/0.70115. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67814/0.70197. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67798/0.70259. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67693/0.70321. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67642/0.70371. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67552/0.70438. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67497/0.70476. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67407/0.70511. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67303/0.70555. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67233/0.70613. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67168/0.70646. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67074/0.70647. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66949/0.70693. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66868/0.70728. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66752/0.70748. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66665/0.70747. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66553/0.70766. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66458/0.70792. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66324/0.70776. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66225/0.70803. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66099/0.70834. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66006/0.70821. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65904/0.70846. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65763/0.70833. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65646/0.70860. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65576/0.70915. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65489/0.70902. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65348/0.70939. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65237/0.70966. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65160/0.71083. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65046/0.71108. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65028/0.71192. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64909/0.71217. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64816/0.71348. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64684/0.71370. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64673/0.71484. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.64519/0.71520. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64415/0.71595. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64353/0.71733. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64349/0.71789. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64251/0.71910. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64176/0.71963. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64089/0.72087. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64066/0.72167. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63871/0.72209. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63857/0.72340. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63785/0.72454. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63678/0.72534. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.63669/0.72595. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63582/0.72672. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63478/0.72813. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63444/0.72847. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63399/0.72925. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63309/0.73083. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63171/0.73086. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63117/0.73231. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63105/0.73321. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.63013/0.73410. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62922/0.73406. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62869/0.73571. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62811/0.73622. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62647/0.73763. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62642/0.73813. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.62599/0.73903. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62513/0.74065. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70596/0.69728. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.70303/0.69654. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.70057/0.69595. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69760/0.69562. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69409/0.69588. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69148/0.69672. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69044/0.69773. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68935/0.69861. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68870/0.69924. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68866/0.69964. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68859/0.69988. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68816/0.70001. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68818/0.70004. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68831/0.70001. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68782/0.69998. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68758/0.69993. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68766/0.69990. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68771/0.69982. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68749/0.69975. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68714/0.69967. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68716/0.69961. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68698/0.69951. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68687/0.69945. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.69932. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68648/0.69925. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68649/0.69919. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68614/0.69909. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68598/0.69898. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68620/0.69887. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68567/0.69880. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68569/0.69865. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68558/0.69859. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68543/0.69855. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68514/0.69853. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68483/0.69847. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68495/0.69841. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68460/0.69836. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68459/0.69830. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.68426/0.69829. Took 0.12 sec\n",
      "Epoch 39, Loss(train/val) 0.68421/0.69827. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68376/0.69826. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68389/0.69826. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68332/0.69825. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68338/0.69827. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68316/0.69840. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68282/0.69839. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68252/0.69850. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68233/0.69858. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.68255/0.69868. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68191/0.69870. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68186/0.69874. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68162/0.69882. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68153/0.69892. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68132/0.69901. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68076/0.69918. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68047/0.69935. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.68019/0.69943. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67991/0.69954. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67968/0.69967. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67948/0.69980. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67940/0.70001. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67889/0.70013. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67861/0.70025. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67823/0.70043. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67830/0.70057. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67761/0.70067. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67753/0.70095. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67753/0.70104. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67662/0.70108. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67703/0.70121. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67610/0.70145. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67595/0.70148. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.67577/0.70164. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67551/0.70174. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67481/0.70191. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67510/0.70205. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67481/0.70215. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67394/0.70217. Took 0.12 sec\n",
      "Epoch 78, Loss(train/val) 0.67420/0.70223. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67349/0.70225. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67339/0.70232. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.67236/0.70236. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67282/0.70237. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67235/0.70237. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.67166/0.70251. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67177/0.70250. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67118/0.70252. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67085/0.70263. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67031/0.70261. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66996/0.70267. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66970/0.70264. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66936/0.70257. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66880/0.70259. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66840/0.70256. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66773/0.70239. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.66757/0.70241. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66762/0.70242. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66663/0.70238. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66636/0.70220. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.66597/0.70217. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70891/0.71296. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.70194/0.70467. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69694/0.69804. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69360/0.69347. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69219/0.69075. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.68920. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69092/0.68822. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69045/0.68753. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69016/0.68700. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69019/0.68657. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68965/0.68617. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68940/0.68578. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68935/0.68542. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68882/0.68510. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68878/0.68479. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68846/0.68449. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68812/0.68417. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68828/0.68388. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68773/0.68358. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68755/0.68332. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68744/0.68306. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68713/0.68281. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68715/0.68258. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68654/0.68233. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68662/0.68213. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68640/0.68193. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68598/0.68173. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68570/0.68155. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68549/0.68137. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68537/0.68123. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68516/0.68106. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68493/0.68096. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68480/0.68087. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68472/0.68071. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68417/0.68060. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68420/0.68052. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68434/0.68043. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68398/0.68037. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68354/0.68031. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68336/0.68026. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68321/0.68020. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68283/0.68018. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68289/0.68013. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68251/0.68011. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68237/0.68002. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68187/0.68001. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68161/0.68003. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68145/0.67997. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68138/0.68000. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68075/0.68006. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68074/0.68001. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68044/0.68002. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68027/0.68001. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67995/0.68001. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67968/0.68006. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67927/0.68011. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67935/0.68018. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67868/0.68017. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67882/0.68024. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67830/0.68030. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67782/0.68035. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67766/0.68039. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67712/0.68045. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67693/0.68053. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67651/0.68060. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67643/0.68073. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67581/0.68077. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67544/0.68086. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67496/0.68096. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67457/0.68108. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67457/0.68121. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67406/0.68136. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67376/0.68149. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67335/0.68166. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67322/0.68182. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.67231/0.68195. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67196/0.68222. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67192/0.68234. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67154/0.68253. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67109/0.68268. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67035/0.68301. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66990/0.68314. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66977/0.68355. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66919/0.68370. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66887/0.68404. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66847/0.68419. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66804/0.68461. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.66680/0.68487. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66670/0.68523. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66610/0.68548. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66561/0.68586. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66510/0.68611. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66451/0.68661. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.66417/0.68706. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66378/0.68748. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.66281/0.68794. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66267/0.68834. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66243/0.68877. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66189/0.68922. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66108/0.68975. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69036/0.69725. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68971/0.69685. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68933/0.69639. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68841/0.69582. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68748/0.69523. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68732/0.69465. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68658/0.69413. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68639/0.69378. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68599/0.69355. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68549/0.69348. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68493/0.69347. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68492/0.69354. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68456/0.69368. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68414/0.69385. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68380/0.69406. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68374/0.69431. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68390/0.69457. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68327/0.69484. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68321/0.69514. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68322/0.69545. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68242/0.69580. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68230/0.69608. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68179/0.69642. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.69672. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68169/0.69706. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68126/0.69736. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68104/0.69772. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68078/0.69804. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68031/0.69835. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68014/0.69869. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67991/0.69902. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67939/0.69927. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67953/0.69957. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67904/0.69989. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67839/0.70018. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67833/0.70049. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67772/0.70075. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67762/0.70094. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67714/0.70124. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67660/0.70147. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67626/0.70168. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67563/0.70206. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67489/0.70239. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67474/0.70266. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67407/0.70289. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67400/0.70328. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67301/0.70366. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.67249/0.70394. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67195/0.70418. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67146/0.70466. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67081/0.70495. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66984/0.70534. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66959/0.70556. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66898/0.70583. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66864/0.70614. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66714/0.70660. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66747/0.70706. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.66577/0.70753. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66499/0.70807. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66383/0.70864. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66356/0.70899. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66313/0.70941. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66176/0.70986. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66077/0.71049. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66014/0.71086. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65983/0.71138. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65871/0.71173. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65750/0.71215. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65629/0.71251. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65545/0.71284. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65431/0.71309. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65370/0.71355. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65206/0.71388. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65171/0.71413. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65064/0.71442. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64947/0.71486. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64859/0.71498. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64706/0.71511. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64536/0.71537. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64573/0.71572. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64358/0.71576. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64270/0.71604. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64144/0.71610. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63949/0.71606. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.63937/0.71616. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63864/0.71641. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63602/0.71645. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63564/0.71680. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63401/0.71705. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63387/0.71678. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63316/0.71676. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63068/0.71700. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63027/0.71734. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62854/0.71762. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62697/0.71753. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62632/0.71722. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62527/0.71752. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62460/0.71718. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.62267/0.71738. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.62174/0.71722. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69483/0.69565. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69449/0.69526. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69438/0.69477. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69412/0.69411. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69374/0.69311. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69295/0.69164. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69187/0.68959. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.68721. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68928/0.68498. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68825/0.68311. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68741/0.68179. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68711/0.68090. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68677/0.68032. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68659/0.67992. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68615/0.67962. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68633/0.67944. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68615/0.67927. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68614/0.67914. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68582/0.67906. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68584/0.67896. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68563/0.67890. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68544/0.67882. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68557/0.67874. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68544/0.67866. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68517/0.67860. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68497/0.67854. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68506/0.67846. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68494/0.67840. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68448/0.67838. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68452/0.67834. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68455/0.67825. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68397/0.67818. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68367/0.67814. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68369/0.67811. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68372/0.67803. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68343/0.67798. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68320/0.67789. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68275/0.67781. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68264/0.67779. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68242/0.67771. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68186/0.67771. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68133/0.67771. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68120/0.67769. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68102/0.67765. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68040/0.67764. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68020/0.67763. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67969/0.67761. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67931/0.67759. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67919/0.67760. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67870/0.67763. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67796/0.67770. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67780/0.67773. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67681/0.67788. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67682/0.67789. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67619/0.67791. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67564/0.67818. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67538/0.67804. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67468/0.67837. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67383/0.67838. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67350/0.67822. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67297/0.67847. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67259/0.67856. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67207/0.67846. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67134/0.67857. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67081/0.67863. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67038/0.67862. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66964/0.67882. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66906/0.67889. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66816/0.67891. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66768/0.67909. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66739/0.67873. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66672/0.67887. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66622/0.67887. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66524/0.67882. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66468/0.67882. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66402/0.67919. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66337/0.67921. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66283/0.67934. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66220/0.67925. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66189/0.67937. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66087/0.67945. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.66009/0.67963. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66006/0.67987. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65885/0.68002. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65855/0.67985. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65786/0.68008. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65707/0.68050. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65649/0.68043. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65551/0.68088. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65541/0.68099. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65432/0.68153. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65375/0.68195. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 0.65323/0.68222. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65309/0.68222. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65261/0.68280. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.65131/0.68276. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65093/0.68295. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64925/0.68343. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64958/0.68430. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64856/0.68448. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.70264/0.70441. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.70271/0.70407. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.70236/0.70342. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.70150/0.70205. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69984/0.69904. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69662/0.69357. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.68739. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68750/0.68326. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68543/0.68119. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68440/0.68022. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68404/0.67974. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68387/0.67949. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68354/0.67931. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68344/0.67922. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68317/0.67920. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68320/0.67918. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68303/0.67914. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68308/0.67912. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68282/0.67907. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68296/0.67904. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68290/0.67901. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68261/0.67902. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.68240/0.67900. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68208/0.67900. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68220/0.67900. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68210/0.67899. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68181/0.67901. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68166/0.67905. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68143/0.67908. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68127/0.67915. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68097/0.67923. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68081/0.67933. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68056/0.67946. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68069/0.67955. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68026/0.67971. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68029/0.67987. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67963/0.68004. Took 0.12 sec\n",
      "Epoch 37, Loss(train/val) 0.67970/0.68019. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67948/0.68041. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67924/0.68064. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67893/0.68090. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67828/0.68111. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67824/0.68133. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67800/0.68163. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67752/0.68193. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67711/0.68225. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67704/0.68260. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67671/0.68292. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67656/0.68327. Took 0.12 sec\n",
      "Epoch 49, Loss(train/val) 0.67614/0.68362. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67593/0.68400. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67551/0.68443. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67515/0.68487. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67449/0.68541. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67414/0.68596. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67395/0.68634. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67357/0.68685. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67276/0.68755. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67283/0.68804. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67211/0.68860. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67178/0.68930. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67136/0.69002. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67052/0.69065. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67021/0.69127. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67012/0.69183. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66988/0.69234. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66872/0.69310. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66842/0.69372. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66826/0.69432. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66804/0.69494. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66681/0.69551. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66640/0.69605. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66589/0.69653. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66556/0.69742. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66546/0.69796. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66448/0.69821. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66421/0.69876. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66355/0.69940. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66290/0.69964. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66199/0.70031. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66173/0.70051. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66086/0.70088. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66033/0.70124. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65949/0.70162. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65916/0.70194. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65848/0.70255. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65805/0.70233. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65755/0.70282. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65637/0.70313. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65572/0.70341. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65530/0.70399. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65496/0.70386. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65346/0.70444. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65337/0.70472. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65203/0.70531. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65193/0.70528. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65080/0.70619. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65028/0.70617. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65011/0.70677. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64940/0.70636. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69623/0.69582. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69416/0.69448. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69212/0.69312. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68935/0.69174. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68669/0.69076. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68478/0.69041. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68348/0.69038. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68312/0.69040. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68271/0.69038. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68278/0.69030. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68249/0.69021. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68215/0.69011. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68235/0.68996. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68206/0.68980. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68187/0.68962. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68140/0.68947. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68146/0.68934. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68131/0.68918. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68117/0.68903. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68093/0.68885. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68080/0.68865. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68101/0.68847. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68051/0.68827. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68054/0.68808. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67999/0.68786. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67995/0.68766. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.67968/0.68743. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67952/0.68720. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67919/0.68696. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67867/0.68672. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67859/0.68646. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67816/0.68616. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67784/0.68584. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67750/0.68549. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67707/0.68513. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67719/0.68479. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67678/0.68446. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67617/0.68417. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67607/0.68380. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67526/0.68345. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67451/0.68315. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67451/0.68280. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67422/0.68246. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67373/0.68212. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67330/0.68185. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67273/0.68150. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67256/0.68115. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67213/0.68081. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67185/0.68054. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67120/0.68022. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67043/0.67996. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66985/0.67967. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66971/0.67941. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66916/0.67919. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66863/0.67895. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66811/0.67879. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66730/0.67858. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66691/0.67835. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66627/0.67810. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66603/0.67785. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66518/0.67758. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66454/0.67739. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66369/0.67718. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66320/0.67690. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66288/0.67692. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66209/0.67659. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66125/0.67633. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66088/0.67614. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65974/0.67585. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65911/0.67569. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65870/0.67560. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65754/0.67547. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.65677/0.67530. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65666/0.67544. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65565/0.67545. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65531/0.67523. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65471/0.67527. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65348/0.67529. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65319/0.67544. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65244/0.67538. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65246/0.67541. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.65054/0.67552. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65092/0.67560. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64990/0.67571. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64919/0.67579. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64817/0.67613. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64822/0.67626. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64782/0.67635. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64686/0.67656. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64579/0.67679. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64506/0.67686. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64452/0.67695. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64479/0.67722. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64308/0.67744. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64311/0.67773. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64157/0.67798. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.64178/0.67839. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64050/0.67864. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63994/0.67897. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63991/0.67925. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.68842/0.68788. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68846/0.68770. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68822/0.68743. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68770/0.68708. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68735/0.68661. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68678/0.68603. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68621/0.68543. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68568/0.68488. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68511/0.68447. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68482/0.68424. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68465/0.68411. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68426/0.68405. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68425/0.68404. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68393/0.68406. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68393/0.68412. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68338/0.68418. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68344/0.68426. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68343/0.68436. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68299/0.68449. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68281/0.68462. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68270/0.68476. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68239/0.68492. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68199/0.68509. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68212/0.68528. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68159/0.68551. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68134/0.68575. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68070/0.68601. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68064/0.68631. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68005/0.68664. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67987/0.68699. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67951/0.68737. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67913/0.68779. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67863/0.68826. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67806/0.68876. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67749/0.68931. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67698/0.68991. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67643/0.69051. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67589/0.69115. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67539/0.69186. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67489/0.69262. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67384/0.69341. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67348/0.69425. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67295/0.69507. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67236/0.69596. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67150/0.69684. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67047/0.69771. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66990/0.69864. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66948/0.69961. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66856/0.70055. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66775/0.70155. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66697/0.70254. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66582/0.70359. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66559/0.70462. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66424/0.70566. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66338/0.70675. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66253/0.70791. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66198/0.70899. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66111/0.71012. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65946/0.71141. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65862/0.71269. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65770/0.71406. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65709/0.71539. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65639/0.71690. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65501/0.71825. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65399/0.71968. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65278/0.72117. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65217/0.72251. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65172/0.72401. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65108/0.72547. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64983/0.72705. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64865/0.72857. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64779/0.73001. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64667/0.73160. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64573/0.73314. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64583/0.73466. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64475/0.73620. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64388/0.73773. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64293/0.73927. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64197/0.74085. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64081/0.74243. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64005/0.74405. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63920/0.74545. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63874/0.74698. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63860/0.74851. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63636/0.75022. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63566/0.75194. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63445/0.75356. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63498/0.75512. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63295/0.75664. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63317/0.75813. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63175/0.75979. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62992/0.76134. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62941/0.76295. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62907/0.76449. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62785/0.76626. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62693/0.76797. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62623/0.76952. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62495/0.77104. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62471/0.77241. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62395/0.77405. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.70486/0.69372. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.70197/0.69307. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69788/0.69274. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69353. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68789/0.69553. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68508/0.69793. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68354/0.69983. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68276/0.70115. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68242/0.70199. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68227/0.70247. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68191/0.70269. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68172/0.70275. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68171/0.70270. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68151/0.70263. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68143/0.70254. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68122/0.70234. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68120/0.70223. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68101/0.70207. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68104/0.70186. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68079/0.70169. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68078/0.70153. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68051/0.70129. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68024/0.70105. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68020/0.70086. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68005/0.70072. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67994/0.70055. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67997/0.70045. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67971/0.70039. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67971/0.70032. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67926/0.70012. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67916/0.69994. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67910/0.69986. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67899/0.69960. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67869/0.69948. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67844/0.69934. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67846/0.69919. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67801/0.69913. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67807/0.69908. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67773/0.69902. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67758/0.69885. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67743/0.69875. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67745/0.69886. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67693/0.69880. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67660/0.69873. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67649/0.69867. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67638/0.69854. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67628/0.69871. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67602/0.69877. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67594/0.69886. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67554/0.69878. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67561/0.69899. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67519/0.69896. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67520/0.69913. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67454/0.69928. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67461/0.69939. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67426/0.69932. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67384/0.69962. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67367/0.69972. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67352/0.69983. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67360/0.69997. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67313/0.70021. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67285/0.70063. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67286/0.70097. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67261/0.70115. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67234/0.70132. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67226/0.70184. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67184/0.70181. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67142/0.70209. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67177/0.70252. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67148/0.70261. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67096/0.70298. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67082/0.70315. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67065/0.70352. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67046/0.70399. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66971/0.70401. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66968/0.70430. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66928/0.70470. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66921/0.70491. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66857/0.70523. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66868/0.70564. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66810/0.70606. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66815/0.70623. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66806/0.70668. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66758/0.70715. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66724/0.70752. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66702/0.70769. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66696/0.70834. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66670/0.70881. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66600/0.70898. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66572/0.70958. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66556/0.71000. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66521/0.71089. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.66489/0.71098. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66359/0.71114. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66342/0.71187. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66328/0.71233. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66305/0.71318. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66284/0.71349. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66232/0.71409. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66171/0.71441. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69001/0.70083. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68783/0.70176. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68672/0.70269. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68601/0.70336. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68506/0.70367. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68488/0.70362. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68445/0.70323. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68416/0.70263. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68377/0.70195. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68371/0.70112. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68309/0.70025. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68305/0.69936. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68256/0.69837. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68266/0.69737. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68213/0.69634. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68191/0.69537. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68136/0.69439. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68096/0.69338. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68081/0.69237. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68080/0.69140. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68079/0.69049. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68031/0.68960. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68023/0.68867. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67965/0.68783. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.67953/0.68701. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67948/0.68627. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67933/0.68553. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67911/0.68490. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67882/0.68425. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67870/0.68361. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67840/0.68308. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67870/0.68249. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67810/0.68196. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67825/0.68161. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67786/0.68119. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67790/0.68080. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67727/0.68044. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67744/0.67992. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67735/0.67953. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67705/0.67926. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67693/0.67896. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67631/0.67873. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67663/0.67827. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67585/0.67806. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67618/0.67768. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67626/0.67745. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67569/0.67721. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67586/0.67703. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67529/0.67677. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67539/0.67664. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67497/0.67634. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67428/0.67620. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67473/0.67601. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67417/0.67588. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67415/0.67573. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67405/0.67547. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67325/0.67540. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67325/0.67527. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67318/0.67503. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67307/0.67492. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67222/0.67485. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67213/0.67486. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67222/0.67465. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67184/0.67465. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67156/0.67456. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67111/0.67452. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67098/0.67442. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67097/0.67438. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67071/0.67440. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66975/0.67434. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66970/0.67446. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66932/0.67446. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66899/0.67428. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66902/0.67422. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66811/0.67447. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66820/0.67456. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66803/0.67470. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66769/0.67443. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66668/0.67475. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66669/0.67471. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66658/0.67513. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66610/0.67506. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66566/0.67503. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66490/0.67534. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66427/0.67548. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66451/0.67541. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66431/0.67585. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66362/0.67610. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66291/0.67614. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66252/0.67663. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66281/0.67709. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66177/0.67702. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66129/0.67740. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.66095/0.67781. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66087/0.67833. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65979/0.67867. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65944/0.67866. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65947/0.67900. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65844/0.67945. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65801/0.68018. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69418/0.69788. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69098/0.69652. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68950/0.69587. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68814/0.69566. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68739/0.69562. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68725/0.69561. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68653/0.69557. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68650/0.69552. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68615/0.69547. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68592/0.69541. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68539/0.69535. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68533/0.69529. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68518/0.69526. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68504/0.69523. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68477/0.69518. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68433/0.69514. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68403/0.69512. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68390/0.69510. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68365/0.69509. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68324/0.69509. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68325/0.69511. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68288/0.69515. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68268/0.69521. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68220/0.69524. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68206/0.69530. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68188/0.69537. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68176/0.69548. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68140/0.69557. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68119/0.69568. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68096/0.69580. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68057/0.69592. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68071/0.69607. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68022/0.69622. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68022/0.69636. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68000/0.69651. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67969/0.69664. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67933/0.69678. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67935/0.69693. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67917/0.69709. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67912/0.69722. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67901/0.69733. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67870/0.69744. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67891/0.69759. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67835/0.69773. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67816/0.69788. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67813/0.69797. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67824/0.69808. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67798/0.69818. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67777/0.69830. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67759/0.69840. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67736/0.69849. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67726/0.69858. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67732/0.69867. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67718/0.69874. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67701/0.69882. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67676/0.69888. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67661/0.69897. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67644/0.69900. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67627/0.69902. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67621/0.69912. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67597/0.69918. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67586/0.69926. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67570/0.69929. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67540/0.69931. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67556/0.69940. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67505/0.69945. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67504/0.69953. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67517/0.69964. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67472/0.69966. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67446/0.69968. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67453/0.69975. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67424/0.69982. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67394/0.69992. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67386/0.70000. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67332/0.70011. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67341/0.70019. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67315/0.70022. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67319/0.70024. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67285/0.70028. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67272/0.70035. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67260/0.70036. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67262/0.70036. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67212/0.70037. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67198/0.70043. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.67145/0.70052. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.67154/0.70056. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67110/0.70065. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67095/0.70069. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67090/0.70080. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67061/0.70089. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.67048/0.70088. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66968/0.70103. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67001/0.70118. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66945/0.70119. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66903/0.70127. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66880/0.70139. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66829/0.70141. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66791/0.70149. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66787/0.70161. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66793/0.70183. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69224/0.69444. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69447. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69199/0.69449. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69158/0.69449. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69170/0.69449. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69157/0.69449. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69158/0.69449. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69107/0.69449. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.69448. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69105/0.69448. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69054/0.69445. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69055/0.69443. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69016/0.69442. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68982/0.69441. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68973/0.69438. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68932/0.69435. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68870/0.69433. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68828/0.69435. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68795/0.69439. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68704/0.69445. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68625/0.69458. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68568/0.69480. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68510/0.69511. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68409/0.69553. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68330/0.69600. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68225/0.69662. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68152/0.69730. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68047/0.69813. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67943/0.69900. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.67844/0.69998. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67790/0.70091. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67708/0.70187. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67595/0.70272. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67521/0.70358. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67464/0.70442. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67391/0.70521. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67307/0.70604. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67206/0.70671. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67175/0.70750. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67024/0.70819. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66970/0.70872. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66904/0.70929. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66833/0.70995. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66723/0.71052. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.66700/0.71120. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66616/0.71161. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66496/0.71224. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66401/0.71290. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66346/0.71336. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66251/0.71402. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66121/0.71446. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66065/0.71486. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65927/0.71551. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65856/0.71599. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65752/0.71655. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65592/0.71696. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65506/0.71774. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65378/0.71831. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65273/0.71881. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65130/0.71935. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65030/0.71998. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64900/0.72080. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64802/0.72179. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64600/0.72272. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64562/0.72348. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64341/0.72430. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64306/0.72547. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64183/0.72636. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64046/0.72767. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63953/0.72854. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63770/0.72993. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63663/0.73105. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.63627/0.73213. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63447/0.73348. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63323/0.73499. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63166/0.73670. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63036/0.73813. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62964/0.73969. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.62764/0.74145. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62744/0.74291. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62584/0.74405. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62430/0.74553. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62369/0.74725. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62228/0.74843. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.62095/0.75001. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62014/0.75125. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61920/0.75263. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61764/0.75406. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61617/0.75548. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61421/0.75696. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61404/0.75826. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61273/0.75962. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61174/0.76115. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.61000/0.76276. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60880/0.76397. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60877/0.76587. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60739/0.76671. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60608/0.76749. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60311/0.76888. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.60301/0.77044. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69185/0.69226. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69084/0.69215. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69056/0.69209. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69039/0.69208. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69031/0.69212. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68940/0.69217. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68952/0.69227. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68917/0.69237. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68897/0.69251. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68870/0.69266. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68814/0.69285. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68803/0.69305. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68787/0.69330. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68756/0.69356. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68705/0.69388. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68698/0.69421. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68651/0.69460. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68599/0.69501. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68538/0.69543. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68541/0.69590. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68502/0.69641. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68457/0.69692. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68435/0.69744. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68390/0.69794. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68398/0.69847. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68368/0.69897. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68330/0.69947. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68314/0.69998. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68289/0.70047. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68284/0.70090. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68200/0.70134. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68199/0.70174. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68217/0.70215. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68175/0.70257. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68137/0.70299. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68153/0.70333. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68102/0.70368. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68083/0.70400. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68067/0.70433. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68045/0.70464. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68027/0.70493. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68023/0.70524. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68004/0.70551. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68005/0.70578. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67964/0.70604. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67949/0.70632. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67898/0.70660. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67946/0.70681. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67927/0.70702. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67899/0.70722. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67856/0.70741. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67833/0.70762. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67842/0.70790. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67816/0.70812. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67748/0.70835. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67732/0.70858. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67734/0.70885. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67717/0.70909. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67693/0.70933. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67635/0.70953. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67631/0.70979. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67626/0.70994. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67604/0.71018. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67562/0.71047. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67530/0.71074. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67516/0.71097. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67462/0.71128. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67396/0.71152. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67438/0.71175. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67364/0.71196. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.67354/0.71226. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67319/0.71247. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67303/0.71269. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67233/0.71298. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67256/0.71328. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67215/0.71358. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67122/0.71392. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67146/0.71426. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67072/0.71466. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67054/0.71499. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67042/0.71533. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66989/0.71563. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66931/0.71589. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66887/0.71627. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66851/0.71666. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66808/0.71704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66786/0.71743. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66750/0.71787. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66687/0.71825. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66647/0.71867. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66630/0.71903. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66583/0.71942. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66510/0.71983. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66506/0.72034. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66440/0.72081. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66413/0.72117. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66334/0.72167. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66373/0.72215. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66322/0.72261. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66231/0.72307. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69051/0.70407. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69045/0.70414. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69021/0.70416. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69012/0.70420. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68999/0.70425. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68979/0.70436. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.70440. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68941/0.70447. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68926/0.70456. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68890/0.70461. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68900/0.70462. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68875/0.70464. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68840/0.70467. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68808/0.70468. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68832/0.70474. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68799/0.70475. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68750/0.70477. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68713/0.70476. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68690/0.70470. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68669/0.70468. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68646/0.70473. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68571/0.70470. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68577/0.70474. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68508/0.70474. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68496/0.70477. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68440/0.70471. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68396/0.70479. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68371/0.70483. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68336/0.70505. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68316/0.70500. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68277/0.70498. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68223/0.70517. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68216/0.70547. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68149/0.70552. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68138/0.70541. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68075/0.70556. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68039/0.70557. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68007/0.70571. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67952/0.70585. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67922/0.70611. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67883/0.70580. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67858/0.70585. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67800/0.70587. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67770/0.70587. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67705/0.70587. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67671/0.70595. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67654/0.70602. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67596/0.70617. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67574/0.70611. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67526/0.70585. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67438/0.70587. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67407/0.70570. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67407/0.70562. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67291/0.70555. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67263/0.70550. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67214/0.70552. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67159/0.70528. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67145/0.70525. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67072/0.70516. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66989/0.70522. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66925/0.70518. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66903/0.70524. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66852/0.70510. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66821/0.70503. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66773/0.70499. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66749/0.70505. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66614/0.70485. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66640/0.70499. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66541/0.70522. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66489/0.70530. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66451/0.70558. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66343/0.70556. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66312/0.70551. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66255/0.70565. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66196/0.70569. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66101/0.70575. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66077/0.70610. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66027/0.70609. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65935/0.70638. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65881/0.70680. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65853/0.70734. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65725/0.70742. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65696/0.70813. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65614/0.70838. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65553/0.70890. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65476/0.70995. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65390/0.71003. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65286/0.71064. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65231/0.71122. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65165/0.71174. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65082/0.71271. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64914/0.71334. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64843/0.71398. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64830/0.71469. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64745/0.71516. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64672/0.71597. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64552/0.71674. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64458/0.71784. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64340/0.71888. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64321/0.72009. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69325/0.68994. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.68965. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69234/0.68943. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69223/0.68930. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.68919. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69196/0.68911. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.68904. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69143/0.68901. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69126/0.68899. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69097/0.68898. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69114/0.68900. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69084/0.68902. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69046/0.68904. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69034/0.68908. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68999/0.68914. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68993/0.68922. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68972/0.68930. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68963/0.68938. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68941/0.68949. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68919/0.68965. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68903/0.68982. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68834/0.69002. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68825/0.69022. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68763/0.69047. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68754/0.69074. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68716/0.69105. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68673/0.69132. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68632/0.69163. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68603/0.69198. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68573/0.69230. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68556/0.69266. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68497/0.69300. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68465/0.69344. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68415/0.69385. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68416/0.69418. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68405/0.69452. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68357/0.69491. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68284/0.69531. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68298/0.69565. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68227/0.69598. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68204/0.69628. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68176/0.69664. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68137/0.69696. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68101/0.69729. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68042/0.69766. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68038/0.69800. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67944/0.69834. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67933/0.69867. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67870/0.69897. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67853/0.69929. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67794/0.69963. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67706/0.70003. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67723/0.70039. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67603/0.70088. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67538/0.70133. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67560/0.70176. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67455/0.70209. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67401/0.70259. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67321/0.70306. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67225/0.70344. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67151/0.70397. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67096/0.70445. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67029/0.70499. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66928/0.70547. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66796/0.70610. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66769/0.70678. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66699/0.70750. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66610/0.70835. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66540/0.70913. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66434/0.70981. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66377/0.71046. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66248/0.71144. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66199/0.71231. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66125/0.71294. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65989/0.71396. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65985/0.71480. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65905/0.71562. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65847/0.71634. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65767/0.71721. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65691/0.71822. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65633/0.71905. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65540/0.72014. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65524/0.72109. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65372/0.72168. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65314/0.72247. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65234/0.72351. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65215/0.72425. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65184/0.72486. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65080/0.72586. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64951/0.72664. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64928/0.72736. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64889/0.72795. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64811/0.72910. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64735/0.72961. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64659/0.73043. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64611/0.73118. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.64605/0.73194. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64414/0.73249. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64452/0.73352. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64412/0.73424. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69180/0.69447. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69160/0.69424. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69125/0.69402. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69383. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69100/0.69363. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69080/0.69344. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69060/0.69326. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69059/0.69308. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68993/0.69289. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68997/0.69270. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68927/0.69252. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68880/0.69234. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68884/0.69216. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68826/0.69200. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68795/0.69184. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68744/0.69167. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68671/0.69152. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68634/0.69139. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68581/0.69132. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68461/0.69131. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68418/0.69135. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68328/0.69150. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68229/0.69170. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68172/0.69196. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68092/0.69234. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67986/0.69279. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67917/0.69327. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67843/0.69376. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67782/0.69425. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67718/0.69481. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67619/0.69533. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67571/0.69590. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67525/0.69635. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67509/0.69683. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67437/0.69733. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.67381/0.69782. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67309/0.69838. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67327/0.69891. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67207/0.69943. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67179/0.70002. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67122/0.70060. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67069/0.70115. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67047/0.70174. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67004/0.70232. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66914/0.70292. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66872/0.70360. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66841/0.70426. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66809/0.70486. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66753/0.70546. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66724/0.70604. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66685/0.70663. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66635/0.70728. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66591/0.70790. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66539/0.70854. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66462/0.70915. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66449/0.70978. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66388/0.71046. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66313/0.71110. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66327/0.71171. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66212/0.71231. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66197/0.71296. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66139/0.71356. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66053/0.71422. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66111/0.71472. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65985/0.71537. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65918/0.71599. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65903/0.71652. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65841/0.71691. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65789/0.71746. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65656/0.71805. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65705/0.71855. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65560/0.71888. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65499/0.71939. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65399/0.71993. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65385/0.72055. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65349/0.72099. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.65314/0.72142. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65173/0.72187. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65118/0.72226. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65046/0.72272. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65022/0.72309. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64907/0.72354. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64843/0.72394. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64761/0.72438. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64798/0.72478. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64565/0.72534. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64486/0.72585. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64444/0.72614. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64388/0.72665. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64301/0.72723. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64212/0.72785. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64103/0.72830. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64007/0.72891. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63892/0.72945. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63835/0.73026. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63782/0.73091. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63639/0.73167. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63533/0.73235. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63588/0.73307. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63453/0.73358. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69294/0.68996. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69203/0.68987. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69142/0.68981. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69083/0.68975. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69061/0.68970. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69037/0.68965. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69012/0.68958. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68986/0.68949. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68909/0.68946. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68899/0.68941. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68890/0.68937. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68827/0.68933. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68772/0.68929. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68748/0.68924. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68679/0.68917. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68632/0.68909. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68598/0.68902. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68576/0.68893. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.68878. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68469/0.68863. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68392/0.68855. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68347/0.68840. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.68823. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68324/0.68802. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68288/0.68778. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68257/0.68751. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68220/0.68728. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68239/0.68696. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68198/0.68664. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68186/0.68631. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68154/0.68599. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68097/0.68564. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68054/0.68534. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68050/0.68515. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68045/0.68490. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68045/0.68463. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67976/0.68438. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67936/0.68414. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67927/0.68395. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67920/0.68376. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67876/0.68352. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67853/0.68330. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67806/0.68324. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67821/0.68300. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67798/0.68274. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67731/0.68269. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67681/0.68250. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67638/0.68244. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67659/0.68225. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67568/0.68223. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67556/0.68208. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67519/0.68216. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67497/0.68215. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67431/0.68209. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67439/0.68217. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67314/0.68199. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67311/0.68191. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67219/0.68175. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67185/0.68184. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67135/0.68189. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67086/0.68201. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67018/0.68176. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67064/0.68184. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66917/0.68200. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66919/0.68204. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66838/0.68210. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66778/0.68224. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66729/0.68253. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66637/0.68268. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66622/0.68260. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66469/0.68271. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66412/0.68275. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66393/0.68293. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66304/0.68311. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66218/0.68349. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66132/0.68352. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66131/0.68374. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65994/0.68369. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65916/0.68402. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65847/0.68421. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65802/0.68460. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65676/0.68493. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65561/0.68517. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65567/0.68527. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65480/0.68557. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65376/0.68587. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65166/0.68593. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65254/0.68637. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65159/0.68700. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65174/0.68734. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64950/0.68747. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64935/0.68777. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64898/0.68812. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64756/0.68836. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64659/0.68854. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64716/0.68893. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64590/0.68888. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64518/0.68944. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64387/0.68995. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64294/0.69072. Took 0.10 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69714/0.66999. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69319/0.68173. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.69176. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68985/0.69841. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68868/0.70180. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68831/0.70331. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68777/0.70399. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68733/0.70410. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68715/0.70452. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.68700/0.70466. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68632/0.70493. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68613/0.70513. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68575/0.70528. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68525/0.70549. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68503/0.70579. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68531/0.70607. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68468/0.70623. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68465/0.70618. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68438/0.70645. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68408/0.70663. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68383/0.70670. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68378/0.70679. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68360/0.70701. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68318/0.70719. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68298/0.70716. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68295/0.70725. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68258/0.70738. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68219/0.70749. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68183/0.70760. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68212/0.70782. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68223/0.70792. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68146/0.70790. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68114/0.70829. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68126/0.70852. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68115/0.70847. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68063/0.70855. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68008/0.70876. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68044/0.70897. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67997/0.70902. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67965/0.70939. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67964/0.70946. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67937/0.70958. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67930/0.70974. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67896/0.70979. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67852/0.71003. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.67808/0.71005. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67789/0.71001. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67757/0.71042. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67733/0.71090. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67687/0.71080. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67655/0.71081. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67640/0.71113. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67573/0.71127. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67596/0.71133. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67524/0.71242. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67481/0.71211. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67450/0.71256. Took 0.08 sec\n",
      "Epoch 57, Loss(train/val) 0.67407/0.71252. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67370/0.71271. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67339/0.71337. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67267/0.71299. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67212/0.71395. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67180/0.71442. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67172/0.71446. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67112/0.71529. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67013/0.71537. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66997/0.71561. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66954/0.71670. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66898/0.71664. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66873/0.71771. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66798/0.71768. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66716/0.71808. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66715/0.72004. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66659/0.71948. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66606/0.71967. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66538/0.72092. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66466/0.72168. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66468/0.72244. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66409/0.72298. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66272/0.72333. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66238/0.72424. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66186/0.72481. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66145/0.72655. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66074/0.72615. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66051/0.72785. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65948/0.72961. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65933/0.72785. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65822/0.73052. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65787/0.73150. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65744/0.73183. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65706/0.73164. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65706/0.73388. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65532/0.73355. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65503/0.73511. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65440/0.73680. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65421/0.73669. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65347/0.73825. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65300/0.73880. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65242/0.73962. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65208/0.74074. Took 0.10 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69439/0.70223. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.70034. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69170/0.69919. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69072/0.69854. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69044/0.69831. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68974/0.69842. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68928/0.69877. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68908/0.69927. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68850/0.69989. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68824/0.70061. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68806/0.70138. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.68715/0.70226. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68693/0.70320. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68653/0.70417. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68608/0.70524. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68586/0.70623. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68524/0.70725. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68516/0.70833. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68482/0.70934. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68445/0.71037. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68387/0.71144. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68377/0.71234. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68331/0.71330. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68299/0.71423. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68257/0.71512. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68228/0.71601. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68193/0.71675. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68177/0.71757. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68158/0.71828. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68126/0.71897. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68093/0.71967. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68048/0.72029. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68050/0.72087. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68018/0.72150. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67980/0.72207. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67971/0.72264. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67919/0.72309. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67925/0.72359. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67893/0.72409. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67847/0.72457. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67794/0.72500. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67790/0.72549. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67771/0.72591. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67725/0.72631. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67703/0.72675. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67704/0.72716. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67624/0.72758. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67621/0.72804. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67552/0.72830. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67528/0.72869. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67477/0.72933. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67447/0.72982. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67433/0.73028. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67390/0.73072. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67333/0.73119. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67350/0.73163. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67253/0.73200. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67177/0.73259. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67163/0.73315. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67136/0.73365. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67088/0.73413. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67051/0.73470. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67007/0.73534. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66934/0.73595. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66830/0.73649. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66798/0.73728. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66729/0.73785. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66692/0.73848. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66661/0.73917. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66542/0.73987. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66503/0.74082. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66470/0.74154. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66355/0.74236. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66360/0.74316. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66262/0.74409. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66190/0.74498. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66088/0.74601. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66056/0.74711. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65934/0.74838. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65896/0.74936. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65836/0.75058. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65705/0.75138. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65596/0.75287. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65587/0.75417. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65475/0.75538. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65447/0.75704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65333/0.75846. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65233/0.75993. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65172/0.76165. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65129/0.76280. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65032/0.76417. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64934/0.76627. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64846/0.76781. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64795/0.76944. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64750/0.77070. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64639/0.77227. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64516/0.77385. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64530/0.77554. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64405/0.77734. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64288/0.77897. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69442/0.69119. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69408/0.69131. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69409/0.69144. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69364/0.69154. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69364/0.69165. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69347/0.69175. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69323/0.69184. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69317/0.69195. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69299/0.69206. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69304/0.69218. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69291/0.69231. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69257/0.69244. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69224/0.69258. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69220/0.69274. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69200/0.69292. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69186/0.69312. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69142/0.69333. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69095/0.69359. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69087/0.69389. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69041/0.69425. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68977/0.69466. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68942/0.69516. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68853/0.69567. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68791/0.69628. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68700/0.69700. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68574/0.69781. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68478/0.69865. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68327/0.69933. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68153/0.69994. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68032/0.70056. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67794/0.70107. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67592/0.70155. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67364/0.70208. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67146/0.70247. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66952/0.70305. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66759/0.70352. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66520/0.70412. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66365/0.70471. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.66176/0.70559. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65990/0.70640. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65848/0.70723. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65715/0.70817. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65556/0.70898. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65458/0.70964. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65335/0.71079. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65257/0.71217. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65081/0.71234. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65024/0.71336. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64916/0.71386. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64818/0.71492. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64718/0.71526. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64656/0.71653. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64533/0.71717. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.64442/0.71781. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64349/0.71851. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64286/0.71964. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64132/0.72004. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64144/0.72086. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.64047/0.72151. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63968/0.72215. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.63899/0.72337. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63867/0.72327. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63781/0.72390. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63689/0.72497. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63634/0.72548. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63577/0.72572. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63521/0.72687. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63442/0.72784. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63338/0.72852. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63289/0.72865. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63181/0.73011. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63133/0.73079. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63103/0.73137. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63017/0.73197. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62995/0.73260. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62925/0.73325. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62851/0.73338. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62845/0.73480. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62716/0.73498. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.62684/0.73618. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62599/0.73656. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62552/0.73750. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62490/0.73857. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62428/0.73845. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62411/0.73966. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62327/0.73919. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62170/0.74039. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62094/0.74092. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62061/0.74170. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62062/0.74313. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61933/0.74317. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61871/0.74410. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61820/0.74549. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61840/0.74416. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.61740/0.74615. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61617/0.74559. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61596/0.74705. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61567/0.74710. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61476/0.74958. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61412/0.74913. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69472/0.69509. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69437/0.69504. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69439/0.69499. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69395/0.69495. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69389/0.69490. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69370/0.69485. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69352/0.69480. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69304/0.69474. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69303/0.69466. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69249/0.69460. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69255/0.69454. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69244/0.69447. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69216/0.69441. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69165/0.69435. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69150/0.69427. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69136/0.69418. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69089/0.69412. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69083/0.69405. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69045/0.69399. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69009/0.69397. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68978/0.69393. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68941/0.69391. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68907/0.69391. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68884/0.69392. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68846/0.69395. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68813/0.69403. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68794/0.69415. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68732/0.69430. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68727/0.69446. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68688/0.69469. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68655/0.69495. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68627/0.69523. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68581/0.69555. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68547/0.69589. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68515/0.69625. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68446/0.69663. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68445/0.69704. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68390/0.69747. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68326/0.69793. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68317/0.69844. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68254/0.69890. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68210/0.69939. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68180/0.69984. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68082/0.70025. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68075/0.70073. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68012/0.70125. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67981/0.70174. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67925/0.70218. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67862/0.70256. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67786/0.70298. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67748/0.70341. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67656/0.70379. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67594/0.70422. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67553/0.70463. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67492/0.70501. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67396/0.70544. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67392/0.70594. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67239/0.70633. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67241/0.70676. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67119/0.70719. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67071/0.70755. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67000/0.70794. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66936/0.70838. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66840/0.70876. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66789/0.70928. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66685/0.70990. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66601/0.71058. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66572/0.71122. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66449/0.71185. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66375/0.71250. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66342/0.71313. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66258/0.71374. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66220/0.71453. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66133/0.71520. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66063/0.71591. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66050/0.71654. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65899/0.71714. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65857/0.71806. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65801/0.71887. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65748/0.71951. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65684/0.72019. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65663/0.72096. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65616/0.72174. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65501/0.72242. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65442/0.72325. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65405/0.72383. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65360/0.72450. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65218/0.72530. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65187/0.72604. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65144/0.72672. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65081/0.72751. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64989/0.72841. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64944/0.72917. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64843/0.72992. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64834/0.73075. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64742/0.73140. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64688/0.73210. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.64656/0.73282. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64584/0.73338. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64529/0.73412. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69360/0.70451. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.70385. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69292/0.70335. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69237/0.70296. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69242/0.70265. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69189/0.70238. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69170/0.70218. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69149/0.70202. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69118/0.70189. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69087/0.70178. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.70170. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69069/0.70162. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69002/0.70160. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69004/0.70159. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68971/0.70163. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.70172. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68910/0.70180. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68842/0.70195. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68796/0.70213. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68772/0.70234. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68735/0.70255. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68670/0.70285. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68671/0.70319. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68606/0.70354. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68532/0.70399. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68525/0.70449. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68463/0.70499. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68409/0.70554. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68415/0.70612. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68311/0.70670. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68282/0.70737. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68237/0.70792. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68173/0.70855. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68167/0.70917. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68060/0.70977. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68013/0.71032. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67978/0.71085. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67916/0.71140. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67890/0.71192. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67847/0.71237. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67799/0.71281. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67738/0.71324. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67723/0.71372. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67656/0.71417. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67612/0.71460. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67585/0.71502. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67517/0.71534. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67426/0.71570. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67425/0.71603. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67348/0.71636. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67311/0.71674. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67263/0.71709. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67196/0.71731. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67166/0.71769. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67144/0.71799. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67117/0.71830. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67055/0.71861. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66927/0.71891. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66917/0.71916. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66850/0.71948. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66828/0.71979. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66735/0.72010. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66693/0.72032. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66608/0.72059. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66591/0.72084. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66519/0.72102. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66488/0.72129. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66352/0.72161. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66374/0.72181. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66325/0.72210. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66237/0.72239. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66117/0.72271. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66085/0.72303. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66072/0.72337. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65957/0.72369. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65858/0.72397. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65795/0.72450. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65794/0.72492. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65700/0.72518. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65654/0.72555. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65521/0.72580. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65538/0.72639. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65373/0.72681. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65335/0.72709. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65260/0.72756. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65227/0.72802. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65076/0.72827. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64982/0.72883. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64918/0.72944. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64828/0.72996. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64825/0.73054. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64750/0.73110. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64644/0.73168. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64514/0.73210. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64508/0.73292. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64384/0.73367. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64338/0.73414. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64225/0.73488. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64251/0.73572. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64072/0.73641. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69389/0.68093. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69341/0.68146. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69324/0.68202. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69302/0.68254. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69268/0.68305. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69231/0.68359. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69212/0.68412. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69192/0.68460. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69157/0.68506. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69134/0.68551. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69106/0.68595. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69087/0.68635. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69069/0.68673. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69039/0.68706. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69001/0.68733. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68969/0.68754. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68937/0.68774. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68914/0.68788. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68885/0.68815. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68842/0.68827. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68838/0.68836. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68812/0.68852. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68767/0.68859. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68723/0.68879. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68690/0.68911. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68645/0.68922. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68609/0.68940. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68583/0.68963. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68574/0.68987. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68529/0.69016. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68479/0.69056. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68443/0.69102. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68403/0.69148. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68379/0.69167. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68362/0.69210. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68322/0.69242. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68257/0.69299. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68255/0.69344. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68215/0.69382. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68175/0.69429. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68172/0.69494. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68157/0.69543. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68093/0.69597. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68086/0.69655. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68035/0.69690. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68037/0.69748. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67974/0.69778. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67956/0.69838. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67947/0.69911. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67897/0.69945. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67886/0.69986. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67798/0.70039. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67795/0.70078. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67802/0.70106. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67734/0.70157. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67686/0.70207. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67712/0.70254. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67667/0.70306. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67613/0.70359. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67617/0.70399. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67524/0.70421. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67528/0.70484. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67484/0.70512. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67436/0.70560. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67415/0.70604. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67416/0.70654. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67326/0.70686. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67334/0.70733. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67273/0.70782. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67251/0.70843. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67229/0.70854. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67247/0.70925. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67120/0.70958. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67062/0.70985. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67101/0.71080. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66956/0.71101. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66971/0.71131. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66947/0.71192. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66874/0.71228. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66823/0.71298. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66766/0.71332. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66694/0.71391. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66652/0.71449. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66627/0.71489. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66614/0.71577. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66458/0.71567. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66469/0.71651. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66435/0.71715. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66340/0.71737. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66350/0.71811. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66312/0.71849. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66256/0.71913. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66186/0.71966. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66141/0.72000. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66086/0.72050. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65959/0.72093. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65954/0.72138. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65845/0.72171. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65779/0.72231. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65721/0.72263. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69866/0.69643. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69668/0.69507. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69553/0.69427. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69459/0.69382. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69372/0.69366. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69302/0.69369. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69276/0.69392. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69232/0.69427. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69183/0.69465. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69146/0.69509. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69109/0.69554. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69083/0.69599. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69003/0.69647. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68985/0.69698. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68968/0.69742. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68957/0.69785. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68890/0.69821. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68842/0.69857. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68843/0.69892. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68799/0.69923. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68801/0.69948. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68773/0.69972. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68728/0.69992. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68727/0.70010. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68699/0.70024. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68672/0.70036. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68652/0.70041. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68602/0.70044. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68626/0.70056. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68611/0.70063. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68550/0.70064. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68593/0.70063. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68525/0.70067. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68524/0.70078. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68501/0.70085. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68471/0.70090. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68448/0.70097. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68438/0.70109. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68395/0.70121. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68386/0.70128. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68370/0.70150. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68328/0.70168. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68295/0.70173. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68291/0.70191. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68264/0.70205. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68225/0.70227. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.68197/0.70243. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68178/0.70270. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68147/0.70300. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68130/0.70332. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68094/0.70366. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68112/0.70388. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68029/0.70414. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68015/0.70447. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67978/0.70472. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67967/0.70511. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67938/0.70556. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67893/0.70596. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67856/0.70641. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67827/0.70680. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67811/0.70720. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67736/0.70776. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67725/0.70830. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67664/0.70871. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67660/0.70921. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67595/0.70977. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67560/0.71044. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67565/0.71092. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67480/0.71137. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67444/0.71214. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67412/0.71280. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67345/0.71326. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67295/0.71388. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67261/0.71458. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67210/0.71540. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67116/0.71605. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67082/0.71681. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67065/0.71762. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66976/0.71830. Took 0.08 sec\n",
      "Epoch 79, Loss(train/val) 0.66997/0.71902. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66906/0.71966. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66863/0.72070. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66789/0.72149. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66736/0.72260. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66621/0.72355. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66599/0.72429. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66531/0.72529. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66507/0.72647. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66346/0.72757. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66359/0.72855. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66275/0.73002. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66123/0.73098. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66126/0.73217. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66050/0.73324. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66097/0.73450. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66003/0.73553. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65869/0.73664. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65821/0.73796. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65750/0.73936. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65670/0.74038. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.70136/0.69775. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.70066/0.69746. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.70021/0.69726. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69958/0.69709. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69929/0.69694. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69892/0.69676. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69883/0.69655. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69827/0.69622. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69758/0.69570. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69660/0.69491. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69500/0.69389. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69355/0.69287. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69196/0.69213. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69087/0.69176. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69018/0.69165. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68989/0.69164. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68953/0.69165. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68922/0.69167. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68901/0.69168. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68870/0.69166. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68850/0.69164. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68829/0.69163. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68805/0.69160. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68786/0.69158. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68754/0.69157. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68713/0.69155. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68688/0.69153. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68670/0.69151. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68619/0.69152. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68589/0.69153. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68571/0.69158. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68492/0.69164. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68455/0.69171. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68385/0.69179. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68357/0.69191. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68286/0.69205. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68241/0.69226. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68206/0.69250. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68124/0.69282. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68059/0.69321. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67987/0.69367. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67903/0.69417. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67822/0.69474. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67715/0.69542. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67650/0.69613. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67572/0.69687. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67495/0.69769. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67407/0.69853. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67337/0.69941. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67250/0.70034. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67152/0.70127. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67048/0.70225. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66996/0.70324. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66904/0.70412. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66852/0.70513. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66749/0.70616. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66653/0.70718. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66555/0.70805. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66466/0.70909. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66356/0.71017. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66350/0.71111. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66208/0.71211. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66123/0.71313. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66021/0.71422. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65943/0.71520. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65879/0.71615. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65791/0.71736. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65692/0.71855. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65582/0.71987. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65505/0.72099. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65399/0.72198. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65327/0.72329. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65241/0.72435. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65192/0.72571. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65069/0.72689. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64940/0.72834. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64895/0.72964. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64779/0.73087. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64693/0.73220. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64616/0.73347. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64571/0.73463. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64449/0.73595. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64338/0.73742. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64183/0.73893. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64168/0.74017. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64127/0.74153. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63978/0.74279. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63888/0.74410. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63813/0.74566. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63719/0.74693. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63635/0.74827. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63546/0.74993. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63513/0.75131. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63336/0.75269. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63213/0.75439. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63179/0.75572. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63084/0.75707. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63019/0.75883. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62893/0.76059. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62756/0.76249. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69416/0.69729. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69288/0.69738. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69220/0.69753. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69130/0.69774. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69094/0.69798. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.69821. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68968/0.69843. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68909/0.69864. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68912/0.69880. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68918/0.69889. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68874/0.69900. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.69906. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68820/0.69908. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68785/0.69906. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68764/0.69910. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68772/0.69907. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68729/0.69906. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68711/0.69902. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68702/0.69896. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68657/0.69892. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68639/0.69884. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68624/0.69874. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68602/0.69863. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68548/0.69853. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68516/0.69853. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68508/0.69841. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68468/0.69830. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68419/0.69828. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68364/0.69819. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68375/0.69808. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68365/0.69794. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68317/0.69781. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68353/0.69773. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68309/0.69769. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68213/0.69762. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68206/0.69745. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68203/0.69728. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68194/0.69712. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68134/0.69689. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68139/0.69679. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68108/0.69664. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68061/0.69648. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68055/0.69641. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68018/0.69629. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67943/0.69620. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67921/0.69607. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67886/0.69605. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67863/0.69595. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67852/0.69575. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67850/0.69575. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67784/0.69565. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67745/0.69572. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67764/0.69570. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67664/0.69563. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67607/0.69566. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67611/0.69561. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67612/0.69555. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67515/0.69560. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67559/0.69560. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67508/0.69556. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67435/0.69571. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67394/0.69581. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67359/0.69594. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67350/0.69598. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67292/0.69621. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67166/0.69629. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67182/0.69637. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67125/0.69664. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67070/0.69688. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67058/0.69691. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66990/0.69722. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66991/0.69734. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66885/0.69767. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66833/0.69771. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66775/0.69796. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66763/0.69834. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66700/0.69862. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66628/0.69892. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66551/0.69922. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66546/0.69944. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66436/0.69976. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66386/0.70016. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66325/0.70050. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66363/0.70076. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66261/0.70099. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66171/0.70143. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66086/0.70187. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66109/0.70220. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65920/0.70291. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65957/0.70309. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65901/0.70334. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65767/0.70380. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65808/0.70435. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65665/0.70499. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65601/0.70543. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65597/0.70615. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65463/0.70688. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65496/0.70714. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65340/0.70781. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65326/0.70810. Took 0.11 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69534/0.69506. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69287/0.69434. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69095/0.69401. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68994/0.69401. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68914/0.69421. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68828/0.69448. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68785/0.69472. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68759/0.69494. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68788/0.69506. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68720/0.69512. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68679/0.69516. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68686/0.69514. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68648/0.69516. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68654/0.69513. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68620/0.69510. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68556/0.69504. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68596/0.69505. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68562/0.69501. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68515/0.69499. Took 0.08 sec\n",
      "Epoch 19, Loss(train/val) 0.68497/0.69497. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68502/0.69498. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68476/0.69495. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68446/0.69496. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68399/0.69494. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68388/0.69492. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68373/0.69490. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68328/0.69490. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68321/0.69490. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68261/0.69490. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68263/0.69489. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68251/0.69487. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68215/0.69495. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68212/0.69498. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68185/0.69509. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68084/0.69516. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68081/0.69522. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68038/0.69533. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68013/0.69539. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68001/0.69545. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67956/0.69552. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67943/0.69564. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67901/0.69578. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67849/0.69593. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67841/0.69610. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67758/0.69623. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67746/0.69636. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67802/0.69655. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67668/0.69673. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67675/0.69689. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67546/0.69710. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67593/0.69726. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67535/0.69735. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67490/0.69741. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67474/0.69765. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67433/0.69760. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67422/0.69768. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67373/0.69790. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67352/0.69779. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67285/0.69799. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67202/0.69801. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67207/0.69799. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67113/0.69818. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67083/0.69823. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67087/0.69826. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67018/0.69830. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67005/0.69841. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66936/0.69829. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66866/0.69828. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66885/0.69814. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66807/0.69817. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66801/0.69793. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66758/0.69797. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66731/0.69802. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66675/0.69806. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66622/0.69795. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66574/0.69806. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66544/0.69781. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66464/0.69803. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66473/0.69761. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66315/0.69760. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66339/0.69758. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66201/0.69745. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66215/0.69742. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66142/0.69743. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66047/0.69746. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66006/0.69714. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65950/0.69723. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65965/0.69696. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65906/0.69694. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65759/0.69682. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65771/0.69664. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65606/0.69660. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65598/0.69634. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65526/0.69628. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65441/0.69618. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65425/0.69610. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65369/0.69594. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65286/0.69574. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65214/0.69561. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65147/0.69550. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69145/0.68998. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69069/0.68898. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.68979/0.68794. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68881/0.68690. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68799/0.68597. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68706/0.68529. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68657/0.68487. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68611/0.68471. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68593/0.68470. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68553/0.68477. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68535/0.68492. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68501/0.68511. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68464/0.68532. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68454/0.68556. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68446/0.68583. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68425/0.68613. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68383/0.68644. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.68676. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68330/0.68709. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68308/0.68745. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68295/0.68783. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68260/0.68822. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68238/0.68862. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.68905. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68156/0.68953. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68141/0.69000. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68129/0.69048. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68061/0.69098. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68069/0.69149. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68033/0.69206. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68005/0.69260. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67969/0.69313. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67926/0.69374. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67906/0.69428. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.67855/0.69492. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67844/0.69553. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67811/0.69615. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67740/0.69678. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67751/0.69743. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67679/0.69810. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67648/0.69876. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67612/0.69943. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67593/0.70000. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67555/0.70073. Took 0.08 sec\n",
      "Epoch 44, Loss(train/val) 0.67502/0.70148. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67456/0.70221. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67458/0.70297. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67379/0.70366. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67350/0.70430. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67303/0.70519. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67222/0.70608. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67198/0.70682. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67144/0.70753. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67086/0.70848. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67038/0.70934. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66989/0.71033. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66912/0.71117. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66900/0.71203. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66813/0.71314. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66783/0.71417. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66749/0.71503. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66652/0.71614. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66607/0.71716. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66506/0.71825. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.66482/0.71964. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66407/0.72062. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66351/0.72191. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66283/0.72293. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66203/0.72422. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66144/0.72560. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66075/0.72681. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66021/0.72794. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65970/0.72923. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65828/0.73076. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65803/0.73166. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65682/0.73303. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65624/0.73429. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65584/0.73552. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65478/0.73677. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65470/0.73761. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65326/0.73895. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65321/0.73993. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.65199/0.74156. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65154/0.74248. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65009/0.74380. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64977/0.74500. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64902/0.74606. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64806/0.74773. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64805/0.74863. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64720/0.74995. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64570/0.75067. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64541/0.75214. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64495/0.75279. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64377/0.75404. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64305/0.75518. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64253/0.75665. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64193/0.75745. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64027/0.75857. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64044/0.75968. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63894/0.76093. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69512/0.69696. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69318/0.69610. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69123/0.69549. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68949/0.69518. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68734/0.69524. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68680/0.69550. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68594/0.69581. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68528/0.69600. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68531/0.69614. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68454/0.69618. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68491/0.69625. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68428/0.69622. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68401/0.69616. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68360/0.69608. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68359/0.69603. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68291/0.69601. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68271/0.69595. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68216/0.69590. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68185/0.69582. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68127/0.69573. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68113/0.69570. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68074/0.69560. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67972/0.69552. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67967/0.69541. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67929/0.69533. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67832/0.69526. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67800/0.69518. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67745/0.69512. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67625/0.69506. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67563/0.69495. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67504/0.69488. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67460/0.69482. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67398/0.69479. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67328/0.69476. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67265/0.69472. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67191/0.69464. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67194/0.69468. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67118/0.69474. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.66999/0.69486. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66933/0.69493. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66929/0.69496. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66853/0.69511. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66776/0.69524. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66736/0.69531. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66693/0.69552. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66617/0.69572. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66556/0.69602. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66492/0.69622. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66457/0.69643. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66318/0.69667. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66340/0.69693. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66249/0.69727. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66169/0.69745. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66205/0.69779. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66130/0.69797. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66054/0.69825. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65924/0.69845. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65940/0.69882. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65888/0.69915. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65811/0.69954. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65753/0.69986. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65709/0.70001. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65637/0.70037. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65583/0.70058. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65484/0.70084. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65490/0.70117. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65384/0.70162. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65378/0.70195. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65346/0.70220. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65231/0.70223. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65179/0.70256. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65130/0.70274. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65015/0.70334. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65047/0.70370. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64935/0.70400. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64830/0.70428. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64797/0.70462. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64718/0.70484. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64685/0.70518. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.64529/0.70538. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64501/0.70584. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64460/0.70592. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64279/0.70634. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64199/0.70651. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64156/0.70692. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64055/0.70704. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63944/0.70752. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63879/0.70809. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63890/0.70840. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63664/0.70869. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63549/0.70906. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63547/0.70914. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63397/0.70966. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63267/0.71028. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.63274/0.71070. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63194/0.71083. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63094/0.71093. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62854/0.71125. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62806/0.71198. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62607/0.71222. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69000/0.71050. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68932/0.70917. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.68903/0.70831. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68827/0.70783. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68804/0.70752. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68769/0.70733. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68745/0.70719. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68705/0.70718. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68652/0.70714. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68628/0.70716. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68588/0.70727. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68569/0.70736. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.68503/0.70751. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68518/0.70764. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68454/0.70789. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68441/0.70814. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68382/0.70829. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68380/0.70842. Took 0.08 sec\n",
      "Epoch 18, Loss(train/val) 0.68340/0.70843. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68317/0.70861. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68298/0.70878. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68210/0.70894. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68207/0.70921. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68183/0.70937. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68162/0.70946. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68146/0.70948. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68102/0.70946. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68079/0.70954. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68084/0.70962. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68047/0.70963. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67983/0.70976. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68017/0.70972. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67998/0.70972. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67996/0.70976. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67944/0.70982. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67937/0.70987. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67889/0.70977. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67875/0.70972. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67856/0.70982. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67832/0.70982. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67839/0.70986. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67800/0.70984. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67780/0.70984. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67749/0.70988. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67723/0.70992. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67695/0.71011. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67683/0.70996. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67722/0.70983. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67654/0.70986. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67640/0.70997. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67580/0.71001. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67569/0.70997. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67540/0.71005. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67518/0.71021. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67522/0.71023. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67512/0.71028. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67463/0.71049. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67415/0.71051. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67392/0.71066. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67334/0.71083. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67342/0.71084. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67293/0.71113. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67225/0.71128. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67217/0.71142. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67171/0.71174. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67130/0.71199. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67127/0.71213. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67080/0.71257. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67031/0.71283. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67021/0.71315. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66965/0.71328. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66948/0.71358. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66910/0.71389. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66796/0.71449. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66769/0.71458. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66677/0.71521. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66683/0.71551. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66610/0.71609. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66572/0.71665. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66501/0.71724. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66430/0.71792. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66397/0.71848. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66348/0.71891. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66328/0.71982. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66214/0.72020. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66144/0.72102. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66119/0.72178. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66077/0.72242. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66016/0.72314. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65989/0.72373. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65905/0.72475. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65843/0.72512. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65766/0.72624. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65685/0.72719. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65656/0.72763. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65573/0.72860. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65595/0.72953. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65456/0.73068. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65478/0.73137. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65355/0.73254. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69073/0.69000. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69031/0.68915. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68942/0.68854. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68895/0.68804. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68886/0.68761. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68833/0.68723. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68820/0.68685. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68834/0.68648. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68775/0.68612. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68753/0.68576. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68693/0.68538. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68710/0.68500. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68633/0.68460. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68632/0.68420. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68605/0.68381. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68573/0.68342. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68539/0.68304. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68511/0.68266. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68469/0.68230. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68393/0.68196. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68417/0.68164. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68321/0.68138. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68333/0.68115. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68320/0.68096. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68269/0.68079. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68260/0.68067. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68205/0.68058. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68182/0.68054. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68139/0.68053. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68088/0.68055. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68101/0.68061. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68079/0.68069. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68031/0.68081. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67977/0.68094. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67972/0.68109. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67945/0.68127. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67890/0.68147. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67862/0.68168. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67817/0.68191. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67827/0.68216. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67763/0.68243. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67718/0.68271. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67673/0.68303. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67652/0.68335. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67584/0.68370. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67566/0.68407. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67491/0.68448. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67465/0.68492. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67415/0.68532. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67356/0.68577. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67299/0.68622. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67240/0.68663. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67186/0.68709. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67142/0.68756. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67093/0.68808. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67052/0.68863. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66965/0.68920. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66924/0.68979. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66901/0.69041. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66789/0.69100. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66693/0.69160. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66625/0.69222. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66577/0.69293. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66513/0.69371. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66433/0.69443. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66406/0.69514. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66281/0.69593. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66241/0.69685. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66207/0.69771. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66136/0.69856. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66037/0.69948. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65983/0.70040. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65900/0.70142. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65853/0.70233. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65777/0.70330. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65688/0.70425. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65642/0.70528. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65568/0.70622. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65527/0.70706. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65508/0.70808. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65467/0.70905. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65360/0.71010. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65340/0.71124. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65218/0.71211. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65208/0.71322. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65123/0.71407. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65079/0.71514. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65081/0.71619. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64891/0.71706. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64820/0.71815. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64818/0.71936. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64816/0.72031. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64709/0.72120. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64633/0.72222. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64599/0.72308. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64536/0.72404. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64522/0.72508. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64482/0.72598. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64428/0.72694. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64335/0.72799. Took 0.10 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69223/0.69460. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.69459. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69196/0.69460. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69152/0.69460. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69153/0.69462. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69136/0.69464. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69110/0.69467. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.69469. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69066/0.69472. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69056/0.69478. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69042/0.69484. Took 0.08 sec\n",
      "Epoch 11, Loss(train/val) 0.69015/0.69488. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69017/0.69494. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68969/0.69501. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68977/0.69508. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68943/0.69517. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68918/0.69526. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68912/0.69536. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68893/0.69546. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68854/0.69556. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68837/0.69567. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68823/0.69578. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68783/0.69590. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68740/0.69604. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68761/0.69617. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68709/0.69632. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68686/0.69647. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68641/0.69663. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68615/0.69680. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68598/0.69697. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68616/0.69713. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68549/0.69729. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68526/0.69747. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68492/0.69763. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68482/0.69781. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68454/0.69798. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68413/0.69811. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68424/0.69825. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68384/0.69840. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68363/0.69853. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68336/0.69864. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.68272/0.69881. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68292/0.69894. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68244/0.69905. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68277/0.69918. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68203/0.69926. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68155/0.69936. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68140/0.69942. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68142/0.69957. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68089/0.69969. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68080/0.69979. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68045/0.69990. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68015/0.70002. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67975/0.70010. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67924/0.70017. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67917/0.70033. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67872/0.70036. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67834/0.70055. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67840/0.70062. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67782/0.70076. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67768/0.70083. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67702/0.70101. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67641/0.70122. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67632/0.70133. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67603/0.70150. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67557/0.70171. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67512/0.70200. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67443/0.70230. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67426/0.70259. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67430/0.70290. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67360/0.70313. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67315/0.70351. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67271/0.70380. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67242/0.70410. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67206/0.70440. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67162/0.70482. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67125/0.70526. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67099/0.70571. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67039/0.70607. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66984/0.70651. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66998/0.70686. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.66887/0.70741. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66873/0.70782. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66889/0.70834. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66828/0.70879. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66749/0.70909. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66785/0.70955. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66734/0.71003. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66670/0.71050. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66631/0.71097. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66641/0.71152. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66608/0.71183. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66476/0.71227. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.66510/0.71275. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66465/0.71320. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66425/0.71371. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66402/0.71417. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66315/0.71462. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66295/0.71519. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.66228/0.71575. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69364/0.69536. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69336/0.69542. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69347/0.69548. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69323/0.69553. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69303/0.69559. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69278/0.69562. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69296/0.69567. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69269/0.69571. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69277/0.69574. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69265/0.69578. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69240/0.69583. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69243/0.69588. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69247/0.69593. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69225/0.69599. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69218/0.69605. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69201/0.69610. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69207/0.69618. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69169/0.69626. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69142/0.69637. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69186/0.69648. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69163/0.69664. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69144/0.69678. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69119/0.69693. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69108/0.69711. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.69110/0.69729. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69100/0.69747. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69061/0.69768. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.69024/0.69789. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68997/0.69809. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68974/0.69824. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68964/0.69837. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68917/0.69837. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68898/0.69842. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68832/0.69844. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68798/0.69842. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68737/0.69834. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68682/0.69829. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68604/0.69814. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68561/0.69805. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68497/0.69809. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68414/0.69809. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68300/0.69815. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68259/0.69814. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68153/0.69797. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68060/0.69811. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67975/0.69819. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67895/0.69835. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67778/0.69855. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67699/0.69878. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67592/0.69895. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67493/0.69933. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67413/0.69941. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67328/0.69992. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67157/0.70026. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67139/0.70089. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67042/0.70102. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66957/0.70183. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66843/0.70216. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66811/0.70300. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66664/0.70336. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66574/0.70418. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66491/0.70495. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66377/0.70582. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66249/0.70639. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66191/0.70711. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66131/0.70774. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66067/0.70838. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65947/0.70910. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65838/0.70993. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65749/0.71102. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65620/0.71176. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65558/0.71206. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65492/0.71318. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65348/0.71428. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65362/0.71555. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65227/0.71610. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65112/0.71726. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64990/0.71824. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64948/0.71878. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64834/0.72029. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64791/0.72046. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64679/0.72192. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64675/0.72274. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64486/0.72370. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64331/0.72475. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64268/0.72578. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64202/0.72697. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64120/0.72737. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64117/0.72802. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63857/0.72961. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63845/0.73051. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63690/0.73117. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63631/0.73127. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63560/0.73317. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63443/0.73356. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63357/0.73461. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63274/0.73555. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63284/0.73597. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63140/0.73708. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.63133/0.73772. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69629/0.69733. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69599/0.69718. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69563/0.69706. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69564/0.69695. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69543/0.69684. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69529/0.69675. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69516/0.69665. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69494/0.69655. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69476/0.69644. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69448/0.69632. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69430/0.69614. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69379/0.69590. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69351/0.69553. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69262/0.69501. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69240/0.69438. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69183/0.69367. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69109/0.69299. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69053/0.69241. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69006/0.69203. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68954/0.69175. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68897/0.69163. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68863/0.69155. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68813/0.69149. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68825/0.69141. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68743/0.69144. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68697/0.69146. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68660/0.69136. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68627/0.69133. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68565/0.69127. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68547/0.69123. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68465/0.69127. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68404/0.69125. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68385/0.69125. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68304/0.69133. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68277/0.69134. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68200/0.69136. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.68165/0.69142. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68086/0.69152. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68053/0.69163. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67990/0.69168. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67915/0.69177. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67897/0.69186. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67859/0.69198. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67750/0.69220. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67744/0.69236. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67670/0.69251. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67598/0.69266. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67541/0.69289. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67462/0.69307. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67433/0.69335. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67365/0.69366. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.67275/0.69393. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67262/0.69419. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67197/0.69440. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67095/0.69472. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67069/0.69501. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66956/0.69535. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66919/0.69576. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66856/0.69619. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66743/0.69652. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66696/0.69687. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66677/0.69726. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66590/0.69777. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66569/0.69818. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66490/0.69878. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66394/0.69931. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66341/0.69988. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66192/0.70052. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66085/0.70117. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66013/0.70172. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66009/0.70221. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65895/0.70282. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65808/0.70350. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65727/0.70429. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65626/0.70506. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65512/0.70592. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.65424/0.70680. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65345/0.70767. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65308/0.70850. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65132/0.70938. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65059/0.71020. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65015/0.71107. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64820/0.71215. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64839/0.71303. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64698/0.71405. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64542/0.71525. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64475/0.71653. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64399/0.71774. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64239/0.71904. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64149/0.72029. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63997/0.72165. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63865/0.72287. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63833/0.72422. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63735/0.72555. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63513/0.72716. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63401/0.72867. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63331/0.73031. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.63187/0.73180. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63096/0.73354. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62939/0.73511. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.70333/0.69714. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69823/0.69768. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69581/0.69904. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69400/0.70047. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69299/0.70146. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69291/0.70197. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69292/0.70205. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69232/0.70197. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69220/0.70177. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69196/0.70151. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69207/0.70125. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69161/0.70101. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69163/0.70067. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69118/0.70041. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69101/0.70021. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69047/0.69995. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69075/0.69968. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69009/0.69939. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69015/0.69908. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68986/0.69878. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68937/0.69852. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68929/0.69822. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68922/0.69790. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68859/0.69760. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68855/0.69733. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68817/0.69702. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68786/0.69666. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68719/0.69625. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68708/0.69592. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68677/0.69559. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68650/0.69528. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68520/0.69495. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68498/0.69460. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68507/0.69419. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68441/0.69389. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68418/0.69362. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68374/0.69330. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68319/0.69302. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68277/0.69267. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68198/0.69239. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68143/0.69220. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68096/0.69194. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68073/0.69189. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68004/0.69169. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67964/0.69155. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67885/0.69151. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67822/0.69141. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67806/0.69142. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67720/0.69137. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67651/0.69153. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67634/0.69160. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67546/0.69172. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67514/0.69192. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67494/0.69204. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67394/0.69223. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67361/0.69238. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67245/0.69270. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67204/0.69297. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67163/0.69326. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67081/0.69357. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66987/0.69394. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66966/0.69434. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66932/0.69471. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66823/0.69501. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66807/0.69563. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66743/0.69604. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66640/0.69663. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66576/0.69709. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66491/0.69763. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66431/0.69820. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66413/0.69870. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66305/0.69926. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66268/0.69964. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66178/0.70044. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66167/0.70093. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66042/0.70140. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65970/0.70209. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65884/0.70271. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65845/0.70330. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65770/0.70395. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65694/0.70488. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65633/0.70515. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65598/0.70588. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65532/0.70658. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65328/0.70730. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65334/0.70778. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65099/0.70856. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65117/0.70923. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65105/0.71004. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64918/0.71107. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64855/0.71178. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64780/0.71235. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64717/0.71313. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64669/0.71402. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64501/0.71455. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64522/0.71552. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64376/0.71627. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64319/0.71721. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64135/0.71799. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64003/0.71873. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69540/0.69503. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69499/0.69503. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69479/0.69505. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69443/0.69507. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69454/0.69510. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69437/0.69513. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69429/0.69517. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69426/0.69520. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69408/0.69524. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69400/0.69527. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69385/0.69530. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69366/0.69533. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69345/0.69536. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69323/0.69539. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69307/0.69542. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69307/0.69545. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69284/0.69549. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69250/0.69554. Took 0.12 sec\n",
      "Epoch 18, Loss(train/val) 0.69232/0.69561. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69193/0.69569. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69201/0.69578. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69180/0.69589. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69130/0.69601. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69129/0.69615. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69099/0.69630. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.69086/0.69647. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69049/0.69669. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69002/0.69691. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68989/0.69714. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68952/0.69740. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68909/0.69769. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68877/0.69801. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68779/0.69839. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68721/0.69880. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68660/0.69926. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68607/0.69976. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68512/0.70034. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68410/0.70095. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68280/0.70163. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68186/0.70234. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68052/0.70316. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67882/0.70394. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67796/0.70465. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67587/0.70533. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67469/0.70608. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67302/0.70663. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67176/0.70723. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67078/0.70801. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66899/0.70856. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66801/0.70901. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66598/0.70952. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66497/0.71024. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66385/0.71081. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66255/0.71130. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66199/0.71193. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66058/0.71249. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66012/0.71304. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65888/0.71336. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65795/0.71399. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65657/0.71462. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65588/0.71471. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65483/0.71542. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65407/0.71601. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65326/0.71660. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65267/0.71722. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65164/0.71782. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65113/0.71858. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65052/0.71891. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64913/0.71932. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64805/0.71975. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64774/0.72015. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64688/0.72071. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64568/0.72112. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64507/0.72189. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64439/0.72259. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64320/0.72303. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64350/0.72382. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64154/0.72439. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64041/0.72479. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63975/0.72541. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64009/0.72603. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.63893/0.72678. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63685/0.72778. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63708/0.72804. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63500/0.72849. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63478/0.72918. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63298/0.73002. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63416/0.73061. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63216/0.73138. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63076/0.73260. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63053/0.73322. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.62937/0.73442. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62721/0.73520. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.62784/0.73611. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62715/0.73664. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62542/0.73793. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.62486/0.73921. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62300/0.73982. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62224/0.74100. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62120/0.74219. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69433/0.69107. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69418/0.69074. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69383/0.69048. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69332/0.69025. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69377/0.69009. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69345/0.68995. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69349/0.68983. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69317/0.68970. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69302/0.68962. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69302/0.68955. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69272/0.68951. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69268/0.68948. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69232/0.68948. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69241/0.68946. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69240/0.68944. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69216/0.68953. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69210/0.68961. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69184/0.68969. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.69175/0.68974. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69141/0.68979. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69143/0.68987. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69120/0.69001. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69090/0.69024. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69052/0.69041. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69056/0.69061. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69022/0.69074. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68996/0.69093. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68956/0.69123. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68917/0.69144. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68900/0.69172. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68868/0.69203. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68826/0.69237. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68797/0.69266. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68780/0.69281. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68739/0.69330. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68706/0.69353. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68676/0.69401. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68613/0.69426. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68543/0.69451. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.68539/0.69481. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68514/0.69505. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68453/0.69524. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68445/0.69541. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.68388/0.69564. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68352/0.69583. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.68315/0.69611. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68196/0.69659. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68191/0.69662. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68142/0.69703. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68101/0.69699. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68043/0.69720. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68005/0.69737. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67913/0.69762. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67896/0.69779. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67871/0.69795. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67799/0.69805. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67786/0.69799. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67689/0.69830. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67662/0.69881. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67605/0.69864. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67532/0.69892. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67499/0.69901. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67436/0.69900. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67369/0.69922. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67295/0.69974. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67307/0.69929. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67202/0.69943. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67195/0.69970. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67068/0.69965. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67000/0.69943. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66964/0.69948. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66968/0.70003. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66864/0.70032. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66828/0.69961. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66750/0.70032. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66688/0.69993. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66625/0.69989. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66542/0.69994. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66489/0.69971. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66414/0.69975. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66393/0.69999. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66260/0.69949. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66166/0.69986. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66110/0.69968. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66106/0.70006. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66048/0.69950. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65973/0.70001. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.65931/0.69962. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65845/0.69958. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65867/0.69974. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.65656/0.70025. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65530/0.70018. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65583/0.70024. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65506/0.70025. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65399/0.69992. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.65373/0.69980. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65278/0.70007. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65244/0.70038. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65126/0.70025. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65002/0.70043. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69341/0.69502. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69341/0.69512. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69305/0.69523. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69320/0.69533. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69299/0.69542. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69282/0.69553. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69266/0.69562. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69270/0.69573. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69277/0.69582. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69247/0.69591. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69255/0.69601. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69228/0.69611. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69231/0.69619. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69183/0.69630. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69194/0.69639. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.69185/0.69650. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69171/0.69660. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69148/0.69671. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69148/0.69683. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69131/0.69695. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69101/0.69707. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69082/0.69718. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69066/0.69733. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69045/0.69748. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68999/0.69762. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68942/0.69776. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68969/0.69786. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68923/0.69791. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68866/0.69801. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68854/0.69802. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68839/0.69808. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68788/0.69815. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68718/0.69818. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68673/0.69824. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68640/0.69827. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68583/0.69829. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68535/0.69820. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68492/0.69826. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68447/0.69832. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68393/0.69844. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68351/0.69851. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68297/0.69844. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68238/0.69857. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68196/0.69849. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68165/0.69856. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68090/0.69846. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68012/0.69854. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67956/0.69880. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67940/0.69897. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67868/0.69906. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67796/0.69910. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67790/0.69932. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67708/0.69928. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67666/0.69952. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67619/0.69986. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67570/0.70002. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67439/0.70018. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67436/0.70021. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67390/0.70047. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67375/0.70067. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67311/0.70120. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67238/0.70111. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67165/0.70126. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67076/0.70177. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67065/0.70180. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67004/0.70223. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66899/0.70228. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66853/0.70248. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66795/0.70252. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66700/0.70304. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66679/0.70325. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66585/0.70328. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66501/0.70358. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.66451/0.70360. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66373/0.70337. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66403/0.70416. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66199/0.70362. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66171/0.70459. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66144/0.70391. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65995/0.70474. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.65935/0.70418. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65856/0.70396. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65652/0.70434. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65615/0.70471. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65571/0.70485. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65467/0.70455. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65397/0.70440. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65238/0.70503. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65154/0.70452. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.65125/0.70418. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64984/0.70467. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64875/0.70465. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64696/0.70451. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64717/0.70460. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64492/0.70427. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64436/0.70465. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64278/0.70536. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64218/0.70501. Took 0.12 sec\n",
      "Epoch 98, Loss(train/val) 0.63964/0.70529. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63943/0.70534. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69518/0.69528. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69514/0.69509. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69462/0.69491. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69427/0.69469. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69414/0.69448. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69361/0.69422. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69322/0.69391. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69277/0.69356. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69229/0.69321. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69169/0.69286. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69122/0.69254. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69082/0.69230. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68967/0.69209. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68927/0.69195. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68853/0.69183. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68759/0.69179. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68656/0.69183. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68584/0.69191. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68505/0.69206. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68422/0.69220. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68290/0.69246. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68216/0.69282. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68137/0.69328. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.67992/0.69366. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67886/0.69407. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67801/0.69465. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67628/0.69568. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67557/0.69655. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67431/0.69763. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67318/0.69874. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67213/0.70010. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67101/0.70145. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66965/0.70285. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66897/0.70451. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66762/0.70624. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66677/0.70799. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66586/0.70914. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.66491/0.71079. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66387/0.71239. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66276/0.71426. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66218/0.71530. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66200/0.71713. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66029/0.71864. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65962/0.71994. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65894/0.72130. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65829/0.72290. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65700/0.72435. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.65622/0.72537. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.65531/0.72683. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65457/0.72850. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65309/0.72958. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65274/0.73116. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65202/0.73216. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65086/0.73354. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65006/0.73519. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64943/0.73647. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.64942/0.73696. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64783/0.73895. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.64721/0.74006. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64485/0.74113. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64473/0.74326. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64461/0.74464. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.64204/0.74642. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64222/0.74745. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64170/0.74915. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64004/0.75027. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63879/0.75256. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.63753/0.75380. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63594/0.75549. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63565/0.75665. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63389/0.75818. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63266/0.75983. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63169/0.76200. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.63181/0.76388. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63017/0.76455. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62816/0.76695. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62638/0.76873. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.62664/0.77142. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62512/0.77164. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62255/0.77396. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62165/0.77512. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62080/0.77849. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61999/0.78010. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61722/0.78280. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61729/0.78523. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61501/0.78683. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61368/0.78961. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61126/0.79292. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.61069/0.79528. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60987/0.79748. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.60803/0.79917. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.60693/0.80263. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60567/0.80421. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60348/0.80733. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60316/0.81139. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60072/0.81187. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59805/0.81506. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.59754/0.81935. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59545/0.82331. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59426/0.82594. Took 0.12 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69638/0.69021. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69473/0.69081. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69292/0.69183. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69193/0.69300. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69142/0.69401. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69081/0.69474. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69046/0.69526. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69027/0.69557. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68984/0.69576. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68987/0.69594. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68950/0.69601. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68933/0.69606. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68900/0.69614. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68884/0.69619. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68824/0.69623. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68810/0.69621. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68764/0.69621. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68743/0.69619. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68701/0.69607. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68683/0.69599. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68611/0.69584. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68593/0.69574. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68552/0.69555. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68519/0.69535. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68488/0.69513. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68464/0.69481. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68394/0.69456. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68364/0.69422. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68321/0.69395. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68252/0.69353. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68197/0.69322. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68126/0.69297. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68098/0.69268. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68037/0.69233. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67981/0.69200. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67925/0.69177. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67874/0.69152. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67834/0.69130. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67770/0.69124. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67685/0.69118. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67640/0.69124. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67585/0.69115. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67522/0.69118. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67432/0.69126. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67404/0.69151. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67350/0.69141. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67223/0.69171. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67203/0.69181. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67158/0.69222. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67114/0.69250. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67019/0.69258. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66967/0.69298. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66852/0.69311. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66774/0.69363. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66761/0.69394. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66708/0.69395. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66680/0.69419. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66589/0.69444. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66478/0.69481. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66461/0.69510. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66364/0.69599. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66345/0.69610. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66239/0.69666. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66209/0.69694. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.66100/0.69718. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66068/0.69760. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65960/0.69826. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65896/0.69883. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65879/0.69908. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65785/0.69936. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65662/0.70007. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65660/0.70071. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65644/0.70072. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65560/0.70150. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65460/0.70225. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65478/0.70188. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65337/0.70267. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65381/0.70314. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65220/0.70376. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.65202/0.70415. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65187/0.70490. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65091/0.70564. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65066/0.70660. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64999/0.70682. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64958/0.70735. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64957/0.70749. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64833/0.70807. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64750/0.70870. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64649/0.70925. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64739/0.70933. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64631/0.71011. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.64575/0.71047. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64496/0.71062. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64483/0.71087. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64391/0.71149. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64356/0.71251. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64341/0.71259. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64283/0.71386. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64237/0.71383. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64182/0.71420. Took 0.10 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69710/0.69358. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69528/0.69462. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69428/0.69586. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69305/0.69713. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69834. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69934. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69074/0.70008. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.70046. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68981/0.70065. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68924/0.70065. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68875/0.70049. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68833/0.70038. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68745/0.70012. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68730/0.69995. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68676/0.69978. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68615/0.69949. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68585/0.69920. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68502/0.69884. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68435/0.69856. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68435/0.69843. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68347/0.69812. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68303/0.69785. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68211/0.69747. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68204/0.69730. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68144/0.69704. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68088/0.69686. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68014/0.69668. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.67913/0.69659. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67878/0.69642. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67832/0.69619. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.67797/0.69610. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67685/0.69632. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67638/0.69627. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67589/0.69619. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67503/0.69630. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67476/0.69639. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67397/0.69640. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67329/0.69656. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67222/0.69679. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67221/0.69708. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67158/0.69734. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67079/0.69768. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67003/0.69783. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66945/0.69853. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.66853/0.69876. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66842/0.69895. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66737/0.69962. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66613/0.70026. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66553/0.70085. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66502/0.70155. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66415/0.70247. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66468/0.70304. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66315/0.70377. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66227/0.70459. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66190/0.70551. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66125/0.70589. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66080/0.70720. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65983/0.70796. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65956/0.70908. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65871/0.70968. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65800/0.71089. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65804/0.71194. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65768/0.71243. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65685/0.71382. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65549/0.71428. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65548/0.71521. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65462/0.71606. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65493/0.71752. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65309/0.71773. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65280/0.71908. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65255/0.72022. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65182/0.72119. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65069/0.72186. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65030/0.72341. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64947/0.72410. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64921/0.72556. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64843/0.72610. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64784/0.72733. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64738/0.72822. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64675/0.72919. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64598/0.73111. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.64563/0.73167. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64472/0.73257. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64373/0.73368. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64340/0.73491. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64250/0.73600. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64188/0.73710. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64104/0.73853. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64030/0.73952. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63943/0.74027. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.63902/0.74200. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63839/0.74337. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63773/0.74431. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63685/0.74600. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63675/0.74671. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63613/0.74804. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.63502/0.74907. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63476/0.74991. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63344/0.75131. Took 0.13 sec\n",
      "Epoch 99, Loss(train/val) 0.63202/0.75242. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69143/0.70557. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69148/0.70513. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.70466. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69072/0.70418. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69038/0.70367. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69014/0.70311. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68969/0.70259. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68956/0.70203. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68922/0.70141. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68885/0.70086. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68852/0.70039. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68805/0.69990. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68776/0.69947. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68714/0.69902. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68697/0.69865. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68639/0.69828. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68568/0.69793. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68530/0.69768. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68476/0.69741. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68400/0.69721. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68331/0.69713. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68275/0.69711. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68199/0.69704. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68136/0.69704. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68046/0.69714. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67956/0.69724. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67915/0.69734. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.67836/0.69748. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67775/0.69779. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67717/0.69807. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67651/0.69837. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67585/0.69865. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67542/0.69887. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67461/0.69917. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67412/0.69931. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67361/0.69960. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67283/0.69987. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67236/0.70014. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67221/0.70029. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67159/0.70060. Took 0.08 sec\n",
      "Epoch 40, Loss(train/val) 0.67073/0.70083. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67050/0.70107. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66959/0.70135. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66898/0.70144. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66915/0.70171. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66828/0.70198. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66732/0.70224. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66647/0.70251. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66600/0.70281. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66547/0.70305. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66515/0.70349. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66418/0.70379. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66361/0.70422. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66322/0.70460. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66257/0.70505. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66176/0.70554. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66094/0.70606. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65999/0.70652. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65951/0.70681. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.65858/0.70747. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65777/0.70813. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65676/0.70860. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65620/0.70924. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65532/0.70986. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65505/0.71052. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65406/0.71095. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65275/0.71152. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65214/0.71213. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65132/0.71273. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65062/0.71348. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.64983/0.71419. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64932/0.71483. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64853/0.71553. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64727/0.71631. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64709/0.71714. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64632/0.71796. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64498/0.71843. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64441/0.71950. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64428/0.72026. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64301/0.72092. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64189/0.72168. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64153/0.72243. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.64049/0.72279. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63996/0.72324. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63867/0.72403. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63835/0.72451. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63740/0.72555. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63690/0.72613. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.63571/0.72637. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63522/0.72678. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63396/0.72730. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.63307/0.72811. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63296/0.72866. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63247/0.72970. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.63116/0.73045. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63028/0.73091. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62996/0.73107. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62889/0.73185. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62799/0.73203. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62741/0.73280. Took 0.10 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69381/0.69556. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69355/0.69531. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69327/0.69514. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69309/0.69499. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69301/0.69483. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69312/0.69469. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.69459. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69273/0.69453. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69265/0.69447. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69231/0.69444. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69197/0.69441. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69199/0.69433. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69193/0.69434. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69189/0.69434. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69151/0.69437. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69131/0.69443. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69083/0.69448. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69102/0.69455. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69049/0.69462. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69019/0.69473. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69010/0.69490. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68965/0.69510. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68955/0.69526. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68905/0.69547. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68893/0.69574. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68813/0.69602. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68802/0.69638. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68767/0.69671. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68733/0.69713. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68694/0.69757. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68663/0.69791. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68598/0.69831. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68546/0.69876. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68526/0.69927. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68466/0.69963. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68424/0.70015. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68408/0.70062. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68366/0.70109. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68275/0.70159. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68268/0.70207. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68231/0.70258. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68151/0.70300. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68127/0.70348. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68063/0.70399. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68037/0.70440. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67986/0.70477. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67924/0.70522. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67870/0.70574. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67778/0.70613. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67771/0.70647. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67705/0.70686. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67684/0.70703. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67600/0.70755. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67534/0.70783. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67557/0.70803. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67490/0.70830. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67365/0.70851. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67339/0.70893. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67272/0.70917. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67184/0.70922. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67097/0.70952. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67075/0.70969. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67009/0.71000. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66932/0.71026. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66891/0.71070. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66818/0.71057. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66751/0.71106. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66706/0.71107. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66586/0.71147. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66538/0.71169. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66496/0.71194. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66372/0.71232. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66348/0.71271. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66227/0.71286. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66175/0.71332. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66136/0.71348. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66021/0.71393. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65915/0.71446. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65897/0.71456. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65801/0.71486. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.65755/0.71513. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65716/0.71553. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65641/0.71608. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65555/0.71636. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65472/0.71691. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65399/0.71725. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65340/0.71745. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65271/0.71797. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65199/0.71816. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65091/0.71864. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65045/0.71899. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65004/0.71942. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.64931/0.71974. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64817/0.72026. Took 0.12 sec\n",
      "Epoch 94, Loss(train/val) 0.64768/0.72065. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64700/0.72100. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64714/0.72152. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64603/0.72136. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64497/0.72188. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64478/0.72236. Took 0.11 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69599/0.70228. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69558/0.70197. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69521/0.70178. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69491/0.70167. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69517/0.70162. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69455/0.70163. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69447/0.70170. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69452/0.70181. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69405/0.70193. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69398/0.70209. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69374/0.70228. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69347/0.70249. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69326/0.70272. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69317/0.70294. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69268/0.70319. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69236/0.70344. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69227/0.70369. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69189/0.70389. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69146/0.70411. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69136/0.70429. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69095/0.70436. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69026/0.70445. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68984/0.70448. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68951/0.70442. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68891/0.70442. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68833/0.70466. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68770/0.70469. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68737/0.70491. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68708/0.70571. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68631/0.70630. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68562/0.70705. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68507/0.70766. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68459/0.70845. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68398/0.70908. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68346/0.70980. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68282/0.71041. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68238/0.71122. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68160/0.71180. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68099/0.71257. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68004/0.71279. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67974/0.71370. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67911/0.71440. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67828/0.71512. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67752/0.71584. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67724/0.71635. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67647/0.71666. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67550/0.71723. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67492/0.71851. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67430/0.71869. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67316/0.71940. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67293/0.72023. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67225/0.72077. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67122/0.72184. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67056/0.72226. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66967/0.72307. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66799/0.72390. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66735/0.72480. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66661/0.72595. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66556/0.72660. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66456/0.72810. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66338/0.72872. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66258/0.72980. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66132/0.73117. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66023/0.73249. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65874/0.73356. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65794/0.73429. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65639/0.73583. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65507/0.73694. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65376/0.73793. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65284/0.73962. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65166/0.74076. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64980/0.74207. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64890/0.74276. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64734/0.74466. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64583/0.74523. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64440/0.74710. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64333/0.74787. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64278/0.74933. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64139/0.74987. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63960/0.75157. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63882/0.75241. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.63719/0.75324. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.63639/0.75473. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63573/0.75541. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63339/0.75616. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63274/0.75692. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63169/0.75757. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63051/0.75834. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62999/0.75941. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62826/0.75991. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62705/0.76049. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62603/0.76196. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62533/0.76215. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.62436/0.76259. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62321/0.76452. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62284/0.76537. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62074/0.76441. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62074/0.76629. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61886/0.76615. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.61716/0.76675. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69341/0.69482. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69279/0.69444. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69247/0.69411. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69227/0.69380. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69192/0.69352. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69162/0.69326. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.69300. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69077/0.69277. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.69255. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69037/0.69234. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69016/0.69215. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68990/0.69195. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68950/0.69178. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68912/0.69161. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68902/0.69146. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68851/0.69130. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68818/0.69116. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68794/0.69104. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68761/0.69095. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68739/0.69087. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68680/0.69080. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68644/0.69076. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.68610/0.69073. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68573/0.69072. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68553/0.69071. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68521/0.69073. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68474/0.69077. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68387/0.69083. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68431/0.69090. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68371/0.69099. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68317/0.69109. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68279/0.69121. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68210/0.69136. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68165/0.69149. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68108/0.69165. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68112/0.69183. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68036/0.69202. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67969/0.69223. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67955/0.69243. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67878/0.69262. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67812/0.69283. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67816/0.69306. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.67693/0.69325. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67661/0.69348. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67596/0.69370. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67495/0.69392. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67505/0.69415. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67402/0.69434. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67313/0.69452. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67269/0.69471. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67172/0.69492. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67092/0.69513. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66972/0.69535. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66886/0.69565. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66818/0.69592. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66787/0.69614. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66718/0.69636. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66600/0.69660. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66454/0.69693. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66423/0.69726. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66309/0.69762. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66203/0.69795. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66154/0.69838. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66041/0.69879. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.65937/0.69927. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65869/0.69983. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65765/0.70033. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65752/0.70085. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65651/0.70141. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65640/0.70188. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65516/0.70238. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65397/0.70300. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65391/0.70363. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65253/0.70424. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65135/0.70483. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65130/0.70549. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.65097/0.70611. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65061/0.70679. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64972/0.70748. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.64843/0.70820. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64835/0.70874. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64766/0.70933. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64671/0.71007. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64587/0.71075. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64572/0.71151. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64436/0.71207. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64423/0.71271. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64339/0.71342. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64240/0.71416. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64195/0.71480. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64202/0.71535. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64133/0.71606. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63989/0.71673. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63892/0.71748. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63890/0.71804. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63795/0.71874. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63768/0.71946. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63724/0.72006. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63672/0.72073. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63568/0.72143. Took 0.11 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69644/0.70647. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69410/0.69628. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69272/0.68861. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.68314. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69135/0.67973. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69139/0.67771. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69084/0.67656. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.67573. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69013/0.67511. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69021/0.67498. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68986/0.67471. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68943/0.67447. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68913/0.67420. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68919/0.67418. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68865/0.67408. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68837/0.67406. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68783/0.67396. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.67396. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68708/0.67372. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68708/0.67354. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68664/0.67371. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68624/0.67366. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68637/0.67365. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68582/0.67381. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68586/0.67402. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68529/0.67420. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68472/0.67403. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68448/0.67415. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68416/0.67457. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68409/0.67474. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68353/0.67456. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68308/0.67459. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68249/0.67519. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68266/0.67554. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68194/0.67547. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68145/0.67569. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68110/0.67618. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68052/0.67630. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68059/0.67678. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67980/0.67717. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67913/0.67736. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67888/0.67770. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67883/0.67818. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67789/0.67846. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67775/0.67886. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67715/0.67892. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67679/0.67945. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67625/0.68014. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67568/0.68054. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67524/0.68058. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67471/0.68124. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67377/0.68118. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67323/0.68186. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67258/0.68212. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.67205/0.68298. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67145/0.68338. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67076/0.68422. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67038/0.68478. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66950/0.68516. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66892/0.68574. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66784/0.68632. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66732/0.68692. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66700/0.68744. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66644/0.68836. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66518/0.68848. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66473/0.68894. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66382/0.69041. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66296/0.69117. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66240/0.69039. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66191/0.69237. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66060/0.69266. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66078/0.69339. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65985/0.69432. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65924/0.69494. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65829/0.69575. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65816/0.69584. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65741/0.69771. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65666/0.69812. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.65665/0.69845. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65596/0.69912. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65521/0.69987. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65402/0.70115. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65443/0.70173. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65350/0.70232. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65328/0.70313. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65242/0.70361. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65131/0.70473. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65147/0.70512. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65102/0.70622. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65102/0.70616. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.64988/0.70769. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64975/0.70741. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64946/0.70917. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64919/0.70926. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64880/0.70984. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64828/0.71184. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64762/0.71144. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64640/0.71182. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64694/0.71303. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64583/0.71406. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69655/0.70240. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69568/0.70127. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69520/0.70025. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69437/0.69921. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69365/0.69801. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69272/0.69664. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69188/0.69516. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69095/0.69365. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69016/0.69233. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68953/0.69128. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68961/0.69040. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68921/0.68978. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68876/0.68933. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68847/0.68900. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68826/0.68878. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68803/0.68860. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68774/0.68849. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68741/0.68843. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68731/0.68838. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68673/0.68830. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68656/0.68826. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68636/0.68822. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68608/0.68821. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68555/0.68821. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68526/0.68821. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68451/0.68828. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68436/0.68836. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68380/0.68839. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68324/0.68844. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68265/0.68851. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68178/0.68861. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68120/0.68875. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68010/0.68897. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67968/0.68907. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67890/0.68919. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67797/0.68933. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67675/0.68951. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67614/0.68970. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67542/0.68975. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67481/0.68981. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67370/0.68993. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67264/0.69017. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67135/0.69009. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67078/0.68999. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67001/0.68988. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66863/0.68989. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66762/0.68989. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66677/0.68946. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66569/0.68935. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66467/0.68915. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66437/0.68883. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66335/0.68873. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66231/0.68845. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66127/0.68851. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66036/0.68794. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65925/0.68740. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65905/0.68712. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65743/0.68674. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65695/0.68654. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65587/0.68643. Took 0.12 sec\n",
      "Epoch 60, Loss(train/val) 0.65533/0.68610. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65385/0.68595. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65371/0.68571. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65287/0.68544. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65256/0.68529. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65169/0.68579. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65076/0.68579. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64984/0.68555. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64919/0.68615. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64830/0.68571. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64758/0.68599. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64640/0.68577. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.64666/0.68619. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64649/0.68624. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64545/0.68613. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64422/0.68612. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64401/0.68651. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64392/0.68646. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64299/0.68694. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64118/0.68734. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64125/0.68716. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64049/0.68740. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63959/0.68747. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63945/0.68791. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63842/0.68800. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63758/0.68799. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63760/0.68881. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.63565/0.68878. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63521/0.68850. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63486/0.68886. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63409/0.68875. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63300/0.68901. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63334/0.68983. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.63123/0.69018. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63158/0.69067. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63071/0.69158. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62970/0.69133. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62846/0.69200. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62779/0.69230. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62775/0.69261. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69401/0.69650. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69187/0.69887. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69026/0.70128. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.68926/0.70351. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68886/0.70525. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68863/0.70655. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68831/0.70750. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68807/0.70812. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68785/0.70858. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68778/0.70891. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68758/0.70911. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68736/0.70926. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68715/0.70942. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68717/0.70950. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68694/0.70962. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68661/0.70971. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68667/0.70976. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68617/0.70976. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68639/0.70984. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68605/0.70989. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68609/0.70994. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68586/0.71002. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68555/0.71009. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68517/0.71012. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68524/0.71019. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68511/0.71027. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68442/0.71031. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68464/0.71035. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68449/0.71045. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68410/0.71055. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68384/0.71066. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68374/0.71071. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68343/0.71078. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68344/0.71090. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68252/0.71110. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68248/0.71119. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68229/0.71135. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68195/0.71152. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68126/0.71179. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68113/0.71198. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68072/0.71216. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68056/0.71246. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67988/0.71278. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67958/0.71307. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67936/0.71350. Took 0.12 sec\n",
      "Epoch 45, Loss(train/val) 0.67936/0.71398. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67841/0.71436. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67815/0.71478. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67746/0.71533. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67737/0.71578. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67638/0.71628. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67616/0.71680. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67573/0.71721. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67546/0.71780. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.67472/0.71843. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67425/0.71902. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67385/0.71951. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67358/0.72012. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67302/0.72068. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67233/0.72128. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67185/0.72190. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67136/0.72243. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67034/0.72301. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67013/0.72362. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66986/0.72401. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66944/0.72492. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66856/0.72550. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66821/0.72579. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.66752/0.72626. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66738/0.72687. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66647/0.72745. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66593/0.72800. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66553/0.72840. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66496/0.72895. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66492/0.72948. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66477/0.72991. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66363/0.73013. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66312/0.73091. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66202/0.73140. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66234/0.73175. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66161/0.73227. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66079/0.73267. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66096/0.73330. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65991/0.73357. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65945/0.73406. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65878/0.73475. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65875/0.73520. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65803/0.73563. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65776/0.73602. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65719/0.73649. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.65680/0.73668. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65651/0.73744. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65594/0.73767. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65622/0.73794. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65509/0.73824. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65480/0.73879. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65409/0.73929. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65371/0.74001. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65336/0.74001. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.65298/0.74048. Took 0.09 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.71992/0.69848. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.70793/0.69509. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69829/0.69527. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69254/0.69807. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69037/0.70081. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68960/0.70239. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68930/0.70312. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68892/0.70344. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68900/0.70366. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68882/0.70379. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68814/0.70393. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68808/0.70408. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68729/0.70416. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68705/0.70430. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68710/0.70446. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68678/0.70457. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68648/0.70469. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68608/0.70484. Took 0.12 sec\n",
      "Epoch 18, Loss(train/val) 0.68568/0.70499. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68574/0.70517. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68535/0.70530. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68510/0.70544. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68524/0.70562. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68486/0.70578. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68458/0.70599. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68438/0.70618. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68424/0.70634. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68393/0.70652. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68360/0.70670. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68289/0.70695. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68304/0.70721. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68249/0.70744. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68230/0.70760. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68208/0.70781. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68167/0.70802. Took 0.11 sec\n",
      "Epoch 35, Loss(train/val) 0.68142/0.70821. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68129/0.70838. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68102/0.70864. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68043/0.70891. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68039/0.70913. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67982/0.70933. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67960/0.70955. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67946/0.70987. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67900/0.71009. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67854/0.71024. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67825/0.71043. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67831/0.71060. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67834/0.71083. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67744/0.71097. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67698/0.71109. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67720/0.71137. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67662/0.71145. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67639/0.71165. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.67614/0.71180. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67579/0.71183. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67527/0.71201. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67514/0.71221. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67455/0.71227. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67412/0.71235. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67372/0.71248. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67372/0.71249. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67321/0.71255. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.67305/0.71264. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67287/0.71248. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67214/0.71270. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67191/0.71274. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67133/0.71279. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67108/0.71273. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.67078/0.71286. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67018/0.71279. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66993/0.71282. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.66920/0.71275. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66902/0.71298. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66861/0.71294. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66769/0.71293. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66756/0.71297. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66732/0.71309. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.66628/0.71297. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66626/0.71300. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66632/0.71308. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66517/0.71315. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66491/0.71329. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66433/0.71317. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66405/0.71323. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66332/0.71342. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66225/0.71372. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66282/0.71355. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66155/0.71385. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66110/0.71386. Took 0.12 sec\n",
      "Epoch 89, Loss(train/val) 0.66094/0.71406. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66011/0.71436. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66036/0.71437. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65882/0.71454. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65885/0.71474. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65826/0.71481. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65774/0.71523. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65717/0.71521. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65728/0.71567. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.65616/0.71610. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65563/0.71631. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69011/0.69763. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69008/0.69757. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.68990/0.69750. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69005/0.69745. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68945/0.69743. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68944/0.69738. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.68913/0.69732. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68915/0.69727. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68876/0.69722. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68889/0.69715. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68875/0.69712. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68812/0.69708. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68813/0.69702. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68771/0.69698. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68762/0.69695. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68717/0.69691. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68680/0.69682. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68634/0.69671. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68617/0.69667. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68558/0.69669. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68514/0.69669. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68462/0.69666. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68415/0.69669. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68355/0.69666. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68325/0.69673. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68237/0.69681. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68188/0.69697. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68112/0.69710. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68020/0.69723. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67969/0.69744. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67921/0.69754. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67824/0.69762. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67755/0.69769. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67633/0.69782. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67584/0.69797. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67510/0.69807. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67396/0.69825. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67317/0.69830. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67212/0.69833. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67130/0.69840. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67019/0.69854. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.66964/0.69872. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66883/0.69879. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66728/0.69887. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66689/0.69906. Took 0.12 sec\n",
      "Epoch 45, Loss(train/val) 0.66563/0.69922. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66552/0.69927. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66401/0.69933. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66322/0.69942. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66187/0.69967. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66111/0.69995. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66050/0.70018. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65906/0.70028. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65839/0.70053. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.65727/0.70072. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65654/0.70105. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65530/0.70134. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65535/0.70171. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65410/0.70237. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65294/0.70261. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65301/0.70277. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65117/0.70327. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65039/0.70372. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65000/0.70404. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64924/0.70445. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64772/0.70471. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.64671/0.70541. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64634/0.70624. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64566/0.70671. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64490/0.70719. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64491/0.70732. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64295/0.70783. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.64320/0.70845. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64251/0.70891. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64210/0.70948. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64090/0.71036. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63945/0.71067. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63915/0.71114. Took 0.12 sec\n",
      "Epoch 78, Loss(train/val) 0.63869/0.71140. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63795/0.71211. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63757/0.71303. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.63719/0.71342. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63641/0.71418. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63534/0.71498. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63392/0.71544. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63505/0.71582. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63352/0.71656. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.63349/0.71685. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63256/0.71743. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63143/0.71837. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63155/0.71921. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62993/0.71991. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62943/0.72044. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.62860/0.72098. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62823/0.72143. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62824/0.72242. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62714/0.72280. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62629/0.72360. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62541/0.72411. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62580/0.72465. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69349/0.69246. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.69220. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69302/0.69192. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69293/0.69163. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69250/0.69130. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69230/0.69092. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69228/0.69051. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69170/0.69003. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69090/0.68951. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69051/0.68901. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69027/0.68855. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68940/0.68816. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68896/0.68787. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68828/0.68768. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68806/0.68753. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68796/0.68737. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68748/0.68723. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68677/0.68708. Took 0.12 sec\n",
      "Epoch 18, Loss(train/val) 0.68671/0.68692. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68619/0.68678. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68571/0.68662. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68553/0.68648. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68490/0.68634. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68444/0.68615. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68420/0.68599. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68333/0.68588. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68299/0.68576. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68230/0.68560. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68166/0.68552. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68105/0.68545. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68044/0.68543. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67949/0.68546. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67915/0.68551. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67829/0.68562. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67756/0.68585. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67669/0.68608. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67594/0.68641. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67466/0.68677. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67401/0.68722. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67299/0.68783. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67264/0.68834. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67129/0.68896. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67051/0.68959. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.66957/0.69031. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66821/0.69096. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66718/0.69159. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66592/0.69210. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66484/0.69267. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66380/0.69341. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66280/0.69404. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66124/0.69469. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66034/0.69540. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65947/0.69600. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65742/0.69671. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.65657/0.69734. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65564/0.69793. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65446/0.69854. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.65382/0.69921. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65290/0.69965. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65091/0.70026. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64977/0.70063. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.64833/0.70120. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64724/0.70186. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64622/0.70270. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64435/0.70311. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64375/0.70403. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64217/0.70462. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64099/0.70519. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64019/0.70552. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63818/0.70674. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.63696/0.70713. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63652/0.70799. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63458/0.70837. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63307/0.70854. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63316/0.70948. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.63213/0.71011. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63001/0.71111. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62869/0.71105. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62735/0.71198. Took 0.12 sec\n",
      "Epoch 79, Loss(train/val) 0.62625/0.71327. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.62501/0.71366. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.62402/0.71436. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62316/0.71490. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62202/0.71623. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.62131/0.71658. Took 0.12 sec\n",
      "Epoch 85, Loss(train/val) 0.61961/0.71714. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61822/0.71800. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61697/0.71870. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61634/0.71909. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61533/0.72029. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.61418/0.72127. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.61189/0.72207. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61050/0.72315. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.60967/0.72297. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60893/0.72400. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60697/0.72574. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.60543/0.72663. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60418/0.72686. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60364/0.72802. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.60285/0.72856. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69491/0.68985. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69437/0.69002. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69402/0.69020. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69340/0.69040. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69334/0.69061. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69252/0.69084. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69222/0.69108. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69197/0.69132. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69149/0.69155. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69123/0.69177. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69095/0.69197. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69081/0.69217. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69024/0.69235. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69011/0.69253. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68984/0.69267. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68936/0.69280. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68932/0.69292. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68873/0.69307. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68850/0.69320. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68807/0.69326. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68747/0.69333. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68759/0.69336. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68677/0.69343. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68607/0.69347. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68577/0.69354. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68526/0.69357. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68435/0.69360. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68417/0.69366. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68298/0.69376. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68232/0.69393. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68172/0.69405. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68109/0.69428. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68029/0.69455. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67961/0.69479. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67893/0.69519. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67840/0.69557. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67743/0.69580. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67674/0.69620. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.69646. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67538/0.69682. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67496/0.69721. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67398/0.69751. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67332/0.69783. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67273/0.69797. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67186/0.69823. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67159/0.69857. Took 0.12 sec\n",
      "Epoch 46, Loss(train/val) 0.67039/0.69878. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66962/0.69894. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66947/0.69911. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66837/0.69937. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66787/0.69960. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66686/0.69987. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66625/0.69998. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66514/0.70031. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66461/0.70034. Took 0.12 sec\n",
      "Epoch 55, Loss(train/val) 0.66369/0.70068. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66279/0.70083. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66212/0.70107. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66119/0.70117. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66015/0.70152. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65881/0.70184. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65815/0.70214. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65772/0.70246. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65579/0.70284. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65483/0.70326. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65433/0.70360. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65254/0.70393. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.65149/0.70441. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65081/0.70497. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64953/0.70566. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.64871/0.70628. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64733/0.70694. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64532/0.70755. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.64518/0.70838. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64388/0.70924. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64270/0.71025. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64146/0.71113. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64013/0.71211. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63941/0.71300. Took 0.12 sec\n",
      "Epoch 79, Loss(train/val) 0.63795/0.71400. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63760/0.71490. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.63589/0.71615. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.63460/0.71739. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63349/0.71862. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63216/0.71966. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63063/0.72079. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63009/0.72198. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62871/0.72306. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62729/0.72425. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62609/0.72540. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62453/0.72674. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.62363/0.72782. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62323/0.72928. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62213/0.73066. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62074/0.73188. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61935/0.73305. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61782/0.73452. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.61658/0.73579. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61527/0.73668. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61479/0.73792. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.70271/0.71013. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69960/0.70553. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69701/0.70151. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69519/0.69829. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69382/0.69593. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69306/0.69431. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69265/0.69330. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69256/0.69270. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69244/0.69235. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69230/0.69213. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69226/0.69199. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69220/0.69191. Took 0.12 sec\n",
      "Epoch 12, Loss(train/val) 0.69218/0.69189. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69176/0.69190. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69178/0.69191. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69197/0.69193. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69181/0.69197. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.69180/0.69203. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.69163/0.69206. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69122/0.69210. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69131/0.69211. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69126/0.69213. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69125/0.69218. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69137/0.69224. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.69093/0.69229. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69099/0.69234. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69072/0.69239. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69074/0.69241. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69076/0.69247. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69044/0.69253. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69033/0.69260. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68989/0.69263. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69008/0.69266. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69005/0.69274. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69004/0.69282. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68970/0.69288. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68981/0.69292. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68953/0.69297. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68940/0.69301. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68933/0.69309. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68917/0.69319. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68924/0.69323. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.68894/0.69329. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68878/0.69340. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68839/0.69349. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.68826/0.69358. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68795/0.69367. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68832/0.69379. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68766/0.69385. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68748/0.69391. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68733/0.69402. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.68746/0.69414. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68715/0.69419. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.68673/0.69435. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68683/0.69446. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68648/0.69457. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68658/0.69475. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68627/0.69492. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68586/0.69502. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68580/0.69515. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68537/0.69529. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68512/0.69545. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68506/0.69553. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.68484/0.69570. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68455/0.69591. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68441/0.69611. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.68391/0.69624. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68384/0.69638. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68341/0.69662. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.68322/0.69683. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.68290/0.69707. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68255/0.69729. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.68239/0.69763. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68185/0.69789. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68157/0.69823. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.68140/0.69848. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68060/0.69879. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.68040/0.69913. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.68002/0.69963. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67940/0.70005. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67867/0.70059. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.67846/0.70104. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67788/0.70161. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67741/0.70214. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.67710/0.70271. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67616/0.70345. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67578/0.70420. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.67531/0.70491. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67479/0.70573. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67383/0.70662. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.67301/0.70766. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67247/0.70859. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67130/0.70975. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.67096/0.71081. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67044/0.71212. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66954/0.71318. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66879/0.71455. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66805/0.71606. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66701/0.71723. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.66650/0.71865. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69741/0.69265. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69659/0.69207. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69559/0.69156. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69466/0.69116. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69375/0.69093. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69330/0.69080. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69292/0.69080. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69239/0.69086. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69191/0.69094. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69179/0.69104. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69145/0.69114. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69113/0.69126. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69080/0.69139. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69042/0.69152. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69043/0.69165. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68989/0.69179. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68955/0.69195. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68917/0.69214. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68894/0.69236. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68846/0.69260. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68796/0.69284. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68755/0.69308. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68741/0.69338. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68698/0.69374. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68651/0.69412. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68607/0.69456. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68551/0.69502. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68526/0.69550. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68442/0.69602. Took 0.11 sec\n",
      "Epoch 29, Loss(train/val) 0.68407/0.69656. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68364/0.69715. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68312/0.69771. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68274/0.69833. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68226/0.69895. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68227/0.69957. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68169/0.70015. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68116/0.70081. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68067/0.70138. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68029/0.70197. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67951/0.70260. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67965/0.70318. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67936/0.70377. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67881/0.70435. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67834/0.70487. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67824/0.70529. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67742/0.70580. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67726/0.70636. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67661/0.70683. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67648/0.70733. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67581/0.70773. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67512/0.70823. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.67530/0.70854. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67460/0.70899. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67429/0.70934. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67407/0.70989. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67367/0.71016. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67340/0.71049. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67340/0.71080. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67150/0.71121. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67136/0.71164. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67147/0.71181. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67101/0.71224. Took 0.08 sec\n",
      "Epoch 62, Loss(train/val) 0.67031/0.71267. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66991/0.71287. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66995/0.71333. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66890/0.71356. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66873/0.71405. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66783/0.71440. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66743/0.71481. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66663/0.71528. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66600/0.71570. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66521/0.71612. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66498/0.71638. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66454/0.71711. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66315/0.71738. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66277/0.71796. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66247/0.71844. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66160/0.71898. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66096/0.71921. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66127/0.71986. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65930/0.72065. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65922/0.72123. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65867/0.72185. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65800/0.72254. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65729/0.72313. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65717/0.72372. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65615/0.72464. Took 0.12 sec\n",
      "Epoch 87, Loss(train/val) 0.65507/0.72536. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65487/0.72590. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65392/0.72665. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65314/0.72720. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65299/0.72839. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65123/0.72903. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65091/0.72982. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65060/0.73061. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65003/0.73147. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.64944/0.73200. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64826/0.73294. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64836/0.73360. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64657/0.73533. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69319/0.68909. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69225/0.68773. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69178/0.68663. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69120/0.68574. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69077/0.68509. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69071/0.68458. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69033/0.68416. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69021/0.68393. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69020/0.68374. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68991/0.68362. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68989/0.68354. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68989/0.68347. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68981/0.68340. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68962/0.68342. Took 0.08 sec\n",
      "Epoch 14, Loss(train/val) 0.68957/0.68343. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68937/0.68345. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68934/0.68347. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68926/0.68351. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68920/0.68353. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68907/0.68355. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68890/0.68361. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68864/0.68362. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68855/0.68367. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68846/0.68378. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68844/0.68383. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68807/0.68385. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68816/0.68391. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68794/0.68398. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68808/0.68406. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68796/0.68411. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68806/0.68419. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68777/0.68428. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68763/0.68442. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68729/0.68447. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68706/0.68458. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68698/0.68474. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68694/0.68484. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68660/0.68496. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68630/0.68507. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68629/0.68516. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68608/0.68527. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68608/0.68537. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68597/0.68543. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68586/0.68547. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68557/0.68554. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68510/0.68562. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68510/0.68575. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68472/0.68580. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68475/0.68583. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68440/0.68593. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68394/0.68600. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68374/0.68597. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.68344/0.68598. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68341/0.68613. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68262/0.68620. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.68251/0.68623. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68236/0.68622. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68219/0.68630. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.68158/0.68638. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.68179/0.68637. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68111/0.68638. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.68075/0.68644. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68044/0.68645. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68004/0.68641. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67982/0.68647. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67938/0.68638. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67886/0.68650. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.67828/0.68653. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67815/0.68669. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67776/0.68659. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67704/0.68659. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67638/0.68664. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67622/0.68654. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67580/0.68646. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67468/0.68661. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67470/0.68651. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67382/0.68654. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67299/0.68646. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67246/0.68649. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67180/0.68641. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67116/0.68654. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67052/0.68653. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.67019/0.68666. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66928/0.68663. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66836/0.68687. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66791/0.68684. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66718/0.68708. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66655/0.68695. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66576/0.68733. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66463/0.68718. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66412/0.68768. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.66344/0.68762. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.66270/0.68762. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66191/0.68795. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.66118/0.68791. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66075/0.68787. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65972/0.68811. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65875/0.68829. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65846/0.68848. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65783/0.68896. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69165/0.68459. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69152/0.68486. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69122/0.68513. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69123/0.68540. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.68565. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69054/0.68591. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69039/0.68614. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.68636. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69012/0.68657. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68998/0.68678. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68999/0.68701. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68986/0.68725. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68958/0.68747. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68957/0.68771. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68929/0.68798. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68929/0.68823. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68928/0.68852. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68911/0.68884. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68874/0.68916. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68874/0.68951. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68876/0.68994. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68822/0.69033. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68787/0.69081. Took 0.11 sec\n",
      "Epoch 23, Loss(train/val) 0.68750/0.69133. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68749/0.69193. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68698/0.69263. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68684/0.69341. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68625/0.69419. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68647/0.69506. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68584/0.69606. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68572/0.69712. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68511/0.69826. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68501/0.69947. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68480/0.70070. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68402/0.70204. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68384/0.70333. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68356/0.70458. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68321/0.70580. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68259/0.70705. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68255/0.70823. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68214/0.70940. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68154/0.71052. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68134/0.71158. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68074/0.71265. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68078/0.71359. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68041/0.71446. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67981/0.71543. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67957/0.71625. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67896/0.71708. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67851/0.71794. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67816/0.71877. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67787/0.71950. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67738/0.72022. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67687/0.72093. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67634/0.72167. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67603/0.72241. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67547/0.72302. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67499/0.72371. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67445/0.72437. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67408/0.72508. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67315/0.72572. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67210/0.72638. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67235/0.72685. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67161/0.72733. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.67136/0.72779. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66976/0.72838. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66954/0.72888. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66898/0.72929. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66805/0.72982. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66722/0.73035. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.66660/0.73079. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66570/0.73120. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66451/0.73135. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66413/0.73186. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66309/0.73226. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66238/0.73283. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66156/0.73310. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66096/0.73337. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65994/0.73386. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.65900/0.73434. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65815/0.73459. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65716/0.73473. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65575/0.73539. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65535/0.73586. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65427/0.73653. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65387/0.73701. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65281/0.73758. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65130/0.73878. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65044/0.73955. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64920/0.73981. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64828/0.74068. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.64682/0.74152. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64651/0.74240. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64514/0.74361. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64442/0.74370. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64353/0.74468. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64302/0.74522. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64131/0.74628. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63995/0.74743. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63880/0.74774. Took 0.11 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69013/0.68702. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68993/0.68704. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68987/0.68706. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68955/0.68708. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68934/0.68710. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68924/0.68713. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68947/0.68717. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68911/0.68720. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68871/0.68727. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68893/0.68734. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68789/0.68741. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68770/0.68749. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68785/0.68760. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68758/0.68773. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68708/0.68787. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68681/0.68804. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68634/0.68825. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68580/0.68849. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68547/0.68879. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68470/0.68913. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68423/0.68955. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68376/0.69003. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68309/0.69058. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68244/0.69121. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68171/0.69197. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68078/0.69278. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68021/0.69370. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67869/0.69477. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67823/0.69589. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67766/0.69706. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67635/0.69833. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67592/0.69960. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67461/0.70094. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67443/0.70235. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67306/0.70376. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67198/0.70518. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67203/0.70656. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67082/0.70790. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66960/0.70928. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66899/0.71060. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66845/0.71184. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.66760/0.71305. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.66631/0.71429. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66541/0.71559. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66462/0.71686. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66437/0.71807. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66323/0.71919. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66209/0.72036. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.66158/0.72139. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66070/0.72253. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.65957/0.72355. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65866/0.72471. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65832/0.72567. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65655/0.72679. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.65609/0.72777. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65496/0.72880. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65388/0.72980. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.65313/0.73070. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65247/0.73175. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65222/0.73268. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65016/0.73353. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64963/0.73446. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64907/0.73538. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.64679/0.73636. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64643/0.73734. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64562/0.73805. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64445/0.73920. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64394/0.74008. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64311/0.74075. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.64215/0.74169. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64047/0.74263. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63952/0.74346. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63910/0.74423. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63758/0.74497. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63661/0.74586. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63623/0.74684. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63514/0.74777. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63371/0.74862. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.63232/0.74939. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63079/0.75021. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63089/0.75125. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.62993/0.75206. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.62841/0.75269. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62671/0.75346. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62576/0.75436. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62460/0.75527. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62409/0.75635. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62258/0.75728. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62201/0.75848. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62037/0.75911. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61980/0.76004. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61899/0.76083. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61645/0.76196. Took 0.12 sec\n",
      "Epoch 93, Loss(train/val) 0.61645/0.76305. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61489/0.76329. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61406/0.76430. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61396/0.76478. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61186/0.76585. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60957/0.76712. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.60816/0.76805. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69811/0.69783. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69784/0.69757. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69750/0.69716. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69694/0.69652. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69623/0.69555. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69544/0.69418. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69422/0.69245. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69313/0.69074. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69207/0.68921. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69105/0.68811. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69036/0.68733. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.68685. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68986/0.68654. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68948/0.68636. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68961/0.68631. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68949/0.68628. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68907/0.68629. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68932/0.68632. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68893/0.68634. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68900/0.68641. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68869/0.68647. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68861/0.68654. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68857/0.68661. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68848/0.68666. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68829/0.68669. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68820/0.68673. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68808/0.68679. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68774/0.68687. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68770/0.68691. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68768/0.68701. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68755/0.68713. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68739/0.68724. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68719/0.68736. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68690/0.68753. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68692/0.68770. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68626/0.68792. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68589/0.68808. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68539/0.68833. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68565/0.68841. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68490/0.68864. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68454/0.68896. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68426/0.68915. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68335/0.68949. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68310/0.68977. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68258/0.69000. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68221/0.69019. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68123/0.69081. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68110/0.69089. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68035/0.69115. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67963/0.69167. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67895/0.69205. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67852/0.69204. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67808/0.69228. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67662/0.69261. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67633/0.69283. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67579/0.69278. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67524/0.69326. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.67495/0.69382. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67372/0.69387. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67318/0.69396. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67342/0.69395. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67262/0.69485. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67125/0.69517. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67081/0.69486. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67083/0.69494. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67066/0.69541. Took 0.12 sec\n",
      "Epoch 66, Loss(train/val) 0.66934/0.69586. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66911/0.69632. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66851/0.69615. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66811/0.69696. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66716/0.69766. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66639/0.69739. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66621/0.69817. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66534/0.69848. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66545/0.69884. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66461/0.69985. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66339/0.70042. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66380/0.70031. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66345/0.70121. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66204/0.70144. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.66195/0.70258. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66101/0.70269. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66006/0.70350. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66007/0.70423. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65988/0.70547. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65923/0.70514. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65805/0.70602. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65740/0.70736. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65761/0.70797. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65608/0.70849. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65563/0.70948. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65522/0.71054. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65376/0.71134. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65334/0.71213. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65261/0.71352. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65221/0.71376. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65136/0.71517. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65133/0.71582. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65060/0.71677. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64873/0.71843. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.70328/0.68851. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69939/0.68478. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69562/0.68163. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69245/0.68000. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69082/0.67982. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68971/0.68029. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.68094. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68855/0.68160. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68845/0.68221. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68793/0.68276. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68787/0.68332. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68732/0.68383. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68749/0.68431. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68691/0.68479. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68685/0.68527. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68677/0.68572. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68643/0.68616. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68601/0.68662. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68598/0.68706. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68603/0.68747. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68560/0.68789. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68544/0.68831. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68499/0.68873. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68511/0.68916. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68479/0.68961. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68472/0.69005. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68458/0.69043. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68407/0.69082. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68396/0.69127. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68394/0.69174. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68356/0.69217. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68333/0.69264. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68322/0.69306. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68313/0.69352. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68309/0.69392. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68266/0.69434. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68279/0.69472. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68246/0.69509. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68249/0.69548. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68171/0.69592. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68227/0.69621. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68150/0.69653. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68171/0.69689. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68123/0.69724. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68121/0.69766. Took 0.08 sec\n",
      "Epoch 45, Loss(train/val) 0.68106/0.69799. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68087/0.69832. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68087/0.69877. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68050/0.69907. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68048/0.69941. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68022/0.69979. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67962/0.70010. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67973/0.70050. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67961/0.70089. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67930/0.70124. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67919/0.70155. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67892/0.70195. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67877/0.70236. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67856/0.70274. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67831/0.70317. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67811/0.70353. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67798/0.70387. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.67778/0.70438. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67738/0.70493. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67735/0.70550. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67694/0.70588. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67661/0.70645. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67659/0.70694. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67600/0.70744. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67577/0.70809. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67557/0.70874. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67503/0.70944. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67497/0.71004. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67483/0.71072. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67421/0.71139. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67408/0.71209. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67351/0.71307. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.67273/0.71409. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67276/0.71489. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67295/0.71559. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67174/0.71678. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67165/0.71780. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67084/0.71898. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67083/0.72018. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66992/0.72127. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66934/0.72242. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66841/0.72379. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66805/0.72506. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.66768/0.72630. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66715/0.72768. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66652/0.72890. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66604/0.73038. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66541/0.73121. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66492/0.73329. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66371/0.73512. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66311/0.73670. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66251/0.73836. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.66191/0.73971. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66163/0.74154. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66092/0.74289. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.68896/0.69379. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.68868/0.69371. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.68862/0.69364. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68845/0.69355. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68836/0.69347. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68850/0.69337. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68782/0.69325. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68783/0.69312. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68727/0.69296. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68726/0.69285. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68687/0.69272. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68661/0.69250. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68604/0.69223. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68579/0.69187. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68524/0.69152. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68479/0.69112. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68409/0.69067. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68365/0.69019. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68311/0.68957. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68201/0.68890. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68179/0.68821. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68108/0.68759. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68038/0.68701. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67952/0.68662. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.67923/0.68616. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67841/0.68582. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67805/0.68528. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.67758/0.68505. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67682/0.68486. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67680/0.68463. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.67598/0.68437. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67536/0.68418. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67504/0.68423. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67472/0.68418. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67441/0.68406. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67384/0.68418. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67350/0.68424. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67314/0.68414. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67297/0.68403. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67212/0.68421. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67213/0.68434. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67153/0.68437. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67121/0.68454. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67066/0.68459. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67042/0.68471. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67014/0.68496. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66948/0.68522. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.66898/0.68545. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66867/0.68555. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66792/0.68566. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66787/0.68620. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66712/0.68640. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66655/0.68675. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66629/0.68673. Took 0.12 sec\n",
      "Epoch 54, Loss(train/val) 0.66546/0.68723. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66532/0.68755. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66488/0.68767. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66394/0.68783. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66335/0.68815. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66251/0.68850. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66269/0.68890. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66242/0.68925. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66148/0.68965. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66037/0.69019. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65984/0.69034. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65906/0.69116. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65890/0.69140. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65827/0.69192. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65747/0.69239. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65658/0.69304. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65528/0.69333. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65542/0.69404. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.65431/0.69456. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65364/0.69487. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65284/0.69564. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65248/0.69625. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65173/0.69685. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65078/0.69740. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65004/0.69808. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64939/0.69874. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64821/0.69959. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64742/0.70011. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64695/0.70084. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64628/0.70143. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64567/0.70252. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64428/0.70305. Took 0.11 sec\n",
      "Epoch 86, Loss(train/val) 0.64390/0.70394. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64311/0.70448. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64212/0.70568. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.64147/0.70594. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64061/0.70681. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64024/0.70752. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63897/0.70831. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63886/0.70893. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63717/0.70976. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.63663/0.71066. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63562/0.71139. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63475/0.71203. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.63372/0.71273. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63367/0.71360. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69056/0.69355. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69032/0.69329. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68986/0.69307. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68977/0.69287. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68969/0.69267. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68923/0.69245. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68916/0.69223. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68868/0.69204. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68844/0.69183. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68792/0.69164. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68785/0.69146. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68709/0.69131. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68650/0.69117. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68624/0.69100. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68602/0.69087. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68524/0.69074. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68492/0.69062. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68429/0.69054. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68368/0.69046. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68329/0.69040. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68247/0.69040. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68192/0.69044. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68135/0.69046. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68071/0.69053. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68006/0.69058. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68002/0.69058. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67903/0.69061. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67875/0.69063. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67826/0.69063. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67776/0.69068. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67653/0.69060. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67660/0.69058. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67586/0.69055. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67581/0.69045. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67502/0.69040. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67456/0.69028. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67398/0.69020. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67319/0.69004. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67281/0.68989. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67225/0.68982. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67142/0.68969. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67124/0.68957. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67076/0.68932. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67007/0.68911. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66959/0.68878. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66868/0.68863. Took 0.12 sec\n",
      "Epoch 46, Loss(train/val) 0.66841/0.68861. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66775/0.68822. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66782/0.68801. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66660/0.68780. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66610/0.68729. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66555/0.68702. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66504/0.68688. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66486/0.68663. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66314/0.68664. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66261/0.68620. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66238/0.68605. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.66200/0.68554. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66080/0.68541. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66037/0.68506. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65846/0.68487. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65893/0.68450. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65727/0.68455. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65685/0.68396. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65563/0.68365. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.65568/0.68358. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65454/0.68326. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65347/0.68299. Took 0.12 sec\n",
      "Epoch 68, Loss(train/val) 0.65260/0.68247. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65083/0.68257. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65038/0.68243. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64934/0.68193. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64877/0.68217. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64752/0.68233. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.64586/0.68233. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64536/0.68195. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64363/0.68195. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64290/0.68171. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64170/0.68160. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64023/0.68196. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.63895/0.68206. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63799/0.68222. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63715/0.68220. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.63490/0.68218. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63431/0.68251. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63230/0.68303. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.63116/0.68365. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62889/0.68394. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62959/0.68465. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.62697/0.68455. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62593/0.68532. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62470/0.68572. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 0.62176/0.68666. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62120/0.68679. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62067/0.68773. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.61857/0.68880. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61706/0.68903. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61534/0.69073. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.61374/0.69142. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.61225/0.69219. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69692/0.69821. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69624/0.69764. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69533/0.69717. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69422/0.69681. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69349/0.69660. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69289/0.69650. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69223/0.69651. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69215/0.69656. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.69183/0.69662. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69145/0.69668. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69143/0.69672. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69117/0.69675. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69105/0.69679. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69082/0.69680. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69039/0.69682. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69038/0.69684. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68993/0.69687. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68955/0.69687. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68941/0.69688. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68903/0.69689. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68887/0.69691. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68835/0.69693. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68840/0.69694. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68787/0.69697. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68719/0.69699. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68707/0.69702. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68659/0.69707. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68616/0.69711. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68575/0.69716. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68534/0.69723. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68505/0.69736. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68437/0.69749. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68433/0.69767. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68370/0.69784. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68330/0.69805. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68288/0.69826. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68234/0.69848. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68218/0.69872. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.69896. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68111/0.69924. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68075/0.69953. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68078/0.69980. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67992/0.70006. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67968/0.70034. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67923/0.70067. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67880/0.70091. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67879/0.70114. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67822/0.70136. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67827/0.70161. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67757/0.70182. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67763/0.70207. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67677/0.70234. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67648/0.70260. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67569/0.70284. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67578/0.70308. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67492/0.70332. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67456/0.70360. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67433/0.70387. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67362/0.70414. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67334/0.70440. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67287/0.70465. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67234/0.70499. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67206/0.70524. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67149/0.70548. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67105/0.70583. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67081/0.70614. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66968/0.70642. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66979/0.70685. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66919/0.70716. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66869/0.70746. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66829/0.70785. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66756/0.70817. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66690/0.70862. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66636/0.70894. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.66546/0.70941. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66563/0.70981. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66486/0.71026. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66453/0.71084. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66414/0.71134. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66295/0.71179. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66255/0.71235. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66234/0.71290. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.66118/0.71344. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66059/0.71395. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66023/0.71447. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65965/0.71517. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65813/0.71584. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65753/0.71651. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65703/0.71714. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65639/0.71800. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65645/0.71886. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.65497/0.71943. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65439/0.72034. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65386/0.72116. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65286/0.72183. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65251/0.72254. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65127/0.72354. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65116/0.72451. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64967/0.72543. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64958/0.72598. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69836/0.70417. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69619/0.70090. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69512/0.69851. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69405/0.69671. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69335/0.69543. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69316/0.69448. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69291/0.69377. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69257/0.69327. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69253/0.69291. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69249/0.69260. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69211/0.69237. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69221/0.69217. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69177/0.69195. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69180/0.69176. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69179/0.69160. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69154/0.69144. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.69143/0.69123. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69113/0.69111. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69094/0.69093. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69110/0.69074. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.69075/0.69053. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69028/0.69039. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.69033/0.69024. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68997/0.69010. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68989/0.68988. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68955/0.68968. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68933/0.68955. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68912/0.68936. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68874/0.68914. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68845/0.68900. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68809/0.68879. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68763/0.68862. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68745/0.68841. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68712/0.68822. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.68665/0.68809. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68635/0.68803. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68599/0.68786. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68543/0.68777. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68456/0.68779. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.68461/0.68770. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68459/0.68760. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68401/0.68754. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68386/0.68759. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.68318/0.68762. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68303/0.68765. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68299/0.68780. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68241/0.68782. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68193/0.68789. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68165/0.68791. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.68122/0.68806. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68081/0.68823. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68043/0.68833. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68006/0.68848. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68014/0.68859. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67949/0.68870. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67902/0.68885. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67877/0.68905. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67874/0.68911. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.67826/0.68912. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67808/0.68949. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67776/0.68970. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67711/0.68997. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67740/0.69009. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67649/0.69042. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67639/0.69048. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67610/0.69087. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67581/0.69105. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67510/0.69127. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67484/0.69147. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67460/0.69183. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67393/0.69198. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.67388/0.69232. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67284/0.69263. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.67268/0.69284. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67249/0.69335. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67158/0.69354. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.67167/0.69391. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67176/0.69429. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67046/0.69475. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.67055/0.69493. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67041/0.69538. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66919/0.69579. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66896/0.69619. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66853/0.69652. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66835/0.69707. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66751/0.69753. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66744/0.69789. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66703/0.69830. Took 0.12 sec\n",
      "Epoch 88, Loss(train/val) 0.66601/0.69872. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66651/0.69928. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66525/0.69978. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66551/0.70004. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66440/0.70055. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.66426/0.70092. Took 0.12 sec\n",
      "Epoch 94, Loss(train/val) 0.66409/0.70175. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66339/0.70209. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66302/0.70280. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66252/0.70337. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66151/0.70381. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66134/0.70441. Took 0.10 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69826/0.69040. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69826/0.69052. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69740/0.69065. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69714/0.69074. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69631/0.69078. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69598/0.69079. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69544/0.69078. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69482/0.69079. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69410/0.69088. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69328/0.69110. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69281/0.69139. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69286/0.69175. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69190/0.69216. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69196/0.69259. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69137/0.69304. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69153/0.69348. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69146/0.69396. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69070/0.69446. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69065/0.69496. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69053/0.69550. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.69008/0.69606. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68956/0.69667. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68961/0.69730. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68888/0.69794. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68876/0.69867. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68843/0.69943. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68813/0.70017. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68745/0.70102. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68706/0.70180. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68658/0.70259. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68587/0.70341. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68530/0.70423. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68500/0.70493. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68438/0.70562. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68430/0.70623. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68368/0.70687. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68273/0.70760. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68246/0.70831. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68178/0.70884. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68111/0.70930. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68079/0.70969. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68034/0.71008. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67915/0.71047. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67862/0.71091. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67820/0.71153. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67731/0.71194. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67672/0.71245. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67625/0.71254. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67492/0.71286. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67459/0.71319. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67396/0.71352. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67345/0.71375. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67282/0.71374. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67265/0.71413. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67083/0.71466. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67079/0.71497. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67032/0.71487. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66950/0.71519. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66890/0.71520. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66766/0.71564. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66739/0.71602. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66646/0.71647. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66584/0.71669. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66487/0.71682. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66442/0.71771. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66345/0.71804. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66239/0.71818. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66245/0.71859. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66137/0.71894. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66059/0.71895. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65972/0.71981. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65808/0.71994. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65805/0.72078. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65735/0.72110. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65623/0.72187. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65577/0.72223. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65534/0.72294. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65368/0.72354. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.65307/0.72274. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65208/0.72353. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65112/0.72467. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65018/0.72549. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64940/0.72542. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64773/0.72602. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64740/0.72645. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64624/0.72680. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64618/0.72742. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64486/0.72887. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64418/0.72876. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64268/0.72889. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64147/0.72976. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63982/0.73037. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64006/0.73096. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.63805/0.73068. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63742/0.73215. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63628/0.73202. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63525/0.73270. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63468/0.73362. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63278/0.73354. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63150/0.73556. Took 0.09 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.70424/0.71164. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.70015/0.70561. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69772/0.70124. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69611/0.69829. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69518/0.69644. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69451/0.69528. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69444/0.69454. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69383/0.69401. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69374/0.69367. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69367/0.69342. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69343/0.69322. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69314/0.69307. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69289/0.69289. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69269/0.69277. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69270/0.69268. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69257/0.69262. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69244/0.69259. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69215/0.69251. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69219/0.69245. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69182/0.69242. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69150/0.69240. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69146/0.69241. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.69121/0.69240. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69121/0.69239. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69098/0.69244. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69065/0.69244. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69046/0.69247. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69026/0.69256. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69013/0.69268. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68984/0.69275. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.69006/0.69284. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68973/0.69298. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68931/0.69313. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68908/0.69328. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68902/0.69348. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68878/0.69373. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68867/0.69394. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68811/0.69420. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68795/0.69448. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68769/0.69479. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68760/0.69507. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68783/0.69545. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68725/0.69579. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68720/0.69615. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68684/0.69654. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68677/0.69696. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68669/0.69734. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68662/0.69773. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68627/0.69812. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68605/0.69852. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68593/0.69888. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68560/0.69925. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68532/0.69964. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68533/0.70001. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68503/0.70036. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68514/0.70070. Took 0.08 sec\n",
      "Epoch 56, Loss(train/val) 0.68465/0.70107. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.68445/0.70144. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68443/0.70175. Took 0.08 sec\n",
      "Epoch 59, Loss(train/val) 0.68414/0.70204. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68387/0.70241. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68368/0.70277. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68333/0.70309. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68324/0.70343. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68300/0.70374. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68298/0.70417. Took 0.08 sec\n",
      "Epoch 66, Loss(train/val) 0.68273/0.70444. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.68251/0.70474. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68211/0.70507. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.68196/0.70535. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68181/0.70572. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68158/0.70609. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.68127/0.70635. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68081/0.70677. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68060/0.70710. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.68021/0.70743. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.68040/0.70774. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67995/0.70802. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67929/0.70841. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67937/0.70868. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67901/0.70914. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.67847/0.70954. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67818/0.70983. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67808/0.71024. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.67771/0.71060. Took 0.08 sec\n",
      "Epoch 85, Loss(train/val) 0.67764/0.71102. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67690/0.71136. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67672/0.71177. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67633/0.71209. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67566/0.71248. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.67551/0.71298. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67515/0.71347. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67484/0.71376. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.67436/0.71420. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67392/0.71464. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.67373/0.71501. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.67360/0.71537. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.67299/0.71566. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.67242/0.71606. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.67212/0.71649. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69954/0.68905. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69601/0.68827. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69417/0.68813. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69334/0.68822. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69286/0.68831. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69183/0.68835. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69173/0.68834. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69118/0.68829. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69113/0.68820. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69055/0.68811. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69028/0.68804. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69017/0.68796. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68952/0.68788. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68954/0.68782. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68919/0.68775. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68847/0.68772. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68833/0.68770. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68802/0.68766. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68795/0.68764. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68771/0.68763. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68749/0.68768. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68734/0.68775. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68691/0.68783. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68680/0.68790. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68667/0.68799. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68642/0.68806. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68622/0.68817. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68596/0.68829. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68611/0.68841. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68577/0.68852. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68542/0.68865. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68523/0.68876. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68516/0.68887. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68492/0.68898. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68495/0.68908. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68480/0.68918. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68464/0.68926. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68422/0.68932. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68424/0.68946. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68424/0.68952. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68396/0.68959. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68363/0.68972. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68363/0.68979. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68358/0.68990. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68332/0.68996. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68324/0.69000. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68340/0.69007. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68303/0.69014. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68271/0.69017. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68237/0.69026. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.68241/0.69035. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68199/0.69043. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68180/0.69051. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68162/0.69054. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.68152/0.69061. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68103/0.69064. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68090/0.69073. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68090/0.69077. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68061/0.69082. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68010/0.69084. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68022/0.69087. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67963/0.69095. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68010/0.69099. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67926/0.69109. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67925/0.69111. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67855/0.69117. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67817/0.69125. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67815/0.69133. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67818/0.69137. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67786/0.69144. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67750/0.69149. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67741/0.69155. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67706/0.69160. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67649/0.69170. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67582/0.69177. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.67608/0.69186. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67597/0.69195. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67492/0.69206. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67463/0.69219. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67393/0.69221. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67401/0.69238. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.67375/0.69248. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67301/0.69259. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67306/0.69265. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.67229/0.69282. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67158/0.69295. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67151/0.69311. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.67078/0.69328. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67047/0.69345. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66993/0.69362. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66936/0.69379. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66913/0.69397. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66858/0.69418. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66787/0.69445. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66756/0.69460. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66717/0.69484. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66596/0.69500. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66627/0.69529. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66527/0.69556. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.66499/0.69581. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69981/0.70346. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69875/0.70241. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69816/0.70123. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69705/0.69975. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69559/0.69794. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69469/0.69619. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69384/0.69468. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69334/0.69359. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.69260/0.69287. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69235/0.69248. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69263/0.69220. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69249/0.69210. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69216/0.69206. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69189/0.69207. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69183/0.69209. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69176/0.69214. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69166/0.69219. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69197/0.69228. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69183/0.69239. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69153/0.69253. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69144/0.69264. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69142/0.69272. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69119/0.69286. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.69151/0.69297. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.69100/0.69314. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.69092/0.69328. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.69086/0.69344. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.69078/0.69362. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.69079/0.69382. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.69090/0.69397. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.69047/0.69412. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.69056/0.69431. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.69064/0.69448. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.69035/0.69470. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.69034/0.69491. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.69027/0.69506. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68997/0.69529. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.69000/0.69552. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68984/0.69576. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68956/0.69599. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68962/0.69623. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68955/0.69649. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68916/0.69676. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68944/0.69706. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68898/0.69746. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68892/0.69772. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68873/0.69801. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68852/0.69846. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.68818/0.69889. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68826/0.69925. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68809/0.69970. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68750/0.70017. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68756/0.70054. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68730/0.70093. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.68713/0.70145. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.68665/0.70185. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68653/0.70236. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68618/0.70295. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68592/0.70345. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68563/0.70397. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.68556/0.70450. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68486/0.70499. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68475/0.70562. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.68457/0.70607. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.68404/0.70658. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.68416/0.70715. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.68322/0.70771. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.68290/0.70831. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.68294/0.70892. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.68233/0.70943. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.68195/0.71004. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.68175/0.71067. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.68098/0.71116. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.68027/0.71174. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.68059/0.71232. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.68003/0.71312. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67975/0.71387. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67903/0.71427. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.67904/0.71463. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67845/0.71510. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67795/0.71589. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.67717/0.71632. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.67648/0.71678. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.67610/0.71708. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.67558/0.71789. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.67526/0.71825. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.67474/0.71891. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.67365/0.71920. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.67379/0.72001. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.67317/0.72007. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.67216/0.72091. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.67143/0.72128. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.67115/0.72167. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.67094/0.72258. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.67009/0.72304. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66946/0.72348. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.66870/0.72447. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66789/0.72497. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66743/0.72584. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.66662/0.72614. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69355/0.69262. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69276/0.69176. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69154/0.69107. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69125/0.69065. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69059/0.69047. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.69048. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68988/0.69063. Took 0.08 sec\n",
      "Epoch 7, Loss(train/val) 0.68980/0.69085. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68954/0.69110. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68958/0.69137. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68916/0.69168. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.69196. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68893/0.69228. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68865/0.69264. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68865/0.69303. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68837/0.69343. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68808/0.69385. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68804/0.69428. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68792/0.69475. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68772/0.69520. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68762/0.69569. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68753/0.69618. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68710/0.69670. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68672/0.69724. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68665/0.69777. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68643/0.69831. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68608/0.69884. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68604/0.69934. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68571/0.69985. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68529/0.70037. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68528/0.70085. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68489/0.70128. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68504/0.70176. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68462/0.70226. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68471/0.70272. Took 0.08 sec\n",
      "Epoch 35, Loss(train/val) 0.68429/0.70318. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68435/0.70358. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68403/0.70400. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.68373/0.70441. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68335/0.70482. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68345/0.70518. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68301/0.70556. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68278/0.70583. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68289/0.70617. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68268/0.70646. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68222/0.70672. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68195/0.70700. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68193/0.70723. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68156/0.70741. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68140/0.70762. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68092/0.70786. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68081/0.70817. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68025/0.70837. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68004/0.70858. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67981/0.70879. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67954/0.70895. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67933/0.70905. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67910/0.70910. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67860/0.70919. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67841/0.70936. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67796/0.70943. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67764/0.70947. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67701/0.70943. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67659/0.70940. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67674/0.70934. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67611/0.70923. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67551/0.70913. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67496/0.70924. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67455/0.70910. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67415/0.70894. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67340/0.70872. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67293/0.70844. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67302/0.70823. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67190/0.70813. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67156/0.70789. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.67117/0.70756. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67056/0.70744. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67004/0.70701. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66935/0.70665. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66864/0.70631. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66813/0.70611. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66712/0.70606. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66670/0.70545. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66610/0.70498. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66519/0.70456. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66511/0.70399. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66431/0.70360. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66364/0.70337. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66254/0.70307. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66181/0.70277. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.66166/0.70243. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66076/0.70206. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65989/0.70204. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65873/0.70156. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65865/0.70138. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65775/0.70116. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65673/0.70076. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65595/0.70068. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65551/0.70049. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65500/0.70025. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69225/0.70409. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69202/0.70398. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69178/0.70387. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69154/0.70377. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69161/0.70368. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69108/0.70358. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.70349. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69048/0.70342. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69007/0.70340. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68968/0.70339. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68912/0.70341. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68873/0.70352. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68852/0.70378. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.70402. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68698/0.70425. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68616/0.70433. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68505/0.70446. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68409/0.70444. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68289/0.70463. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68138/0.70494. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68025/0.70535. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67891/0.70592. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67727/0.70584. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67608/0.70613. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67422/0.70636. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67346/0.70682. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67168/0.70765. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67188/0.70749. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67052/0.70810. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.66983/0.70864. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.66839/0.70843. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66663/0.70847. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66647/0.70875. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66631/0.70979. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66570/0.70997. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66417/0.71033. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.66361/0.71089. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.66252/0.71098. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.66288/0.71152. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.66158/0.71212. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.66004/0.71252. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65937/0.71329. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65906/0.71378. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.65836/0.71454. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.65718/0.71506. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.65709/0.71542. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.65564/0.71600. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.65454/0.71664. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.65384/0.71736. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.65301/0.71786. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65231/0.71877. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.65106/0.71995. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.64972/0.71997. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64942/0.72142. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.64781/0.72246. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64675/0.72318. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64600/0.72469. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.64512/0.72528. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.64363/0.72670. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64308/0.72802. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64208/0.72879. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.63984/0.72955. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63879/0.73079. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.63762/0.73216. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.63691/0.73386. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63655/0.73537. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.63403/0.73705. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.63320/0.73807. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63253/0.74016. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63122/0.74065. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62969/0.74248. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.62939/0.74373. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.62821/0.74523. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62743/0.74718. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62570/0.74860. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62396/0.74934. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.62315/0.75060. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62299/0.75213. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62033/0.75291. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61936/0.75544. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61816/0.75598. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61616/0.75741. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.61602/0.75863. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61499/0.76072. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61334/0.76251. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61271/0.76395. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61074/0.76452. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60973/0.76619. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.60958/0.76680. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60754/0.76801. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60622/0.76990. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.60609/0.77160. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60327/0.77296. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60425/0.77407. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60186/0.77542. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60122/0.77591. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60003/0.77726. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59982/0.77885. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.59805/0.77932. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59663/0.78201. Took 0.10 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.70160/0.69320. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69792/0.69037. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69522/0.68798. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69272/0.68653. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69156/0.68602. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69043/0.68590. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69035/0.68585. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.68576. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68987/0.68567. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68993/0.68557. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68980/0.68544. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68969/0.68531. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68943/0.68517. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68930/0.68502. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68904/0.68488. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68893/0.68472. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68861/0.68453. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68857/0.68438. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68835/0.68424. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68812/0.68408. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68776/0.68389. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68783/0.68371. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68769/0.68349. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68729/0.68331. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68702/0.68310. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68694/0.68291. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68679/0.68273. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68640/0.68252. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68630/0.68232. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68580/0.68212. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68549/0.68192. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68535/0.68176. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68520/0.68156. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68469/0.68143. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68459/0.68127. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68435/0.68112. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68421/0.68098. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68393/0.68082. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68339/0.68068. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68332/0.68055. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68308/0.68046. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68244/0.68042. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68208/0.68033. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68202/0.68022. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68127/0.68016. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68096/0.68011. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68120/0.68006. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68054/0.68001. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68015/0.67998. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67974/0.67997. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67931/0.67990. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67909/0.67989. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67855/0.67994. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67816/0.67993. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67791/0.67995. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67719/0.68003. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67663/0.68002. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67611/0.68009. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67567/0.68020. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67519/0.68029. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67481/0.68030. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67437/0.68044. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67369/0.68058. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67304/0.68072. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67271/0.68090. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67208/0.68097. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67152/0.68110. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67069/0.68125. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67018/0.68135. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67012/0.68129. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66939/0.68157. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66827/0.68154. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66768/0.68175. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66726/0.68184. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66666/0.68192. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66592/0.68208. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66566/0.68233. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66486/0.68241. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66460/0.68250. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66383/0.68281. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66274/0.68285. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66275/0.68302. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66163/0.68330. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66085/0.68347. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65999/0.68356. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65959/0.68377. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65865/0.68405. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65832/0.68432. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65738/0.68437. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65643/0.68468. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65521/0.68505. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65522/0.68527. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65462/0.68562. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65392/0.68587. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65317/0.68593. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65228/0.68637. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65195/0.68651. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65132/0.68706. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65000/0.68712. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64890/0.68737. Took 0.11 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.70317/0.69809. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69986/0.69613. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69836/0.69492. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69651/0.69400. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69572/0.69320. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69513/0.69252. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69401/0.69191. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69330/0.69136. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69285/0.69084. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69207/0.69038. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69147/0.68995. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69106/0.68955. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69045/0.68922. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68974/0.68891. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68959/0.68862. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68892/0.68837. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68827/0.68815. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68816/0.68793. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68781/0.68776. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68746/0.68761. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68704/0.68748. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68666/0.68737. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68652/0.68729. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68607/0.68724. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68575/0.68718. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68519/0.68714. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68519/0.68713. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68524/0.68711. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68491/0.68709. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68448/0.68708. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68452/0.68710. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68401/0.68709. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68371/0.68706. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68400/0.68706. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68382/0.68705. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68336/0.68708. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68304/0.68711. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68305/0.68713. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68265/0.68712. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68255/0.68713. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68234/0.68707. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68192/0.68708. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68198/0.68706. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68156/0.68705. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68182/0.68700. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68142/0.68696. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68117/0.68690. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68091/0.68688. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68097/0.68688. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.68058/0.68683. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68071/0.68674. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68032/0.68665. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67998/0.68663. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68003/0.68658. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67923/0.68655. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67927/0.68651. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67910/0.68639. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67852/0.68635. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.67850/0.68625. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67824/0.68616. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67778/0.68610. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67764/0.68606. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67754/0.68598. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67705/0.68591. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67687/0.68578. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67604/0.68563. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67603/0.68556. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67571/0.68546. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67545/0.68535. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67513/0.68525. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.67512/0.68516. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67449/0.68506. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67446/0.68493. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67403/0.68483. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67344/0.68475. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67321/0.68462. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67294/0.68452. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67267/0.68439. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67213/0.68425. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67126/0.68414. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67151/0.68403. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67130/0.68391. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.67067/0.68383. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66989/0.68372. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66995/0.68368. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66928/0.68357. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66909/0.68348. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66866/0.68340. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.66811/0.68345. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66716/0.68333. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66743/0.68333. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66691/0.68336. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66637/0.68338. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66594/0.68334. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66530/0.68340. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66533/0.68345. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66484/0.68348. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.66365/0.68365. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.66387/0.68369. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.66297/0.68379. Took 0.10 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69706/0.69677. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69635/0.69564. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69537/0.69427. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69442/0.69269. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69349/0.69118. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69259/0.68997. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69192/0.68910. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69150/0.68854. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69098/0.68814. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69070/0.68788. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69069/0.68767. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69047/0.68751. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69023/0.68734. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69007/0.68719. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68980/0.68706. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68935/0.68692. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68925/0.68679. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68919/0.68665. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68889/0.68647. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68859/0.68630. Took 0.11 sec\n",
      "Epoch 20, Loss(train/val) 0.68824/0.68610. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68804/0.68590. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68758/0.68567. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68729/0.68544. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68699/0.68519. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68667/0.68491. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68626/0.68463. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68608/0.68435. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68558/0.68408. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68524/0.68379. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68465/0.68350. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68408/0.68321. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68363/0.68295. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68343/0.68268. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68285/0.68241. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68245/0.68217. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68185/0.68199. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68175/0.68184. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68120/0.68171. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68104/0.68159. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68070/0.68148. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68059/0.68139. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.68009/0.68134. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67951/0.68128. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67943/0.68122. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67925/0.68121. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67889/0.68121. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67839/0.68120. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67780/0.68121. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67761/0.68125. Took 0.08 sec\n",
      "Epoch 50, Loss(train/val) 0.67744/0.68126. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67721/0.68131. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67700/0.68129. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67680/0.68131. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67634/0.68134. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67574/0.68136. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67568/0.68139. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67499/0.68146. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67498/0.68154. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67474/0.68165. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67404/0.68170. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67394/0.68183. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67372/0.68189. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67312/0.68195. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67308/0.68204. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67220/0.68212. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67191/0.68225. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67166/0.68234. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67114/0.68241. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67080/0.68251. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67065/0.68265. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66995/0.68275. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66937/0.68280. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66926/0.68283. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66849/0.68287. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66826/0.68299. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66776/0.68307. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66735/0.68313. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66741/0.68321. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66628/0.68327. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66580/0.68342. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66520/0.68355. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66487/0.68363. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66432/0.68378. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66312/0.68392. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66262/0.68410. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66267/0.68421. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66178/0.68439. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66111/0.68458. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66033/0.68466. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65948/0.68490. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65879/0.68516. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65794/0.68545. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65691/0.68584. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65617/0.68614. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65534/0.68651. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65462/0.68685. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65298/0.68730. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65198/0.68765. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65090/0.68846. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69318/0.69182. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69271/0.69170. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69160. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69150. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.69142. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69174/0.69134. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69153/0.69127. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69112/0.69119. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69091/0.69111. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69058/0.69102. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69037/0.69094. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69007/0.69086. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68963/0.69079. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68938/0.69071. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68881/0.69067. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68840/0.69063. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68784/0.69062. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68740/0.69064. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68682/0.69071. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68621/0.69079. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68592/0.69096. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68523/0.69118. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68462/0.69144. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68410/0.69174. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68347/0.69205. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68285/0.69239. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68256/0.69269. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68209/0.69305. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.69334. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68135/0.69364. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.68081/0.69395. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68076/0.69422. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.69449. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68000/0.69474. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67995/0.69496. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67927/0.69519. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67916/0.69537. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67873/0.69555. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67848/0.69576. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67813/0.69594. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67748/0.69616. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67743/0.69638. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67712/0.69660. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67702/0.69684. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67639/0.69709. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67626/0.69731. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67612/0.69750. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67574/0.69769. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67533/0.69788. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67476/0.69813. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67451/0.69839. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67418/0.69867. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.67403/0.69894. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67374/0.69915. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67299/0.69944. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67308/0.69973. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67223/0.70001. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67214/0.70034. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67194/0.70058. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67167/0.70083. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67110/0.70118. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67054/0.70145. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67031/0.70180. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66974/0.70218. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66958/0.70255. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66914/0.70296. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66870/0.70326. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66818/0.70366. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66784/0.70401. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66715/0.70438. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66680/0.70477. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66589/0.70526. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66521/0.70576. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.66508/0.70618. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66455/0.70660. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66407/0.70705. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66361/0.70747. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66259/0.70799. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66212/0.70858. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66168/0.70906. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66100/0.70950. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65986/0.71006. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65894/0.71059. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65844/0.71127. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65749/0.71186. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65672/0.71248. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65535/0.71304. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65502/0.71366. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65385/0.71423. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65274/0.71499. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65195/0.71569. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65109/0.71645. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64981/0.71714. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64857/0.71785. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64744/0.71853. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64600/0.71930. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64509/0.72001. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64326/0.72088. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64195/0.72162. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64112/0.72237. Took 0.10 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69586/0.68389. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69587/0.68404. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69568/0.68420. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69527/0.68439. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69512/0.68461. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69499/0.68491. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69446/0.68533. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69406/0.68593. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69352/0.68682. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69302/0.68810. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69197/0.68973. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69130/0.69150. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69051/0.69318. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.69003/0.69470. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68953/0.69579. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68922/0.69661. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68906/0.69734. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68851/0.69785. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.69819. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68803/0.69847. Took 0.08 sec\n",
      "Epoch 20, Loss(train/val) 0.68770/0.69867. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68724/0.69884. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68690/0.69912. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68635/0.69934. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68633/0.69933. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68593/0.69955. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68545/0.69973. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68507/0.69994. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68470/0.70012. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68427/0.70029. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68373/0.70025. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68306/0.70035. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68277/0.70065. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68254/0.70090. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68181/0.70115. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68140/0.70152. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68101/0.70201. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68050/0.70242. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67999/0.70265. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67925/0.70318. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67912/0.70382. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67806/0.70419. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67753/0.70485. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67698/0.70502. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67664/0.70576. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67566/0.70659. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67474/0.70702. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67429/0.70768. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67317/0.70866. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67207/0.70950. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67149/0.71050. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67047/0.71137. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66955/0.71260. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66866/0.71360. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66751/0.71444. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66736/0.71561. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66554/0.71652. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66455/0.71757. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66362/0.71837. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66186/0.71923. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66158/0.72095. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65990/0.72127. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65941/0.72299. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65774/0.72375. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65698/0.72481. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65504/0.72587. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65365/0.72682. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65267/0.72851. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65116/0.72927. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.65042/0.73062. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64826/0.73211. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64768/0.73329. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64575/0.73473. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64535/0.73627. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64355/0.73804. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64231/0.73921. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64022/0.74076. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63943/0.74247. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63745/0.74403. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63696/0.74532. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63478/0.74760. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63369/0.74884. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63255/0.75008. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63063/0.75218. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62953/0.75364. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62872/0.75541. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62684/0.75693. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62504/0.75908. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62341/0.76070. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62232/0.76234. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.62111/0.76408. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61953/0.76585. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61846/0.76800. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.61682/0.76998. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61613/0.77113. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61412/0.77274. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.61286/0.77493. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61187/0.77654. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61097/0.77859. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.60953/0.78015. Took 0.09 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69262/0.70084. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69209/0.70212. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69186/0.70316. Took 0.08 sec\n",
      "Epoch 3, Loss(train/val) 0.69146/0.70400. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69137/0.70466. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69124/0.70511. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69100/0.70546. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69097/0.70574. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69064/0.70598. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69059/0.70618. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69065/0.70633. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69051/0.70647. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69026/0.70662. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69038/0.70669. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68993/0.70679. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68988/0.70691. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68962/0.70698. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68964/0.70711. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68955/0.70726. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68946/0.70738. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68941/0.70742. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68909/0.70753. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68897/0.70765. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68894/0.70767. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68874/0.70779. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68861/0.70793. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68859/0.70795. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68841/0.70804. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68840/0.70818. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68823/0.70821. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68776/0.70823. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68774/0.70836. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68765/0.70838. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68740/0.70857. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68732/0.70869. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68701/0.70874. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68711/0.70871. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68683/0.70869. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68654/0.70874. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68625/0.70886. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68618/0.70895. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68633/0.70901. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68581/0.70904. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68546/0.70918. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68520/0.70934. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68516/0.70952. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68482/0.70955. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68455/0.70970. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68452/0.70983. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68393/0.70991. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68366/0.70999. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68324/0.71009. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68305/0.71034. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68257/0.71053. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68240/0.71067. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.68178/0.71091. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68152/0.71105. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68111/0.71147. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68059/0.71188. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68017/0.71196. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67977/0.71231. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67920/0.71269. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67899/0.71302. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67836/0.71325. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67741/0.71395. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67755/0.71435. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67638/0.71478. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67616/0.71534. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67522/0.71564. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67489/0.71643. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67434/0.71679. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67333/0.71748. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67292/0.71787. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67228/0.71839. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67149/0.71914. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67035/0.71935. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.67013/0.72011. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66934/0.72063. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66815/0.72147. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66777/0.72200. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66701/0.72252. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66623/0.72298. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66543/0.72335. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66513/0.72408. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66385/0.72444. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66335/0.72479. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66263/0.72569. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66169/0.72618. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66102/0.72695. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65991/0.72752. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65915/0.72801. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65874/0.72842. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65817/0.72884. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65722/0.72927. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65662/0.72995. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65547/0.73053. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65509/0.73090. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65466/0.73180. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65393/0.73225. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65270/0.73306. Took 0.11 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69725/0.69252. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69537/0.69387. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69448/0.69549. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69344/0.69716. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69309/0.69875. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69236/0.70001. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69237/0.70093. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69202/0.70160. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69169/0.70208. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69146/0.70246. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69140/0.70269. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69118/0.70286. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69115/0.70294. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69099/0.70308. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69077/0.70325. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69060/0.70331. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69036/0.70338. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69034/0.70348. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69007/0.70357. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68997/0.70367. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68981/0.70381. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68960/0.70388. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68965/0.70405. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68951/0.70417. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68934/0.70428. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68908/0.70434. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68893/0.70442. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68874/0.70452. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68829/0.70459. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68834/0.70468. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68804/0.70471. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68816/0.70478. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68823/0.70469. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68784/0.70476. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68762/0.70474. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.68774/0.70468. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68721/0.70475. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68698/0.70467. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68706/0.70461. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68683/0.70463. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68659/0.70458. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68629/0.70451. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68630/0.70456. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68596/0.70454. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68617/0.70447. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68545/0.70436. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68523/0.70424. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68513/0.70422. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68470/0.70407. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68455/0.70393. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68443/0.70385. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68424/0.70377. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.68375/0.70363. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68336/0.70356. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68338/0.70346. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68297/0.70328. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68281/0.70320. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68232/0.70305. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.68178/0.70303. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.68147/0.70308. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68139/0.70290. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68098/0.70292. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68060/0.70290. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.68015/0.70277. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67988/0.70273. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67929/0.70273. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67896/0.70267. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67859/0.70296. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67802/0.70285. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67762/0.70297. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67695/0.70290. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67659/0.70315. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67618/0.70322. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67595/0.70326. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67526/0.70343. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67439/0.70376. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.67418/0.70380. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67384/0.70379. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67359/0.70423. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.67232/0.70407. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67188/0.70449. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67157/0.70445. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.67113/0.70460. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66991/0.70482. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66905/0.70513. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.66938/0.70514. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66796/0.70554. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66770/0.70558. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66730/0.70581. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66675/0.70575. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66552/0.70615. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.66507/0.70642. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66452/0.70673. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66358/0.70717. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.66261/0.70717. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66234/0.70755. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66095/0.70806. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66025/0.70854. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65946/0.70906. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65885/0.70899. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69480/0.69167. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69437/0.69176. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69422/0.69185. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69415/0.69194. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69379/0.69203. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69386/0.69212. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69345/0.69221. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69303/0.69230. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69309/0.69241. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69293/0.69252. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69261/0.69265. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69273/0.69280. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69268/0.69295. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69230/0.69315. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69214/0.69337. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69199/0.69365. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69196/0.69398. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69154/0.69439. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69108/0.69485. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69103/0.69542. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69046/0.69610. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69004/0.69691. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68936/0.69786. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68887/0.69897. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68826/0.70014. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68804/0.70132. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68713/0.70262. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68676/0.70385. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68612/0.70503. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68560/0.70613. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68473/0.70712. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68477/0.70802. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68428/0.70878. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68354/0.70943. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68301/0.71008. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68278/0.71058. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68210/0.71111. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68164/0.71152. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68122/0.71198. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68015/0.71243. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68002/0.71296. Took 0.08 sec\n",
      "Epoch 41, Loss(train/val) 0.67949/0.71334. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67868/0.71375. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67787/0.71412. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67696/0.71451. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67622/0.71496. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67541/0.71559. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67479/0.71606. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67337/0.71657. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67291/0.71719. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67205/0.71793. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67132/0.71867. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67021/0.71943. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66909/0.72017. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66840/0.72100. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66755/0.72177. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66629/0.72278. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66484/0.72380. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66446/0.72503. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66308/0.72613. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66257/0.72725. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66206/0.72847. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66037/0.72994. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65988/0.73126. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65895/0.73244. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65774/0.73382. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65761/0.73485. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65673/0.73638. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65577/0.73790. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65494/0.73927. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65362/0.74050. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65306/0.74214. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65250/0.74377. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65159/0.74491. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65103/0.74633. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65043/0.74781. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64929/0.74918. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64818/0.75040. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64792/0.75190. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64711/0.75321. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64631/0.75476. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64570/0.75614. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64509/0.75751. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64519/0.75805. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64330/0.75950. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64312/0.76052. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64112/0.76225. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64108/0.76320. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64056/0.76465. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64059/0.76581. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63898/0.76738. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63872/0.76862. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63738/0.77013. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63754/0.77091. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63601/0.77271. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63513/0.77332. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63471/0.77468. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63448/0.77565. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63200/0.77711. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63275/0.77822. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69319/0.68986. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.68994. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69285/0.69003. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69013. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69261/0.69021. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69254/0.69030. Took 0.08 sec\n",
      "Epoch 6, Loss(train/val) 0.69224/0.69039. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69216/0.69047. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69201/0.69056. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69194/0.69064. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69170/0.69070. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69154/0.69076. Took 0.08 sec\n",
      "Epoch 12, Loss(train/val) 0.69119/0.69083. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69103/0.69089. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69113/0.69097. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69092/0.69104. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69061/0.69111. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69053/0.69116. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69049/0.69121. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68996/0.69125. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68984/0.69129. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68972/0.69137. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68942/0.69144. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68904/0.69149. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68889/0.69160. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68870/0.69164. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68833/0.69171. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68820/0.69185. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68789/0.69200. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68701/0.69208. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68707/0.69219. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68659/0.69235. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68663/0.69251. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68637/0.69266. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68563/0.69283. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68562/0.69302. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68499/0.69323. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68473/0.69348. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68444/0.69381. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68408/0.69402. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68397/0.69431. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68320/0.69456. Took 0.08 sec\n",
      "Epoch 42, Loss(train/val) 0.68270/0.69489. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68260/0.69521. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68209/0.69556. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68152/0.69587. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68135/0.69629. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68089/0.69680. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68043/0.69712. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67985/0.69760. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67920/0.69801. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67893/0.69861. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67823/0.69923. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67762/0.69964. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67723/0.70012. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67630/0.70075. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67588/0.70130. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67502/0.70193. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67478/0.70255. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67441/0.70336. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67324/0.70404. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67263/0.70453. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67198/0.70535. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67135/0.70624. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67008/0.70681. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66993/0.70781. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66866/0.70832. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66772/0.70942. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66722/0.70986. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66657/0.71071. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66504/0.71147. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66456/0.71251. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66382/0.71340. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66271/0.71441. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66137/0.71520. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66064/0.71623. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65938/0.71692. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65841/0.71774. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65782/0.71887. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65602/0.71978. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65575/0.72080. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65507/0.72184. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65336/0.72271. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65249/0.72384. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65162/0.72451. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.65154/0.72579. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64989/0.72636. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64918/0.72746. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64808/0.72837. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64741/0.72878. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64667/0.72996. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64538/0.73072. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64373/0.73113. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64389/0.73236. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.64279/0.73331. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64180/0.73412. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64027/0.73467. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63944/0.73590. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64022/0.73651. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63836/0.73674. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69475/0.68424. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69314/0.68630. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69235/0.68783. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69248/0.68889. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.68952. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69188/0.69000. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69167/0.69040. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69114/0.69073. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69090/0.69105. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69089/0.69144. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69057/0.69175. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69061/0.69205. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69025/0.69243. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68988/0.69274. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68967/0.69301. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68954/0.69331. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68936/0.69361. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68904/0.69399. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68910/0.69428. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68866/0.69462. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68844/0.69500. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68807/0.69542. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68787/0.69574. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68798/0.69601. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68738/0.69636. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68750/0.69671. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68696/0.69698. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68687/0.69730. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68633/0.69758. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68667/0.69787. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68610/0.69810. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68574/0.69839. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68562/0.69867. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68556/0.69896. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68490/0.69902. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68480/0.69925. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68430/0.69943. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68403/0.69956. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68348/0.69963. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68333/0.69990. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68303/0.69996. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68279/0.70014. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68242/0.70022. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68201/0.70025. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68161/0.70030. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68098/0.70021. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68082/0.70033. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68022/0.70032. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67965/0.70043. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67886/0.70038. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67894/0.70015. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67818/0.70012. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67789/0.69990. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67733/0.69982. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67708/0.69999. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67627/0.69947. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67526/0.69941. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67501/0.69929. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67470/0.69909. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67337/0.69881. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67352/0.69870. Took 0.12 sec\n",
      "Epoch 61, Loss(train/val) 0.67202/0.69858. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67073/0.69814. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67080/0.69798. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66969/0.69780. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66919/0.69768. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66829/0.69752. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66780/0.69718. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66726/0.69726. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66650/0.69701. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66529/0.69684. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66491/0.69676. Took 0.08 sec\n",
      "Epoch 72, Loss(train/val) 0.66434/0.69677. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66364/0.69673. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66338/0.69674. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66160/0.69674. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66185/0.69686. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65993/0.69712. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66041/0.69709. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65929/0.69726. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65909/0.69721. Took 0.08 sec\n",
      "Epoch 81, Loss(train/val) 0.65839/0.69749. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65763/0.69764. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65689/0.69766. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65688/0.69759. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65608/0.69752. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65607/0.69786. Took 0.08 sec\n",
      "Epoch 87, Loss(train/val) 0.65519/0.69799. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65377/0.69829. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65398/0.69848. Took 0.08 sec\n",
      "Epoch 90, Loss(train/val) 0.65276/0.69869. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65338/0.69905. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65201/0.69913. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65158/0.69904. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65097/0.69915. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65001/0.69933. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65004/0.69938. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64837/0.69959. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64856/0.69983. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64873/0.69977. Took 0.09 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69474. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69350/0.69463. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69329/0.69456. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69299/0.69451. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69283/0.69448. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69238/0.69449. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69227/0.69452. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69197/0.69458. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69172/0.69464. Took 0.08 sec\n",
      "Epoch 9, Loss(train/val) 0.69133/0.69471. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69126/0.69481. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69119/0.69490. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69051/0.69501. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69026/0.69513. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68965/0.69524. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68930/0.69537. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68885/0.69547. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68821/0.69558. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68786/0.69569. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68710/0.69576. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68652/0.69587. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68576/0.69600. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68501/0.69613. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68422/0.69628. Took 0.08 sec\n",
      "Epoch 24, Loss(train/val) 0.68371/0.69646. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68302/0.69663. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68236/0.69682. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68174/0.69704. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68077/0.69724. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68035/0.69748. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67958/0.69772. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67913/0.69798. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67835/0.69827. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67787/0.69857. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67745/0.69886. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67663/0.69919. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67634/0.69957. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67529/0.70002. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67448/0.70042. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67408/0.70085. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67404/0.70123. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67278/0.70161. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67206/0.70211. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67178/0.70263. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67093/0.70315. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67088/0.70362. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66994/0.70404. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66969/0.70465. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66872/0.70517. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66866/0.70559. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66754/0.70615. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66688/0.70668. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66631/0.70713. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66596/0.70769. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66482/0.70823. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66449/0.70878. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66362/0.70929. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66342/0.70991. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66231/0.71042. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66144/0.71110. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66106/0.71153. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66025/0.71210. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65953/0.71264. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65876/0.71325. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65811/0.71392. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65685/0.71440. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65608/0.71502. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65490/0.71573. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65399/0.71648. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65350/0.71719. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65311/0.71780. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65125/0.71872. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65055/0.71953. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65023/0.72053. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64890/0.72117. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64801/0.72216. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64678/0.72321. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64702/0.72392. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64463/0.72502. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64425/0.72603. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64286/0.72692. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64219/0.72819. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64078/0.72926. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63971/0.73040. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63811/0.73156. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63745/0.73299. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63710/0.73431. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63532/0.73562. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63404/0.73710. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63337/0.73870. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63182/0.74009. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63106/0.74175. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63091/0.74345. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62992/0.74446. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62702/0.74599. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62650/0.74754. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62571/0.74907. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.62497/0.75069. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62447/0.75225. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62378/0.75387. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69317/0.68983. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.68879. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69132/0.68792. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69038/0.68720. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68948/0.68668. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68893/0.68632. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68847/0.68606. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.68586. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68793/0.68569. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68765/0.68553. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68727/0.68539. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68693/0.68522. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68666/0.68506. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68661/0.68493. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68601/0.68480. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68574/0.68468. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68546/0.68458. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68518/0.68448. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68452/0.68439. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68444/0.68432. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68381/0.68425. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68369/0.68421. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68304/0.68417. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68247/0.68414. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68217/0.68414. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68188/0.68415. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68140/0.68417. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68108/0.68417. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68066/0.68419. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68011/0.68420. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67989/0.68422. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67941/0.68424. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67912/0.68429. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67879/0.68431. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67831/0.68434. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67790/0.68437. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67741/0.68440. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67697/0.68444. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67668/0.68446. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67623/0.68448. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67530/0.68451. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67542/0.68456. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67464/0.68461. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67433/0.68466. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67392/0.68472. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67341/0.68478. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67289/0.68489. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67254/0.68497. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67174/0.68506. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67122/0.68515. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67053/0.68529. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67015/0.68542. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66956/0.68554. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66889/0.68569. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66880/0.68586. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66766/0.68607. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66754/0.68627. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66702/0.68649. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66594/0.68676. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66571/0.68700. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66484/0.68728. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66409/0.68764. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66371/0.68804. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66287/0.68841. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66274/0.68875. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66172/0.68911. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66073/0.68955. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.66076/0.69005. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66024/0.69054. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65951/0.69105. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65900/0.69165. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65815/0.69229. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65747/0.69292. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65701/0.69360. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65624/0.69421. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65569/0.69486. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65487/0.69560. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65453/0.69627. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65432/0.69708. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65345/0.69786. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65292/0.69859. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65240/0.69941. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65117/0.70013. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65074/0.70097. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64978/0.70184. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64917/0.70265. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64891/0.70363. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64863/0.70446. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64736/0.70538. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.64694/0.70635. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64667/0.70728. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.64568/0.70821. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64546/0.70913. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64488/0.71010. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64414/0.71095. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64374/0.71199. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64372/0.71274. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.64303/0.71360. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64232/0.71450. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64128/0.71562. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69417/0.69734. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69348/0.69704. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69275/0.69685. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69258/0.69674. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69212/0.69667. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69156/0.69667. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.69671. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69076/0.69681. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69025/0.69697. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68991/0.69721. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68945/0.69751. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.69787. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68832/0.69835. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68823/0.69892. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68719/0.69961. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68661/0.70043. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68587/0.70130. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68536/0.70222. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68454/0.70324. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68431/0.70425. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68396/0.70528. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68326/0.70632. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68274/0.70728. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68256/0.70820. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68235/0.70909. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68189/0.70978. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68128/0.71054. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68094/0.71123. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68078/0.71188. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68003/0.71248. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68054/0.71306. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67949/0.71355. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67928/0.71401. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67919/0.71443. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67863/0.71486. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67821/0.71526. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67796/0.71559. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67773/0.71596. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67709/0.71629. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67677/0.71660. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67644/0.71691. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67622/0.71721. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67626/0.71743. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67547/0.71776. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67503/0.71808. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67464/0.71831. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67409/0.71850. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67392/0.71877. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67345/0.71901. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67268/0.71926. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67235/0.71948. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67142/0.71971. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67113/0.71990. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67064/0.72023. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67021/0.72063. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66895/0.72095. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66908/0.72106. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66800/0.72139. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66694/0.72161. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66676/0.72196. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66622/0.72220. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66508/0.72253. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66377/0.72295. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66389/0.72342. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66241/0.72369. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66176/0.72406. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66066/0.72455. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66054/0.72491. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65925/0.72548. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65831/0.72604. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65803/0.72643. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65677/0.72723. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65618/0.72770. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65521/0.72818. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65341/0.72873. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65371/0.72947. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65209/0.73019. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65133/0.73108. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65084/0.73154. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65014/0.73238. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64893/0.73312. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64782/0.73354. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64775/0.73452. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64662/0.73489. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64566/0.73559. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64469/0.73623. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64334/0.73733. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64333/0.73797. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64210/0.73861. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64111/0.73917. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63990/0.73981. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63955/0.74024. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63883/0.74071. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63785/0.74193. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63614/0.74253. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63531/0.74346. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63581/0.74434. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63420/0.74452. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63323/0.74567. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.63261/0.74646. Took 0.09 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69375/0.69436. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69339/0.69396. Took 0.11 sec\n",
      "Epoch 2, Loss(train/val) 0.69313/0.69359. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69303/0.69315. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69277/0.69267. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69247/0.69213. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69162. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69171/0.69109. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69157/0.69062. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69106/0.69027. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69105/0.69000. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69068/0.68977. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69023/0.68966. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68986/0.68971. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.68940/0.68981. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68897/0.68997. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68863/0.69037. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68810/0.69081. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68745/0.69139. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68704/0.69203. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68649/0.69272. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68607/0.69341. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68512/0.69422. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68498/0.69504. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68413/0.69602. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68370/0.69690. Took 0.10 sec\n",
      "Epoch 26, Loss(train/val) 0.68330/0.69775. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68275/0.69863. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68253/0.69948. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68230/0.70025. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68181/0.70103. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68152/0.70182. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68132/0.70264. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68081/0.70338. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68037/0.70414. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68013/0.70487. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67957/0.70563. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67933/0.70635. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67854/0.70697. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67882/0.70765. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67810/0.70835. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67792/0.70899. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67734/0.70969. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67706/0.71051. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67674/0.71119. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67630/0.71196. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67581/0.71265. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67488/0.71334. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67475/0.71405. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67420/0.71486. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67368/0.71560. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67370/0.71638. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67279/0.71721. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67200/0.71810. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67152/0.71898. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67139/0.71983. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67052/0.72079. Took 0.12 sec\n",
      "Epoch 57, Loss(train/val) 0.66978/0.72169. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66925/0.72252. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66835/0.72348. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66787/0.72456. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66702/0.72560. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66647/0.72667. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66562/0.72780. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66495/0.72879. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66370/0.73004. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66335/0.73117. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66216/0.73240. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66116/0.73379. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66028/0.73523. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65919/0.73650. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65818/0.73822. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65706/0.73954. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65627/0.74097. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65543/0.74238. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65379/0.74409. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65319/0.74537. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65279/0.74697. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65080/0.74859. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64965/0.75032. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64868/0.75190. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64728/0.75383. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64675/0.75533. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64598/0.75695. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64468/0.75896. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64410/0.76061. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64269/0.76212. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64178/0.76380. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64048/0.76553. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63936/0.76727. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63830/0.76863. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63819/0.77074. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63654/0.77230. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.63511/0.77402. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63390/0.77610. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63394/0.77778. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63220/0.77966. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63178/0.78147. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63037/0.78257. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62900/0.78440. Took 0.09 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69836/0.70511. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69510/0.69893. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69388. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69137/0.69012. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69046/0.68778. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.68641. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69001/0.68566. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.68979/0.68530. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68947/0.68509. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68948/0.68500. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68981/0.68503. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68935/0.68504. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68900/0.68503. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68898/0.68505. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68895/0.68503. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68855/0.68501. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68852/0.68505. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68862/0.68510. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68822/0.68512. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68803/0.68520. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68803/0.68524. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68806/0.68529. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68792/0.68534. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68776/0.68539. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68745/0.68545. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68733/0.68550. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68719/0.68561. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68712/0.68570. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68687/0.68575. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68683/0.68585. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68685/0.68601. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.68630/0.68606. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68638/0.68612. Took 0.08 sec\n",
      "Epoch 33, Loss(train/val) 0.68590/0.68617. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68605/0.68623. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68591/0.68641. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68568/0.68656. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68538/0.68664. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68551/0.68678. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68517/0.68695. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68502/0.68708. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68489/0.68722. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68465/0.68739. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68455/0.68749. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68430/0.68761. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68402/0.68780. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.68381/0.68794. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68361/0.68808. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68331/0.68825. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68324/0.68843. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68321/0.68853. Took 0.08 sec\n",
      "Epoch 51, Loss(train/val) 0.68281/0.68867. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68265/0.68895. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68239/0.68911. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.68221/0.68937. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68176/0.68950. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.68164/0.68967. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.68143/0.68991. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.68109/0.69016. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.68077/0.69045. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.68049/0.69072. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.68033/0.69094. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.68005/0.69114. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67947/0.69140. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67917/0.69181. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.67863/0.69206. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67854/0.69237. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67809/0.69251. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67762/0.69301. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67732/0.69334. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67679/0.69353. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67607/0.69384. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67592/0.69438. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.67523/0.69482. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67487/0.69511. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67411/0.69570. Took 0.11 sec\n",
      "Epoch 76, Loss(train/val) 0.67357/0.69615. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67309/0.69651. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67255/0.69695. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67167/0.69737. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.67095/0.69786. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67070/0.69846. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.66972/0.69907. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66888/0.69974. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66834/0.70050. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66763/0.70108. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66645/0.70161. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66584/0.70218. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66532/0.70302. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66443/0.70400. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66336/0.70472. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.66331/0.70555. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66185/0.70629. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66140/0.70730. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66054/0.70771. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65994/0.70907. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65853/0.70985. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.65778/0.71077. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65731/0.71178. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65628/0.71244. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69710/0.69601. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.69577/0.69452. Took 0.12 sec\n",
      "Epoch 2, Loss(train/val) 0.69480/0.69340. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69425/0.69256. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69334/0.69187. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69278/0.69128. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69220/0.69080. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69194/0.69040. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69146/0.69004. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69095/0.68971. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69061/0.68940. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69011/0.68911. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68952/0.68882. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68890/0.68855. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68875/0.68829. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68851/0.68807. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68771/0.68785. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68728/0.68768. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68684/0.68758. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68662/0.68754. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68608/0.68757. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68567/0.68767. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68545/0.68784. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68510/0.68803. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68497/0.68829. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68439/0.68856. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68433/0.68885. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68414/0.68920. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68409/0.68958. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68352/0.68996. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68340/0.69033. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68319/0.69073. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68310/0.69114. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68225/0.69156. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68241/0.69196. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68227/0.69234. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68216/0.69278. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68201/0.69322. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68174/0.69364. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68125/0.69405. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68119/0.69443. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68082/0.69484. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68066/0.69523. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68023/0.69561. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68046/0.69601. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68038/0.69637. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67978/0.69673. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67967/0.69710. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67977/0.69748. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67919/0.69781. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67910/0.69815. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67858/0.69846. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67855/0.69885. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67820/0.69917. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67813/0.69953. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67716/0.69994. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67731/0.70025. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67703/0.70060. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67692/0.70091. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67622/0.70123. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67642/0.70151. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67613/0.70188. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67546/0.70228. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67471/0.70263. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67486/0.70291. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67434/0.70323. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.67395/0.70354. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67327/0.70389. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67328/0.70413. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67256/0.70447. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67182/0.70472. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67119/0.70498. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.67075/0.70536. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67028/0.70557. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66946/0.70590. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66882/0.70609. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66814/0.70644. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66786/0.70676. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66653/0.70712. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66576/0.70738. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66556/0.70768. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.66501/0.70803. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66319/0.70826. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66321/0.70858. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66134/0.70883. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66166/0.70918. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66044/0.70952. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65963/0.70981. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65847/0.71027. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65730/0.71070. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65638/0.71108. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65570/0.71141. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65414/0.71194. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65356/0.71241. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65195/0.71314. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65138/0.71355. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65090/0.71392. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64990/0.71424. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64785/0.71482. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64799/0.71571. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.68903/0.69530. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.68868/0.69548. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.68866/0.69567. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68854/0.69587. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68841/0.69606. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68804/0.69628. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68788/0.69649. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68753/0.69668. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68776/0.69686. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68761/0.69704. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68718/0.69719. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68735/0.69731. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68719/0.69739. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68666/0.69741. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68647/0.69739. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68644/0.69732. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68627/0.69723. Took 0.08 sec\n",
      "Epoch 17, Loss(train/val) 0.68573/0.69705. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68546/0.69681. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68510/0.69656. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.69625. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68455/0.69589. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68442/0.69550. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68392/0.69509. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68357/0.69473. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68322/0.69430. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68290/0.69383. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68213/0.69341. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68214/0.69296. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68155/0.69262. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68118/0.69217. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68104/0.69172. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68064/0.69138. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68016/0.69099. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67964/0.69070. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67957/0.69035. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67896/0.68993. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67890/0.68956. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67815/0.68925. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67804/0.68888. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67770/0.68869. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67727/0.68841. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67708/0.68808. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67666/0.68774. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67608/0.68763. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67542/0.68747. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67535/0.68724. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67485/0.68700. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67404/0.68670. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67381/0.68668. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67334/0.68647. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67287/0.68615. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67232/0.68634. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67184/0.68616. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67129/0.68603. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67089/0.68605. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67010/0.68599. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66953/0.68603. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66855/0.68633. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66835/0.68628. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66775/0.68660. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66687/0.68678. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.66601/0.68686. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66535/0.68706. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66473/0.68729. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66352/0.68779. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66305/0.68801. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66264/0.68833. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66161/0.68911. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66126/0.68939. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66004/0.68969. Took 0.08 sec\n",
      "Epoch 71, Loss(train/val) 0.65937/0.69028. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65792/0.69103. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65714/0.69175. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65657/0.69222. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65513/0.69341. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65523/0.69388. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65354/0.69481. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65308/0.69568. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65251/0.69654. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65122/0.69730. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65075/0.69833. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64932/0.69946. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64924/0.70041. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64775/0.70104. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64679/0.70258. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64566/0.70316. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64538/0.70461. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64415/0.70577. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64353/0.70633. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64224/0.70776. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64059/0.70944. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64036/0.71013. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63930/0.71102. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63850/0.71299. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63729/0.71413. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63634/0.71564. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63480/0.71654. Took 0.08 sec\n",
      "Epoch 98, Loss(train/val) 0.63387/0.71745. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63287/0.71923. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.68821/0.69758. Took 0.26 sec\n",
      "Epoch 1, Loss(train/val) 0.68727/0.69854. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68634/0.69956. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68587/0.70063. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68545/0.70163. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68485/0.70251. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68450/0.70331. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68442/0.70397. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68389/0.70451. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68374/0.70494. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68342/0.70529. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68318/0.70554. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68299/0.70580. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68281/0.70600. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68240/0.70611. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68232/0.70625. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68185/0.70631. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68165/0.70644. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68123/0.70650. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68109/0.70653. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68083/0.70660. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68027/0.70676. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68017/0.70686. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67985/0.70698. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.67936/0.70714. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.67925/0.70724. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67867/0.70741. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67841/0.70758. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67768/0.70759. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67776/0.70780. Took 0.08 sec\n",
      "Epoch 30, Loss(train/val) 0.67701/0.70793. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67645/0.70793. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67609/0.70795. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67594/0.70809. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67517/0.70822. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67460/0.70810. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67426/0.70815. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67398/0.70800. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67346/0.70799. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67278/0.70784. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67202/0.70764. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67182/0.70747. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67091/0.70728. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67013/0.70688. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67001/0.70686. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66919/0.70646. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66827/0.70579. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66793/0.70552. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66732/0.70541. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66684/0.70498. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66581/0.70463. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66510/0.70412. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66443/0.70383. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66453/0.70343. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66334/0.70301. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66273/0.70250. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66190/0.70218. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66124/0.70166. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66117/0.70186. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66034/0.70136. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65942/0.70057. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65862/0.70055. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65769/0.69998. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65732/0.69982. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65678/0.69942. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65507/0.69862. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65475/0.69850. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65425/0.69840. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65372/0.69822. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65233/0.69741. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65132/0.69737. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65108/0.69692. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.64935/0.69721. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.64978/0.69676. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64825/0.69621. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64783/0.69573. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64636/0.69560. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64554/0.69553. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64464/0.69481. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.64368/0.69475. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64286/0.69441. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64169/0.69433. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64092/0.69472. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64011/0.69454. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.63917/0.69452. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63782/0.69425. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63724/0.69393. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63542/0.69418. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63479/0.69356. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63415/0.69358. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63261/0.69373. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.63127/0.69363. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63067/0.69375. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62946/0.69386. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.62998/0.69447. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62792/0.69478. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.62627/0.69421. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62480/0.69448. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62472/0.69457. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62382/0.69548. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70437/0.69725. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69813/0.69042. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69400/0.68584. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69168/0.68328. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69042/0.68197. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68974/0.68133. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68940/0.68099. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68919/0.68073. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68821/0.68057. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68822/0.68045. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.68032. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68798/0.68024. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68694/0.68019. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68695/0.68012. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68625/0.68005. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68629/0.67999. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68582/0.67992. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68532/0.67989. Took 0.11 sec\n",
      "Epoch 18, Loss(train/val) 0.68507/0.67984. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68485/0.67980. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68462/0.67981. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68407/0.67981. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68356/0.67979. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68384/0.67982. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68343/0.67984. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68261/0.67985. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68260/0.67985. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68230/0.67989. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68216/0.67990. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68216/0.67994. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68138/0.67996. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68118/0.67993. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.68121/0.67992. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68099/0.67993. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68048/0.67990. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68057/0.67981. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67995/0.67977. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67973/0.67974. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67954/0.67969. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67926/0.67961. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67927/0.67955. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67893/0.67946. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67857/0.67934. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67841/0.67921. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67812/0.67909. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67780/0.67896. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67777/0.67883. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67739/0.67868. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67722/0.67851. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67675/0.67837. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67655/0.67821. Took 0.12 sec\n",
      "Epoch 51, Loss(train/val) 0.67627/0.67808. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67610/0.67785. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67565/0.67768. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67549/0.67747. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67533/0.67725. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67508/0.67706. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67452/0.67682. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67408/0.67669. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67379/0.67644. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67367/0.67630. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67304/0.67603. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67272/0.67582. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.67285/0.67556. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67236/0.67536. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67166/0.67518. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67128/0.67500. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67094/0.67479. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67086/0.67469. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.67015/0.67447. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67005/0.67423. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66946/0.67408. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66915/0.67387. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66853/0.67378. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66828/0.67372. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66766/0.67358. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66773/0.67352. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66680/0.67336. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66666/0.67335. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66623/0.67332. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66591/0.67323. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66569/0.67322. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66475/0.67320. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.66474/0.67325. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66423/0.67333. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66358/0.67333. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66315/0.67332. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66295/0.67340. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66281/0.67341. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66218/0.67348. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.66152/0.67355. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66150/0.67369. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.66083/0.67377. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66052/0.67382. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66010/0.67388. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65976/0.67404. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65881/0.67406. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65855/0.67413. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65826/0.67434. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65776/0.67442. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69629/0.69468. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69498/0.69394. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69342/0.69343. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69219/0.69316. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69146/0.69309. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69063/0.69312. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.69316. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69014/0.69317. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68986/0.69315. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68958/0.69312. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68965/0.69301. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68924/0.69291. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68899/0.69279. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68878/0.69265. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68842/0.69253. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68840/0.69238. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68827/0.69222. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68789/0.69204. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68777/0.69184. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68745/0.69164. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68717/0.69145. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68700/0.69125. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68637/0.69101. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68652/0.69078. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68602/0.69053. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68589/0.69024. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68590/0.68995. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68536/0.68963. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68497/0.68929. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68424/0.68897. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68399/0.68862. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68389/0.68827. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68369/0.68788. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68297/0.68749. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68263/0.68707. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68241/0.68667. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68172/0.68624. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68124/0.68579. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68095/0.68534. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.67999/0.68490. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67967/0.68452. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67908/0.68413. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67875/0.68368. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67829/0.68327. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67806/0.68288. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67723/0.68251. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67677/0.68212. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67644/0.68170. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67591/0.68134. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67532/0.68099. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67496/0.68061. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67445/0.68026. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67388/0.67993. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67370/0.67958. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67321/0.67922. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67230/0.67889. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67182/0.67854. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67151/0.67821. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67160/0.67791. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67067/0.67759. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66933/0.67719. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66935/0.67682. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66868/0.67648. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66889/0.67620. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66761/0.67578. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66691/0.67548. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66657/0.67523. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66643/0.67489. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66567/0.67454. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66493/0.67419. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66430/0.67385. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66349/0.67358. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66252/0.67318. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66227/0.67291. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66155/0.67258. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66091/0.67222. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66046/0.67190. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65994/0.67167. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65820/0.67128. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65807/0.67101. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65717/0.67064. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65629/0.67045. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65600/0.67013. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65445/0.66989. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65445/0.66953. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65403/0.66939. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65271/0.66932. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.65195/0.66912. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65080/0.66910. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65004/0.66909. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64957/0.66894. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64882/0.66886. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64832/0.66890. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64710/0.66892. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64608/0.66903. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64473/0.66888. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64496/0.66913. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64394/0.66934. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64333/0.66938. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64174/0.66944. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69612/0.69580. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69477/0.69600. Took 0.14 sec\n",
      "Epoch 2, Loss(train/val) 0.69375/0.69640. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.69698. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69201/0.69774. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69146/0.69866. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69065/0.69963. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68994/0.70053. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68964/0.70130. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.70187. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68952/0.70224. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68880/0.70255. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68866/0.70277. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68814/0.70291. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68789/0.70300. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68758/0.70308. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68724/0.70314. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68712/0.70318. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68702/0.70321. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68677/0.70326. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68648/0.70328. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68612/0.70332. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68594/0.70335. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.70334. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68506/0.70345. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68466/0.70355. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68438/0.70355. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68384/0.70361. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68368/0.70367. Took 0.08 sec\n",
      "Epoch 29, Loss(train/val) 0.68328/0.70379. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68257/0.70392. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68208/0.70407. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68200/0.70421. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68163/0.70425. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68109/0.70433. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68079/0.70450. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68042/0.70464. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68006/0.70487. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67985/0.70507. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67957/0.70517. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67897/0.70527. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67818/0.70539. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67784/0.70553. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67791/0.70565. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67680/0.70586. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67694/0.70604. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67610/0.70607. Took 0.08 sec\n",
      "Epoch 47, Loss(train/val) 0.67568/0.70623. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67572/0.70639. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67470/0.70644. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67455/0.70670. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67430/0.70691. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67337/0.70699. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67305/0.70717. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67251/0.70750. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67223/0.70762. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67156/0.70770. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67120/0.70802. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67063/0.70826. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66995/0.70858. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66944/0.70894. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66860/0.70941. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66825/0.70947. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66770/0.70978. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66693/0.71020. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66678/0.71052. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66548/0.71105. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66461/0.71147. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66450/0.71185. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66322/0.71231. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66307/0.71271. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66221/0.71325. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66129/0.71369. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66084/0.71406. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65965/0.71462. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.65902/0.71504. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65755/0.71539. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65750/0.71605. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65607/0.71666. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65589/0.71707. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65512/0.71763. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65338/0.71782. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65273/0.71819. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65171/0.71876. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.65051/0.71954. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65020/0.71989. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64880/0.72073. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64767/0.72121. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64703/0.72183. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64586/0.72259. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64498/0.72287. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64334/0.72324. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64255/0.72419. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64150/0.72435. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64054/0.72583. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63911/0.72591. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63729/0.72651. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63683/0.72715. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63549/0.72774. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63446/0.72845. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69732/0.69589. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69474/0.69554. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69329/0.69572. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69246/0.69610. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.69651. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69081/0.69685. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69055/0.69715. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69042/0.69742. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68992/0.69765. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68943/0.69790. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68927/0.69814. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.69840. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68852/0.69867. Took 0.08 sec\n",
      "Epoch 13, Loss(train/val) 0.68847/0.69897. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68759/0.69930. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68767/0.69963. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68702/0.69998. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68681/0.70035. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68640/0.70075. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68599/0.70116. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68575/0.70155. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68565/0.70201. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68485/0.70249. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68454/0.70297. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68435/0.70343. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68384/0.70391. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68340/0.70443. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68339/0.70496. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68306/0.70546. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68271/0.70590. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68212/0.70640. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68193/0.70687. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68136/0.70736. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68108/0.70779. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68118/0.70822. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68040/0.70863. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68012/0.70904. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67974/0.70944. Took 0.08 sec\n",
      "Epoch 38, Loss(train/val) 0.67950/0.70984. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67905/0.71025. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67878/0.71063. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67854/0.71098. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67809/0.71130. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67797/0.71161. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67737/0.71193. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67732/0.71217. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67676/0.71248. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67665/0.71273. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67597/0.71308. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67562/0.71339. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67538/0.71364. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67480/0.71394. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67429/0.71427. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67417/0.71457. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67328/0.71487. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67296/0.71510. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67222/0.71538. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67206/0.71568. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67138/0.71599. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67105/0.71630. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67073/0.71662. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67005/0.71689. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66964/0.71719. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66855/0.71754. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66897/0.71779. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66779/0.71811. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66747/0.71844. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66614/0.71885. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66626/0.71912. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66580/0.71945. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66513/0.71976. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66407/0.72006. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.66361/0.72047. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66366/0.72079. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66266/0.72114. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66136/0.72158. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66147/0.72192. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66065/0.72227. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66073/0.72270. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65952/0.72308. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65858/0.72355. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.65815/0.72398. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65751/0.72440. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65721/0.72484. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65615/0.72515. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65508/0.72562. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65488/0.72615. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65402/0.72656. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65353/0.72706. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65287/0.72737. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65275/0.72783. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65136/0.72844. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65090/0.72867. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65038/0.72911. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64968/0.72973. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64905/0.73032. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64819/0.73077. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64806/0.73114. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64710/0.73173. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64688/0.73221. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69318/0.69355. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69304/0.69348. Took 0.13 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69340. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69292/0.69332. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69293/0.69325. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69290/0.69318. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69277/0.69310. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69279/0.69302. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69258/0.69293. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69248/0.69283. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69225/0.69274. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69226/0.69263. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69214/0.69250. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69203/0.69237. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69196/0.69224. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69170/0.69208. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69152/0.69192. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69145/0.69177. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69118/0.69160. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69110/0.69144. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69059/0.69123. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69037/0.69101. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69009/0.69077. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68971/0.69054. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68914/0.69027. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68875/0.69002. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68804/0.68978. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68741/0.68959. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68663/0.68945. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68605/0.68940. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68526/0.68951. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68418/0.68971. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68322/0.69011. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68232/0.69048. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68105/0.69107. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67994/0.69191. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67882/0.69309. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67779/0.69373. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67659/0.69514. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67527/0.69691. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67408/0.69802. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67276/0.69989. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67143/0.70116. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67043/0.70329. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66933/0.70498. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.66804/0.70700. Took 0.08 sec\n",
      "Epoch 46, Loss(train/val) 0.66662/0.70924. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.66520/0.71172. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66410/0.71375. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66275/0.71578. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66165/0.71784. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66060/0.72017. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65913/0.72300. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65736/0.72496. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65637/0.72663. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.65497/0.72900. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65445/0.73190. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.65273/0.73419. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65112/0.73487. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65089/0.73716. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64898/0.73987. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.64758/0.74051. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64653/0.74263. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.64598/0.74432. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.64420/0.74593. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64320/0.74811. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.64190/0.75024. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64017/0.75091. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63986/0.75250. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.63819/0.75427. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63657/0.75581. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63634/0.75786. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.63486/0.75910. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.63361/0.76114. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63140/0.76332. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63083/0.76438. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63113/0.76579. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62846/0.76711. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.62749/0.76911. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62635/0.77119. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62469/0.77317. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.62421/0.77477. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62278/0.77716. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.62205/0.77850. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.62032/0.77980. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61838/0.78177. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61750/0.78448. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.61595/0.78552. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61518/0.78710. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.61422/0.78921. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.61180/0.79024. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61061/0.79211. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60870/0.79395. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.60924/0.79491. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.60621/0.79524. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.60532/0.79716. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.60414/0.79846. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60356/0.79837. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60078/0.80123. Took 0.14 sec\n",
      "Epoch 99, Loss(train/val) 0.60075/0.80165. Took 0.11 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69744/0.70289. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69651/0.70247. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69591/0.70208. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69500/0.70173. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69493/0.70144. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69392/0.70116. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69345/0.70093. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69248/0.70061. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69194/0.70028. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69138/0.70003. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.69969. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69013/0.69941. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68945/0.69912. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68881/0.69881. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68812/0.69859. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68773/0.69844. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68730/0.69833. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68652/0.69814. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68653/0.69798. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68580/0.69790. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68524/0.69790. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68487/0.69795. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68423/0.69785. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68435/0.69796. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68382/0.69801. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68343/0.69802. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68298/0.69806. Took 0.08 sec\n",
      "Epoch 27, Loss(train/val) 0.68266/0.69814. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68258/0.69820. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68180/0.69821. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68154/0.69817. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68114/0.69822. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68061/0.69831. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68033/0.69858. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67941/0.69855. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67973/0.69855. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67893/0.69881. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67870/0.69879. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67833/0.69884. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67802/0.69888. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67752/0.69921. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67732/0.69924. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.67621/0.69940. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67647/0.69973. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67590/0.69983. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67540/0.69985. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67507/0.70011. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67514/0.70046. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67383/0.70029. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67346/0.70057. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67286/0.70050. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67259/0.70072. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67247/0.70110. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67157/0.70112. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67093/0.70168. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67088/0.70166. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66994/0.70220. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66973/0.70230. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66948/0.70241. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66900/0.70308. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66858/0.70326. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.66825/0.70379. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66778/0.70368. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66727/0.70431. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66662/0.70464. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66623/0.70481. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66600/0.70562. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66495/0.70593. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66445/0.70596. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66421/0.70640. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66364/0.70671. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66318/0.70667. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66307/0.70776. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.66228/0.70792. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66163/0.70864. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66149/0.70867. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66137/0.70928. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66059/0.70946. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65998/0.70987. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.65883/0.71036. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65844/0.71076. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65708/0.71121. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.65772/0.71129. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65682/0.71158. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65571/0.71186. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.65570/0.71216. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65535/0.71290. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65417/0.71281. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65400/0.71334. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65301/0.71357. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65266/0.71311. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65215/0.71382. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65146/0.71377. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65067/0.71468. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65030/0.71466. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64976/0.71425. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64866/0.71542. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.64730/0.71518. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64742/0.71579. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64714/0.71540. Took 0.10 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69482/0.69092. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69443/0.69103. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69394/0.69120. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69367/0.69144. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69312/0.69176. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69283/0.69218. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69228/0.69265. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69195/0.69310. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69167/0.69346. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69114/0.69374. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69088/0.69388. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69087/0.69397. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69043/0.69393. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69029/0.69381. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68983/0.69359. Took 0.08 sec\n",
      "Epoch 15, Loss(train/val) 0.68954/0.69335. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68949/0.69304. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68896/0.69273. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68873/0.69245. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68847/0.69205. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68808/0.69159. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68767/0.69121. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68748/0.69085. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68701/0.69047. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68684/0.69005. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68661/0.68971. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68621/0.68930. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68567/0.68893. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68549/0.68867. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68532/0.68854. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68493/0.68823. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68505/0.68790. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68420/0.68767. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68414/0.68734. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68404/0.68715. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68357/0.68704. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68320/0.68682. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68325/0.68660. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68296/0.68639. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68259/0.68628. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68234/0.68618. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68210/0.68606. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68166/0.68583. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68151/0.68562. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68131/0.68547. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68079/0.68525. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68057/0.68512. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.68050/0.68506. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67976/0.68482. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.68031/0.68481. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67960/0.68470. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67900/0.68461. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67894/0.68459. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67856/0.68449. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67812/0.68440. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67824/0.68438. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67725/0.68422. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67761/0.68404. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67683/0.68407. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67675/0.68414. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67605/0.68406. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67571/0.68384. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67535/0.68383. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67547/0.68391. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67483/0.68377. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67401/0.68362. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67383/0.68357. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67353/0.68347. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.67310/0.68358. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.67241/0.68332. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67202/0.68340. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.67195/0.68342. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.67132/0.68315. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67083/0.68322. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67031/0.68330. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66946/0.68312. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66945/0.68319. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66856/0.68327. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.66814/0.68311. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66770/0.68311. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66748/0.68323. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66611/0.68291. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66598/0.68291. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66531/0.68305. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66537/0.68292. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66443/0.68301. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66354/0.68319. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.66316/0.68304. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.66280/0.68298. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.66161/0.68288. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.66154/0.68288. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.66109/0.68297. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65990/0.68318. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65962/0.68320. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65946/0.68296. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65776/0.68281. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65808/0.68313. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65708/0.68293. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65693/0.68307. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.65586/0.68351. Took 0.10 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69610/0.69827. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69525/0.69745. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69456/0.69665. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69347/0.69591. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69274/0.69527. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69202/0.69480. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69204/0.69449. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69135/0.69428. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69142/0.69414. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69090/0.69405. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.69396. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69050/0.69389. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69060/0.69381. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68959/0.69374. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68971/0.69368. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.68966/0.69360. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68927/0.69353. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68902/0.69346. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68858/0.69340. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68848/0.69333. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68809/0.69325. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68793/0.69319. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68726/0.69311. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68705/0.69304. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68666/0.69297. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68643/0.69289. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68618/0.69281. Took 0.11 sec\n",
      "Epoch 27, Loss(train/val) 0.68584/0.69272. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68519/0.69265. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68486/0.69257. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68479/0.69247. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68454/0.69237. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68401/0.69228. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68370/0.69217. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68297/0.69205. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68296/0.69191. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68243/0.69178. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68205/0.69167. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68154/0.69156. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.68140/0.69143. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68119/0.69128. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68042/0.69110. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68056/0.69093. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67942/0.69078. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67954/0.69062. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67894/0.69049. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67852/0.69031. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67820/0.69016. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67779/0.69003. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67762/0.68988. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67724/0.68973. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67677/0.68955. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67573/0.68934. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.67590/0.68917. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67511/0.68899. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67505/0.68880. Took 0.13 sec\n",
      "Epoch 56, Loss(train/val) 0.67521/0.68865. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67429/0.68848. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67380/0.68831. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.67324/0.68819. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67294/0.68807. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67222/0.68798. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67212/0.68788. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67187/0.68773. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.67128/0.68761. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.67114/0.68748. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66994/0.68739. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67029/0.68730. Took 0.12 sec\n",
      "Epoch 68, Loss(train/val) 0.66927/0.68718. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66835/0.68712. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66867/0.68709. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66793/0.68702. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66752/0.68703. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66779/0.68699. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.66656/0.68697. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66662/0.68691. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66634/0.68689. Took 0.11 sec\n",
      "Epoch 77, Loss(train/val) 0.66519/0.68690. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66495/0.68685. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66451/0.68688. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.66394/0.68685. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66357/0.68684. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.66240/0.68690. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.66189/0.68685. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66156/0.68690. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66135/0.68697. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.66019/0.68694. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65995/0.68705. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65992/0.68709. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65891/0.68724. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65823/0.68738. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65675/0.68755. Took 0.12 sec\n",
      "Epoch 92, Loss(train/val) 0.65720/0.68768. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65652/0.68781. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65559/0.68797. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.65451/0.68826. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.65479/0.68830. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65413/0.68858. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65413/0.68871. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65327/0.68900. Took 0.09 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69406/0.69198. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69402/0.69217. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69372/0.69237. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69392/0.69258. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69375/0.69280. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69313/0.69306. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69315/0.69334. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69283/0.69365. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69261/0.69399. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69215/0.69439. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69203/0.69487. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69175/0.69536. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69151/0.69590. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69120/0.69649. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69057/0.69711. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69046/0.69771. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68997/0.69826. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68963/0.69880. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68926/0.69932. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68862/0.69987. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68844/0.70046. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68778/0.70104. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68730/0.70166. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68653/0.70227. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68630/0.70293. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68562/0.70364. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68485/0.70435. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68442/0.70508. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68372/0.70585. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68327/0.70657. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68245/0.70725. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68178/0.70797. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68089/0.70866. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68034/0.70935. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67976/0.70996. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67888/0.71061. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67825/0.71122. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67812/0.71185. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67729/0.71248. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67646/0.71299. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67548/0.71364. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67507/0.71425. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67419/0.71483. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67340/0.71545. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67282/0.71606. Took 0.12 sec\n",
      "Epoch 45, Loss(train/val) 0.67167/0.71666. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67085/0.71732. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67096/0.71791. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66985/0.71854. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66925/0.71920. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66853/0.71980. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66745/0.72051. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66723/0.72126. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66659/0.72198. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66556/0.72262. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66483/0.72330. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66335/0.72414. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.66274/0.72486. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66218/0.72549. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66112/0.72623. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66021/0.72686. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65989/0.72758. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65910/0.72843. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.65824/0.72905. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65722/0.72989. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65666/0.73056. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65604/0.73103. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65533/0.73192. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65389/0.73298. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65353/0.73335. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65207/0.73444. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65187/0.73518. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.65149/0.73606. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65024/0.73661. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64962/0.73748. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.64864/0.73813. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64785/0.73898. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64710/0.73999. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64561/0.74086. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64606/0.74151. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64482/0.74238. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64458/0.74300. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64302/0.74418. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64187/0.74533. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64093/0.74627. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64081/0.74693. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64063/0.74771. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.63942/0.74887. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63862/0.74989. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63901/0.75063. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63711/0.75131. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63651/0.75229. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63598/0.75320. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63506/0.75380. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63500/0.75468. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63330/0.75558. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63278/0.75625. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63177/0.75707. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63106/0.75742. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.63056/0.75857. Took 0.09 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69517. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69297/0.69506. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.69491. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.69479. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69260/0.69468. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69235/0.69453. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69439. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69246/0.69424. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69222/0.69402. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69186/0.69380. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69187/0.69352. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69134/0.69323. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69097/0.69289. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69072/0.69253. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69079/0.69220. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69034/0.69189. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68982/0.69161. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68961/0.69145. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68891/0.69128. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68864/0.69119. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68807/0.69119. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68749/0.69110. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68733/0.69120. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68678/0.69124. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68604/0.69134. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68556/0.69141. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68511/0.69112. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68471/0.69118. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68436/0.69135. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68370/0.69127. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68316/0.69117. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68275/0.69083. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68219/0.69066. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68215/0.69053. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68153/0.69042. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68101/0.69022. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68069/0.69006. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68002/0.68978. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67977/0.69003. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67939/0.68974. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67925/0.68949. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67847/0.68955. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67803/0.68931. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67795/0.68890. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67712/0.68896. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67696/0.68920. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67650/0.68880. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67618/0.68879. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67576/0.68864. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67544/0.68875. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67468/0.68863. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67433/0.68893. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67425/0.68861. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67404/0.68858. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.67339/0.68877. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67280/0.68835. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.67208/0.68859. Took 0.11 sec\n",
      "Epoch 57, Loss(train/val) 0.67180/0.68896. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67127/0.68910. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67089/0.68946. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67028/0.68926. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66976/0.68950. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66952/0.68946. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66855/0.68964. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66812/0.68989. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66770/0.69027. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66686/0.69057. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66662/0.69082. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66591/0.69110. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.66567/0.69114. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66470/0.69144. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66412/0.69170. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66391/0.69245. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66298/0.69276. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66213/0.69341. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66203/0.69358. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66106/0.69382. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66013/0.69414. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66022/0.69517. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65900/0.69533. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65772/0.69573. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.65704/0.69634. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65724/0.69629. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65593/0.69744. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65521/0.69786. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65511/0.69844. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65379/0.69918. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65364/0.69983. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65275/0.69983. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65161/0.70101. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.65086/0.70114. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65009/0.70208. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64851/0.70282. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.64874/0.70334. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64747/0.70434. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64703/0.70440. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.64592/0.70542. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64550/0.70649. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64423/0.70721. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.64309/0.70674. Took 0.10 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69557/0.70194. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69522/0.70133. Took 0.10 sec\n",
      "Epoch 2, Loss(train/val) 0.69443/0.70054. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69378/0.69954. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69308/0.69835. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69709. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69157/0.69588. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69085/0.69482. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69030/0.69400. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68980/0.69345. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68918/0.69315. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68907/0.69295. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68875/0.69288. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68822/0.69293. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68769/0.69297. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68746/0.69310. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68708/0.69326. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68677/0.69336. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68648/0.69353. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68571/0.69366. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68526/0.69380. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68486/0.69403. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68456/0.69425. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68417/0.69435. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68345/0.69448. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68311/0.69472. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68247/0.69487. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68199/0.69497. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68128/0.69516. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68074/0.69534. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68016/0.69543. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67985/0.69561. Took 0.08 sec\n",
      "Epoch 32, Loss(train/val) 0.67924/0.69570. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67872/0.69586. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67800/0.69595. Took 0.10 sec\n",
      "Epoch 35, Loss(train/val) 0.67757/0.69610. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67686/0.69624. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67619/0.69630. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67588/0.69645. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67485/0.69657. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67442/0.69689. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67351/0.69696. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67308/0.69711. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67252/0.69710. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67190/0.69721. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67156/0.69728. Took 0.12 sec\n",
      "Epoch 46, Loss(train/val) 0.67058/0.69751. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66981/0.69767. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66908/0.69772. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.66831/0.69786. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66802/0.69798. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66714/0.69807. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66732/0.69806. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66613/0.69838. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66583/0.69844. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66531/0.69841. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66482/0.69862. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66425/0.69875. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66325/0.69921. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66314/0.69924. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66286/0.69959. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66195/0.69940. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66160/0.69974. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66122/0.69991. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66005/0.70036. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66048/0.70054. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66001/0.70043. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65865/0.70068. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65860/0.70131. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65748/0.70106. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65800/0.70118. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65671/0.70154. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65631/0.70170. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65610/0.70200. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.65607/0.70202. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65468/0.70213. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65490/0.70257. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.65403/0.70265. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.65361/0.70263. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.65271/0.70303. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65196/0.70296. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65214/0.70308. Took 0.12 sec\n",
      "Epoch 82, Loss(train/val) 0.65141/0.70357. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65119/0.70372. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65031/0.70365. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64972/0.70370. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64974/0.70421. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64826/0.70441. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64738/0.70431. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64790/0.70482. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64676/0.70525. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64614/0.70489. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64615/0.70490. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64576/0.70508. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64531/0.70503. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64368/0.70584. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64318/0.70618. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64335/0.70633. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.64181/0.70575. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64152/0.70632. Took 0.09 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69142/0.69503. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69128/0.69516. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69093/0.69531. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69060/0.69546. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69038/0.69561. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69017/0.69578. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68985/0.69596. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68944/0.69618. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68908/0.69640. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68861/0.69663. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68788/0.69690. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68747/0.69721. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.69758. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68622/0.69796. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68550/0.69847. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68488/0.69906. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68399/0.69975. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68340/0.70054. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68268/0.70142. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68199/0.70227. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68094/0.70330. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.67997/0.70442. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.67906/0.70541. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.67797/0.70659. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.67687/0.70784. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.67580/0.70901. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67449/0.71028. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67345/0.71164. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67251/0.71297. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67111/0.71430. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.66996/0.71582. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.66877/0.71724. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.66768/0.71908. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.66623/0.72060. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.66514/0.72239. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.66382/0.72406. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.66253/0.72604. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.66134/0.72774. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.65989/0.72954. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.65873/0.73128. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.65761/0.73319. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.65590/0.73470. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.65512/0.73654. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.65340/0.73831. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.65291/0.73987. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.65186/0.74125. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.65088/0.74280. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.64946/0.74446. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.64831/0.74577. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.64777/0.74740. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.64665/0.74876. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.64511/0.75010. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.64403/0.75121. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.64336/0.75256. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.64298/0.75400. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.64172/0.75489. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.64008/0.75616. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.63973/0.75702. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.63915/0.75878. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.63827/0.75910. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.63693/0.76051. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.63518/0.76143. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.63465/0.76286. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.63401/0.76351. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.63307/0.76453. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.63181/0.76575. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.63075/0.76684. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.63002/0.76842. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.62911/0.76934. Took 0.11 sec\n",
      "Epoch 69, Loss(train/val) 0.62808/0.77072. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.62740/0.77208. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.62597/0.77301. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.62471/0.77461. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.62426/0.77557. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62316/0.77686. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62242/0.77882. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62195/0.77985. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62061/0.78095. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61920/0.78288. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61853/0.78440. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61752/0.78603. Took 0.14 sec\n",
      "Epoch 81, Loss(train/val) 0.61736/0.78766. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61665/0.78944. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.61482/0.79072. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.61336/0.79220. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.61267/0.79421. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.61170/0.79590. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61035/0.79761. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60983/0.79971. Took 0.11 sec\n",
      "Epoch 89, Loss(train/val) 0.60939/0.80132. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60810/0.80340. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.60748/0.80468. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.60662/0.80691. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.60529/0.80865. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.60439/0.81013. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.60231/0.81215. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.60134/0.81373. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.60004/0.81511. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.59915/0.81677. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.59826/0.81820. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69862/0.69920. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69725/0.69830. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69596/0.69721. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69391/0.69606. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69234/0.69519. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69069/0.69483. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68942/0.69479. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68911/0.69487. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68883/0.69497. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68823/0.69500. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68762/0.69504. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68740/0.69504. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68705/0.69505. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68682/0.69506. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68618/0.69504. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68577/0.69499. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68525/0.69495. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68468/0.69493. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68439/0.69491. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68380/0.69488. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68349/0.69484. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68279/0.69480. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68226/0.69476. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68176/0.69472. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68110/0.69469. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68046/0.69463. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.67994/0.69459. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67956/0.69457. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.67887/0.69452. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67831/0.69450. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67776/0.69447. Took 0.12 sec\n",
      "Epoch 31, Loss(train/val) 0.67754/0.69452. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67665/0.69454. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67611/0.69453. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67534/0.69461. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67506/0.69470. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67440/0.69479. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67397/0.69497. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67350/0.69509. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67274/0.69525. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67239/0.69542. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67162/0.69560. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67122/0.69584. Took 0.11 sec\n",
      "Epoch 43, Loss(train/val) 0.67096/0.69609. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67043/0.69634. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66963/0.69651. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66929/0.69680. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66804/0.69709. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66755/0.69737. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66750/0.69778. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66670/0.69818. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66621/0.69863. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66553/0.69901. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66531/0.69942. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66475/0.69988. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66402/0.70036. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66347/0.70083. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66322/0.70126. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66245/0.70179. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66165/0.70227. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66130/0.70290. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66075/0.70352. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66031/0.70395. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.65935/0.70463. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.65888/0.70516. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65818/0.70580. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65825/0.70654. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65694/0.70721. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65625/0.70799. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65598/0.70869. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65543/0.70947. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65525/0.71026. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65380/0.71090. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.65386/0.71159. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65269/0.71240. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65202/0.71331. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65182/0.71409. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65091/0.71508. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65015/0.71597. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.64975/0.71697. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64958/0.71775. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64917/0.71858. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64859/0.71941. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64749/0.72018. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64693/0.72097. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64667/0.72178. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64524/0.72274. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.64468/0.72376. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64442/0.72468. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64473/0.72570. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.64302/0.72659. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64285/0.72748. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64194/0.72839. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64121/0.72924. Took 0.12 sec\n",
      "Epoch 94, Loss(train/val) 0.64077/0.73013. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64081/0.73094. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63944/0.73185. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63911/0.73283. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.63862/0.73352. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63836/0.73441. Took 0.11 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69673/0.69141. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69596/0.69187. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69500/0.69239. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69429/0.69316. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69343/0.69437. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69207/0.69612. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69072/0.69821. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69007/0.70010. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68930/0.70155. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68902/0.70251. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68871/0.70315. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.70356. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.68826/0.70375. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68786/0.70399. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68761/0.70410. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68756/0.70420. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.70426. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68682/0.70442. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68665/0.70457. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68632/0.70472. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68604/0.70480. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68549/0.70490. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68535/0.70505. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68486/0.70519. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68458/0.70546. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68418/0.70564. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68378/0.70587. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68332/0.70607. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68309/0.70625. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68301/0.70651. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68259/0.70681. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68226/0.70706. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68203/0.70734. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68165/0.70760. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.68143/0.70788. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68104/0.70798. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68103/0.70825. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68038/0.70839. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68023/0.70868. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68020/0.70909. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67926/0.70927. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67933/0.70942. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67881/0.70983. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67860/0.71008. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67855/0.71023. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67817/0.71050. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67734/0.71085. Took 0.12 sec\n",
      "Epoch 47, Loss(train/val) 0.67755/0.71109. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67663/0.71147. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67664/0.71194. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67628/0.71223. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67585/0.71271. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67567/0.71334. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67488/0.71349. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67467/0.71402. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67404/0.71465. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67379/0.71524. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67314/0.71573. Took 0.12 sec\n",
      "Epoch 58, Loss(train/val) 0.67303/0.71611. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67213/0.71660. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67231/0.71744. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67122/0.71795. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67076/0.71859. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67059/0.71941. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.66959/0.71998. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66937/0.72094. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66924/0.72170. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66836/0.72246. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66740/0.72302. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66676/0.72395. Took 0.12 sec\n",
      "Epoch 70, Loss(train/val) 0.66629/0.72476. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66594/0.72568. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66511/0.72651. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.66483/0.72763. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66360/0.72827. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66336/0.72906. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66258/0.73049. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66201/0.73132. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66082/0.73216. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.66053/0.73350. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65915/0.73414. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65857/0.73513. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65799/0.73634. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.65701/0.73740. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65627/0.73878. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65479/0.73990. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65437/0.74120. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65369/0.74200. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.65252/0.74331. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65136/0.74374. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65081/0.74537. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.65033/0.74676. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64943/0.74811. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.64748/0.74860. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64707/0.75051. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64567/0.75164. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64556/0.75310. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.64416/0.75446. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64304/0.75598. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64165/0.75739. Took 0.11 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69610/0.69586. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69512/0.69473. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69395/0.69358. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69317/0.69241. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69238/0.69140. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69198/0.69070. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.69027. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69076/0.69008. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69073/0.69001. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69026/0.69000. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.69002. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68985/0.69007. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68984/0.69014. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68961/0.69019. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68933/0.69026. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68926/0.69032. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68874/0.69040. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68863/0.69045. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68843/0.69049. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68803/0.69055. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68781/0.69063. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68727/0.69070. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68735/0.69075. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68698/0.69082. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68639/0.69090. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68617/0.69094. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68571/0.69098. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68524/0.69103. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68468/0.69113. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68451/0.69123. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68401/0.69125. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68359/0.69132. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68326/0.69140. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68214/0.69155. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68189/0.69172. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68102/0.69190. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68033/0.69219. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67979/0.69250. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67925/0.69289. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67800/0.69336. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67736/0.69392. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67672/0.69461. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67523/0.69534. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67478/0.69625. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67397/0.69704. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67294/0.69808. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67218/0.69918. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67117/0.70032. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67031/0.70155. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66940/0.70288. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66852/0.70421. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66759/0.70559. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66632/0.70716. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66597/0.70863. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66479/0.71017. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66385/0.71156. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66294/0.71322. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66150/0.71478. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66106/0.71636. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66066/0.71780. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65940/0.71970. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65864/0.72135. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65767/0.72285. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65729/0.72424. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65621/0.72582. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65531/0.72722. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65515/0.72849. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65270/0.72993. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65302/0.73165. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65194/0.73353. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65167/0.73515. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65006/0.73690. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.64967/0.73839. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64853/0.74005. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64787/0.74172. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64747/0.74329. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64686/0.74506. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64495/0.74649. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64450/0.74818. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64417/0.74953. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64286/0.75091. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64220/0.75253. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64115/0.75408. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64103/0.75592. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63986/0.75777. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63919/0.75951. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63718/0.76114. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.63728/0.76323. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63693/0.76410. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63592/0.76635. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63446/0.76827. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63360/0.76961. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63353/0.77160. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63271/0.77325. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63229/0.77523. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63091/0.77686. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63007/0.77819. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62837/0.78027. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62765/0.78175. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.62670/0.78383. Took 0.09 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70279/0.71179. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69970/0.70755. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69709/0.70328. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69467/0.69922. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69262/0.69606. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69182/0.69403. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69124/0.69293. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69066/0.69235. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69079/0.69203. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69053/0.69194. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.69033/0.69195. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69012/0.69202. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68999/0.69206. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68992/0.69217. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68975/0.69229. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68949/0.69240. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68909/0.69249. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68917/0.69262. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68896/0.69273. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68837/0.69285. Took 0.10 sec\n",
      "Epoch 20, Loss(train/val) 0.68867/0.69300. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68820/0.69312. Took 0.12 sec\n",
      "Epoch 22, Loss(train/val) 0.68766/0.69324. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68777/0.69339. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68770/0.69361. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68722/0.69381. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68713/0.69399. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68679/0.69422. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68639/0.69443. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68593/0.69456. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68581/0.69486. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68555/0.69504. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68484/0.69530. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68456/0.69549. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.68395/0.69563. Took 0.13 sec\n",
      "Epoch 35, Loss(train/val) 0.68344/0.69587. Took 0.17 sec\n",
      "Epoch 36, Loss(train/val) 0.68302/0.69620. Took 0.14 sec\n",
      "Epoch 37, Loss(train/val) 0.68251/0.69644. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68233/0.69689. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68147/0.69713. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.68083/0.69738. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68017/0.69772. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67943/0.69806. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67880/0.69846. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67840/0.69862. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67747/0.69912. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67692/0.69957. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67550/0.69990. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67508/0.70036. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67408/0.70102. Took 0.12 sec\n",
      "Epoch 50, Loss(train/val) 0.67332/0.70146. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67221/0.70179. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67117/0.70256. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67015/0.70332. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66918/0.70388. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66877/0.70464. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66761/0.70600. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66651/0.70639. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66540/0.70735. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.66413/0.70830. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66347/0.70954. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66224/0.71041. Took 0.11 sec\n",
      "Epoch 62, Loss(train/val) 0.66109/0.71180. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66010/0.71277. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65932/0.71411. Took 0.11 sec\n",
      "Epoch 65, Loss(train/val) 0.65802/0.71546. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65640/0.71749. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65601/0.71892. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.65467/0.71991. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65344/0.72145. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65208/0.72336. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65157/0.72486. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65035/0.72657. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65054/0.72828. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.64836/0.73005. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64818/0.73143. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64730/0.73325. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64673/0.73525. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64520/0.73700. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64489/0.73916. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64417/0.73982. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.64403/0.74177. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64286/0.74309. Took 0.11 sec\n",
      "Epoch 83, Loss(train/val) 0.64127/0.74525. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64187/0.74634. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.64011/0.74827. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64020/0.74921. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63906/0.75067. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63871/0.75224. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63816/0.75353. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.63693/0.75573. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63629/0.75665. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63597/0.75893. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63472/0.76045. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63470/0.76043. Took 0.11 sec\n",
      "Epoch 95, Loss(train/val) 0.63433/0.76301. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63388/0.76389. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63188/0.76557. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.63231/0.76623. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63172/0.76788. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69090/0.68637. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69083/0.68646. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69049/0.68656. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69054/0.68669. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69028/0.68682. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.68698. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.69018/0.68712. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68983/0.68730. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68972/0.68751. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68964/0.68773. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68958/0.68798. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68915/0.68826. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68909/0.68857. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68879/0.68887. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68860/0.68924. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68844/0.68967. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68834/0.69014. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68837/0.69065. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68763/0.69114. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68782/0.69183. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68742/0.69247. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68725/0.69315. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68703/0.69386. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68640/0.69470. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68619/0.69548. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68580/0.69634. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68546/0.69716. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68536/0.69793. Took 0.10 sec\n",
      "Epoch 28, Loss(train/val) 0.68485/0.69872. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68467/0.69946. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68408/0.70024. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68348/0.70102. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68335/0.70180. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68307/0.70245. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68246/0.70297. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68192/0.70381. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68139/0.70440. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68085/0.70468. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68031/0.70523. Took 0.11 sec\n",
      "Epoch 39, Loss(train/val) 0.68002/0.70587. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67950/0.70626. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67893/0.70667. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67813/0.70683. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67752/0.70719. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67716/0.70724. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67642/0.70739. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67562/0.70756. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67432/0.70738. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67403/0.70746. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67324/0.70729. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67238/0.70702. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67131/0.70713. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67019/0.70675. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66970/0.70660. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66847/0.70633. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66761/0.70594. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66653/0.70586. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66600/0.70538. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66515/0.70488. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66392/0.70466. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66291/0.70419. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66204/0.70400. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66116/0.70378. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.65977/0.70348. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.65903/0.70348. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65793/0.70350. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65731/0.70370. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65605/0.70390. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65553/0.70379. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65445/0.70415. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65389/0.70448. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65255/0.70481. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.65176/0.70489. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65036/0.70531. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64968/0.70511. Took 0.12 sec\n",
      "Epoch 75, Loss(train/val) 0.64936/0.70550. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64760/0.70607. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64676/0.70620. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64646/0.70725. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64502/0.70782. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64410/0.70776. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64321/0.70804. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.64195/0.70854. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64093/0.70910. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63973/0.70972. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63984/0.71015. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63798/0.71090. Took 0.12 sec\n",
      "Epoch 87, Loss(train/val) 0.63831/0.71159. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63605/0.71195. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.63566/0.71260. Took 0.12 sec\n",
      "Epoch 90, Loss(train/val) 0.63505/0.71323. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63389/0.71345. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63239/0.71412. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63116/0.71503. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62969/0.71577. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.62960/0.71650. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62833/0.71718. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62771/0.71772. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62716/0.71850. Took 0.12 sec\n",
      "Epoch 99, Loss(train/val) 0.62598/0.71919. Took 0.09 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69218/0.69739. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69231/0.69733. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69727. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69169/0.69722. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69163/0.69719. Took 0.11 sec\n",
      "Epoch 5, Loss(train/val) 0.69125/0.69717. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69110/0.69717. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69084/0.69720. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69057/0.69728. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69033/0.69738. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69001/0.69753. Took 0.11 sec\n",
      "Epoch 11, Loss(train/val) 0.68968/0.69770. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68934/0.69785. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68902/0.69800. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68855/0.69817. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68819/0.69831. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68796/0.69843. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.68758/0.69851. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68700/0.69857. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68678/0.69859. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68624/0.69858. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68563/0.69854. Took 0.08 sec\n",
      "Epoch 22, Loss(train/val) 0.68527/0.69848. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68448/0.69832. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68401/0.69812. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68347/0.69793. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68255/0.69774. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68182/0.69755. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68102/0.69741. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68041/0.69723. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67951/0.69716. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67891/0.69715. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.67820/0.69706. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67773/0.69711. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67674/0.69708. Took 0.13 sec\n",
      "Epoch 35, Loss(train/val) 0.67661/0.69713. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67501/0.69719. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67487/0.69724. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67420/0.69724. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67358/0.69737. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67290/0.69732. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67183/0.69739. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67135/0.69739. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67053/0.69757. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66955/0.69763. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66899/0.69772. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.66829/0.69776. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.66724/0.69783. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66684/0.69785. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66568/0.69798. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66471/0.69799. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66400/0.69821. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66331/0.69825. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66211/0.69842. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66139/0.69859. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66040/0.69898. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.65937/0.69919. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.65859/0.69941. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.65740/0.69995. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65721/0.70013. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65553/0.70063. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.65406/0.70114. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65374/0.70164. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65246/0.70223. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65155/0.70272. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65082/0.70324. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64928/0.70392. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.64897/0.70459. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64773/0.70508. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.64674/0.70569. Took 0.12 sec\n",
      "Epoch 70, Loss(train/val) 0.64558/0.70624. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64497/0.70689. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64378/0.70729. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64330/0.70781. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64191/0.70861. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.64137/0.70909. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64053/0.70966. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.63892/0.71040. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.63843/0.71129. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.63717/0.71182. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.63688/0.71261. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63550/0.71320. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.63468/0.71371. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63417/0.71451. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63248/0.71465. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.63169/0.71532. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63041/0.71574. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62996/0.71658. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.62886/0.71716. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62801/0.71786. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.62673/0.71827. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62529/0.71859. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.62451/0.71873. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.62344/0.71990. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.62304/0.72023. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62122/0.72075. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.62062/0.72139. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61857/0.72214. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.61876/0.72249. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61713/0.72342. Took 0.09 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.68961/0.69544. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68939/0.69551. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68932/0.69557. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68900/0.69564. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68901/0.69573. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.68891/0.69582. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68855/0.69593. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68822/0.69608. Took 0.10 sec\n",
      "Epoch 8, Loss(train/val) 0.68823/0.69628. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68793/0.69652. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68757/0.69681. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68738/0.69714. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68706/0.69748. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68667/0.69785. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68623/0.69820. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68598/0.69850. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.68542/0.69879. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68479/0.69915. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68444/0.69951. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68381/0.69991. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68388/0.70028. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68303/0.70061. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68260/0.70104. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.70162. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68151/0.70219. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68079/0.70273. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68046/0.70328. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67987/0.70375. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67949/0.70424. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67875/0.70471. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67819/0.70516. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67758/0.70569. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67680/0.70602. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67667/0.70628. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67565/0.70664. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67530/0.70681. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67466/0.70705. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67393/0.70746. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67293/0.70749. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67233/0.70771. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67151/0.70782. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67090/0.70813. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67003/0.70823. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66937/0.70818. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66821/0.70824. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66768/0.70835. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66639/0.70842. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66520/0.70846. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66439/0.70873. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66296/0.70889. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66228/0.70885. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66079/0.70960. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.65982/0.70969. Took 0.11 sec\n",
      "Epoch 53, Loss(train/val) 0.65857/0.71031. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65762/0.71120. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.65684/0.71140. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65496/0.71231. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65350/0.71318. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65282/0.71402. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65114/0.71519. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.64972/0.71638. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.64905/0.71740. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64761/0.71820. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.64632/0.71932. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64558/0.72026. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64410/0.72159. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64335/0.72254. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64210/0.72414. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.64064/0.72599. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.64013/0.72716. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63893/0.72832. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63744/0.72971. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.63652/0.73108. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.63563/0.73308. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.63470/0.73425. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.63352/0.73544. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.63205/0.73688. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.63095/0.73873. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.63029/0.74099. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.62880/0.74235. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62836/0.74428. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.62701/0.74572. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.62618/0.74766. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.62511/0.74944. Took 0.13 sec\n",
      "Epoch 84, Loss(train/val) 0.62365/0.75069. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.62292/0.75321. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62246/0.75367. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.62175/0.75639. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.61972/0.75881. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.61826/0.76097. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.61810/0.76204. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.61695/0.76330. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.61546/0.76592. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.61416/0.76797. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61362/0.76950. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.61215/0.77163. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61101/0.77315. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61030/0.77537. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.60936/0.77771. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.60774/0.77923. Took 0.09 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69159/0.69193. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69035/0.69000. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68979/0.68867. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68937/0.68766. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.68907/0.68685. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68841/0.68617. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68838/0.68555. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.68796/0.68501. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68761/0.68449. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68726/0.68397. Took 0.10 sec\n",
      "Epoch 10, Loss(train/val) 0.68702/0.68344. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68673/0.68292. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68661/0.68240. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68606/0.68188. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68561/0.68134. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68545/0.68084. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68501/0.68030. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68475/0.67979. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68434/0.67926. Took 0.11 sec\n",
      "Epoch 19, Loss(train/val) 0.68398/0.67877. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68365/0.67828. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68295/0.67782. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68274/0.67733. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68255/0.67691. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68211/0.67651. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68169/0.67614. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68145/0.67586. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68125/0.67562. Took 0.11 sec\n",
      "Epoch 28, Loss(train/val) 0.68049/0.67534. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68046/0.67512. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68038/0.67488. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67990/0.67481. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67958/0.67473. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67924/0.67467. Took 0.11 sec\n",
      "Epoch 34, Loss(train/val) 0.67868/0.67464. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67845/0.67468. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67790/0.67476. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67781/0.67484. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67744/0.67488. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67698/0.67494. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67655/0.67496. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67624/0.67507. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67532/0.67519. Took 0.12 sec\n",
      "Epoch 43, Loss(train/val) 0.67523/0.67532. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67455/0.67558. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67400/0.67570. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67379/0.67588. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67298/0.67607. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67302/0.67620. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67189/0.67644. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67172/0.67665. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67105/0.67681. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67036/0.67702. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66996/0.67728. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66930/0.67761. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66836/0.67778. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66816/0.67804. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66710/0.67822. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66679/0.67853. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.66607/0.67878. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66552/0.67905. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66493/0.67934. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66372/0.67961. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66344/0.67982. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.66250/0.68007. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66232/0.68048. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.66184/0.68065. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.66076/0.68105. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66039/0.68118. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65998/0.68141. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.65898/0.68181. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65859/0.68208. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65746/0.68250. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.65713/0.68273. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65632/0.68306. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65580/0.68343. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.65523/0.68365. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65438/0.68401. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65403/0.68421. Took 0.11 sec\n",
      "Epoch 79, Loss(train/val) 0.65345/0.68470. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65260/0.68486. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65207/0.68523. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.65197/0.68571. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65060/0.68602. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65040/0.68635. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64930/0.68692. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64891/0.68683. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64850/0.68748. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.64759/0.68786. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64685/0.68811. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64664/0.68859. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.64623/0.68878. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64547/0.68924. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64444/0.68973. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.64364/0.68997. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.64346/0.69027. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.64271/0.69047. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.64245/0.69105. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.64094/0.69102. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64036/0.69162. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68957/0.69164. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68897/0.69242. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.68867/0.69313. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68807/0.69383. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68815/0.69442. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68784/0.69501. Took 0.10 sec\n",
      "Epoch 6, Loss(train/val) 0.68758/0.69545. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68741/0.69584. Took 0.08 sec\n",
      "Epoch 8, Loss(train/val) 0.68720/0.69614. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68703/0.69640. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68667/0.69660. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68667/0.69680. Took 0.12 sec\n",
      "Epoch 12, Loss(train/val) 0.68654/0.69696. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68633/0.69717. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68611/0.69736. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68619/0.69751. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68565/0.69769. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68533/0.69787. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68547/0.69801. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68505/0.69814. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68475/0.69835. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68456/0.69855. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68411/0.69877. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68393/0.69899. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68374/0.69913. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68335/0.69924. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68313/0.69950. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68281/0.69983. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68259/0.70007. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68208/0.70032. Took 0.11 sec\n",
      "Epoch 30, Loss(train/val) 0.68218/0.70070. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68163/0.70100. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68105/0.70143. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68075/0.70181. Took 0.08 sec\n",
      "Epoch 34, Loss(train/val) 0.68038/0.70226. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67992/0.70274. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.67966/0.70312. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67929/0.70368. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67899/0.70410. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67830/0.70462. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67822/0.70507. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67715/0.70565. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67724/0.70617. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67626/0.70680. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67600/0.70742. Took 0.11 sec\n",
      "Epoch 45, Loss(train/val) 0.67559/0.70800. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67495/0.70842. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67413/0.70890. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67412/0.70981. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67322/0.71040. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67312/0.71090. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67243/0.71146. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67136/0.71208. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67061/0.71266. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67035/0.71300. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66949/0.71375. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66903/0.71416. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66836/0.71460. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66738/0.71525. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66769/0.71573. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66629/0.71623. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66600/0.71659. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66478/0.71709. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.66421/0.71743. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66351/0.71793. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66305/0.71853. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66244/0.71900. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66125/0.71944. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66094/0.72006. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66051/0.72059. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65973/0.72109. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65936/0.72189. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.65803/0.72230. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.65774/0.72279. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65744/0.72337. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65707/0.72380. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.65575/0.72439. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65499/0.72497. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65523/0.72549. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65412/0.72591. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65378/0.72646. Took 0.12 sec\n",
      "Epoch 81, Loss(train/val) 0.65304/0.72703. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65233/0.72769. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65201/0.72829. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.65148/0.72864. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65067/0.72899. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65050/0.72957. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64934/0.72991. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.64956/0.73022. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64890/0.73104. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.64799/0.73165. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.64775/0.73188. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64682/0.73260. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64624/0.73282. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.64609/0.73366. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.64583/0.73443. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.64487/0.73484. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64428/0.73492. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.64433/0.73567. Took 0.11 sec\n",
      "Epoch 99, Loss(train/val) 0.64305/0.73575. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69031/0.70188. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68965/0.70236. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.68984/0.70286. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.68978/0.70339. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.68938/0.70397. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68903/0.70456. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68874/0.70521. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68846/0.70587. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68815/0.70657. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.68778/0.70732. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68775/0.70812. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68742/0.70893. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68708/0.70976. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68629/0.71062. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68598/0.71152. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68571/0.71252. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68532/0.71355. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68477/0.71462. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68433/0.71579. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68401/0.71701. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68328/0.71832. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68302/0.71960. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68258/0.72090. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68185/0.72222. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68146/0.72360. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68096/0.72500. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68022/0.72642. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.67995/0.72780. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.67943/0.72918. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.67842/0.73064. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.67859/0.73197. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67821/0.73311. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67749/0.73436. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67664/0.73578. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67657/0.73693. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67594/0.73821. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67513/0.73931. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67491/0.74053. Took 0.10 sec\n",
      "Epoch 38, Loss(train/val) 0.67435/0.74158. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67392/0.74260. Took 0.11 sec\n",
      "Epoch 40, Loss(train/val) 0.67358/0.74373. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67314/0.74475. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67257/0.74586. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67210/0.74684. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.67160/0.74781. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67147/0.74855. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67092/0.74953. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67066/0.75042. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66955/0.75124. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66850/0.75212. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66828/0.75276. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66816/0.75380. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.66821/0.75446. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66705/0.75508. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66675/0.75592. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66601/0.75652. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66564/0.75765. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66468/0.75804. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.66405/0.75862. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66379/0.75923. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66354/0.76002. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66274/0.76050. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66260/0.76119. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66173/0.76158. Took 0.12 sec\n",
      "Epoch 64, Loss(train/val) 0.66100/0.76219. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.66010/0.76267. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66005/0.76334. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65939/0.76367. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65859/0.76437. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65797/0.76502. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65765/0.76577. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65668/0.76618. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65569/0.76670. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65535/0.76705. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65486/0.76735. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.65398/0.76806. Took 0.12 sec\n",
      "Epoch 76, Loss(train/val) 0.65349/0.76868. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.65247/0.76891. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.65217/0.76929. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.65108/0.76983. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65036/0.77018. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.65046/0.77047. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64916/0.77136. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64868/0.77210. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64700/0.77252. Took 0.11 sec\n",
      "Epoch 85, Loss(train/val) 0.64698/0.77298. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64570/0.77364. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.64493/0.77432. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.64425/0.77469. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.64306/0.77557. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.64283/0.77565. Took 0.12 sec\n",
      "Epoch 91, Loss(train/val) 0.64245/0.77656. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.64107/0.77680. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.64017/0.77743. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.63949/0.77779. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63847/0.77887. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63781/0.77914. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63631/0.78042. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.63476/0.78061. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.63402/0.78184. Took 0.11 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69392/0.69465. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69265/0.69624. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69167/0.69789. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69093/0.69968. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69014/0.70156. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.68958/0.70343. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.68917/0.70511. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.68888/0.70650. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.70753. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.68805/0.70829. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.68790/0.70895. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68739/0.70942. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.68734/0.70980. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68713/0.71018. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68677/0.71055. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68624/0.71090. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68597/0.71129. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68569/0.71166. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68524/0.71201. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68499/0.71240. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68454/0.71284. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.68429/0.71314. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68380/0.71370. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68335/0.71411. Took 0.11 sec\n",
      "Epoch 24, Loss(train/val) 0.68321/0.71455. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68253/0.71497. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68196/0.71537. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68184/0.71572. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68105/0.71618. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68078/0.71663. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68056/0.71705. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.67984/0.71732. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67939/0.71777. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.67886/0.71827. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67860/0.71853. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67804/0.71897. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67772/0.71918. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67711/0.71952. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67633/0.72007. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67579/0.72052. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67540/0.72068. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67460/0.72103. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67384/0.72132. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.67349/0.72166. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67267/0.72185. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67218/0.72216. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67132/0.72275. Took 0.10 sec\n",
      "Epoch 47, Loss(train/val) 0.67054/0.72340. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66992/0.72376. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66940/0.72444. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66856/0.72534. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66786/0.72549. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66738/0.72609. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.66642/0.72662. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66560/0.72726. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66436/0.72794. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66424/0.72787. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66329/0.72930. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66246/0.73033. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66161/0.73150. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66115/0.73235. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66010/0.73325. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65939/0.73365. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65862/0.73393. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65777/0.73542. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65626/0.73631. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65643/0.73736. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65553/0.73852. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65426/0.73981. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65383/0.74042. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65268/0.74211. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.65204/0.74262. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.65078/0.74432. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.65025/0.74516. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64967/0.74657. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64864/0.74720. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64825/0.74884. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64722/0.74992. Took 0.11 sec\n",
      "Epoch 78, Loss(train/val) 0.64611/0.75088. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64620/0.75247. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64493/0.75395. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64366/0.75579. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.64326/0.75641. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.64271/0.75799. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.64145/0.75832. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64098/0.75997. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.64011/0.76164. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.63954/0.76267. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63959/0.76470. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63875/0.76599. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.63672/0.76516. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.63583/0.76742. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63524/0.76913. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.63461/0.76965. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.63352/0.77159. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.63242/0.77178. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.63120/0.77305. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.63066/0.77541. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62982/0.77663. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62867/0.77802. Took 0.09 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69347/0.68814. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69294/0.68848. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69269/0.68881. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69261/0.68916. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69242/0.68951. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69197/0.68987. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69188/0.69025. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69156/0.69064. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69098/0.69107. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69084/0.69153. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69072/0.69204. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69034/0.69259. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68954/0.69317. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68955/0.69382. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68896/0.69457. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68853/0.69538. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68808/0.69626. Took 0.11 sec\n",
      "Epoch 17, Loss(train/val) 0.68733/0.69723. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68684/0.69829. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68645/0.69941. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68567/0.70059. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68515/0.70184. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68420/0.70314. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68396/0.70441. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68329/0.70570. Took 0.10 sec\n",
      "Epoch 25, Loss(train/val) 0.68263/0.70698. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68197/0.70820. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68187/0.70936. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68136/0.71051. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68056/0.71168. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68015/0.71277. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.67969/0.71377. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67912/0.71469. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67908/0.71565. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67833/0.71651. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67775/0.71735. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67745/0.71817. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67656/0.71898. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67631/0.71980. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67572/0.72060. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67535/0.72146. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67515/0.72227. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67405/0.72302. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67364/0.72381. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67311/0.72463. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67262/0.72541. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.67170/0.72613. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67128/0.72698. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67090/0.72776. Took 0.10 sec\n",
      "Epoch 49, Loss(train/val) 0.67028/0.72858. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.66954/0.72942. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.66905/0.73021. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.66834/0.73099. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66737/0.73193. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66713/0.73291. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66578/0.73386. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66516/0.73480. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66463/0.73567. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.66345/0.73660. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66342/0.73756. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.66223/0.73846. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66215/0.73949. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.66078/0.74044. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.66034/0.74143. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.65928/0.74242. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65892/0.74337. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.65804/0.74428. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.65696/0.74546. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65608/0.74649. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65545/0.74746. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.65457/0.74837. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.65272/0.74952. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.65248/0.75057. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.65145/0.75167. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.65115/0.75270. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.65051/0.75394. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.64922/0.75500. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64814/0.75620. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64796/0.75715. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.64663/0.75834. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.64593/0.75951. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.64491/0.76061. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.64380/0.76173. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.64326/0.76310. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.64261/0.76415. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.64192/0.76533. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63990/0.76648. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.63961/0.76764. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63856/0.76865. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63798/0.76977. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63734/0.77091. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.63603/0.77261. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.63496/0.77396. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.63325/0.77554. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.63288/0.77661. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.63196/0.77806. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.63126/0.77979. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.63025/0.78085. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.62827/0.78284. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.62744/0.78415. Took 0.11 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69120/0.69954. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69134/0.69956. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.69960. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69096/0.69966. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69087/0.69972. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69058/0.69979. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69061/0.69989. Took 0.11 sec\n",
      "Epoch 7, Loss(train/val) 0.69046/0.69999. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69022/0.70012. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69009/0.70029. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69018/0.70047. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.68960/0.70069. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.68967/0.70094. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.68952/0.70118. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68943/0.70149. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68880/0.70180. Took 0.10 sec\n",
      "Epoch 16, Loss(train/val) 0.68858/0.70209. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68834/0.70238. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68784/0.70278. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68774/0.70322. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68772/0.70360. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68675/0.70403. Took 0.11 sec\n",
      "Epoch 22, Loss(train/val) 0.68635/0.70448. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68563/0.70496. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68547/0.70547. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68510/0.70596. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68380/0.70672. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68308/0.70718. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68215/0.70769. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68171/0.70805. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68117/0.70885. Took 0.11 sec\n",
      "Epoch 31, Loss(train/val) 0.67976/0.70932. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67870/0.70969. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67791/0.71030. Took 0.10 sec\n",
      "Epoch 34, Loss(train/val) 0.67693/0.71116. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67611/0.71178. Took 0.08 sec\n",
      "Epoch 36, Loss(train/val) 0.67479/0.71234. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67358/0.71322. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67271/0.71399. Took 0.08 sec\n",
      "Epoch 39, Loss(train/val) 0.67132/0.71489. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67038/0.71563. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.66903/0.71674. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.66782/0.71771. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.66713/0.71854. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66624/0.71919. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.66481/0.72008. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.66389/0.72107. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66261/0.72183. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.66190/0.72254. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.66040/0.72337. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.65887/0.72423. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.65803/0.72508. Took 0.11 sec\n",
      "Epoch 52, Loss(train/val) 0.65679/0.72553. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65582/0.72682. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.65474/0.72748. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.65372/0.72863. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65270/0.72978. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65127/0.73026. Took 0.11 sec\n",
      "Epoch 58, Loss(train/val) 0.65082/0.73127. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.64855/0.73264. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.64770/0.73335. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.64548/0.73439. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64556/0.73528. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64355/0.73667. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.64383/0.73777. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.64190/0.73901. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64049/0.73981. Took 0.11 sec\n",
      "Epoch 67, Loss(train/val) 0.63911/0.74030. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63807/0.74193. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63634/0.74308. Took 0.11 sec\n",
      "Epoch 70, Loss(train/val) 0.63567/0.74495. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63339/0.74598. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.63196/0.74783. Took 0.11 sec\n",
      "Epoch 73, Loss(train/val) 0.63189/0.74909. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62886/0.74962. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62881/0.75132. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.62760/0.75264. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62600/0.75382. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62410/0.75585. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62323/0.75684. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.62093/0.75979. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.62113/0.75947. Took 0.11 sec\n",
      "Epoch 82, Loss(train/val) 0.61872/0.76171. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61709/0.76334. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61564/0.76472. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61537/0.76722. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.61338/0.76798. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.61188/0.77056. Took 0.11 sec\n",
      "Epoch 88, Loss(train/val) 0.61066/0.77101. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60980/0.77337. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60714/0.77396. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.60596/0.77678. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.60424/0.77926. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.60248/0.78019. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.60109/0.78363. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59988/0.78667. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59770/0.78852. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59632/0.79152. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.59545/0.79569. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.59399/0.79687. Took 0.10 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69447/0.69163. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69436/0.69164. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69402/0.69167. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69402/0.69169. Took 0.11 sec\n",
      "Epoch 4, Loss(train/val) 0.69360/0.69170. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69358/0.69172. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69357/0.69175. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69370/0.69177. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69336/0.69179. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69352/0.69183. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69325/0.69186. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69301/0.69193. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69290/0.69199. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69265/0.69204. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69277/0.69211. Took 0.11 sec\n",
      "Epoch 15, Loss(train/val) 0.69221/0.69216. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69228/0.69224. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69214/0.69232. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.69191/0.69237. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69157/0.69250. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69135/0.69262. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.69122/0.69275. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.69098/0.69286. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.69057/0.69299. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.69025/0.69320. Took 0.08 sec\n",
      "Epoch 25, Loss(train/val) 0.68999/0.69350. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68938/0.69368. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68931/0.69389. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68890/0.69412. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68833/0.69446. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68802/0.69483. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68730/0.69523. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68690/0.69583. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68626/0.69623. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68568/0.69667. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68513/0.69741. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.68432/0.69787. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68373/0.69835. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68307/0.69879. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68259/0.69958. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68155/0.70010. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68098/0.70093. Took 0.11 sec\n",
      "Epoch 42, Loss(train/val) 0.68003/0.70151. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67939/0.70215. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67823/0.70267. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67736/0.70315. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67680/0.70380. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67545/0.70436. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67482/0.70504. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67331/0.70587. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67267/0.70661. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67127/0.70736. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67011/0.70774. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66882/0.70841. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.66734/0.70979. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.66617/0.71069. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66466/0.71134. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66249/0.71271. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66125/0.71315. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65882/0.71449. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.65716/0.71575. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65614/0.71646. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65371/0.71758. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.65191/0.71886. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65004/0.71989. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64744/0.72054. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.64484/0.72238. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64299/0.72309. Took 0.10 sec\n",
      "Epoch 68, Loss(train/val) 0.64087/0.72463. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.63870/0.72554. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.63726/0.72670. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63372/0.72802. Took 0.12 sec\n",
      "Epoch 72, Loss(train/val) 0.63252/0.72975. Took 0.10 sec\n",
      "Epoch 73, Loss(train/val) 0.62981/0.73058. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62752/0.73231. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.62553/0.73379. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62388/0.73600. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62273/0.73748. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.61970/0.73895. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.61686/0.74121. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61462/0.74363. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.61402/0.74575. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.61098/0.74765. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.60907/0.75099. Took 0.12 sec\n",
      "Epoch 84, Loss(train/val) 0.60719/0.75336. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.60464/0.75663. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60256/0.75965. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.59925/0.76218. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.59825/0.76477. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.59723/0.76867. Took 0.11 sec\n",
      "Epoch 90, Loss(train/val) 0.59492/0.77148. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.59398/0.77332. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59091/0.77719. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.58742/0.78087. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.58758/0.78269. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.58430/0.78525. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.58125/0.79039. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.58105/0.79356. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.57836/0.79646. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.57591/0.79998. Took 0.09 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69233/0.69612. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69221/0.69640. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.69670. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69701. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69180/0.69734. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69158/0.69769. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69141/0.69806. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69118/0.69847. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69103/0.69890. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69076/0.69937. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69066/0.69988. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69041/0.70038. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69036/0.70094. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69011/0.70152. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68970/0.70212. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68931/0.70273. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68922/0.70338. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68893/0.70400. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68897/0.70465. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68857/0.70527. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68838/0.70586. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68800/0.70645. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68777/0.70696. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68752/0.70749. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68735/0.70797. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68686/0.70844. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68661/0.70888. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68639/0.70927. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68616/0.70963. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68608/0.70995. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68552/0.71025. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68533/0.71055. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68515/0.71077. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68487/0.71096. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68468/0.71117. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68414/0.71135. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68399/0.71146. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.68350/0.71164. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68317/0.71174. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68272/0.71183. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68245/0.71188. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68236/0.71196. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68200/0.71209. Took 0.10 sec\n",
      "Epoch 43, Loss(train/val) 0.68153/0.71224. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68112/0.71230. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68093/0.71233. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.68040/0.71242. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67979/0.71261. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67955/0.71265. Took 0.11 sec\n",
      "Epoch 49, Loss(train/val) 0.67917/0.71274. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67876/0.71278. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67852/0.71287. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67789/0.71296. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67777/0.71314. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67720/0.71330. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.67670/0.71348. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67597/0.71352. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67566/0.71368. Took 0.10 sec\n",
      "Epoch 58, Loss(train/val) 0.67539/0.71374. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67469/0.71403. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67413/0.71422. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.67392/0.71424. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67315/0.71470. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67236/0.71485. Took 0.11 sec\n",
      "Epoch 64, Loss(train/val) 0.67252/0.71524. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67167/0.71535. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67119/0.71572. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67039/0.71592. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67003/0.71641. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66929/0.71686. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66862/0.71711. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.66816/0.71770. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66774/0.71825. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.66722/0.71836. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66647/0.71899. Took 0.10 sec\n",
      "Epoch 75, Loss(train/val) 0.66532/0.71918. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66549/0.72003. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66473/0.72043. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66407/0.72091. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66422/0.72139. Took 0.11 sec\n",
      "Epoch 80, Loss(train/val) 0.66330/0.72229. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66299/0.72258. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66258/0.72321. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66176/0.72339. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66140/0.72427. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.66110/0.72473. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66046/0.72526. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65998/0.72598. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65939/0.72625. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65917/0.72688. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65870/0.72721. Took 0.11 sec\n",
      "Epoch 91, Loss(train/val) 0.65787/0.72799. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65708/0.72915. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65712/0.72961. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65670/0.73038. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65607/0.73086. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65589/0.73106. Took 0.11 sec\n",
      "Epoch 97, Loss(train/val) 0.65560/0.73188. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65413/0.73267. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65451/0.73361. Took 0.10 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69665/0.69389. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69560/0.69238. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69462/0.69136. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69435/0.69075. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69390/0.69039. Took 0.10 sec\n",
      "Epoch 5, Loss(train/val) 0.69371/0.69015. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69327/0.69003. Took 0.12 sec\n",
      "Epoch 7, Loss(train/val) 0.69299/0.68996. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69281/0.68997. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69262/0.69001. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69209/0.69013. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69176/0.69021. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69189/0.69033. Took 0.10 sec\n",
      "Epoch 13, Loss(train/val) 0.69165/0.69049. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69128/0.69060. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69140/0.69074. Took 0.11 sec\n",
      "Epoch 16, Loss(train/val) 0.69115/0.69095. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69071/0.69112. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69042/0.69125. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.69019/0.69146. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69030/0.69167. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68987/0.69183. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68961/0.69202. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68943/0.69229. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68924/0.69256. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68908/0.69274. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68906/0.69290. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68852/0.69306. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68842/0.69326. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68810/0.69342. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68792/0.69364. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68781/0.69389. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68750/0.69414. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68725/0.69436. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68714/0.69456. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68701/0.69475. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68640/0.69496. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68649/0.69516. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68598/0.69541. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.68575/0.69558. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68545/0.69580. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.68525/0.69601. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68473/0.69620. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68459/0.69645. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.68408/0.69669. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.68404/0.69694. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68345/0.69711. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.68355/0.69741. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.68335/0.69766. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68259/0.69783. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.68197/0.69804. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.68169/0.69833. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68157/0.69856. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.68132/0.69882. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.68034/0.69914. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.68052/0.69934. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.68001/0.69969. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67947/0.70000. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67929/0.70025. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67871/0.70050. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.67806/0.70085. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67791/0.70108. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67766/0.70141. Took 0.11 sec\n",
      "Epoch 63, Loss(train/val) 0.67640/0.70171. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67644/0.70205. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67605/0.70252. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.67555/0.70296. Took 0.10 sec\n",
      "Epoch 67, Loss(train/val) 0.67501/0.70337. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67405/0.70373. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67369/0.70429. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67326/0.70462. Took 0.10 sec\n",
      "Epoch 71, Loss(train/val) 0.67248/0.70510. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.67257/0.70567. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67157/0.70620. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67073/0.70672. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67023/0.70732. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66988/0.70784. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66931/0.70851. Took 0.12 sec\n",
      "Epoch 78, Loss(train/val) 0.66830/0.70923. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66828/0.70999. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.66730/0.71074. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.66714/0.71134. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66605/0.71206. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66523/0.71286. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.66431/0.71373. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66422/0.71459. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66357/0.71539. Took 0.11 sec\n",
      "Epoch 87, Loss(train/val) 0.66232/0.71617. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66209/0.71713. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66098/0.71810. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.66109/0.71900. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66013/0.72001. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.65929/0.72074. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.65865/0.72168. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65768/0.72256. Took 0.10 sec\n",
      "Epoch 95, Loss(train/val) 0.65682/0.72346. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.65652/0.72451. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65577/0.72540. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.65506/0.72631. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.65457/0.72714. Took 0.10 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69648/0.71501. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69529/0.71321. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69502/0.71160. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69413/0.71012. Took 0.10 sec\n",
      "Epoch 4, Loss(train/val) 0.69390/0.70867. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69343/0.70712. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69294/0.70561. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69234/0.70397. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69204/0.70246. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69149/0.70110. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.69989. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69036/0.69890. Took 0.12 sec\n",
      "Epoch 12, Loss(train/val) 0.69033/0.69815. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.68973/0.69755. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68935/0.69706. Took 0.10 sec\n",
      "Epoch 15, Loss(train/val) 0.68879/0.69657. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68842/0.69614. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68792/0.69568. Took 0.10 sec\n",
      "Epoch 18, Loss(train/val) 0.68735/0.69532. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68713/0.69491. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68644/0.69455. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68573/0.69426. Took 0.10 sec\n",
      "Epoch 22, Loss(train/val) 0.68523/0.69394. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68471/0.69354. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68391/0.69324. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68381/0.69287. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68319/0.69270. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68292/0.69229. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68263/0.69211. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68235/0.69174. Took 0.10 sec\n",
      "Epoch 30, Loss(train/val) 0.68182/0.69154. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68154/0.69145. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68109/0.69131. Took 0.11 sec\n",
      "Epoch 33, Loss(train/val) 0.68086/0.69120. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68061/0.69112. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68039/0.69094. Took 0.10 sec\n",
      "Epoch 36, Loss(train/val) 0.67998/0.69082. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67950/0.69075. Took 0.11 sec\n",
      "Epoch 38, Loss(train/val) 0.67916/0.69072. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67883/0.69062. Took 0.10 sec\n",
      "Epoch 40, Loss(train/val) 0.67876/0.69057. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67844/0.69073. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67805/0.69071. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67766/0.69071. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67734/0.69075. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67745/0.69077. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67735/0.69074. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67653/0.69087. Took 0.11 sec\n",
      "Epoch 48, Loss(train/val) 0.67604/0.69083. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67609/0.69086. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.67572/0.69091. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67542/0.69098. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.67508/0.69117. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.67459/0.69123. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67418/0.69117. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.67372/0.69126. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67392/0.69133. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67287/0.69147. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67282/0.69140. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.67251/0.69149. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.67216/0.69175. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.67181/0.69201. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.67109/0.69188. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.67069/0.69200. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67043/0.69201. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66984/0.69232. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.66940/0.69236. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66911/0.69260. Took 0.11 sec\n",
      "Epoch 68, Loss(train/val) 0.66850/0.69267. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.66783/0.69255. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.66787/0.69288. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.66727/0.69288. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.66638/0.69293. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66574/0.69299. Took 0.11 sec\n",
      "Epoch 74, Loss(train/val) 0.66567/0.69313. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.66538/0.69324. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.66436/0.69339. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.66359/0.69332. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.66307/0.69330. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.66209/0.69369. Took 0.12 sec\n",
      "Epoch 80, Loss(train/val) 0.66218/0.69340. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.66136/0.69361. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66085/0.69394. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66041/0.69366. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.65998/0.69386. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65926/0.69388. Took 0.10 sec\n",
      "Epoch 86, Loss(train/val) 0.65834/0.69401. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.65736/0.69424. Took 0.10 sec\n",
      "Epoch 88, Loss(train/val) 0.65747/0.69397. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.65682/0.69463. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.65597/0.69449. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65530/0.69459. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.65476/0.69466. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.65388/0.69477. Took 0.10 sec\n",
      "Epoch 94, Loss(train/val) 0.65296/0.69496. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65243/0.69503. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.65169/0.69515. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.65070/0.69499. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65081/0.69552. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.64919/0.69550. Took 0.11 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69307/0.69142. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69340/0.69140. Took 0.15 sec\n",
      "Epoch 2, Loss(train/val) 0.69298/0.69138. Took 0.10 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.69139. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69140. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69217/0.69141. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69219/0.69145. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69212/0.69149. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69160/0.69155. Took 0.10 sec\n",
      "Epoch 9, Loss(train/val) 0.69117/0.69160. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69091/0.69171. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69053/0.69181. Took 0.10 sec\n",
      "Epoch 12, Loss(train/val) 0.69039/0.69193. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69025/0.69208. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68996/0.69225. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68959/0.69246. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68873/0.69268. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68880/0.69294. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68823/0.69325. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68789/0.69361. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68763/0.69400. Took 0.11 sec\n",
      "Epoch 21, Loss(train/val) 0.68683/0.69444. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68664/0.69495. Took 0.08 sec\n",
      "Epoch 23, Loss(train/val) 0.68613/0.69549. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68570/0.69607. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68514/0.69671. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68493/0.69738. Took 0.10 sec\n",
      "Epoch 27, Loss(train/val) 0.68409/0.69809. Took 0.08 sec\n",
      "Epoch 28, Loss(train/val) 0.68335/0.69883. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68306/0.69953. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68290/0.70023. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68220/0.70097. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68167/0.70168. Took 0.10 sec\n",
      "Epoch 33, Loss(train/val) 0.68128/0.70234. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68077/0.70298. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68015/0.70360. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67961/0.70421. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.67936/0.70478. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67908/0.70538. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.67884/0.70590. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67822/0.70637. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67792/0.70685. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67743/0.70736. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67702/0.70779. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67641/0.70827. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67634/0.70876. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67555/0.70916. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67547/0.70957. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.67465/0.71000. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67434/0.71043. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67386/0.71082. Took 0.10 sec\n",
      "Epoch 51, Loss(train/val) 0.67328/0.71125. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67299/0.71160. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67273/0.71200. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.67190/0.71230. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67189/0.71261. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67110/0.71290. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.67064/0.71322. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67065/0.71361. Took 0.10 sec\n",
      "Epoch 59, Loss(train/val) 0.66992/0.71395. Took 0.11 sec\n",
      "Epoch 60, Loss(train/val) 0.66970/0.71431. Took 0.10 sec\n",
      "Epoch 61, Loss(train/val) 0.66890/0.71470. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66845/0.71490. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.66788/0.71521. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.66768/0.71549. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.66709/0.71580. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.66673/0.71612. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.66622/0.71641. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.66553/0.71652. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.66503/0.71679. Took 0.10 sec\n",
      "Epoch 70, Loss(train/val) 0.66462/0.71709. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.66428/0.71733. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.66354/0.71761. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.66265/0.71762. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.66273/0.71795. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.66244/0.71814. Took 0.10 sec\n",
      "Epoch 76, Loss(train/val) 0.66196/0.71831. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.66086/0.71846. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.66073/0.71849. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.66054/0.71863. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.65989/0.71881. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.65946/0.71907. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.65869/0.71925. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.65834/0.71936. Took 0.12 sec\n",
      "Epoch 84, Loss(train/val) 0.65780/0.71942. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.65732/0.71933. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.65626/0.71941. Took 0.10 sec\n",
      "Epoch 87, Loss(train/val) 0.65598/0.71949. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.65535/0.71949. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.65577/0.71963. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.65369/0.71981. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.65330/0.71987. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.65367/0.71990. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.65291/0.71974. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.65242/0.71988. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.65164/0.71984. Took 0.12 sec\n",
      "Epoch 96, Loss(train/val) 0.65124/0.71992. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.64977/0.71983. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.65055/0.71990. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.64960/0.72003. Took 0.09 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69163. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69295/0.69187. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69277/0.69211. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69264/0.69237. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69201/0.69267. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69304. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69168/0.69342. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69147/0.69385. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69132/0.69432. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.69482. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69058/0.69534. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69047/0.69588. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69025/0.69639. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.68976/0.69696. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.68942/0.69750. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.68934/0.69804. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68924/0.69857. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68872/0.69907. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68819/0.69959. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68766/0.70016. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68746/0.70065. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68727/0.70116. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68661/0.70175. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.70227. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68547/0.70279. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68505/0.70356. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68469/0.70445. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68392/0.70516. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68341/0.70587. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68254/0.70661. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68214/0.70752. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68160/0.70845. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68123/0.70928. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68044/0.71026. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67975/0.71136. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67894/0.71237. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67848/0.71351. Took 0.11 sec\n",
      "Epoch 37, Loss(train/val) 0.67804/0.71465. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67661/0.71569. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67649/0.71692. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67598/0.71791. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67504/0.71929. Took 0.10 sec\n",
      "Epoch 42, Loss(train/val) 0.67431/0.72037. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67381/0.72137. Took 0.11 sec\n",
      "Epoch 44, Loss(train/val) 0.67291/0.72277. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.67197/0.72431. Took 0.11 sec\n",
      "Epoch 46, Loss(train/val) 0.67129/0.72550. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67052/0.72656. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67005/0.72805. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66886/0.72933. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66738/0.73067. Took 0.11 sec\n",
      "Epoch 51, Loss(train/val) 0.66716/0.73242. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.66569/0.73397. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66553/0.73562. Took 0.11 sec\n",
      "Epoch 54, Loss(train/val) 0.66383/0.73721. Took 0.10 sec\n",
      "Epoch 55, Loss(train/val) 0.66325/0.73888. Took 0.10 sec\n",
      "Epoch 56, Loss(train/val) 0.66265/0.74005. Took 0.10 sec\n",
      "Epoch 57, Loss(train/val) 0.66148/0.74190. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66109/0.74370. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65951/0.74602. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.65873/0.74730. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.65829/0.74932. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.65695/0.75062. Took 0.12 sec\n",
      "Epoch 63, Loss(train/val) 0.65563/0.75285. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65535/0.75446. Took 0.10 sec\n",
      "Epoch 65, Loss(train/val) 0.65424/0.75602. Took 0.10 sec\n",
      "Epoch 66, Loss(train/val) 0.65397/0.75789. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65179/0.75928. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65106/0.76113. Took 0.10 sec\n",
      "Epoch 69, Loss(train/val) 0.65054/0.76293. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.64957/0.76434. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64964/0.76618. Took 0.11 sec\n",
      "Epoch 72, Loss(train/val) 0.64780/0.76817. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64745/0.76930. Took 0.10 sec\n",
      "Epoch 74, Loss(train/val) 0.64540/0.77119. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64509/0.77253. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64401/0.77443. Took 0.10 sec\n",
      "Epoch 77, Loss(train/val) 0.64374/0.77645. Took 0.10 sec\n",
      "Epoch 78, Loss(train/val) 0.64252/0.77765. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.64230/0.77958. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.64072/0.78129. Took 0.11 sec\n",
      "Epoch 81, Loss(train/val) 0.64087/0.78260. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63976/0.78423. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63854/0.78578. Took 0.10 sec\n",
      "Epoch 84, Loss(train/val) 0.63873/0.78709. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63736/0.78837. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.63699/0.78917. Took 0.12 sec\n",
      "Epoch 87, Loss(train/val) 0.63605/0.79069. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.63398/0.79246. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.63439/0.79397. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.63224/0.79578. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.63269/0.79670. Took 0.10 sec\n",
      "Epoch 92, Loss(train/val) 0.63210/0.79831. Took 0.10 sec\n",
      "Epoch 93, Loss(train/val) 0.63135/0.79975. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.62960/0.80034. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.62928/0.80183. Took 0.11 sec\n",
      "Epoch 96, Loss(train/val) 0.62900/0.80335. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.62773/0.80488. Took 0.10 sec\n",
      "Epoch 98, Loss(train/val) 0.62639/0.80603. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.62597/0.80712. Took 0.09 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69588/0.69337. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69547/0.69339. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69554/0.69344. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69513/0.69355. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69495/0.69371. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69477/0.69395. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69466/0.69429. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69414/0.69480. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69377/0.69549. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69350/0.69642. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69311/0.69761. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69230/0.69907. Took 0.11 sec\n",
      "Epoch 12, Loss(train/val) 0.69221/0.70063. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69176/0.70224. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69136/0.70372. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69088/0.70506. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69061/0.70624. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.69007/0.70723. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68964/0.70803. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.68930/0.70869. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68920/0.70936. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68880/0.70998. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68834/0.71045. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68786/0.71110. Took 0.10 sec\n",
      "Epoch 24, Loss(train/val) 0.68775/0.71172. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68720/0.71237. Took 0.08 sec\n",
      "Epoch 26, Loss(train/val) 0.68665/0.71301. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68596/0.71357. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68556/0.71425. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68487/0.71485. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68470/0.71576. Took 0.08 sec\n",
      "Epoch 31, Loss(train/val) 0.68402/0.71672. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.68352/0.71783. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68274/0.71877. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68227/0.71990. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68154/0.72098. Took 0.11 sec\n",
      "Epoch 36, Loss(train/val) 0.68125/0.72205. Took 0.09 sec\n",
      "Epoch 37, Loss(train/val) 0.68051/0.72306. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67990/0.72425. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67917/0.72526. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67868/0.72675. Took 0.09 sec\n",
      "Epoch 41, Loss(train/val) 0.67798/0.72817. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67716/0.72929. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.67668/0.73073. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.67601/0.73179. Took 0.10 sec\n",
      "Epoch 45, Loss(train/val) 0.67517/0.73312. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.67486/0.73406. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.67381/0.73536. Took 0.10 sec\n",
      "Epoch 48, Loss(train/val) 0.67295/0.73669. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.67227/0.73763. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.67146/0.73877. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.67098/0.74034. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.67015/0.74180. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.66947/0.74275. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.66848/0.74357. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.66779/0.74518. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.66675/0.74572. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.66532/0.74685. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.66451/0.74770. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.66337/0.74895. Took 0.10 sec\n",
      "Epoch 60, Loss(train/val) 0.66261/0.74979. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.66170/0.75151. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.66036/0.75173. Took 0.10 sec\n",
      "Epoch 63, Loss(train/val) 0.65962/0.75305. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.65813/0.75377. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.65738/0.75478. Took 0.11 sec\n",
      "Epoch 66, Loss(train/val) 0.65638/0.75553. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.65503/0.75726. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.65423/0.75757. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.65207/0.75887. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.65126/0.75926. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.64996/0.76095. Took 0.10 sec\n",
      "Epoch 72, Loss(train/val) 0.64886/0.76165. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.64719/0.76298. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.64609/0.76413. Took 0.11 sec\n",
      "Epoch 75, Loss(train/val) 0.64492/0.76513. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.64334/0.76525. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.64251/0.76760. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.64101/0.76830. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.63911/0.76968. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.63743/0.77149. Took 0.10 sec\n",
      "Epoch 81, Loss(train/val) 0.63640/0.77260. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.63418/0.77414. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.63355/0.77554. Took 0.11 sec\n",
      "Epoch 84, Loss(train/val) 0.63151/0.77813. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.63028/0.77863. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.62916/0.78067. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.62735/0.78170. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.62671/0.78415. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.62512/0.78605. Took 0.10 sec\n",
      "Epoch 90, Loss(train/val) 0.62327/0.78707. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.62231/0.79030. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.62064/0.79131. Took 0.11 sec\n",
      "Epoch 93, Loss(train/val) 0.61919/0.79374. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.61763/0.79576. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.61585/0.79789. Took 0.10 sec\n",
      "Epoch 96, Loss(train/val) 0.61599/0.80036. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.61360/0.80263. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.61236/0.80406. Took 0.10 sec\n",
      "Epoch 99, Loss(train/val) 0.61088/0.80664. Took 0.09 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69303/0.69424. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69430. Took 0.09 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69437. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69221/0.69442. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69449. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69237/0.69455. Took 0.09 sec\n",
      "Epoch 6, Loss(train/val) 0.69206/0.69461. Took 0.10 sec\n",
      "Epoch 7, Loss(train/val) 0.69185/0.69469. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69189/0.69478. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69178/0.69487. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69145/0.69499. Took 0.09 sec\n",
      "Epoch 11, Loss(train/val) 0.69111/0.69513. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69102/0.69529. Took 0.11 sec\n",
      "Epoch 13, Loss(train/val) 0.69058/0.69547. Took 0.09 sec\n",
      "Epoch 14, Loss(train/val) 0.69040/0.69567. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69016/0.69589. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.68977/0.69615. Took 0.09 sec\n",
      "Epoch 17, Loss(train/val) 0.68958/0.69650. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.68903/0.69689. Took 0.10 sec\n",
      "Epoch 19, Loss(train/val) 0.68868/0.69739. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.68795/0.69790. Took 0.09 sec\n",
      "Epoch 21, Loss(train/val) 0.68761/0.69850. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.68764/0.69909. Took 0.09 sec\n",
      "Epoch 23, Loss(train/val) 0.68656/0.69975. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68597/0.70044. Took 0.11 sec\n",
      "Epoch 25, Loss(train/val) 0.68520/0.70121. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68423/0.70201. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68348/0.70275. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68298/0.70360. Took 0.09 sec\n",
      "Epoch 29, Loss(train/val) 0.68193/0.70448. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68121/0.70536. Took 0.10 sec\n",
      "Epoch 31, Loss(train/val) 0.68032/0.70621. Took 0.09 sec\n",
      "Epoch 32, Loss(train/val) 0.67943/0.70706. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.67869/0.70797. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.67771/0.70875. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.67719/0.70949. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.67590/0.71036. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.67491/0.71118. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.67430/0.71199. Took 0.09 sec\n",
      "Epoch 39, Loss(train/val) 0.67324/0.71272. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.67229/0.71350. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.67159/0.71419. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.67050/0.71487. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.66898/0.71559. Took 0.09 sec\n",
      "Epoch 44, Loss(train/val) 0.66818/0.71617. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.66752/0.71673. Took 0.10 sec\n",
      "Epoch 46, Loss(train/val) 0.66644/0.71725. Took 0.09 sec\n",
      "Epoch 47, Loss(train/val) 0.66496/0.71766. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.66354/0.71834. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.66264/0.71897. Took 0.09 sec\n",
      "Epoch 50, Loss(train/val) 0.66175/0.71947. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.66052/0.71985. Took 0.10 sec\n",
      "Epoch 52, Loss(train/val) 0.65949/0.72008. Took 0.09 sec\n",
      "Epoch 53, Loss(train/val) 0.65812/0.72071. Took 0.10 sec\n",
      "Epoch 54, Loss(train/val) 0.65674/0.72079. Took 0.11 sec\n",
      "Epoch 55, Loss(train/val) 0.65552/0.72116. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.65495/0.72168. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.65321/0.72180. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.65273/0.72250. Took 0.09 sec\n",
      "Epoch 59, Loss(train/val) 0.65097/0.72296. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.65012/0.72375. Took 0.11 sec\n",
      "Epoch 61, Loss(train/val) 0.64839/0.72387. Took 0.09 sec\n",
      "Epoch 62, Loss(train/val) 0.64753/0.72411. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.64594/0.72510. Took 0.10 sec\n",
      "Epoch 64, Loss(train/val) 0.64499/0.72573. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.64286/0.72608. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.64205/0.72667. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.64079/0.72727. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.63914/0.72764. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.63822/0.72882. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.63682/0.73035. Took 0.09 sec\n",
      "Epoch 71, Loss(train/val) 0.63372/0.73101. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.63300/0.73200. Took 0.12 sec\n",
      "Epoch 73, Loss(train/val) 0.63170/0.73321. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.62982/0.73369. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.62877/0.73483. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.62637/0.73598. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.62521/0.73750. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.62349/0.73838. Took 0.10 sec\n",
      "Epoch 79, Loss(train/val) 0.62179/0.73974. Took 0.09 sec\n",
      "Epoch 80, Loss(train/val) 0.61990/0.74164. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.61834/0.74292. Took 0.10 sec\n",
      "Epoch 82, Loss(train/val) 0.61624/0.74479. Took 0.09 sec\n",
      "Epoch 83, Loss(train/val) 0.61457/0.74608. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.61331/0.74899. Took 0.10 sec\n",
      "Epoch 85, Loss(train/val) 0.61165/0.75015. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.60991/0.75269. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.60876/0.75430. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.60528/0.75550. Took 0.09 sec\n",
      "Epoch 89, Loss(train/val) 0.60325/0.75862. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.60371/0.76078. Took 0.10 sec\n",
      "Epoch 91, Loss(train/val) 0.60025/0.76465. Took 0.09 sec\n",
      "Epoch 92, Loss(train/val) 0.59776/0.76486. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.59742/0.76792. Took 0.11 sec\n",
      "Epoch 94, Loss(train/val) 0.59480/0.76926. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.59282/0.77245. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.59143/0.77377. Took 0.10 sec\n",
      "Epoch 97, Loss(train/val) 0.59027/0.77753. Took 0.09 sec\n",
      "Epoch 98, Loss(train/val) 0.58642/0.77937. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.58630/0.78379. Took 0.11 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69639/0.69722. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69602/0.69655. Took 0.16 sec\n",
      "Epoch 2, Loss(train/val) 0.69552/0.69587. Took 0.09 sec\n",
      "Epoch 3, Loss(train/val) 0.69514/0.69527. Took 0.09 sec\n",
      "Epoch 4, Loss(train/val) 0.69498/0.69470. Took 0.09 sec\n",
      "Epoch 5, Loss(train/val) 0.69473/0.69420. Took 0.11 sec\n",
      "Epoch 6, Loss(train/val) 0.69427/0.69375. Took 0.09 sec\n",
      "Epoch 7, Loss(train/val) 0.69381/0.69335. Took 0.09 sec\n",
      "Epoch 8, Loss(train/val) 0.69374/0.69307. Took 0.09 sec\n",
      "Epoch 9, Loss(train/val) 0.69352/0.69281. Took 0.09 sec\n",
      "Epoch 10, Loss(train/val) 0.69312/0.69259. Took 0.10 sec\n",
      "Epoch 11, Loss(train/val) 0.69290/0.69241. Took 0.09 sec\n",
      "Epoch 12, Loss(train/val) 0.69275/0.69229. Took 0.09 sec\n",
      "Epoch 13, Loss(train/val) 0.69267/0.69220. Took 0.10 sec\n",
      "Epoch 14, Loss(train/val) 0.69217/0.69215. Took 0.09 sec\n",
      "Epoch 15, Loss(train/val) 0.69197/0.69214. Took 0.09 sec\n",
      "Epoch 16, Loss(train/val) 0.69191/0.69218. Took 0.10 sec\n",
      "Epoch 17, Loss(train/val) 0.69162/0.69221. Took 0.09 sec\n",
      "Epoch 18, Loss(train/val) 0.69135/0.69229. Took 0.09 sec\n",
      "Epoch 19, Loss(train/val) 0.69100/0.69237. Took 0.09 sec\n",
      "Epoch 20, Loss(train/val) 0.69062/0.69241. Took 0.10 sec\n",
      "Epoch 21, Loss(train/val) 0.69051/0.69255. Took 0.09 sec\n",
      "Epoch 22, Loss(train/val) 0.69001/0.69272. Took 0.10 sec\n",
      "Epoch 23, Loss(train/val) 0.68984/0.69286. Took 0.09 sec\n",
      "Epoch 24, Loss(train/val) 0.68929/0.69311. Took 0.09 sec\n",
      "Epoch 25, Loss(train/val) 0.68933/0.69343. Took 0.09 sec\n",
      "Epoch 26, Loss(train/val) 0.68872/0.69376. Took 0.09 sec\n",
      "Epoch 27, Loss(train/val) 0.68848/0.69410. Took 0.09 sec\n",
      "Epoch 28, Loss(train/val) 0.68764/0.69437. Took 0.10 sec\n",
      "Epoch 29, Loss(train/val) 0.68737/0.69482. Took 0.09 sec\n",
      "Epoch 30, Loss(train/val) 0.68687/0.69529. Took 0.09 sec\n",
      "Epoch 31, Loss(train/val) 0.68673/0.69566. Took 0.10 sec\n",
      "Epoch 32, Loss(train/val) 0.68622/0.69600. Took 0.09 sec\n",
      "Epoch 33, Loss(train/val) 0.68556/0.69648. Took 0.09 sec\n",
      "Epoch 34, Loss(train/val) 0.68527/0.69706. Took 0.09 sec\n",
      "Epoch 35, Loss(train/val) 0.68483/0.69759. Took 0.09 sec\n",
      "Epoch 36, Loss(train/val) 0.68450/0.69829. Took 0.10 sec\n",
      "Epoch 37, Loss(train/val) 0.68446/0.69872. Took 0.09 sec\n",
      "Epoch 38, Loss(train/val) 0.68385/0.69950. Took 0.10 sec\n",
      "Epoch 39, Loss(train/val) 0.68356/0.69976. Took 0.09 sec\n",
      "Epoch 40, Loss(train/val) 0.68301/0.70036. Took 0.10 sec\n",
      "Epoch 41, Loss(train/val) 0.68291/0.70086. Took 0.09 sec\n",
      "Epoch 42, Loss(train/val) 0.68271/0.70148. Took 0.09 sec\n",
      "Epoch 43, Loss(train/val) 0.68299/0.70188. Took 0.10 sec\n",
      "Epoch 44, Loss(train/val) 0.68226/0.70242. Took 0.09 sec\n",
      "Epoch 45, Loss(train/val) 0.68152/0.70307. Took 0.09 sec\n",
      "Epoch 46, Loss(train/val) 0.68180/0.70365. Took 0.11 sec\n",
      "Epoch 47, Loss(train/val) 0.68129/0.70389. Took 0.09 sec\n",
      "Epoch 48, Loss(train/val) 0.68101/0.70434. Took 0.09 sec\n",
      "Epoch 49, Loss(train/val) 0.68071/0.70474. Took 0.10 sec\n",
      "Epoch 50, Loss(train/val) 0.68032/0.70518. Took 0.09 sec\n",
      "Epoch 51, Loss(train/val) 0.68002/0.70551. Took 0.09 sec\n",
      "Epoch 52, Loss(train/val) 0.68011/0.70576. Took 0.10 sec\n",
      "Epoch 53, Loss(train/val) 0.67964/0.70616. Took 0.09 sec\n",
      "Epoch 54, Loss(train/val) 0.67910/0.70677. Took 0.09 sec\n",
      "Epoch 55, Loss(train/val) 0.67935/0.70699. Took 0.09 sec\n",
      "Epoch 56, Loss(train/val) 0.67873/0.70729. Took 0.09 sec\n",
      "Epoch 57, Loss(train/val) 0.67858/0.70773. Took 0.09 sec\n",
      "Epoch 58, Loss(train/val) 0.67803/0.70828. Took 0.11 sec\n",
      "Epoch 59, Loss(train/val) 0.67809/0.70842. Took 0.09 sec\n",
      "Epoch 60, Loss(train/val) 0.67798/0.70856. Took 0.09 sec\n",
      "Epoch 61, Loss(train/val) 0.67746/0.70908. Took 0.10 sec\n",
      "Epoch 62, Loss(train/val) 0.67714/0.70929. Took 0.09 sec\n",
      "Epoch 63, Loss(train/val) 0.67653/0.70971. Took 0.09 sec\n",
      "Epoch 64, Loss(train/val) 0.67626/0.71008. Took 0.09 sec\n",
      "Epoch 65, Loss(train/val) 0.67628/0.71015. Took 0.09 sec\n",
      "Epoch 66, Loss(train/val) 0.67553/0.71045. Took 0.09 sec\n",
      "Epoch 67, Loss(train/val) 0.67534/0.71120. Took 0.09 sec\n",
      "Epoch 68, Loss(train/val) 0.67505/0.71146. Took 0.09 sec\n",
      "Epoch 69, Loss(train/val) 0.67479/0.71155. Took 0.09 sec\n",
      "Epoch 70, Loss(train/val) 0.67466/0.71166. Took 0.11 sec\n",
      "Epoch 71, Loss(train/val) 0.67390/0.71220. Took 0.09 sec\n",
      "Epoch 72, Loss(train/val) 0.67363/0.71281. Took 0.09 sec\n",
      "Epoch 73, Loss(train/val) 0.67373/0.71299. Took 0.09 sec\n",
      "Epoch 74, Loss(train/val) 0.67323/0.71327. Took 0.09 sec\n",
      "Epoch 75, Loss(train/val) 0.67227/0.71387. Took 0.09 sec\n",
      "Epoch 76, Loss(train/val) 0.67212/0.71441. Took 0.09 sec\n",
      "Epoch 77, Loss(train/val) 0.67187/0.71465. Took 0.09 sec\n",
      "Epoch 78, Loss(train/val) 0.67098/0.71532. Took 0.09 sec\n",
      "Epoch 79, Loss(train/val) 0.67068/0.71574. Took 0.10 sec\n",
      "Epoch 80, Loss(train/val) 0.67069/0.71558. Took 0.09 sec\n",
      "Epoch 81, Loss(train/val) 0.67007/0.71681. Took 0.09 sec\n",
      "Epoch 82, Loss(train/val) 0.66937/0.71701. Took 0.10 sec\n",
      "Epoch 83, Loss(train/val) 0.66881/0.71747. Took 0.09 sec\n",
      "Epoch 84, Loss(train/val) 0.66847/0.71809. Took 0.09 sec\n",
      "Epoch 85, Loss(train/val) 0.66753/0.71893. Took 0.09 sec\n",
      "Epoch 86, Loss(train/val) 0.66710/0.71938. Took 0.09 sec\n",
      "Epoch 87, Loss(train/val) 0.66632/0.71997. Took 0.09 sec\n",
      "Epoch 88, Loss(train/val) 0.66610/0.72032. Took 0.10 sec\n",
      "Epoch 89, Loss(train/val) 0.66566/0.72119. Took 0.09 sec\n",
      "Epoch 90, Loss(train/val) 0.66513/0.72153. Took 0.09 sec\n",
      "Epoch 91, Loss(train/val) 0.66434/0.72280. Took 0.11 sec\n",
      "Epoch 92, Loss(train/val) 0.66411/0.72316. Took 0.09 sec\n",
      "Epoch 93, Loss(train/val) 0.66362/0.72398. Took 0.09 sec\n",
      "Epoch 94, Loss(train/val) 0.66302/0.72429. Took 0.09 sec\n",
      "Epoch 95, Loss(train/val) 0.66217/0.72506. Took 0.09 sec\n",
      "Epoch 96, Loss(train/val) 0.66159/0.72594. Took 0.09 sec\n",
      "Epoch 97, Loss(train/val) 0.66074/0.72661. Took 0.11 sec\n",
      "Epoch 98, Loss(train/val) 0.65957/0.72731. Took 0.09 sec\n",
      "Epoch 99, Loss(train/val) 0.65941/0.72832. Took 0.09 sec\n",
      "ACC: 0.3958333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\MM_Transformer_early_map\\data\\kdd17\\price_long_50\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Multimodal_transformer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Multimodal_transformer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "\n",
    "            ACC_cv = []\n",
    "\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model\n",
    "                Transformer = args.Transformer(args.input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.dropout, args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Transformer_optimizer = optim.Adam(Transformer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Transformer, train_loss = train(Transformer,Transformer_optimizer, args, partition)\n",
    "                    Transformer, validation_loss = validation(Transformer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Transformer = args.Transformer(args.input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.dropout,args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_transformer' + '.pt'))\n",
    "\n",
    "                ACC, AP_list = test(Transformer, args, partition)\n",
    "                www.writerow([ACC])\n",
    "                print('ACC: {}'.format(ACC))\n",
    "\n",
    "                \n",
    "                args.attention_map_path = args.split_file_path  +'\\\\attention_map'\n",
    "                os.makedirs(args.attention_map_path)\n",
    "\n",
    "\n",
    "                j=0\n",
    "                for b_att_map in AP_list:\n",
    "                    for h_att_map in b_att_map:\n",
    "                        for att_map in h_att_map:\n",
    "                            j += 1\n",
    "\n",
    "                            att_map = att_map.cpu().detach().numpy()   \n",
    "                            scaler  = preprocessing.MinMaxScaler().fit(att_map)\n",
    "                            att_map = scaler.transform(att_map)\n",
    "\n",
    "\n",
    "                            sns.heatmap(att_map, cmap='crest')\n",
    "                            plt.savefig(args.attention_map_path + '\\\\' + str(j)+ '_attmap' + '.png', dpi = 300)\n",
    "                            plt.clf()\n",
    "\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "                \n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "            \n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"MM_Transformer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
